{"meta":{"title":"Marlowe","subtitle":"","description":"","author":"John Doe","url":"https://xmmarlowe.github.io","root":"/"},"pages":[{"title":"","date":"2020-12-02T14:11:24.545Z","updated":"2020-12-02T14:11:24.545Z","comments":true,"path":"404.html","permalink":"https://xmmarlowe.github.io/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"关于","date":"2020-10-12T12:20:20.000Z","updated":"2021-08-26T15:38:25.533Z","comments":true,"path":"about/index.html","permalink":"https://xmmarlowe.github.io/about/index.html","excerpt":"","text":"有梦不觉人生寒~ 个人博客https://xmmarlowe.github.io/ Marlowe's Githubhttps://github.com/xmmarlowe 毕业学校https://www.cqut.edu.cn/ 个人邮箱mailto:marlowe246@qq.com"},{"title":"所有分类","date":"2020-12-01T14:23:54.558Z","updated":"2020-12-01T14:23:54.558Z","comments":true,"path":"categories/index.html","permalink":"https://xmmarlowe.github.io/categories/index.html","excerpt":"","text":""},{"title":"友链","date":"2021-05-18T12:22:03.377Z","updated":"2021-05-18T12:22:03.377Z","comments":true,"path":"friends/index.html","permalink":"https://xmmarlowe.github.io/friends/index.html","excerpt":"以下是一些有趣的博客，可以前往学习。","text":"以下是一些有趣的博客，可以前往学习。 友链添加流程 Step 1 >Step 2 >Step 3 请先添加本站链接 名称：Marlowe链接：https://xmmarlowe.github.io/头像：https://cdn.jsdelivr.net/gh/moonoonoom/CDN@0.3.1/images/MarloweAvatar.jpg 下方评论区按此格式申请友链 12345- title: # 名称- avatar: # 头像- url: # 链接- screenshot: # 封面图（若不提供则将自动取用博客首页截图）- description: # 描述（可选）s 等待本站添加贵站 请先添加本站友链 申请的友链将经过筛选 链接无法打开或存在质量低的内容时，将不会添加 对于站点内存资源占用巨高的博客酌情添加 原则上最好使用https协议站点 至少拥有三篇原创文章"},{"title":"所有标签","date":"2020-12-01T14:23:54.500Z","updated":"2020-12-01T14:23:54.500Z","comments":true,"path":"tags/index.html","permalink":"https://xmmarlowe.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"一文详解Redis中BigKey、HotKey的发现与处理","slug":"NoSQL/一文详解Redis中BigKey、HotKey的发现与处理","date":"2021-08-26T13:37:24.000Z","updated":"2021-08-26T10:09:44.591Z","comments":true,"path":"2021/08/26/NoSQL/一文详解Redis中BigKey、HotKey的发现与处理/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/26/NoSQL/%E4%B8%80%E6%96%87%E8%AF%A6%E8%A7%A3Redis%E4%B8%ADBigKey%E3%80%81HotKey%E7%9A%84%E5%8F%91%E7%8E%B0%E4%B8%8E%E5%A4%84%E7%90%86/","excerpt":"转载与阿里技术公众号：一文详解Redis中BigKey、HotKey的发现与处理","text":"转载与阿里技术公众号：一文详解Redis中BigKey、HotKey的发现与处理 一 前言在Redis的使用过程中，我们经常会遇到BigKey（下文将其称为“大key”）及HotKey（下文将其称为“热key”）。大Key与热Key如果未能及时发现并进行处理，很可能会使服务性能下降、用户体验变差，甚至引发大面积故障。 二 大Key与热Key的定义我们经常能够在公司内部的Redis开发使用规范手册，或网络中大量的Redis最佳实践文章里看到有关大Key、热Key的定义，然而这些资料中的大Key热Key判定标准却不尽相同，但可以明确的是，它们的判定维度是一致的：大Key通常都会以数据大小与成员数量来判定，而热Key则以其接收到的请求频率、数量来判定。 1 什么是大Key通常我们会将含有较大数据或含有大量成员、列表数的Key称之为大Key，下面我们将用几个实际的例子对大Key的特征进行描述： 一个STRING类型的Key，它的值为5MB（数据过大） 一个LIST类型的Key，它的列表数量为20000个（列表数量过多） 一个ZSET类型的Key，它的成员数量为10000个（成员数量过多） 一个HASH格式的Key，它的成员数量虽然只有1000个但这些成员的value总大小为100MB（成员体积过大） 需要注意的是，在以上的例子中，为了方便理解，我们对大Key的数据、成员、列表数给出了具体的数字。为了避免误导，在实际业务中，大Key的判定仍然需要根据Redis的实际使用场景、业务场景来进行综合判断。 2 什么是热Key在某个Key接收到的访问次数、显著高于其它Key时，我们可以将其称之为热Key，常见的热Key如： 某Redis实例的每秒总访问量为10000，而其中一个Key的每秒访问量达到了7000（访问次数显著高于其它Key） 对一个拥有上千个成员且总大小为1MB的HASH Key每秒发送大量的HGETALL（带宽占用显著高于其它Key） 对一个拥有数万个成员的ZSET Key每秒发送大量的ZRANGE（CPU时间占用显著高于其它Key） 三 大Key与热Key带来的问题在Redis的使用中，大Key及热Key会给Redis带来各种各样的问题，而最常见的问题为性能下降、访问超时、数据不均衡等。 1 大Key带来的常见问题 Client发现Redis变慢； Redis内存不断变大引发OOM，或达到maxmemory设置值引发写阻塞或重要Key被逐出； Redis Cluster中的某个node内存远超其余node，但因Redis Cluster的数据迁移最小粒度为Key而无法将node上的内存均衡化； 大Key上的读请求使Redis占用服务器全部带宽，自身变慢的同时影响到该服务器上的其它服务； 删除一个大Key造成主库较长时间的阻塞并引发同步中断或主从切换； 2 热Key带来的常见问题 热Key占用大量的Redis CPU时间使其性能变差并影响其它请求； Redis Cluster中各node流量不均衡造成Redis Cluster的分布式优势无法被Client利用，一个分片负载很高而其它分片十分空闲从而产生读/写热点问题； 在抢购、秒杀活动中，由于商品对应库存Key的请求量过大超出Redis处理能力造成超卖； 热Key的请求压力数量超出Redis的承受能力造成缓存击穿，此时大量强求将直接指向后端存储将其打挂并影响到其它业务； 四 大Key与热Key的常见产生原因 业务规划不足、Redis不正确的使用、无效数据的堆积、访问突增等都会产生大Key与热Key，如： 将Redis用在并不适合其能力的场景，造成Key的value过大，如使用String类型的Key存放大体积二进制文件型数据（大Key）； 业务上线前规划设计考虑不足没有对Key中的成员进行合理的拆分，造成个别Key中的成员数量过多（大Key）； 没有对无效数据进行定期清理，造成如HASH类型Key中的成员持续不断的增加（大Key）； 预期外的访问量陡增，如突然出现的爆款商品、访问量暴涨的热点新闻、直播间某大主播搞活动带来的大量刷屏点赞、游戏中某区域发生多个工会间的战斗涉及大量玩家等（热Key）； 使用LIST类型Key的业务消费侧代码故障，造成对应Key的成员只增不减（大Key）； 五 找出Redis中的大Key与热Key大Key与热Key的分析并不困难，我们有多种途径和手段来对Redis中的Key进行分析并找出其中的“问题”Key，如Redis的内置功能、开源工具、阿里云Redis控制台中的Key分析功能等。 1 使用Redis内置功能发现大Key及热KeyRedis内置的一些命令、工具都可以帮助我们来发现这些问题Key。当你对Redis的大Key热Key已有明确的分析目标时，可以通过如下命令对对应Key进行分析。 通过Redis内置命令对目标Key进行分析 可能你会选择使用debug object命令对Key进行分析。该命令能够根据传入的对象（Key的名称）来对Key进行分析并返回大量数据，其中serializedlength的值为该Key的序列化长度，你可能会选择通过该数据来判断对应Key是否符合你的大Key判定标准。 需要注意的是，Key的序列化长度并不等同于它在内存空间中的真实长度，此外，debug object属于调试命令，运行代价较大，并且在其运行时，进入Redis的其余请求将会被阻塞直到其执行完毕。而该命令的运行的时间长短取决于传入对象（Key名）序列化长度的大小，因此，在线上环境中并不推荐使用该命令来分析大Key，这可能引发故障。 Redis自4.0起提供了MEMORY USAGE命令来帮助分析Key的内存占用，相对debug object它的执行代价更低，但由于其时间复杂度为O(N)因此在分析大Key时仍有阻塞风险。 我们建议通过风险更低方式来对Key进行分析，Redis对于不同的数据结构提供了不同的命令来返回其长度或成员数量，如下表： 通过以上Redis内置命令我们可以方便且安全的对Key进行分析而不会影响线上服务，但由于它们返回的结果非Key的真实内存占用数据，因此不够精确，仅可作为参考。 通过Redis官方客户端redis-cli的bigkeys参数发现大Key 如果你并无明确的目标Key用于分析，而是希望通过工具找出整个Redis实例中的大Key，此时redis-cli的bigkeys参数能够方便的帮你实现这个目标。 Redis提供了bigkeys参数能够使redis-cli以遍历的方式分析整个Redis实例中的所有Key并汇总以报告的方式返回结果。该方案的优势在于方便及安全，而缺点也非常明显：分析结果不可定制化。 bigkeys仅能分别输出Redis六种数据结构中的最大Key，如果你想只分析STRING类型或是找出全部成员数量超过10的HASH Key，那么bigkeys在此类需求场景下将无能为力。 GitHub上有大量的开源项目能够实现bigkeys的加强版使结果能够按照配置定制化输出，另外你可也以动手使用SCAN + TYPE并配合上文表格中的命令自己实现一个Redis实例级的大Key分析工具。 同样，该方案的实现方式及返回结果使其不具备精确性与实时性，建议仅作为参考。 通过Redis官方客户端redis-cli的hotkeys参数发现热Key Redis自4.0起提供了hotkeys参数来方便用户进行实例级的热Key分析功，该参数能够返回所有Key的被访问次数，它的缺点同样为不可定制化输出报告，大量的信息会使你在分析结果时复杂度较大，另外，使用该方案的前提条件是将redis-server的maxmemory-policy参数设置为LFU。 通过业务层定位热Key 指向Redis的每一次访问都来自业务层，因此我们可以通过在业务层增加相应的代码对Redis的访问进行记录并异步汇总分析。该方案的优势为能够准确并及时的分析出热Key的存在，缺点为业务代码复杂度的增加，同时可能会降低一些性能。 使用monitor命令在紧急情况时找出热Key Redis的monitor命令能够忠实的打印Redis中的所有请求，包括时间信息、Client信息、命令以及Key信息。在发生紧急情况时，我们可以通过短暂执行monitor命令并将输出重定向至文件，在关闭monitor命令后通过对文件中请求进行归类分析即可找出这段时间中的热Key。 由于monitor命令对Redis的CPU、内存、网络资源均有一定的占用。因此，对于一个已处于高压状态的Redis，monitor可能会起到雪上加霜的作用。同时，这种异步收集并分析的方案的时效性较差，并且由于分析的精确度取决于monitor的执行时间，因此在多数无法长时间执行该命令的线上场景中本方案的精确度也不够好。 2 使用开源工具发现大KeyRedis的高度流行使我们能够方便的找到大量开源方案来解决我们当前遇到的难题：在不影响线上服务的同时得到精确的分析报告。 使用redis-rdb-tools工具以定制化方式找出大Key 如果你希望按照自己的标准精确的分析一个Redis实例中所有Key的真实内存占用并避免影响线上服务，在分析结束后得到一份简洁易懂的报告，redis-rdb-tools是非常好的选择。 该工具能够对Redis的RDB文件进行定制化的分析，但由于分析RDB文件为离线工作，因此对线上服务不会有任何影响，这是它的最大优点但同时也是它的最大缺点：离线分析代表着分析结果的较差时效性。对于一个较大的RDB文件，它的分析可能会持续很久很久。 3 依靠公有云的Redis分析服务发现大Key及热Key如果你期望能够实时的对Redis实例中的所有Key进行分析并发现当前存在的大Key及热Key、了解Redis在运行时间线中曾出现过哪些大Key热Key，使自己对整个Redis实例的运行状态有一个全面而又准确的判断，那么公有云的Redis控制台将能满足这个需求。 阿里云Redis控制台中的CloudDBA CloudDBA是阿里云的数据库智能服务系统，它支持Redis大Key与热Key的实时分析、发现。 大Key及热Key分析底层为阿里云Redis内核的Key分析功能，该功能通过Redis内核直接发现并输出大Key热Key的相关信息，因此，该功能的分析结果准确性高效且对性能几乎无任何影响，你可以通过点击CloudDBA中的“Key分析”进入该功能，如下图： Key分析功能共有两个页面，它们允许在不同的时间维度对对应Redis实例中的Key进行分析： 实时：对当前实例立即开始分析当前实例，展示当前存在的所有大Key及热Key。 历史：展示该实例近期曾出现过的大Key及热Key，在历史页面中，所有出现过的大Key及热Key都会被记录，哪怕这些Key当前已经不存在。该功能能够很好的反映Redis的历史Key状态，帮助追溯过去或现场已遭破坏的问题。 六 大Key与热Key的处理现在，我们已经通过多种手段找到了Redis中的问题Key，那么我们应当立即着手对他们进行处理，避免它们在之后的时间中引发问题。 1 大Key的常见处理办法对大Key进行拆分 如将一个含有数万成员的HASH Key拆分为多个HASH Key，并确保每个Key的成员数量在合理范围，在Redis Cluster结构中，大Key的拆分对node间的内存平衡能够起到显著作用。 对大Key进行清理 将不适合Redis能力的数据存放至其它存储，并在Redis中删除此类数据。需要注意的是，我们已在上文提到一个过大的Key可能引发Redis集群同步的中断，Redis自4.0起提供了UNLINK命令，该命令能够以非阻塞的方式缓慢逐步的清理传入的Key，通过UNLINK，你可以安全的删除大Key甚至特大Key。 时刻监控Redis的内存水位 突然出现的大Key问题会让我们措手不及，因此，在大Key产生问题前发现它并进行处理是保持服务稳定的重要手段。我们可以通过监控系统并设置合理的Redis内存报警阈值来提醒我们此时可能有大Key正在产生，如：Redis内存使用率超过70%，Redis内存1小时内增长率超过20%等。 通过此类监控手段我们可以在问题发生前解决问题，如：LIST的消费程序故障造成对应Key的列表数量持续增长，将告警转变为预警从而避免故障的发生。 对失效数据进行定期清理 例如我们会在HASH结构中以增量的形式不断写入大量数据而忽略了这些数据的时效性，这些大量堆积的失效数据会造成大Key的产生，可以通过定时任务的方式对失效数据进行清理。在此类场景中，建议使用HSCAN并配合HDEL对失效数据进行清理，这种方式能够在不阻塞的前提下清理无效数据。 使用阿里云的Tair(Redis企业版)服务避开失效数据的清理工作 如果你的HASH Key过多，同时存在大量的成员失效需要被清理的问题。由于大量Key与大量失效数据的叠加，在此类场景中定时任务已无法做到对无效数据进行及时的清理，阿里云的Tair服务能够很好的解决此类问题。 Tair是阿里云的Redis企业版，它在具备Redis所有特性（包括Redis的高性能特点）的同时提供了大量额外的高级功能。 TairHash是一种可为field设置过期时间和版本的hash类型数据结构，它不但和Redis Hash一样支持丰富的数据接口和高处理性能，还改变了hash只能为key设置过期时间的限制：TairHash允许为field设置过期时间和版本。这极大地提高了hash数据结构的灵活性，简化了很多场景下的业务开发工作。 TairHash使用高效的Active Expire算法，实现了在对响应时间几乎无影响的前提下，高效完成对field过期判断和删除的功能。此类高级功能的合理使用能够解放大量Redis的运维、故障处理工作并降低业务的代码复杂度，让运维将精力投入到其它更有价值的工作中，让研发有更多的时间来写更有价值的代码。 2 热Key的常见处理办法在Redis Cluster结构中对热Key进行复制 在Redis Cluster中，热Key由于迁移粒度问题造成请求无法打散使单一node的压力无法下降。此时可以将对应热Key进行复制并迁移至其他node，例如为热Key foo复制出3个内容完全一样的Key并名为foo2，foo3，foo4，然后将这三个Key迁移到其他node来解决单一node的热Key压力。 该方案的缺点在于代码需要联动修改，同时，Key一变多带来了数据一致性挑战：由更新一个Key演变为需要同时更新多个Key，在很多时候，该方案仅建议用来临时解决当前的棘手问题。 使用读写分离架构 如果热Key的产生来自于读请求，那么读写分离是一个很好的解决方案。在使用读写分离架构时可以通过不断的增加从节点来降低每个Redis实例中的读请求压力。 然而，读写分离架构在业务代码复杂度增加的同时，同样带来了Redis集群架构复杂度的增加：我们不仅要为多个从节点提供转发层（如Proxy，LVS等）来实现负载均衡，还要考虑从节点数量显著增加后带来的故障率增加的问题，Redis集群架构变更的同时为监控、运维、故障处理带来了更大的挑战。 但是，这一切在阿里云Redis服务中显得极为简单，阿里云Redis服务以开箱即用的方式提供服务。同时，在业务的发展发生变化时，阿里云的Redis服务允许用户通过变配的方式调整集群架构来轻松应对，如：主从转变为读写分离，读写分构转变为集群，主从转变为支持读写分离的集群，以及由社区版直接转变为支持大量高级特性的企业版Redis（Tair）。 读写分离架构同样存在缺点，在请求量极大的场景下，读写分离架构会产生不可避免的延迟，此时会有读取到脏数据的问题，因此，在读写压力都较大写对数据一致性要求很高的场景下，读写分离架构并不合适。 使用阿里云Tair的QueryCache特性 QueryCache是阿里云Tair（Redis企业版）服务的企业级特性之一，它的原理如下图： 阿里云数据库Redis会根据高效的排序和统计算法识别出实例中存在的热点Key，开启该功能后，Proxy点会根据设定的规则缓存热点Key的请求和查询结果（仅缓存热点Key的查询结果，无需缓存整个Key），当在缓存有效时间内收到相同的请求时Proxy会直接返回结果至客户端，无需和后端的Redis分片执行交互。在提升读取速度的同时，降低了热点Key对数据分片的性能影响，避免发生请求倾斜。 至此，来自客户端的同样的请求无需再与Proxy后端的Redis进行交互而由Proxy直接返回数据，指向热Key的请求由一个Redis节点承担转为多个Proxy共同承担，能够大幅度降低Redis节点的热Key压力，同时Tair的QueryCache功能还提供了大量的命令来方便用户查看、管理，如通过querycache keys命令查看所有被缓存热Key，通过querycache listall获取所有已缓存的所有命令等。 Tair QueryCache智能化的热Key判定与缓存联动功同样能够降低运维及研发的工作负担。 使用阿里云Redis全球分布式缓存服务 如果热Key的产生来自于写请求，那么传统的Redis读写分离、集群结构都难以解决这一问题。在此类场景下，可以通过阿里云Redis全球分布式缓存架构来扩展同一Key的写能力，从而降低写热Key的单机压力。 阿里云Redis全球分布式缓存（又称全球多活）是基于阿里云数据库Redis自研的多活数据库系统，可轻松支持异地多个站点同时对外提供服务的业务场景，助力企业快速复制阿里巴巴异地多活架构。阿里云Redis全球分布式缓存在解决跨地域、跨国多活问题的同时，因为其支持多点写入，因此我们可以使用该架构来实现最多三倍的同一Key的写入性能的扩展，Redis全球分布式缓存架构如下图： 与传统的Redis同步中间件相比，阿里云Redis全球分布式缓存具有高可靠性、高吞吐低延迟、同步正确性高等特点。","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"}],"author":"Marlowe"},{"title":"微服务架构设计之六边形架构","slug":"架构/微服务架构之六边形架构","date":"2021-08-26T04:23:22.000Z","updated":"2021-08-26T10:09:44.516Z","comments":true,"path":"2021/08/26/架构/微服务架构之六边形架构/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/26/%E6%9E%B6%E6%9E%84/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E4%B9%8B%E5%85%AD%E8%BE%B9%E5%BD%A2%E6%9E%B6%E6%9E%84/","excerpt":"以前接触的web开发都是三层架构，实习的这家公司采用的是六边形架构，此文将简单介绍一下微服务架构设计之六边形架构。","text":"以前接触的web开发都是三层架构，实习的这家公司采用的是六边形架构，此文将简单介绍一下微服务架构设计之六边形架构。 前言六边形架构又称”端口适配器架构“，本质上也是一种分层架构，跟传统的MVC架构不同的是，从上下层转为了内外层。 内部代表了应用的业务逻辑，外部代表应用的驱动逻辑、基础设施或其他应用。 核心理念是应用通过端口与外部进行交互。核心的业务逻辑与外部资源完全隔离，仅通过适配器进行交互。 六边形架构原理 左侧，应用程序端，输入适配器这是用户或外部程序与应用程序交互的一面。它包含允许这些交互的代码。通常，您的用户界面代码，API的HTTP路由，以及使用您的应用程序的程序的JSON序列化都在这里。 内部，业务逻辑这是我们想要从左侧和右侧隔离的部分。它包含所有关注和实现业务逻辑的代码。业务词汇和纯粹的业务逻辑，与解决应用程序的具体问题，使其丰富和具体的所有内容相关联，处于中心位置。 在这里，我们选择根据业务逻辑组织其模块（目录），理想状态下可以考虑采用领域驱动设计。 右侧，基础设施方面，输出适配器取决于您的应用程序需要什么，它驱动的工作。它包含必要的基础结构详细信息，例如与数据库交互的代码，调用文件系统或处理对您所依赖的其他应用程序的HTTP调用的代码。 特性 外部可替换一个端口对应多个适配器，是对一类外部系统的归纳，它体现了对外部的抽象。应用通过端口为外界提供服务，这些端口需要被良好的设计和测试。内部不关心外部如何使用端口，从一开始就要假定外部使用者是可替换的。六边形的六并没有实质意义，只是为了留足够的空间放置端口和适配器。适配器可以分为2类，“主”、“从”适配器，也可称为“驱动者”和“被驱动者”。 自动测试在六边形架构中，自动化测试和用户具有同等的地位，在实现用户界面的同时就需要考虑自动化测试。它们对应相同的端口。六边形架构不仅让自动化测试这件事情成为设计第一要素，同时自动化测试也保证应用逻辑不会泄露到用户界面，在技术上保证了层次的分界。 依赖倒置六边形架构必须遵循如下规则：内部相关的代码不能泄露到外部。所谓的泄露是指不能出现内部依赖外部的情况，只能外部依赖内部，这样才能保证外部是可以替换的。对于驱动者适配器，就是外部依赖内部的。但是对于被驱动者适配器，实际是内部依赖外部，这时需要使用依赖倒置，由驱动者适配器将被驱动者适配器注入到应用内部，这时端口的定义在应用内部，但是实现是由适配器实现。 实现细节右侧的依赖倒置 定义实现所需的接口 逻辑层的成员变量直接引用抽象接口，依赖注入 逻辑服务使用抽象出来的方法 输出适配器具体实现抽象接口 如果不做依赖倒置，后续要把nsq替换成kafka的话，就需要将所有用到nsq的地方全部重写。 有了依赖倒置，新的中间件只需要实现定义好的接口即可。 六边形结构测试 在一般情况下，左侧代码的作用可以由测试框架直接扮演。实际上，测试代码可以直接驱动业务逻辑代码。 右边的代码必须由业务驱动。通常，如果要编写单元测试，可以使用模拟或任何其他形式的测试双重替换它，具体取决于要测试的内容。 项目结构 项目结构图参考","categories":[{"name":"架构","slug":"架构","permalink":"https://xmmarlowe.github.io/categories/%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"微服务","slug":"微服务","permalink":"https://xmmarlowe.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"六边形架构","slug":"六边形架构","permalink":"https://xmmarlowe.github.io/tags/%E5%85%AD%E8%BE%B9%E5%BD%A2%E6%9E%B6%E6%9E%84/"}],"author":"Marlowe"},{"title":"大厂都是怎么调优的？","slug":"数据库/大厂都是怎么调优的？","date":"2021-08-25T23:55:18.000Z","updated":"2021-08-26T00:27:36.238Z","comments":true,"path":"2021/08/26/数据库/大厂都是怎么调优的？/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/26/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%A4%A7%E5%8E%82%E9%83%BD%E6%98%AF%E6%80%8E%E4%B9%88%E8%B0%83%E4%BC%98%E7%9A%84%EF%BC%9F/","excerpt":"转载与大厂都是怎么SQL调优的？ – 敖丙","text":"转载与大厂都是怎么SQL调优的？ – 敖丙 前言这天我正在午休呢，公司DBA就把我喊醒了，说某库出现大量慢SQL，很快啊，很快，我还没反应过来，库就挂了，我心想现在的用户不讲武德啊，怎么在我睡觉的时候大量请求呢。 这是很常见的一个场景哈，因为很多业务开始数据量级不大，所以写sql的时候就没注意性能，等量级上去，很多业务就需要做调优了，在电商公司工作的这几年我也总结了不少，下面就分享给大家吧。 在代码开发过程中，我们都会遵循一些SQL开发规范去编写高质量SQL，来提高接口的Response Time(RT)，对一些核心接口要求RT在100ms以内甚至更低。 由于业务前期数据量比较小，基本都能满足这个要求，但随着业务量的增长，数据量也随之增加，对应接口的SQL耗时也在变长，直接影响了用户的体验，这时候就需要对SQL进行优化。 优化点主要包括SQL规范性检查，表结构索引检查，SQL优化案例分析，下面从这三方面结合实际案例聊聊如何优化SQL。 SQL规范性检查每个公司都有自己的MySQL开发规范，基本上大同小异，这里罗列一些比较重要的，我工作期间经常接触的给大家。 select检查UDF用户自定义函数 SQL语句的select后面使用了自定义函数UDF，SQL返回多少行，那么UDF函数就会被调用多少次，这是非常影响性能的。 12#getOrderNo是用户自定义一个函数用户来根据order_sn来获取订单编号select id, payment_id, order_sn, getOrderNo(order_sn) from payment_transaction where status = 1 and create_time between &#x27;2020-10-01 10:00:00&#x27; and &#x27;2020-10-02 10:00:00&#x27;; text类型检查 如果select出现text类型的字段，就会消耗大量的网络和IO带宽，由于返回的内容过大超过max_allowed_packet设置会导致程序报错，需要评估谨慎使用。 12#表request_log的中content是text类型。select user_id, content, status, url, type from request_log where user_id = 32121; group_concat谨慎使用 gorup_concat是一个字符串聚合函数，会影响SQL的响应时间，如果返回的值过大超过了max_allowed_packet设置会导致程序报错。 1select batch_id, group_concat(name) from buffer_batch where status = 0 and create_time between &#x27;2020-10-01 10:00:00&#x27; and &#x27;2020-10-02 10:00:00&#x27;; 内联子查询 在select后面有子查询的情况称为内联子查询，SQL返回多少行，子查询就需要执行过多少次，严重影响SQL性能。 1select id,(select rule_name from member_rule limit 1) as rule_name, member_id, member_type, member_name, status from member_info m where status = 1 and create_time between &#x27;2020-09-02 10:00:00&#x27; and &#x27;2020-10-01 10:00:00&#x27;; from检查表的链接方式 在MySQL中不建议使用Left Join，即使ON过滤条件列索引，一些情况也不会走索引，导致大量的数据行被扫描，SQL性能变得很差，同时要清楚ON和Where的区别。 12SELECT a.member_id,a.create_time,b.active_time FROM operation_log a LEFT JOIN member_info b ON a.member_id = b.member_id where b.`status` = 1and a.create_time between &#x27;2020-10-01 00:00:00&#x27; and &#x27;2020-10-30 00:00:00&#x27; limit 100, 0; 子查询 由于MySQL的基于成本的优化器CBO对子查询的处理能力比较弱，不建议使用子查询，可以改写成Inner Join。 12select b.member_id,b.member_type, a.create_time,a.device_model from member_operation_log a inner join (select member_id,member_type from member_base_info where `status` = 1and create_time between &#x27;2020-10-01 00:00:00&#x27; and &#x27;2020-10-30 00:00:00&#x27;) as b on a.member_id = b.member_id; where检查索引列被运算 当一个字段被索引，同时出现where条件后面，是不能进行任何运算，会导致索引失效。 1234#device_no列上有索引，由于使用了ltrim函数导致索引失效select id, name , phone, address, device_no from users where ltrim(device_no) = &#x27;Hfs1212121&#x27;;#balance列有索引,由于做了运算导致索引失效select account_no, balance from accounts where balance + 100 = 10000 and status = 1; 类型转换 对于Int类型的字段，传varchar类型的值是可以走索引，MySQL内部自动做了隐式类型转换；相反对于varchar类型字段传入Int值是无法走索引的，应该做到对应的字段类型传对应的值总是对的。 1234#user_id是bigint类型，传入varchar值发生了隐式类型转换，可以走索引。select id, name , phone, address, device_no from users where user_id = &#x27;23126&#x27;;#card_no是varchar(20)，传入int值是无法走索引select id, name , phone, address, device_no from users where card_no = 2312612121; 列字符集 从MySQL 5.6开始建议所有对象字符集应该使用用utf8mb4，包括MySQL实例字符集，数据库字符集，表字符集，列字符集。避免在关联查询Join时字段字符集不匹配导致索引失效，同时目前只有utf8mb4支持emoji表情存储。 1234character_set_server = utf8mb4 #数据库实例字符集character_set_connection = utf8mb4 #连接字符集character_set_database = utf8mb4 #数据库字符集character_set_results = utf8mb4 #结果集字符集 group by检查前缀索引 group by后面的列有索引，索引可以消除排序带来的CPU开销，如果是前缀索引，是不能消除排序的。 1234#device_no字段类型varchar(200)，创建了前缀索引。mysql&gt; alter table users add index idx_device_no(device_no(64));mysql&gt; select device_no, count(*) from users where create_time between &#x27;2020-10-01 00:00:00&#x27; and &#x27;2020-10-30 00:00:00&#x27; group by device_no; 函数运算 假设需要统计某月每天的新增用户量，参考如下SQL语句，虽然可以走create_time的索引，但是不能消除排序，可以考虑冗余一个字段stats_date date类型来解决这种问题。 1select DATE_FORMAT(create_time, &#x27;%Y-%m-%d&#x27;), count(*) from users where create_time between &#x27;2020-09-01 00:00:00&#x27; and &#x27;2020-09-30 23:59:59&#x27; group by DATE_FORMAT(create_time, &#x27;%Y-%m-%d&#x27;); order by检查前缀索引 order by后面的列有索引，索引可以消除排序带来的CPU开销，如果是前缀索引，是不能消除排序的。 字段顺序 排序字段顺序，asc/desc升降要跟索引保持一致，充分利用索引的有序性来消除排序带来的CPU开销。 limit检查limit m,n要慎重 对于limit m, n分页查询，越往后面翻页即m越大的情况下SQL的耗时会越来越长，对于这种应该先取出主键id，然后通过主键id跟原表进行Join关联查询。 表结构检查表&amp;列名关键字在数据库设计建模阶段，对表名及字段名设置要合理，不能使用MySQL的关键字，如desc, order, status, group等。同时建议设置lower_case_table_names = 1表名不区分大小写。 表存储引擎对于OLTP业务系统，建议使用InnoDB引擎获取更好的性能，可以通过参数default_storage_engine控制。 AUTO_INCREMENT属性建表的时候主键id带有AUTO_INCREMENT属性，而且AUTO_INCREMENT=1，在InnoDB内部是通过一个系统全局变量dict_sys.row_id来计数，row_id是一个8字节的bigint unsigned，InnoDB在设计时只给row_id保留了6个字节的长度，这样row_id取值范围就是0到2^48 - 1，如果id的值达到了最大值，下一个值就从0开始继续循环递增，在代码中禁止指定主键id值插入。 12345#新插入的id值会从10001开始，这是不对的，应该从1开始。create table booking( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#x27;主键id&#x27;,......) engine = InnoDB auto_increment = 10000;#指定了id值插入，后续自增就会从该值开始+1，索引禁止指定id值插入。insert into booking(id, book_sn) values(1234551121, &#x27;N12121&#x27;); NOT NULL属性根据业务含义，尽量将字段都添加上NOT NULL DEFAULT VALUE属性，如果列值存储了大量的NULL，会影响索引的稳定性。 DEFAULT属性在创建表的时候，建议每个字段尽量都有默认值，禁止DEFAULT NULL，而是对字段类型填充响应的默认值。 COMMENT属性字段的备注要能明确该字段的作用，尤其是某些表示状态的字段，要显式的写出该字段所有可能的状态数值以及该数值的含义。 TEXT类型不建议使用Text数据类型，一方面由于传输大量的数据包可能会超过max_allowed_packet设置导致程序报错，另一方面表上的DML操作都会变的很慢，建议采用es或者对象存储OSS来存储和检索。 索引检查索引属性索引基数指的是被索引的列唯一值的个数，唯一值越多接近表的count(*)说明索引的选择率越高，通过索引扫描的行数就越少，性能就越高，例如主键id的选择率是100%，在MySQL中尽量所有的update都使用主键id去更新，因为id是聚集索引存储着整行数据，不需要回表，性能是最高的。 1234567891011121314151617181920212223242526272829mysql&gt; select count(*) from member_info;+----------+| count(*) |+----------+| 148416 |+----------+1 row in set (0.35 sec)mysql&gt; show index from member_base_info;+------------------+------------+----------------------------+--------------+-------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |+------------------+------------+----------------------------+--------------+-------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| member_info | 0 | PRIMARY | 1 | id | A | 131088 | NULL | NULL | | BTREE | | || member_info | 0 | uk_member_id | 1 | member_id | A | 131824 | NULL | NULL | | BTREE | | || member_info | 1 | idx_create_time | 1 | create_time | A | 6770 | NULL | NULL | | BTREE | | |+------------------+------------+----------------------------+--------------+-------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+#Table： 表名#Non_unique ：是否为unique index，0-是，1-否。#Key_name：索引名称#Seq_in_index：索引中的顺序号，单列索引-都是1；复合索引-根据索引列的顺序从1开始递增。#Column_name：索引的列名#Collation：排序顺序，如果没有指定asc/desc，默认都是升序ASC。#Cardinality：索引基数-索引列唯一值的个数。#sub_part：前缀索引的长度；例如index (member_name(10)，长度就是10。#Packed：索引的组织方式，默认是NULL。#Null：YES:索引列包含Null值；&#x27;&#x27;:索引不包含Null值。#Index_type：默认是BTREE，其他的值FULLTEXT，HASH，RTREE。#Comment：在索引列中没有被描述的信息，例如索引被禁用。#Index_comment：创建索引时的备注。 前缀索引对于变长字符串类型varchar(m)，为了减少key_len，可以考虑创建前缀索引，但是前缀索引不能消除group by， order by带来排序开销。如果字段的实际最大值比m小很多，建议缩小字段长度。 1alter table member_info add index idx_member_name_part(member_name(10)); 复合索引顺序有很多人喜欢在创建复合索引的时候，总以为前导列一定是唯一值多的列，例如索引index idx_create_time_status(create_time, status)，这个索引往往是无法命中，因为扫描的IO次数太多，总体的cost的比全表扫描还大，CBO最终的选择是走full table scan。 MySQL遵循的是索引最左匹配原则，对于复合索引，从左到右依次扫描索引列，到遇到第一个范围查询（&gt;=, &gt;,&lt;, &lt;=, between ….. and ….）就停止扫描，索引正确的索引顺序应该是index idx_status_create_time(status, create_time)。 1select account_no, balance from accounts where status = 1 and create_time between &#x27;2020-09-01 00:00:00&#x27; and &#x27;2020-09-30 23:59:59&#x27;; 时间列索引对于默认字段created_at(create_time)、updated_at(update_time)这种默认就应该创建索引，这一般来说是默认的规则。 SQL优化案例通过对慢查询的监控告警，经常发现一些SQL语句where过滤字段都有索引，但是由于SQL写法的问题导致索引失效，下面二个案例告诉大家如何通过SQL改写来查询。可以通过以下SQL来捞取最近5分钟的慢查询进行告警。 1select CONCAT( &#x27;# Time: &#x27;, DATE_FORMAT(start_time, &#x27;%y%m%d %H%i%s&#x27;), &#x27;\\n&#x27;, &#x27;# User@Host: &#x27;, user_host, &#x27;\\n&#x27;, &#x27;# Query_time: &#x27;, TIME_TO_SEC(query_time), &#x27; Lock_time: &#x27;, TIME_TO_SEC(lock_time), &#x27; Rows_sent: &#x27;, rows_sent, &#x27; Rows_examined: &#x27;, rows_examined, &#x27;\\n&#x27;, sql_text, &#x27;;&#x27; ) FROM mysql.slow_log where start_time between current_timestamp and date_add(CURRENT_TIMESTAMP,INTERVAL -5 MINUTE); 慢查询SQL1| 2020-10-02 19:17:23 | w_mini_user[w_mini_user] @ [10.200.20.11] | 00:00:02 | 00:00:00 | 9 | 443117 | mini_user | 0 | 0 | 168387936 | select id,club_id,reason,status,type,created_time,invite_id,falg_admin,file_id from t_user_msg where 1 and (team_id in (3212) and app_id is not null) or (invite_id=12395 or applicant_id=12395) order by created_time desc limit 0,10; | 1219921665 | 从慢查询slow_log可以看到，执行时间2s，扫描了443117行，只返回了9行，这是不合理的。 SQL分析12345678910#原始SQL，频繁访问的接口，目前执行时间2s。select id,team_id,reason,status,type,created_time,invite_id,falg_admin,file_id from t_user_msg where 1 and (team_id in (3212) and app_id is not null) or (invite_id=12395 or app_id=12395) order by created_time desc limit 0,10;#执行计划+----+-------------+--------------+-------+---------------------------------+------------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------------+-------+---------------------------------+------------+---------+------+------+-------------+| 1 | SIMPLE | t_user_msg | index | invite_id,app_id,team_id | created_time | 5 | NULL | 10 | Using where |+----+-------------+--------------+-------+---------------------------------+------------+---------+------+------+-------------+1 row in set (0.00 sec) 从执行计划可以看到，表上有单列索引invite_id,app_id,team_id,created_time，走的是create_time的索引，而且type=index索引全扫描，因为create_time没有出现在where条件后，只出现在order by后面，只能是type=index，这也预示着表数据量越大该SQL越慢，我们期望是走三个单列索引invite_id，app_id，team_id，然后type=index_merge操作。 按照常规思路，对于OR条件拆分两部分，分别进行分析。 1select id, ……. from t_user_msg where 1 and **(team_id in (3212) and app_id is not null)** order by created_time desc limit 0,10; 从执行计划看走的是team_id的索引，没有问题。 123| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------------+------+----------------------+---------+---------+-------+------+-----------------------------+| 1 | SIMPLE | t_user_msg | ref | app_id,team_id | team_id | 8 | const | 30 | Using where; Using filesort | 再看另外一个sql语句： 1select id, ……. from t_user_msg where 1 and **(invite_id=12395 or app_id=12395)** order by created_time desc limit 0,10; 从执行计划上看，分别走的是invite_id,app_id的单列索引，同时做了index_merge合并操作，也没有问题。 123| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------------+-------------+-------------------------+-------------------------+---------+------+------+-------------------------------------------------------------------+| 1 | SIMPLE | t_user_msg | index_merge | invite_id,app_id | invite_id,app_id | 9,9 | NULL | 2 | Using union(invite_id,app_id); Using where; Using filesort | 通过上面的分析，第一部分SQL走的执行计划走team_id索引没问题，第二部分SQL分别走invite_id,app_id索引并且index_merge也没问题，为什么两部分SQL进行OR关联之后走create_time的单列索引呢，不应该是三个单列索引的index_merge吗？ index_merge默认是在优化器选项是开启的，主要是将多个范围扫描的结果集合并成一个，可以通过变量查看。 12mysql &gt;select @@optimizer_switch;| index_merge=on,index_merge_union=on,index_merge_sort_union=on,index_merge_intersection=on, 其他三个字段都传入的是具体的值，而且都走了相应的索引，只能怀疑app_id is not null这个条件影响了CBO对最终执行计划的选择，去掉这个条件来看执行计划，竟然走了三个单列索引且type=index_merge，那下面只要搞定app_id is not null这个条件就OK了吧。 123| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+--------------+-------------+---------------------------------+---------------------------------+---------+------+------+---------------------------------------------------------------------------+| 1 | SIMPLE | t_user_msg | index_merge | invite_id,app_id,teadm_id | team_id,invite_id,app_id | 8,9,9 | NULL | 32 | Using union(team_id,invite_id,app_id); Using where; Using filesort | SQL改写通过上面分析得知，条件app_id is not null影响了CBO的选择，下面进行改造。 改写优化1 根据SQL开发规范改写，将OR改写成Union All方式即可，最终的SQL如下： 12345select id, ……. from (select id, ……. from t_user_msg where **1 and (club_id in (5821) and applicant_id is not null)** **union all** select id, ……. from t_user_msg where **1 and invitee_id=&#x27;146737&#x27;** **union all** select id, ……. from t_user_msg where **1 and app_id=&#x27;146737&#x27;** ) as a order by created_time desc limit 0,10; 一般情况下，Java代码和SQL是分开的，SQL是配置在xml文件中，根据业务需求，除了team_id是必填，其他两个都是可选的，所以这种改写虽然能提高SQL执行效率，但不适合这种业务场景。 改写优化2 1app_id is not null 改写为IFNULL(app_id, 0) &gt;0)，最终的SQL为： 改写优化3 将字段app_id bigint(20) DEFAULT NULL，变更为app_id bigint(20) NOT NULL DEFAULT 0，同时更新将app_id is null的时候全部更新成0，就可以将条件app_id is not null 转换为app_id &gt; 0，最终的SQL为： 1select id,team_id,reason,status,type,created_at,invite_id,falg_admin,file_id from t_user_msg where 1 and (team_id in (3212) and **app_id &gt; 0)**) or (invite_id=12395 or app_id=12395) order by created_time desc limit 0,10; 从执行计划看，两种改写优化方式都走三个单列索引，执行时间从2s降低至10ms，线上采用的是优化1的方式，如果一开始能遵循MySQL开发规范就就会避免问题的发生。 总结上面介绍了SQL规范性检查，表结构检查，索引检查以及通过SQL改写来优化查询，在编写代码的过程，如果能提前做这些规范性检查，评估出自己认为理想的执行计划，然后通过explain解析出MySQL CBO的执行计划，两者做对比分析差异，弄清楚自己的选择和CBO的不同，不但能够编写高质量的SQL，同时也能清楚CBO的工作原理。 参考大厂都是怎么SQL调优的？","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://xmmarlowe.github.io/tags/MySQL/"},{"name":"调优","slug":"调优","permalink":"https://xmmarlowe.github.io/tags/%E8%B0%83%E4%BC%98/"}],"author":"Marlowe"},{"title":"一文了解web无状态会话token技术JWT","slug":"个人项目/一文了解web无状态会话token技术JWT","date":"2021-08-24T14:38:13.000Z","updated":"2021-08-25T15:37:34.054Z","comments":true,"path":"2021/08/24/个人项目/一文了解web无状态会话token技术JWT/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/24/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3web%E6%97%A0%E7%8A%B6%E6%80%81%E4%BC%9A%E8%AF%9Dtoken%E6%8A%80%E6%9C%AFJWT/","excerpt":"目前web开发前后端已经算非常的普及了。前后端分离要求我们对用户会话状态要进行一个无状态处理。我们都知道通常管理用户会话是session。用户每次从服务器认证成功后，服务器会发送一个sessionid给用户，session是保存在服务端 的，服务器通过session辨别用户，然后做权限认证等。那如何才知道用户的session是哪个？这时候cookie就出场了，浏览器第一次与服务器建立连接的时候，服务器会生成一个sessionid返回浏览器，浏览器把这个sessionid存储到cookie当中，以后每次发起请求都会在请求头cookie中带上这个sessionid信息，所以服务器就是根据这个sessionid作为索引获取到具体session。","text":"目前web开发前后端已经算非常的普及了。前后端分离要求我们对用户会话状态要进行一个无状态处理。我们都知道通常管理用户会话是session。用户每次从服务器认证成功后，服务器会发送一个sessionid给用户，session是保存在服务端 的，服务器通过session辨别用户，然后做权限认证等。那如何才知道用户的session是哪个？这时候cookie就出场了，浏览器第一次与服务器建立连接的时候，服务器会生成一个sessionid返回浏览器，浏览器把这个sessionid存储到cookie当中，以后每次发起请求都会在请求头cookie中带上这个sessionid信息，所以服务器就是根据这个sessionid作为索引获取到具体session。 痛点上面的场景会有一个痛点。对于前后端分离来说。比如前端都是部署在一台服务器的nginx上，后端部署在另一台服务器的web容器上。甚至 前端不能直接访问后端，中间还加了一层代理层。 大概如下所示： 也就是说前后端分离在应用解耦后增加了部署的复杂性。通常用户一次请求就要转发多次。如果用session 每次携带sessionid 到服务器，服务器还要查询用户信息。同时如果用户很多。这些信息存储在服务器内存中，给服务器增加负担。还有就是CSRF（跨站伪造请求攻击）攻击，session是基于cookie进行用户识别的, cookie如果被截获，用户就会很容易受到跨站请求伪造的攻击。还有就是sessionid就是一个特征值，表达的信息不够丰富。不容易扩展。而且如果你后端应用是多节点部署,那么就需要实现session共享机制(基于session复制)。不方便集群应用。 什么是JWT JSON Web Token (JWT)是一个开放标准(RFC 7519)，它定义了一种紧凑的、自包含的方式，用于作为JSON对象在各方之间安全地传输信息。该信息可以被验证和信任，因为它是数字签名的。 所以JSON WEB TOKEN（以下称JWT）可以解决上面的问题。JWT还是一种token。token 是服务器颁发给客户端的。就像户籍管理部门给你发的身份证一样。你拿着这个证件就能去其他部门办事。其他部门验证你这个身份证是否过期，是否真假。不用每次都让户籍来认可。同时token 天然防止CSRF攻击。而且JWT可以携带一些不敏感的用户信息。这样服务器不用每次都去查询用户信息。开箱即用，方便服务器处理鉴权逻辑。 是为了在网络应用环境间传递声明而执行的一种基于JSON的开放标准（(RFC 7519)。该token被设计为紧凑且安全的，特别适用于分布式站点的单点登录（SSO）场景。 JWT的声明一般被用来在身份提供者和服务提供者间传递被认证的用户身份信息，以便于从资源服务器获取资源，也可以增加一些额外的其它业务逻辑所必须的声明信息，该token也可直接被用于认证，也可被加密。 JWT的特点： 简洁(Compact): 可以通过URL，POST参数或者在HTTP header发送，因为数据量小，传输速度也很快。 自包含(Self-contained)：负载中包含了所有用户所需要的信息，避免了多次查询数据库或缓存。 JWT消息结构：JWT有3个组成部分，分别是: 头部（header) 声明类型以及加密算法 如 {“alg”:”HS256”,”typ”:”JWT”} 用Base64进行了处理 载荷（payload) 携带一些用户身份信息，用户id，颁发机构，颁发时间，过期时间等。用Base64进行了处理。这一段其实是明文，所以一定不要放敏感信息。 签证（signature) 签名信息，使用了自定义的一个密钥然后加密后的结果，目的就是为了保证签名的信息没有被别人改过，这个一般是让服务器验证的。 从上面的例子可以看出来JWT的规则是这样的 &lt;header&gt;.&lt;payload&gt;.&lt;signature&gt;三部分通过”.”进行拼接 。JWT.io提供解析的方法 我们可以拿上面那个token去玩一玩 所以JWT不是简单的token，比session+cookie机制更加丰富。应用场景更加丰富。 JWT不足之处：JWT并不是完美的。 比如说有可能一个用户同时出现两个可用的token情况。 还有如果失效过期了如何进行续期的问题。 同样会出现token被盗用的问题。 注销如何让token失效的问题。 用户信息修改让token同步的问题。 一些问题什么时候你应该用JSON Web Token？下列场景中使用JSON Web Token是很有用的： Authorization (授权) : 这是使用JWT的最常见场景。一旦用户登录，后续每个请求都将包含JWT，允许用户访问该令牌允许的路由、服务和资源。单点登录是现在广泛使用的JWT的一个特性，因为它的开销很小，并且可以轻松地跨域使用。 Information Exchange (信息交换) : 对于安全的在各方之间传输信息而言，JSON Web Tokens无疑是一种很好的方式。因为JWT可以被签名，例如，用公钥/私钥对，你可以确定发送人就是它们所说的那个人。另外，由于签名是使用头和有效负载计算的，您还可以验证内容没有被篡改。 JSON Web Tokens是如何工作的？在认证的时候，当用户用他们的凭证成功登录以后，一个JSON Web Token将会被返回。此后，token就是用户凭证了，你必须非常小心以防止出现安全问题。一般而言，你保存令牌的时候不应该超过你所需要它的时间。 无论何时用户想要访问受保护的路由或者资源的时候，用户代理（通常是浏览器）都应该带上JWT，典型的，通常放在Authorization header中，用Bearer schema。 header应该看起来是这样的： Authorization: Bearer 服务器上的受保护的路由将会检查Authorization header中的JWT是否有效，如果有效，则用户可以访问受保护的资源。如果JWT包含足够多的必需的数据，那么就可以减少对某些操作的数据库查询的需要，尽管可能并不总是如此。 如果token是在授权头（Authorization header）中发送的，那么跨源资源共享(CORS)将不会成为问题，因为它不使用cookie。 基于Token的身份认证 与 基于服务器的身份认证1.基于服务器的身份认证在讨论基于Token的身份认证是如何工作的以及它的好处之前，我们先来看一下以前我们是怎么做的： HTTP协议是无状态的，也就是说，如果我们已经认证了一个用户，那么他下一次请求的时候，服务器不知道我是谁，我们必须再次认证 传统的做法是将已经认证过的用户信息存储在服务器上，比如Session。用户下次请求的时候带着Session ID，然后服务器以此检查用户是否认证过。 这种基于服务器的身份认证方式存在一些问题： Sessions : 每次用户认证通过以后，服务器需要创建一条记录保存用户信息，通常是在内存中，随着认证通过的用户越来越多，服务器的在这里的开销就会越来越大。 Scalability : 由于Session是在内存中的，这就带来一些扩展性的问题。 CORS : 当我们想要扩展我们的应用，让我们的数据被多个移动设备使用时，我们必须考虑跨资源共享问题。当使用AJAX调用从另一个域名下获取资源时，我们可能会遇到禁止请求的问题。 CSRF : 用户很容易受到CSRF攻击。 2.JWT与Session的差异相同点是:它们都是存储用户信息；然而，Session是在服务器端的，而JWT是在客户端的。 Session方式存储用户信息的最大问题在于要占用大量服务器内存，增加服务器的开销。 而JWT方式将用户状态分散到了客户端中，可以明显减轻服务端的内存压力。 Session的状态是存储在服务器端，客户端只有session id；而Token的状态是存储在客户端。 3.基于Token的身份认证是如何工作的基于Token的身份认证是无状态的，服务器或者Session中不会存储任何用户信息。 没有会话信息意味着应用程序可以根据需要扩展和添加更多的机器，而不必担心用户登录的位置。 虽然这一实现可能会有所不同，但其主要流程如下： 用户携带用户名和密码请求访问 -服务器校验用户凭据 -应用提供一个token给客户端 -客户端存储token，并且在随后的每一次请求中都带着它 -服务器校验token并返回数据 注意： 每一次请求都需要token -Token应该放在请求header中 -我们还需要将服务器设置为接受来自所有域的请求，用Access-Control-Allow-Origin: * 4.用Token的好处 - 无状态和可扩展性Tokens存储在客户端。完全无状态，可扩展。我们的负载均衡器可以将用户传递到任意服务器，因为在任何地方都没有状态或会话信息。 - 安全：Token不是Cookie。（The token, not a cookie.）每次请求的时候Token都会被发送。而且，由于没有Cookie被发送，还有助于防止CSRF攻击。即使在你的实现中将token存储到客户端的Cookie中，这个Cookie也只是一种存储机制，而非身份认证机制。没有基于会话的信息可以操作，因为我们没有会话! 还有一点，token在一段时间以后会过期，这个时候用户需要重新登录。这有助于我们保持安全。还有一个概念叫token撤销，它允许我们根据相同的授权许可使特定的token甚至一组token无效。 5.JWT与OAuth的区别?OAuth2是一种授权框架,JWT是一种认证协议,无论使用哪种方式切记用HTTPS来保证数据的安全性,OAuth2用在使用第三方账号登录的情况(比如使用weibo, qq, github登录某个app)，而JWT是用在前后端分离, 需要简单的对后台API进行保护时使用。 参考一文了解web无状态会话token技术JWT五分钟带你了解啥是JWT","categories":[{"name":"个人项目","slug":"个人项目","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"token","slug":"token","permalink":"https://xmmarlowe.github.io/tags/token/"},{"name":"Jwt","slug":"Jwt","permalink":"https://xmmarlowe.github.io/tags/Jwt/"}],"author":"Marlowe"},{"title":"有状态（SESSION）和无状态（JWT）登录验证","slug":"个人项目/有状态（SESSION）和无状态（JWT）登录验证","date":"2021-08-24T14:33:41.000Z","updated":"2021-08-25T15:37:34.062Z","comments":true,"path":"2021/08/24/个人项目/有状态（SESSION）和无状态（JWT）登录验证/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/24/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/%E6%9C%89%E7%8A%B6%E6%80%81%EF%BC%88SESSION%EF%BC%89%E5%92%8C%E6%97%A0%E7%8A%B6%E6%80%81%EF%BC%88JWT%EF%BC%89%E7%99%BB%E5%BD%95%E9%AA%8C%E8%AF%81/","excerpt":"简单对比Session和Jwt的优缺点","text":"简单对比Session和Jwt的优缺点 有状态（SESSION）所谓有状态，就是的就是传统的 cookie session ，cookie的身份验证是有状态的。这意味着验证的记录或者会话(session)必须同时保存在服务器端和客户端。服务器端需要跟踪记录session并存至数据库，同时前端需要在cookie中保存一个sessionID，作为session的唯一标识符，可看做是session的“身份证”。前端退出的话就清cookie。后端强制前端重新认证的话就清或者修改session。 优点 相较于无状态的验证机制，传统的session可以直接从后端主动控制下线，删除session，方便实现互t等功能 session保存在服务端，相对比较安全 有状态的session可以较为准确统计在线人数 缺点 有状态的存储session需要服务器空间 扩展不方便，需要session同步，借助redis实现共享等 无状态（JWT）优点 json通用性，方便跨语言 不需要在服务端保存会话信息, 易于应用的扩展 占用字节很小，方便传输 缺点 JWT的加解密耗费CPU计算资源 不能方便的管理会话 注销没有即时性，如果token泄露，注销状态下仍可登录操作","categories":[{"name":"个人项目","slug":"个人项目","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"Jwt","slug":"Jwt","permalink":"https://xmmarlowe.github.io/tags/Jwt/"},{"name":"Session","slug":"Session","permalink":"https://xmmarlowe.github.io/tags/Session/"}],"author":"Marlowe"},{"title":"Springboot整合Shiro：实现Redis缓存","slug":"个人项目/Springboot整合Shiro：实现Redis缓存","date":"2021-08-24T13:59:15.000Z","updated":"2021-08-25T15:37:34.058Z","comments":true,"path":"2021/08/24/个人项目/Springboot整合Shiro：实现Redis缓存/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/24/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/Springboot%E6%95%B4%E5%90%88Shiro%EF%BC%9A%E5%AE%9E%E7%8E%B0Redis%E7%BC%93%E5%AD%98/","excerpt":"项目整合Shiro后，在没有配置缓存的时候，会存在这样的问题。每发起一个请求，就会调用一次授权方法。用户基数大请求多的时候，会对数据库造成很大的压力。所以我们需要配置缓存，将用户信息放在缓存里，从而减小数据库压力。","text":"项目整合Shiro后，在没有配置缓存的时候，会存在这样的问题。每发起一个请求，就会调用一次授权方法。用户基数大请求多的时候，会对数据库造成很大的压力。所以我们需要配置缓存，将用户信息放在缓存里，从而减小数据库压力。 自定义Realm中两个核心方法认证：doGetAuthenticationInfo123456789101112131415161718192021222324252627282930313233343536373839@Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken auth) throws AuthenticationException &#123; // 处理的是 JWTToken, .getPrincipal() 和 .getCredentials() 都是返回token字符串 String token = (String) auth.getPrincipal(); String username; try &#123; // 从令牌中获取username值 这里可能发生解码异常 username = this.jwtUtils.getUsername(token); // token不包含username信息 || token值中的密钥 是胡乱编造 if (username == null || !this.jwtUtils.verify(token, username, this.jwtUtils.getSecret())) &#123; // 因为token过期了 verify会直接判断false, 所以不能在之后判断是否过期 isExpire也有可能发生解码异常 if (this.jwtUtils.isExpire(token)) &#123; throw new ExpiredCredentialsException(&quot;token过期，请重新登入！&quot;); &#125; // 校验未通过 throw new IncorrectCredentialsException(&quot;token值异常(2)!!!&quot;); &#125; &#125; catch (JWTDecodeException | IllegalArgumentException e) &#123; // token的3部分缺失 / 根本解不了码 e.printStackTrace(); throw new IncorrectCredentialsException(&quot;token值异常(1)!!!!&quot;); &#125; catch (AuthenticationException e) &#123; // 过期/值异常 等 e.printStackTrace(); throw new IncorrectCredentialsException(e.getMessage()); &#125; // 数据库查询用户并返回 User user = this.userService.findUserByUsername(username); if (user == null) &#123; throw new UnknownAccountException(&quot;账号不存在!&quot;); &#125; // 通过认证 直接将user传递给授权过程 反正都通过了认证了 就不需要再在授权过程再走一遍 token-&gt;username-&gt;user 的过程了 return new SimpleAuthenticationInfo(user, token, this.getName()); &#125; 授权：doGetAuthorizationInfo123456789101112131415161718192021222324252627282930@Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) &#123; //获取身份信息 User user = (User) principals.getPrimaryPrincipal(); System.out.println(&quot;调用授权验证：&quot; + user.getUsername()); // 根据主身份信息获取角色和权限信息 User realUser = userService.findRolesByUserName(user.getUsername()); // 授权角色信息 if (!CollectionUtil.isEmpty(realUser.getRoles())) &#123; //权限信息对象info,用来存放查出的用户的所有的角色（Role）及权限（permission） SimpleAuthorizationInfo simpleAuthorizationInfo = new SimpleAuthorizationInfo(); System.out.println(realUser.getRoles()+&quot;-------------------------&quot;); realUser.getRoles().forEach(role -&gt; &#123; // 添加角色信息 simpleAuthorizationInfo.addRole(role.getName()); System.out.println(role.getName()+&quot;================================&quot;); //添加权限信息 List&lt;Permission&gt; permissions = roleService.findPermissionsByRoleId(role.getId()); if (!CollectionUtil.isEmpty(permissions)) &#123; permissions.forEach(permission -&gt; &#123; simpleAuthorizationInfo.addStringPermission(permission.getName()); System.out.println(permission.getName()+&quot;==============AAAAAAAAAA==================&quot;); &#125;); &#125; &#125;); return simpleAuthorizationInfo; &#125; return null; &#125; 自定义缓存管理器我们这里用redis做缓存，下面说下配置redis缓存的方法。 （1）application.yml中配置redis的相关参数 123456789101112###redisspring: redis: host: localhost port: 6379 jedis: pool: max-idle: 8 min-idle: 0 max-active: 8 max-wait: -1 timeout: 0 （2）pom.xml文件中引入shiro-redis依赖 123456&lt;!-- shiro+redis缓存插件 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.crazycake&lt;/groupId&gt; &lt;artifactId&gt;shiro-redis&lt;/artifactId&gt; &lt;version&gt;2.4.2.1-RELEASE&lt;/version&gt;&lt;/dependency&gt; （3）ShiroConfig.java中添加相应的配置 123456789101112131415161718192021222324252627282930313233343536373839404142/** * redisManager * * @return */ public RedisManager redisManager() &#123; RedisManager redisManager = new RedisManager(); redisManager.setHost(host); redisManager.setPort(port); // 配置过期时间 redisManager.setExpire(1800); return redisManager; &#125; /** * 设置cacheManager为redisManager * * @return */ public RedisCacheManager cacheManager() &#123; RedisCacheManager redisCacheManager = new RedisCacheManager(); redisCacheManager.setRedisManager(redisManager()); return redisCacheManager; &#125; /** * redisSessionDAO */ public RedisSessionDAO redisSessionDAO() &#123; RedisSessionDAO redisSessionDAO = new RedisSessionDAO(); redisSessionDAO.setRedisManager(redisManager()); return redisSessionDAO; &#125; /** * sessionManager */ public DefaultWebSessionManager SessionManager() &#123; DefaultWebSessionManager sessionManager = new DefaultWebSessionManager(); sessionManager.setSessionDAO(redisSessionDAO()); return sessionManager; &#125; （4）将session管理器和cache管理器注入到SecurityManager中 1234567891011@Bean public SecurityManager securityManager()&#123; DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(); //将自定义的realm交给SecurityManager管理 securityManager.setRealm(new CustomRealm()); // 自定义缓存实现 使用redis securityManager.setCacheManager(cacheManager()); // 自定义session管理 使用redis securityManager.setSessionManager(SessionManager()); return securityManager; &#125; （5）redis-server.exe启动redis，启动项目，完成。未登录时，在redis中查看数据，得到空的结果。（empty list or set）完成认证和授权后可以在redis中得到相应的信息。 参考Springboot整合Shiro：实现Redis缓存","categories":[{"name":"个人项目","slug":"个人项目","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://xmmarlowe.github.io/tags/SpringBoot/"},{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"},{"name":"Shiro","slug":"Shiro","permalink":"https://xmmarlowe.github.io/tags/Shiro/"}],"author":"Marlowe"},{"title":"初识RPC及其基本原理","slug":"分布式/初识RPC及其基本原理","date":"2021-08-23T14:31:43.000Z","updated":"2021-08-25T15:37:34.065Z","comments":true,"path":"2021/08/23/分布式/初识RPC及其基本原理/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/23/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%9D%E8%AF%86RPC%E5%8F%8A%E5%85%B6%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/","excerpt":"","text":"什么是 RPC?RPC原理是什么?什么是 RPC？RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。比如两个不同的服务 A、B 部署在两台不同的机器上，那么服务 A 如果想要调用服务 B 中的某个方法该怎么办呢？使用 HTTP请求 当然可以，但是可能会比较慢而且一些优化做的并不好。 RPC 的出现就是为了解决这个问题。 RPC原理是什么？ 服务消费端（client）以本地调用的方式调用远程服务； 客户端 Stub（client stub） 接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体（序列化）：RpcRequest； 客户端 Stub（client stub） 找到远程服务的地址，并将消息发送到服务提供端； 服务端 Stub（桩）收到消息将消息反序列化为Java对象: RpcRequest； 服务端 Stub（桩）根据RpcRequest中的类、方法、方法参数等信息调用本地的方法； 服务端 Stub（桩）得到方法执行结果并将组装成能够进行网络传输的消息体：RpcResponse（序列化）发送至消费方； 客户端 Stub（client stub）接收到消息并将消息反序列化为Java对象:RpcResponse ，这样也就得到了最终结果。 下面再贴一个网上的时序图，辅助理解： RPC 解决了什么问题？从上面对 RPC 介绍的内容中，概括来讲RPC 主要解决了：让分布式或者微服务系统中不同服务之间的调用像本地调用一样简单。 常见的 RPC 框架总结 RMI（JDK自带）： JDK自带的RPC，有很多局限性，不推荐使用。 Dubbo: Dubbo是 阿里巴巴公司开源的一个高性能优秀的服务框架，使得应用可通过高性能的 RPC 实现服务的输出和输入功能，可以和 Spring框架无缝集成。目前 Dubbo 已经成为 Spring Cloud Alibaba 中的官方组件。 gRPC ：gRPC是可以在任何环境中运行的现代开源高性能RPC框架。它可以通过可插拔的支持来有效地连接数据中心内和跨数据中心的服务，以实现负载平衡，跟踪，运行状况检查和身份验证。它也适用于分布式计算的最后一英里，以将设备，移动应用程序和浏览器连接到后端服务。 Hessian： Hessian是一个轻量级的remotingonhttp工具，使用简单的方法提供了RMI的功能。 相比WebService，Hessian更简单、快捷。采用的是二进制RPC协议，因为采用的是二进制协议，所以它很适合于发送二进制数据。 Thrift： Apache Thrift是Facebook开源的跨语言的RPC通信框架，目前已经捐献给Apache基金会管理，由于其跨语言特性和出色的性能，在很多互联网公司得到应用，有能力的公司甚至会基于thrift研发一套分布式服务框架，增加诸如服务注册、服务发现等功能。 RPC学习材料跟着 Guide 哥造轮子 既有 HTTP ,为啥用 RPC 进行服务调用?RPC 只是一种设计而已RPC 只是一种概念、一种设计，就是为了解决 不同服务之间的调用问题, 它一般会包含有 传输协议 和 序列化协议 这两个。 但是，HTTP 是一种协议，RPC框架可以使用 HTTP协议作为传输协议或者直接使用TCP作为传输协议，使用不同的协议一般也是为了适应不同的场景。 HTTP 和 TCP可能现在很多对计算机网络不太熟悉的朋友已经被搞蒙了，要想真正搞懂，还需要来简单复习一下计算机网络基础知识： 我们通常谈计算机网络的五层协议的体系结构是指：应用层、传输层、网络层、数据链路层、物理层。 应用层(application-layer）的任务是通过应用进程间的交互来完成特定网络应用。 HTTP 属于应用层协议，它会基于TCP/IP通信协议来传递数据（HTML 文件, 图片文件, 查询结果等）。HTTP协议工作于客户端-服务端架构为上。浏览器作为HTTP客户端通过 URL 向HTTP服务端即WEB服务器发送所有请求。Web服务器根据接收到的请求后，向客户端发送响应信息。HTTP协议建立在 TCP 协议之上。 运输层(transport layer)的主要任务就是负责向两台主机进程之间的通信提供通用的数据传输服务。 TCP是传输层协议，主要解决数据如何在网络中传输。相比于UDP,TCP 提供的是面向连接的，可靠的数据传输服务。 RPC框架功能更齐全成熟的 RPC框架还提供好了“服务自动注册与发现”、”智能负载均衡”、“可视化的服务治理和运维”、“运行期流量调度”等等功能，这些也算是选择 RPC 进行服务注册和发现的一方面原因吧！ 相关阅读： HTTP 协议入门- 阮一峰 一个常见的错误观点很多文章中还会提到说 HTTP 协议相较于自定义 TCP 报文协议，增加的开销在于连接的建立与断开，但是这个观点已经被否认，下面截取自知乎中一个回答，原回答地址：https://www.zhihu.com/question/41609070/answer/191965937 首先要否认一点 HTTP 协议相较于自定义 TCP 报文协议，增加的开销在于连接的建立与断开。HTTP 协议是支持连接池复用的，也就是建立一定数量的连接不断开，并不会频繁的创建和销毁连接。二一要说的是 HTTP 也可以使用 Protobuf 这种二进制编码协议对内容进行编码，因此二者最大的区别还是在传输协议上。 参考什么是 RPC?RPC原理是什么?","categories":[{"name":"分布式","slug":"分布式","permalink":"https://xmmarlowe.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"RPC","slug":"RPC","permalink":"https://xmmarlowe.github.io/tags/RPC/"}],"author":"Marlowe"},{"title":"","slug":"中间件/为什么使用消息队列？","date":"2021-08-23T14:17:19.876Z","updated":"2021-08-25T15:38:57.502Z","comments":true,"path":"2021/08/23/中间件/为什么使用消息队列？/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/23/%E4%B8%AD%E9%97%B4%E4%BB%B6/%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%EF%BC%9F/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Redis分布式锁（二）","slug":"NoSQL/Redis分布式锁（二）","date":"2021-08-22T06:45:29.000Z","updated":"2021-08-25T15:37:34.015Z","comments":true,"path":"2021/08/22/NoSQL/Redis分布式锁（二）/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/22/NoSQL/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%EF%BC%88%E4%BA%8C%EF%BC%89/","excerpt":"","text":"Redis分布式锁01JVM层面的加锁，单机版的锁 synchronized ReentraLock 123456789101112131415161718192021222324252627class X &#123; private final ReentrantLock lock = new ReentrantLock(); // ... public void m() &#123; lock.lock(); // block until condition holds//不见不散 try &#123; // ... method body &#125; finally &#123; lock.unlock() &#125; &#125; public void m2() &#123; if(lock.tryLock(timeout, unit))&#123;//过时不候 try &#123; // ... method body &#125; finally &#123; lock.unlock() &#125; &#125;else&#123; // perform alternative actions &#125; &#125; &#125; Redis分布式锁02分布式部署后，单机锁还是出现超卖现象，需要分布式锁 Redis具有极高的性能，且其命令对分布式锁支持友好，借助SET命令即可实现加锁处理。 SET EX seconds – Set the specified expire time, in seconds. PX milliseconds – Set the specified expire time, in milliseconds. NX – Only set the key if it does not already exist. XX – Only set the key if it already exist. 在Java层面 12345678910111213141516171819public static final String REDIS_LOCK = &quot;redis_lock&quot;;@Autowiredprivate StringRedisTemplate stringRedisTemplate;public void m()&#123; String value = UUID.randomUUID().toString() + Thread.currentThread().getName(); Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(REDIS_LOCK, value); if(!flag) &#123; return &quot;抢锁失败&quot;; &#125; ...//业务逻辑 stringRedisTemplate.delete(REDIS_LOCK);&#125; Redis分布式锁03上面Java源码分布式锁问题： 出现异常的话，可能无法释放锁，必须要在代码层面finally释放锁。 解决方法：try…finally… 123456789101112131415161718192021public static final String REDIS_LOCK = &quot;redis_lock&quot;;@Autowiredprivate StringRedisTemplate stringRedisTemplate;public void m()&#123; String value = UUID.randomUUID().toString() + Thread.currentThread().getName(); try&#123; Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(REDIS_LOCK, value); if(!flag) &#123; return &quot;抢锁失败&quot;; &#125; ...//业务逻辑 &#125;finally&#123; stringRedisTemplate.delete(REDIS_LOCK); &#125;&#125; 另一个问题： 部署了微服务jar包的机器挂了，代码层面根本没有走到finally这块，没办法保证解锁，这个key没有被删除，需要加入一个过期时间限定key。 1234567891011121314151617181920212223public static final String REDIS_LOCK = &quot;redis_lock&quot;;@Autowiredprivate StringRedisTemplate stringRedisTemplate;public void m()&#123; String value = UUID.randomUUID().toString() + Thread.currentThread().getName(); try&#123; Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(REDIS_LOCK, value); //设定时间 stringRedisTemplate.expire(REDIS_LOCK, 10L, TimeUnit.SECONDS); if(!flag) &#123; return &quot;抢锁失败&quot;; &#125; ...//业务逻辑 &#125;finally&#123; stringRedisTemplate.delete(REDIS_LOCK); &#125;&#125; Redis分布式锁04新问题：设置key+过期时间分开了，必须要合并成一行具备原子性。 解决方法： 123456789101112131415161718192021222324public static final String REDIS_LOCK = &quot;redis_lock&quot;;@Autowiredprivate StringRedisTemplate stringRedisTemplate;public void m()&#123; String value = UUID.randomUUID().toString() + Thread.currentThread().getName(); try&#123; Boolean flag = stringRedisTemplate.opsForValue()//使用另一个带有设置超时操作的方法 .setIfAbsent(REDIS_LOCK, value, 10L, TimeUnit.SECONDS); //设定时间 //stringRedisTemplate.expire(REDIS_LOCK, 10L, TimeUnit.SECONDS); if(!flag) &#123; return &quot;抢锁失败&quot;; &#125; ...//业务逻辑 &#125;finally&#123; stringRedisTemplate.delete(REDIS_LOCK); &#125;&#125; 另一个新问题： 张冠李戴，删除了别人的锁 解决方法：只能自己删除自己的，不许动别人的。 1234567891011121314151617181920212223242526public static final String REDIS_LOCK = &quot;redis_lock&quot;;@Autowiredprivate StringRedisTemplate stringRedisTemplate;public void m()&#123; String value = UUID.randomUUID().toString() + Thread.currentThread().getName(); try&#123; Boolean flag = stringRedisTemplate.opsForValue()//使用另一个带有设置超时操作的方法 .setIfAbsent(REDIS_LOCK, value, 10L, TimeUnit.SECONDS); //设定时间 //stringRedisTemplate.expire(REDIS_LOCK, 10L, TimeUnit.SECONDS); if(!flag) &#123; return &quot;抢锁失败&quot;; &#125; ...//业务逻辑 &#125;finally&#123; if(stringRedisTemplate.opsForValue().get(REDIS_LOCK).equals(value)) &#123; stringRedisTemplate.delete(REDIS_LOCK); &#125; &#125;&#125; Redis分布式锁05问题： finally块的判断 + del删除操作不是原子性的 用lua脚本 用redis自身的事务 事务介绍 Redis的事条是通过MULTI，EXEC，DISCARD和WATCH这四个命令来完成。 Redis的单个命令都是原子性的，所以这里确保事务性的对象是命令集合。 Redis将命令集合序列化并确保处于一事务的命令集合连续且不被打断的执行。 Redis不支持回滚的操作。 Redis分布式锁06继续上一章节，解决之道 12345678910111213141516171819202122232425262728293031323334353637public static final String REDIS_LOCK = &quot;redis_lock&quot;;@Autowiredprivate StringRedisTemplate stringRedisTemplate;public void m()&#123; String value = UUID.randomUUID().toString() + Thread.currentThread().getName(); try&#123; Boolean flag = stringRedisTemplate.opsForValue()//使用另一个带有设置超时操作的方法 .setIfAbsent(REDIS_LOCK, value, 10L, TimeUnit.SECONDS); //设定时间 //stringRedisTemplate.expire(REDIS_LOCK, 10L, TimeUnit.SECONDS); if(!flag) &#123; return &quot;抢锁失败&quot;; &#125; ...//业务逻辑 &#125;finally&#123; while(true)&#123; stringRedisTemplate.watch(REDIS_LOCK); if(stringRedisTemplate.opsForValue().get(REDIS_LOCK).equalsIgnoreCase(value))&#123; stringRedisTemplate.setEnableTransactionSupport(true); stringRedisTemplate.multi(); stringRedisTemplate.delete(REDIS_LOCK); List&lt;Object&gt; list = stringRedisTemplate.exec(); if (list == null) &#123; continue; &#125; &#125; stringRedisTemplate.unwatch(); break; &#125; &#125;&#125; Redis分布式锁07Redis调用Lua脚本通过eval命令保证代码执行的原子性 RedisUtils： 123456789101112131415161718192021import redis.clients.jedis.JedisPool;import redis.clients.jedis.JedisPoolConfig;public class RedisUtils &#123; private static JedisPool jedisPool; static &#123; JedisPoolConfig jpc = new JedisPoolConfig(); jpc.setMaxTotal(20); jpc.setMaxIdle(10); jedisPool = new JedisPool(jpc); &#125; public static JedisPool getJedis() throws Exception&#123; if(jedisPool == null) throw new NullPointerException(&quot;JedisPool is not OK.&quot;); return jedisPool; &#125; &#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public static final String REDIS_LOCK = &quot;redis_lock&quot;;@Autowiredprivate StringRedisTemplate stringRedisTemplate;public void m()&#123; String value = UUID.randomUUID().toString() + Thread.currentThread().getName(); try&#123; Boolean flag = stringRedisTemplate.opsForValue()//使用另一个带有设置超时操作的方法 .setIfAbsent(REDIS_LOCK, value, 10L, TimeUnit.SECONDS); //设定时间 //stringRedisTemplate.expire(REDIS_LOCK, 10L, TimeUnit.SECONDS); if(!flag) &#123; return &quot;抢锁失败&quot;; &#125; ...//业务逻辑 &#125;finally&#123; Jedis jedis = RedisUtils.getJedis(); String script = &quot;if redis.call(&#x27;get&#x27;, KEYS[1]) == ARGV[1] &quot; + &quot;then &quot; + &quot; return redis.call(&#x27;del&#x27;, KEYS[1]) &quot; + &quot;else &quot; + &quot; return 0 &quot; + &quot;end&quot;; try &#123; Object o = jedis.eval(script, Collections.singletonList(REDIS_LOCK),// Collections.singletonList(value)); if(&quot;1&quot;.equals(o.toString())) &#123; System.out.println(&quot;---del redis lock ok.&quot;); &#125;else &#123; System.out.println(&quot;---del redis lock error.&quot;); &#125; &#125;finally &#123; if(jedis != null) jedis.close(); &#125; &#125;&#125; Redis分布式锁08确保RedisLock过期时间大于业务执行时间的问题 Redis分布式锁如何续期？ 集群 + CAP对比ZooKeeper 对比ZooKeeper，重点，CAP Redis - AP -redis异步复制造成的锁丢失，比如：主节点没来的及把刚刚set进来这条数据给从节点，就挂了。 ZooKeeper - CP CAP C：Consistency（强一致性） A：Availability（可用性） P：Partition tolerance（分区容错性） 综上所述 Redis集群环境下，我们自己写的也不OK，直接上RedLock之Redisson落地实现。 Redis分布式锁09Redisson官方网站 Redisson配置类: 1234567891011121314151617import org.redisson.Redisson;import org.redisson.config.Config;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class RedisConfig &#123; @Bean public Redisson redisson() &#123; Config config = new Config(); config.useSingleServer().setAddress(&quot;redis://127.0.0.1:6379&quot;).setDatabase(0); return (Redisson)Redisson.create(config); &#125; &#125; Redisson模板 12345678910111213141516public static final String REDIS_LOCK = &quot;REDIS_LOCK&quot;;@Autowiredprivate Redisson redisson;@GetMapping(&quot;/doSomething&quot;)public String doSomething()&#123; RLock redissonLock = redisson.getLock(REDIS_LOCK); redissonLock.lock(); try &#123; //doSomething &#125;finally &#123; redissonLock.unlock(); &#125;&#125; 回到实例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import org.redisson.Redisson;import org.redisson.api.RLock;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Value;import org.springframework.data.redis.core.StringRedisTemplate;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;@RestControllerpublic class GoodController&#123; public static final String REDIS_LOCK = &quot;REDIS_LOCK&quot;; @Autowired private StringRedisTemplate stringRedisTemplate; @Value(&quot;$&#123;server.port&#125;&quot;) private String serverPort; @Autowired private Redisson redisson; @GetMapping(&quot;/buy_goods&quot;) public String buy_Goods()&#123; //String value = UUID.randomUUID().toString() + Thread.currentThread().getName(); RLock redissonLock = redisson.getLock(REDIS_LOCK); redissonLock.lock(); try &#123; String result = stringRedisTemplate.opsForValue().get(&quot;goods:001&quot;);// get key ====看看库存的数量够不够 int goodsNumber = result == null ? 0 : Integer.parseInt(result); if(goodsNumber &gt; 0)&#123; int realNumber = goodsNumber - 1; stringRedisTemplate.opsForValue().set(&quot;goods:001&quot;, String.valueOf(realNumber)); System.out.println(&quot;成功买到商品，库存还剩下: &quot;+ realNumber + &quot; 件&quot; + &quot;\\t服务提供端口&quot; + serverPort); return &quot;成功买到商品，库存还剩下:&quot; + realNumber + &quot; 件&quot; + &quot;\\t服务提供端口&quot; + serverPort; &#125;else&#123; System.out.println(&quot;商品已经售完/活动结束/调用超时,欢迎下次光临&quot; + &quot;\\t服务提供端口&quot; + serverPort); &#125; return &quot;商品已经售完/活动结束/调用超时,欢迎下次光临&quot; + &quot;\\t服务提供端口&quot; + serverPort; &#125;finally &#123; redissonLock.unlock(); &#125; &#125; &#125; Redis分布式锁10让代码更加严谨 12345678910111213141516171819public static final String REDIS_LOCK = &quot;REDIS_LOCK&quot;;@Autowiredprivate Redisson redisson;@GetMapping(&quot;/doSomething&quot;)public String doSomething()&#123; RLock redissonLock = redisson.getLock(REDIS_LOCK); redissonLock.lock(); try &#123; //doSomething &#125;finally &#123; //添加后，更保险 if(redissonLock.isLocked() &amp;&amp; redissonLock.isHeldByCurrentThread()) &#123; redissonLock.unlock(); &#125; &#125;&#125; 可避免如下异常： 1IllegalMonitorStateException: attempt to unlock lock，not loked by current thread by node id:da6385f-81a5-4e6c-b8c0 Redis分布式锁总结回顾synchronized单机版oK，上分布式 nginx分布式微服务单机锁不行 取消单机锁，上Redis分布式锁setnx 只加了锁，没有释放锁，出异常的话，可能无法释放锁,必须要在代码层面finally释放锁 宕机了，部署了微服务代码层面根本没有走到finally这块，没办法保证解锁，这个key没有被删除，需要有lockKey的过期时间设定 为redis的分布式锁key，增加过期时间，此外，还必须要setnx+过期时间必须同一行 必须规定只能自己删除自己的锁,你不能把别人的锁删除了，防止张冠李戴，1删2，2删3 Redis集群环境下，我们自己写的也不oK直接上RedLock之Redisson落地实现 参考Java开发常见面试题详解（LockSupport，AQS，Spring循环依赖，Redis）","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"}],"author":"Marlowe"},{"title":"手写LFU算法","slug":"算法与数据结构/手写LFU算法","date":"2021-08-22T02:13:11.000Z","updated":"2021-08-25T15:37:34.087Z","comments":true,"path":"2021/08/22/算法与数据结构/手写LFU算法/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/22/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%89%8B%E5%86%99LFU%E7%AE%97%E6%B3%95/","excerpt":"","text":"","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"https://xmmarlowe.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"LFU","slug":"LFU","permalink":"https://xmmarlowe.github.io/tags/LFU/"}],"author":"Marlowe"},{"title":"Spring通过set注入解决循环依赖","slug":"Spring/Spring通过set注入解决循环依赖","date":"2021-08-21T09:24:04.000Z","updated":"2021-08-21T09:33:05.548Z","comments":true,"path":"2021/08/21/Spring/Spring通过set注入解决循环依赖/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/21/Spring/Spring%E9%80%9A%E8%BF%87set%E6%B3%A8%E5%85%A5%E8%A7%A3%E5%86%B3%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96/","excerpt":"循环依赖是指两个bean相互依赖,如下面的A和B: A依赖于B,B又依赖于A.如果未加处理这会导致无限递归程序崩溃,然而在实例项目中这种情况循环依赖的情况并不少见.为此Spring做了一些努力,解决了setter注入方式的循环依赖,对于构造器注入方式的循环只能检测并提前崩溃.","text":"循环依赖是指两个bean相互依赖,如下面的A和B: A依赖于B,B又依赖于A.如果未加处理这会导致无限递归程序崩溃,然而在实例项目中这种情况循环依赖的情况并不少见.为此Spring做了一些努力,解决了setter注入方式的循环依赖,对于构造器注入方式的循环只能检测并提前崩溃. 前言12345678910111213141516171819202122232425262728293031//setter注入方式的循环依赖@Componentpublic class A &#123; private B b; @Autowired public void setB(B b)&#123; this.b=b; &#125;&#125;@Componentpublic class B &#123; private A a; @Autowired public void setA(A a)&#123; this.a=a; &#125;&#125;//另外一种setter注入public class A &#123; @Autowired private B b;&#125;@Componentpublic class B &#123; @Autowired private A a;&#125; 123456789101112131415161718192021222324//构造器注入方式的循环依赖@Componentpublic class A &#123; private B b; public A(B b)&#123; this.b=b; &#125;&#125;@Componentpublic class B &#123; private A a; public B(A a)&#123; this.a=a; &#125;&#125;//报错┌─────┐| cycleReferenceDemo.A defined in file [/Users/alonwang/IdeaProjects/spring-lifecycle-example/target/classes/com/github/alonwang/springlifecycle/CycleReferenceDemo$A.class]↑ ↓| cycleReferenceDemo.B defined in file [/Users/alonwang/IdeaProjects/spring-lifecycle-example/target/classes/com/github/alonwang/springlifecycle/CycleReferenceDemo$B.class]└─────┘ 本文将简述Spring是如何支持setter注入方式的循环依赖的,并解释为何对构造器注入方式的循环依赖无能为力. 两种方式下bean的创建流程 在setter注入方式下的详细流程为： 实例化: 调用bean的无参构建函数生成实例. 注入依赖属性: 从容器中获取该bean依赖属性的实例(如果没有,进入依赖属性对应bean的创建流程),进行注入. 初始化: 如果bean实现了InitializingBean或@PostConstruct形式的初始化方法,进行调用 在构造器注入方式下的详细流程为 获取依赖属性&amp;实例化: 在容器中找到有参构造器中声明的参数的实例((如果没有,进入依赖属性对应bean的创建流程)),并用这些这些参数调用这个构造函数生成实例 初始化: 同setter注入 构造器注入无法解决循环依赖的原因构造器注入必须先获取依赖属性才能完成实例化,这是其无法解决循环依赖的根本原因.用上面的例子说明: 开始创建A 获取A的依赖属性b对应的实例B,发现还没有,开始创建B 获取B的依赖属性a对应的实例A,发现它正在创建中(无法获取到A的实例),Spring检测到这一点立刻报错,提示发生无法解决的循环依赖. setter注入解决循环依赖的方式setter注入下实例化和依赖属性注入是分开的,这是其可以解决循环依赖的根本原因,还用上面的例子说明 开始创建A 调用A的无参构造函数实例化A,把A存放在某个地方X,标识它是一个尚不完备但是可获取的bean 开始注入A的属性,获取A的依赖属性发现b对应的实例B还没创建,开始创建B 与2类似,调用B的无参构造函数实例化B,把B存放在某个地方X,标识它是一个尚不完备但是可获取的bean 开始注入B的属性,获取B的依赖属性发现b对应的实例B还没在容器但是在X已经有了,就从X中获取到a,B的注入完成 完成B的初始化,放到容器中. 返回B,给到步骤3,A的注入完成 完成A的初始化,放到容器中. 流程如下图: setter方式解决循环依赖的核心就是提前将仅完成实例化的bean暴露出来,提供给其他bean,这个暴露的地方就是图中的地方X, 这个地方X,在Spring代码中,对应的是DefaultSingletonBeanRegistry的两个属性 1234/** Cache of singleton factories: bean name to ObjectFactory. */private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories /** Cache of early singleton objects: bean name to bean instance. */private final Map&lt;String, Object&gt; earlySingletonObjects singletonFactories存储的是生成bean的工厂,工厂签名如下及添加逻辑如下 123456public interface ObjectFactory&lt;T&gt; &#123; T getObject() throws BeansException;&#125;//实例化之后添加到singletonFactory//getEarlyBeanReference会对bean做修改,例如代理或mock,因此返回的对象和传入的bean可能是不同的addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean)); earlySingletonObjects存储的则是从这个工厂生成的bean. 12345678910111213141516171819//DefaultSingletonBeanRegistry //当A被添加到SingletonFactories后,B需要注入A时,会通过这个方法获取Aprotected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123; synchronized (this.singletonObjects) &#123; singletonObject = this.earlySingletonObjects.get(beanName); //用singletonFactories生成A的bean对象,&quot;转移&quot;到earlySingletonObjects中 if (singletonObject == null &amp;&amp; allowEarlyReference) &#123; ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) &#123; singletonObject = singletonFactory.getObject(); this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); &#125; &#125; &#125; return singletonObject; &#125; 到这里,setter注入方式解决循环依赖的原因就已经搞清楚了.但是我还有一个疑问,为什么要在两个对象,而非直接用earlySingletonObjects存储getEarlyBeanReference生成的对象呢?,再次翻阅源码后,我的结论如下: getEarlyBeanReference是一个相对耗时的操作,(生成代理,mock都不是简单操作),它仅在发生循环依赖的情况下被调用,而大部分bean不会有循环依赖存在,也就不会调用到getEarlyBeanReference,进而节省资源. 后记本文解析了Spring是如何支持setter注入方式的循环依赖,其核心就是提前暴露出不完备的bean供其他bean使用. 参考Spring是如何支持setter注入方式的循环依赖","categories":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/tags/Spring/"},{"name":"循环依赖","slug":"循环依赖","permalink":"https://xmmarlowe.github.io/tags/%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96/"},{"name":"注入","slug":"注入","permalink":"https://xmmarlowe.github.io/tags/%E6%B3%A8%E5%85%A5/"}],"author":"Marlowe"},{"title":"JVM调优(二)","slug":"Java/JVM调优(二)","date":"2021-08-21T07:53:00.000Z","updated":"2021-08-26T10:59:36.275Z","comments":true,"path":"2021/08/21/Java/JVM调优(二)/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/21/Java/JVM%E8%B0%83%E4%BC%98(%E4%BA%8C)/","excerpt":"JVM调优相关步骤","text":"JVM调优相关步骤 1、调优原则JVM调优听起来很高大上，但是要认识到，JVM调优应该是Java性能优化的最后一颗子弹。 比较认可廖雪峰老师的观点，要认识到JVM调优不是常规手段，性能问题一般第一选择是优化程序，最后的选择才是进行JVM调优。 JVM的自动内存管理本来就是为了将开发人员从内存管理的泥潭里拉出来。即使不得不进行JVM调优，也绝对不能拍脑门就去调整参数，一定要全面监控，详细分析性能数据。 2、JVM调优的时机不得不考虑进行JVM调优的是那些情况呢？ Heap内存（老年代）持续上涨达到设置的最大内存值； Full GC 次数频繁； GC 停顿时间过长（超过1秒）； 应用出现OutOfMemory 等内存异常； 应用中有使用本地缓存且占用大量内存空间； 系统吞吐量与响应性能不高或下降。 3、JVM调优的目标吞吐量、延迟、内存占用三者类似CAP，构成了一个不可能三角，只能选择其中两个进行调优，不可三者兼得。 延迟：GC低停顿和GC低频率； 低内存占用； 高吞吐量; 选择了其中两个，必然会会以牺牲另一个为代价。 下面展示了一些JVM调优的量化目标参考实例： Heap 内存使用率 &lt;= 70%; Old generation内存使用率&lt;= 70%; avgpause &lt;= 1秒; Full gc 次数0 或 avg pause interval &gt;= 24小时 ; 注意：不同应用的JVM调优量化目标是不一样的。 4、JVM调优的步骤一般情况下，JVM调优可通过以下步骤进行： 分析系统系统运行情况：分析GC日志及dump文件，判断是否需要优化，确定瓶颈问题点； 确定JVM调优量化目标； 确定JVM调优参数（根据历史JVM参数来调整）； 依次确定调优内存、延迟、吞吐量等指标； 对比观察调优前后的差异； 不断的分析和调整，直到找到合适的JVM参数配置； 找到最合适的参数，将这些参数应用到所有服务器，并进行后续跟踪。 以上操作步骤中，某些步骤是需要多次不断迭代完成的。一般是从满足程序的内存使用需求开始的，之后是时间延迟的要求，最后才是吞吐量的要求，要基于这个步骤来不断优化，每一个步骤都是进行下一步的基础，不可逆行。 5、主要工具JDK自带了很多性能监控工具，我们可以用这些工具来监测系统和排查内存性能问题。 6、常用调优策略这里还是要提一下，及时确定要进行JVM调优，也不要陷入“知见障”，进行分析之后，发现可以通过优化程序提升性能，仍然首选优化程序。 6.1、选择合适的垃圾回收器CPU单核，那么毫无疑问Serial 垃圾收集器是你唯一的选择。 CPU多核，关注吞吐量 ，那么选择PS+PO组合。 CPU多核，关注用户停顿时间，JDK版本1.6或者1.7，那么选择CMS。 CPU多核，关注用户停顿时间，JDK1.8及以上，JVM可用内存6G以上，那么选择G1。 参数配置： 1234567891011&#x2F;&#x2F;设置Serial垃圾收集器（新生代） 开启：-XX:+UseSerialGC ​ &#x2F;&#x2F;设置PS+PO,新生代使用功能Parallel Scavenge 老年代将会使用Parallel Old收集器 开启 -XX:+UseParallelOldGC ​ &#x2F;&#x2F;CMS垃圾收集器（老年代） 开启 -XX:+UseConcMarkSweepGC ​ &#x2F;&#x2F;设置G1垃圾收集器 开启 -XX:+UseG1GC 6.2、调整内存大小现象：垃圾收集频率非常频繁。 原因：如果内存太小，就会导致频繁的需要进行垃圾收集才能释放出足够的空间来创建新的对象，所以增加堆内存大小的效果是非常显而易见的。 注意：如果垃圾收集次数非常频繁，但是每次能回收的对象非常少，那么这个时候并非内存太小，而可能是内存泄露导致对象无法回收，从而造成频繁GC。 参数配置： 1234567891011&#x2F;&#x2F;设置堆初始值 指令1：-Xms2g 指令2：-XX:InitialHeapSize&#x3D;2048m ​ &#x2F;&#x2F;设置堆区最大值 指令1：&#96;-Xmx2g&#96; 指令2： -XX:MaxHeapSize&#x3D;2048m ​ &#x2F;&#x2F;新生代内存配置 指令1：-Xmn512m 指令2：-XX:MaxNewSize&#x3D;512m 6.3、设置符合预期的停顿时间现象：程序间接性的卡顿 原因：如果没有确切的停顿时间设定，垃圾收集器以吞吐量为主，那么垃圾收集时间就会不稳定。 注意：不要设置不切实际的停顿时间，单次时间越短也意味着需要更多的GC次数才能回收完原有数量的垃圾. 参数配置： 12&#x2F;&#x2F;GC停顿时间，垃圾收集器会尝试用各种手段达到这个时间 -XX:MaxGCPauseMillis 6.4、调整内存区域大小比率现象：某一个区域的GC频繁，其他都正常。 原因：如果对应区域空间不足，导致需要频繁GC来释放空间，在JVM堆内存无法增加的情况下，可以调整对应区域的大小比率。 注意：也许并非空间不足，而是因为内存泄造成内存无法回收。从而导致GC频繁。 参数配置： 12345&#x2F;&#x2F;survivor区和Eden区大小比率 指令：-XX:SurvivorRatio&#x3D;6 &#x2F;&#x2F;S区和Eden区占新生代比率为1:6,两个S区2:6 ​ &#x2F;&#x2F;新生代和老年代的占比 -XX:NewRatio&#x3D;4 &#x2F;&#x2F;表示新生代:老年代 &#x3D; 1:4 即老年代占整个堆的4&#x2F;5；默认值&#x3D;2 6.5、调整对象升老年代的年龄现象：老年代频繁GC，每次回收的对象很多。 原因：如果升代年龄小，新生代的对象很快就进入老年代了，导致老年代对象变多，而这些对象其实在随后的很短时间内就可以回收，这时候可以调整对象的升级代年龄，让对象不那么容易进入老年代解决老年代空间不足频繁GC问题。 注意：增加了年龄之后，这些对象在新生代的时间会变长可能导致新生代的GC频率增加，并且频繁复制这些对象新生的GC时间也可能变长。 配置参数： 12&#x2F;&#x2F;进入老年代最小的GC年龄,年轻代对象转换为老年代对象最小年龄值，默认值7 -XX:InitialTenuringThreshol&#x3D;7 6.6、调整大对象的标准现象：老年代频繁GC，每次回收的对象很多,而且单个对象的体积都比较大。 原因：如果大量的大对象直接分配到老年代，导致老年代容易被填满而造成频繁GC，可设置对象直接进入老年代的标准。 注意：这些大对象进入新生代后可能会使新生代的GC频率和时间增加。 配置参数： 12&#x2F;&#x2F;新生代可容纳的最大对象,大于则直接会分配到老年代，0代表没有限制。 -XX:PretenureSizeThreshold&#x3D;1000000 6.7、调整GC的触发时机现象：CMS，G1 经常 Full GC，程序卡顿严重。 原因：G1和CMS 部分GC阶段是并发进行的，业务线程和垃圾收集线程一起工作，也就说明垃圾收集的过程中业务线程会生成新的对象，所以在GC的时候需要预留一部分内存空间来容纳新产生的对象，如果这个时候内存空间不足以容纳新产生的对象，那么JVM就会停止并发收集暂停所有业务线程（STW）来保证垃圾收集的正常运行。这个时候可以调整GC触发的时机（比如在老年代占用60%就触发GC），这样就可以预留足够的空间来让业务线程创建的对象有足够的空间分配。 注意：提早触发GC会增加老年代GC的频率。 配置参数： 12345&#x2F;&#x2F;使用多少比例的老年代后开始CMS收集，默认是68%，如果频繁发生SerialOld卡顿，应该调小 -XX:CMSInitiatingOccupancyFraction ​&#x2F;&#x2F;G1混合垃圾回收周期中要包括的旧区域设置占用率阈值。默认占用率为 65% -XX:G1MixedGCLiveThresholdPercent&#x3D;65 6.8、调整 JVM本地内存大小现象：GC的次数、时间和回收的对象都正常，堆内存空间充足，但是报OOM 原因： JVM除了堆内存之外还有一块堆外内存，这片内存也叫本地内存，可是这块内存区域不足了并不会主动触发GC，只有在堆内存区域触发的时候顺带会把本地内存回收了，而一旦本地内存分配不足就会直接报OOM异常。 注意： 本地内存异常的时候除了上面的现象之外，异常信息可能是OutOfMemoryError：Direct buffer memory。 解决方式除了调整本地内存大小之外，也可以在出现此异常时进行捕获，手动触发GC（System.gc()）。 配置参数： 1XX:MaxDirectMemorySize 7、JVM调优实例以下是整理自网络的一些JVM调优实例： 7.1、网站流量浏览量暴增后，网站反应页面响很慢1、问题推测：在测试环境测速度比较快，但是一到生产就变慢，所以推测可能是因为垃圾收集导致的业务线程停顿。 2、定位：为了确认推测的正确性，在线上通过jstat -gc 指令 看到JVM进行GC 次数频率非常高，GC所占用的时间非常长，所以基本推断就是因为GC频率非常高，所以导致业务线程经常停顿，从而造成网页反应很慢。 3、解决方案：因为网页访问量很高，所以对象创建速度非常快，导致堆内存容易填满从而频繁GC，所以这里问题在于新生代内存太小，所以这里可以增加JVM内存就行了，所以初步从原来的2G内存增加到16G内存。 4、第二个问题：增加内存后的确平常的请求比较快了，但是又出现了另外一个问题，就是不定期的会间断性的卡顿，而且单次卡顿的时间要比之前要长很多。 5、问题推测：练习到是之前的优化加大了内存，所以推测可能是因为内存加大了，从而导致单次GC的时间变长从而导致间接性的卡顿。 6、定位：还是通过jstat -gc 指令 查看到 的确FGC次数并不是很高，但是花费在FGC上的时间是非常高的,根据GC日志 查看到单次FGC的时间有达到几十秒的。 7、解决方案： 因为JVM默认使用的是PS+PO的组合，PS+PO垃圾标记和收集阶段都是STW，所以内存加大了之后，需要进行垃圾回收的时间就变长了，所以这里要想避免单次GC时间过长，所以需要更换并发类的收集器，因为当前的JDK版本为1.7，所以最后选择CMS垃圾收集器，根据之前垃圾收集情况设置了一个预期的停顿的时间，上线后网站再也没有了卡顿问题。 7.2、后台导出数据引发的OOM问题描述： 公司的后台系统，偶发性的引发OOM异常，堆内存溢出。 1、因为是偶发性的，所以第一次简单的认为就是堆内存不足导致，所以单方面的加大了堆内存从4G调整到8G。 2、但是问题依然没有解决，只能从堆内存信息下手，通过开启了-XX:+HeapDumpOnOutOfMemoryError参数 获得堆内存的dump文件。 3、VisualVM 对 堆dump文件进行分析，通过VisualVM查看到占用内存最大的对象是String对象，本来想跟踪着String对象找到其引用的地方，但dump文件太大，跟踪进去的时候总是卡死，而String对象占用比较多也比较正常，最开始也没有认定就是这里的问题，于是就从线程信息里面找突破点。 4、通过线程进行分析，先找到了几个正在运行的业务线程，然后逐一跟进业务线程看了下代码，发现有个引起我注意的方法，导出订单信息。 5、因为订单信息导出这个方法可能会有几万的数据量，首先要从数据库里面查询出来订单信息，然后把订单信息生成excel，这个过程会产生大量的String对象。 6、为了验证自己的猜想，于是准备登录后台去测试下，结果在测试的过程中发现到处订单的按钮前端居然没有做点击后按钮置灰交互事件，结果按钮可以一直点，因为导出订单数据本来就非常慢，使用的人员可能发现点击后很久后页面都没反应，结果就一直点，结果就大量的请求进入到后台，堆内存产生了大量的订单对象和EXCEL对象，而且方法执行非常慢，导致这一段时间内这些对象都无法被回收，所以最终导致内存溢出。 7、知道了问题就容易解决了，最终没有调整任何JVM参数，只是在前端的导出订单按钮上加上了置灰状态，等后端响应之后按钮才可以进行点击，然后减少了查询订单信息的非必要字段来减少生成对象的体积，然后问题就解决了。 7.3、单个缓存数据过大导致的系统CPU飚高1、系统发布后发现CPU一直飚高到600%，发现这个问题后首先要做的是定位到是哪个应用占用CPU高，通过top 找到了对应的一个java应用占用CPU资源600%。 2、如果是应用的CPU飚高，那么基本上可以定位可能是锁资源竞争，或者是频繁GC造成的。 3、所以准备首先从GC的情况排查，如果GC正常的话再从线程的角度排查，首先使用jstat -gc PID 指令打印出GC的信息，结果得到得到的GC 统计信息有明显的异常，应用在运行了才几分钟的情况下GC的时间就占用了482秒，那么问这很明显就是频繁GC导致的CPU飚高。 4、定位到了是GC的问题，那么下一步就是找到频繁GC的原因了，所以可以从两方面定位了，可能是哪个地方频繁创建对象，或者就是有内存泄露导致内存回收不掉。 5、根据这个思路决定把堆内存信息dump下来看一下，使用jmap -dump 指令把堆内存信息dump下来（堆内存空间大的慎用这个指令否则容易导致会影响应用，因为我们的堆内存空间才2G所以也就没考虑这个问题了）。 6、把堆内存信息dump下来后，就使用visualVM进行离线分析了，首先从占用内存最多的对象中查找，结果排名第三看到一个业务VO占用堆内存约10%的空间，很明显这个对象是有问题的。 7、通过业务对象找到了对应的业务代码，通过代码的分析找到了一个可疑之处，这个业务对象是查看新闻资讯信息生成的对象，由于想提升查询的效率，所以把新闻资讯保存到了redis缓存里面，每次调用资讯接口都是从缓存里面获取。 8、把新闻保存到redis缓存里面这个方式是没有问题的，有问题的是新闻的50000多条数据都是保存在一个key里面，这样就导致每次调用查询新闻接口都会从redis里面把50000多条数据都拿出来，再做筛选分页拿出10条返回给前端。50000多条数据也就意味着会产生50000多个对象，每个对象280个字节左右，50000个对象就有13.3M，这就意味着只要查看一次新闻信息就会产生至少13.3M的对象，那么并发请求量只要到10，那么每秒钟都会产生133M的对象，而这种大对象会被直接分配到老年代，这样的话一个2G大小的老年代内存，只需要几秒就会塞满，从而触发GC。 9、知道了问题所在后那么就容易解决了，问题是因为单个缓存过大造成的，那么只需要把缓存减小就行了，这里只需要把缓存以页的粒度进行缓存就行了，每个key缓存10条作为返回给前端1页的数据，这样的话每次查询新闻信息只会从缓存拿出10条数据，就避免了此问题的 产生。 7.4、CPU经常100% 问题定位问题分析：CPU高一定是某个程序长期占用了CPU资源。 1、所以先需要找出那个进行占用CPU高。 1top 列出系统各个进程的资源占用情况。 2、然后根据找到对应进行里哪个线程占用CPU高。 1top -Hp 进程ID 列出对应进程里面的线程占用资源情况 3、找到对应线程ID后，再打印出对应线程的堆栈信息 12printf &quot;%x\\n&quot; PID 把线程ID转换为16进制。jstack PID 打印出进程的所有线程信息，从打印出来的线程信息中找到上一步转换为16进制的线程ID对应的线程信息。 4、最后根据线程的堆栈信息定位到具体业务方法,从代码逻辑中找到问题所在。 12查看是否有线程长时间的watting 或blocked如果线程长期处于watting状态下， 关注watting on xxxxxx，说明线程在等待这把锁，然后根据锁的地址找到持有锁的线程。 7.5、内存飚高问题定位分析： 内存飚高如果是发生在java进程上，一般是因为创建了大量对象所导致，持续飚高说明垃圾回收跟不上对象创建的速度，或者内存泄露导致对象无法回收。 1、先观察垃圾回收的情况 123jstat -gc PID 1000 查看GC次数，时间等信息，每隔一秒打印一次。 jmap -histo PID | head -20 查看堆内存占用空间最大的前20个对象类型,可初步查看是哪个对象占用了内存。 如果每次GC次数频繁，而且每次回收的内存空间也正常，那说明是因为对象创建速度快导致内存一直占用很高；如果每次回收的内存非常少，那么很可能是因为内存泄露导致内存一直无法被回收。 2、导出堆内存文件快照 1jmap -dump:live,format&#x3D;b,file&#x3D;&#x2F;home&#x2F;myheapdump.hprof PID dump堆内存信息到文件。 3、使用visualVM对dump文件进行离线分析,找到占用内存高的对象，再找到创建该对象的业务代码位置，从代码和业务场景中定位具体问题。 7.6、数据分析平台系统频繁 Full GC平台主要对用户在 App 中行为进行定时分析统计，并支持报表导出，使用 CMS GC 算法。 数据分析师在使用中发现系统页面打开经常卡顿，通过 jstat 命令发现系统每次 Young GC 后大约有 10% 的存活对象进入老年代。 原来是因为 Survivor 区空间设置过小，每次 Young GC 后存活对象在 Survivor 区域放不下，提前进入老年代。 通过调大 Survivor 区，使得 Survivor 区可以容纳 Young GC 后存活对象，对象在 Survivor 区经历多次 Young GC 达到年龄阈值才进入老年代。 调整之后每次 Young GC 后进入老年代的存活对象稳定运行时仅几百 Kb，Full GC 频率大大降低。 7.7、业务对接网关 OOM网关主要消费 Kafka 数据，进行数据处理计算然后转发到另外的 Kafka 队列，系统运行几个小时候出现 OOM，重启系统几个小时之后又 OOM。 通过 jmap 导出堆内存，在 eclipse MAT 工具分析才找出原因：代码中将某个业务 Kafka 的 topic 数据进行日志异步打印，该业务数据量较大，大量对象堆积在内存中等待被打印，导致 OOM。 7.8、鉴权系统频繁长时间 Full GC系统对外提供各种账号鉴权服务，使用时发现系统经常服务不可用，通过 Zabbix 的监控平台监控发现系统频繁发生长时间 Full GC，且触发时老年代的堆内存通常并没有占满，发现原来是业务代码中调用了 System.gc()。 参考【JVM进阶之路】十：JVM调优总结","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://xmmarlowe.github.io/tags/JVM/"}],"author":"Marlowe"},{"title":"Leaf——美团点评分布式ID生成系统","slug":"分布式/Leaf——美团点评分布式ID生成系统","date":"2021-08-21T03:26:49.000Z","updated":"2021-08-21T03:32:12.508Z","comments":true,"path":"2021/08/21/分布式/Leaf——美团点评分布式ID生成系统/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/21/%E5%88%86%E5%B8%83%E5%BC%8F/Leaf%E2%80%94%E2%80%94%E7%BE%8E%E5%9B%A2%E7%82%B9%E8%AF%84%E5%88%86%E5%B8%83%E5%BC%8FID%E7%94%9F%E6%88%90%E7%B3%BB%E7%BB%9F/","excerpt":"转载于美团技术团队，原文链接Leaf——美团点评分布式ID生成系统","text":"转载于美团技术团队，原文链接Leaf——美团点评分布式ID生成系统","categories":[{"name":"分布式","slug":"分布式","permalink":"https://xmmarlowe.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"美团","slug":"美团","permalink":"https://xmmarlowe.github.io/tags/%E7%BE%8E%E5%9B%A2/"},{"name":"ID","slug":"ID","permalink":"https://xmmarlowe.github.io/tags/ID/"}],"author":"Marlowe"},{"title":"手写LRU算法","slug":"算法与数据结构/手写LRU算法","date":"2021-08-20T15:33:34.000Z","updated":"2021-08-21T03:29:42.440Z","comments":true,"path":"2021/08/20/算法与数据结构/手写LRU算法/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/20/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%89%8B%E5%86%99LRU%E7%AE%97%E6%B3%95/","excerpt":"","text":"题目描述146. LRU 缓存机制 12345678910111213141516171819202122232425262728293031323334353637运用你所掌握的数据结构，设计和实现一个 LRU (最近最少使用) 缓存机制 。实现 LRUCache 类：LRUCache(int capacity) 以正整数作为容量 capacity 初始化 LRU 缓存int get(int key) 如果关键字 key 存在于缓存中，则返回关键字的值，否则返回 -1 。void put(int key, int value) 如果关键字已经存在，则变更其数据值；如果关键字不存在，则插入该组「关键字-值」。当缓存容量达到上限时，它应该在写入新数据之前删除最久未使用的数据值，从而为新的数据值留出空间。 进阶：你是否可以在 O(1) 时间复杂度内完成这两种操作？示例：输入[&quot;LRUCache&quot;, &quot;put&quot;, &quot;put&quot;, &quot;get&quot;, &quot;put&quot;, &quot;get&quot;, &quot;put&quot;, &quot;get&quot;, &quot;get&quot;, &quot;get&quot;][[2], [1, 1], [2, 2], [1], [3, 3], [2], [4, 4], [1], [3], [4]]输出[null, null, null, 1, null, -1, null, -1, 3, 4]解释LRUCache lRUCache &#x3D; new LRUCache(2);lRUCache.put(1, 1); &#x2F;&#x2F; 缓存是 &#123;1&#x3D;1&#125;lRUCache.put(2, 2); &#x2F;&#x2F; 缓存是 &#123;1&#x3D;1, 2&#x3D;2&#125;lRUCache.get(1); &#x2F;&#x2F; 返回 1lRUCache.put(3, 3); &#x2F;&#x2F; 该操作会使得关键字 2 作废，缓存是 &#123;1&#x3D;1, 3&#x3D;3&#125;lRUCache.get(2); &#x2F;&#x2F; 返回 -1 (未找到)lRUCache.put(4, 4); &#x2F;&#x2F; 该操作会使得关键字 1 作废，缓存是 &#123;4&#x3D;4, 3&#x3D;3&#125;lRUCache.get(1); &#x2F;&#x2F; 返回 -1 (未找到)lRUCache.get(3); &#x2F;&#x2F; 返回 3lRUCache.get(4); &#x2F;&#x2F; 返回 4 提示：1 &lt;&#x3D; capacity &lt;&#x3D; 30000 &lt;&#x3D; key &lt;&#x3D; 100000 &lt;&#x3D; value &lt;&#x3D; 105最多调用 2 * 105 次 get 和 put 解法1：LinkedHashMap1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.marlowe.demos;import java.util.LinkedHashMap;import java.util.Map;/** * @program: JavaThreadDemo * @description: LinkedHashMap实现LRU算法 * @author: Marlowe * @create: 2021-08-20 14:18 **/public class LRUCacheDemo&lt;K, V&gt; extends LinkedHashMap &#123; private int capacity; /** * initialCapacity – the initial capacity * loadFactor – the load factor * accessOrder – the ordering mode - true for access-order, false for insertion-order * * @param capacity */ public LRUCacheDemo(int capacity) &#123; super(capacity, 0.75F, true); this.capacity = capacity; &#125; @Override protected boolean removeEldestEntry(Map.Entry eldest) &#123; return super.size() &gt; capacity; &#125; public static void main(String[] args) &#123; LRUCacheDemo lruCacheDemo = new LRUCacheDemo(3); lruCacheDemo.put(1, &quot;a&quot;); lruCacheDemo.put(2, &quot;b&quot;); lruCacheDemo.put(3, &quot;c&quot;); System.out.println(lruCacheDemo.keySet()); lruCacheDemo.put(4, &quot;d&quot;); System.out.println(lruCacheDemo.keySet()); lruCacheDemo.put(3, &quot;c&quot;); System.out.println(lruCacheDemo.keySet()); lruCacheDemo.put(3, &quot;c&quot;); System.out.println(lruCacheDemo.keySet()); lruCacheDemo.put(3, &quot;c&quot;); System.out.println(lruCacheDemo.keySet()); lruCacheDemo.put(5, &quot;x&quot;); System.out.println(lruCacheDemo.keySet()); &#125;&#125; 解法2：map+双向链表123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200package com.marlowe.demos;import java.util.HashMap;import java.util.Map;/** * @program: JavaThreadDemo * @description: 链表+hash实现LRU算法 * @author: Marlowe * @create: 2021-08-20 14:18 **/public class LRUCacheDemo1 &#123; /** * map 负责查找，构建一个虚拟的双向链表，它里面装的就是一个个Node节点，作为数据载体 */ /** * 1.构造一个Node节点，作为数据载体 * * @param &lt;K&gt; * @param &lt;V&gt; */ class Node&lt;K, V&gt; &#123; K key; V value; Node&lt;K, V&gt; prev; Node&lt;K, V&gt; next; /** * 无参构造，初始化节点 */ public Node() &#123; this.prev = this.next = null; &#125; /** * 有参构造，初始化节点 * * @param key * @param value */ public Node(K key, V value) &#123; this.key = key; this.value = value; this.prev = this.next = null; &#125; &#125; /** * 2.构建一个虚拟的双向链表,里面安放得就是我们的Node * * @param &lt;K&gt; * @param &lt;V&gt; */ class DoubleLinkedList&lt;K, V&gt; &#123; Node&lt;K, V&gt; head; Node&lt;K, V&gt; tail; /** * 2.1构造方法，初始化双向链表 */ public DoubleLinkedList() &#123; head = new Node&lt;&gt;(); tail = new Node&lt;&gt;(); head.next = tail; tail.next = head; &#125; /** * 2.2添加到头 * * @param node */ public void addHead(Node&lt;K, V&gt; node) &#123; node.next = head.next; head.next.prev = node; node.prev = head; head.next = node; &#125; /** * 2.3 删除节点 * * @param node */ public void removeNode(Node&lt;K, V&gt; node) &#123; node.next.prev = node.prev; node.prev.next = node.next; node.prev = null; node.next = null; &#125; /** * 2.4 获取最后一个节点 * * @return */ public Node getLast() &#123; return tail.prev; &#125; &#125; /** * LRU容量 */ private int cacheSize; Map&lt;Integer, Node&lt;Integer, Integer&gt;&gt; map; DoubleLinkedList&lt;Integer, Integer&gt; doubleLinkedList; /** * 初始化LRU * @param cacheSize */ public LRUCacheDemo1(int cacheSize) &#123; this.cacheSize = cacheSize; map = new HashMap&lt;&gt;(); doubleLinkedList = new DoubleLinkedList&lt;&gt;(); &#125; /** * 获取值 * * @param key * @return */ public int get(int key) &#123; if (!map.containsKey(key)) &#123; return -1; &#125; Node&lt;Integer, Integer&gt; node = map.get(key); doubleLinkedList.removeNode(node); doubleLinkedList.addHead(node); return node.value; &#125; /** * 向LRU中放值 * @param key * @param value */ public void put(int key, int value) &#123; // 如果map里面有key，更新value值，放回map，并移动到队首 if (map.containsKey(key)) &#123; Node&lt;Integer, Integer&gt; node = map.get(key); // 更新node的value node.value = value; // 更新node map.put(key, node); // 将当前节点移动到队首 doubleLinkedList.removeNode(node); doubleLinkedList.addHead(node); &#125; else &#123; if (map.size() == cacheSize) &#123; // 如果map满了，map和双向链表都移除最后一个元素 Node lastNode = doubleLinkedList.getLast(); // map和链表都移除当前最后一个node map.remove(lastNode.key); doubleLinkedList.removeNode(lastNode); &#125; // 如果链表没有满，新建节点并从头部加入 Node&lt;Integer, Integer&gt; newNode = new Node&lt;&gt;(key, value); map.put(key, newNode); doubleLinkedList.addHead(newNode); &#125; &#125; public static void main(String[] args) &#123; LRUCacheDemo1 lruCacheDemo = new LRUCacheDemo1(3); System.out.println(&quot;缓存容量:&quot; + lruCacheDemo.cacheSize); lruCacheDemo.put(1, 1); System.out.println(&quot;map大小：&quot; + lruCacheDemo.map.size()); lruCacheDemo.put(2, 2); System.out.println(&quot;map大小：&quot; + lruCacheDemo.map.size()); lruCacheDemo.put(3, 3); System.out.println(&quot;map大小：&quot; + lruCacheDemo.map.size()); lruCacheDemo.put(4, 4); System.out.println(lruCacheDemo.map.keySet()); System.out.println(&quot;map大小：&quot; + lruCacheDemo.map.size()); lruCacheDemo.put(3, 3); System.out.println(lruCacheDemo.map.keySet()); System.out.println(&quot;map大小：&quot; + lruCacheDemo.map.size()); lruCacheDemo.put(3, 3); System.out.println(lruCacheDemo.map.keySet()); System.out.println(&quot;map大小：&quot; + lruCacheDemo.map.size()); lruCacheDemo.put(3, 3); System.out.println(lruCacheDemo.map.keySet()); System.out.println(&quot;map大小：&quot; + lruCacheDemo.map.size()); lruCacheDemo.put(5, 5); System.out.println(lruCacheDemo.map.keySet()); System.out.println(&quot;map大小：&quot; + lruCacheDemo.map.size()); &#125;&#125;","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"https://xmmarlowe.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"LRU","slug":"LRU","permalink":"https://xmmarlowe.github.io/tags/LRU/"}],"author":"Marlowe"},{"title":"Spring三级缓存","slug":"Spring/Spring三级缓存","date":"2021-08-20T00:17:14.000Z","updated":"2021-08-21T03:29:42.447Z","comments":true,"path":"2021/08/20/Spring/Spring三级缓存/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/20/Spring/Spring%E4%B8%89%E7%BA%A7%E7%BC%93%E5%AD%98/","excerpt":"我们都知道 Spring 是通过三级缓存来解决循环依赖的，但是解决循环依赖真的需要使用到三级缓冲吗？只使用两级缓存是否可以呢？本篇文章就 Spring 是如何使用三级缓存解决循环依赖作为引子，验证两级缓存是否可以解决循环依赖。","text":"我们都知道 Spring 是通过三级缓存来解决循环依赖的，但是解决循环依赖真的需要使用到三级缓冲吗？只使用两级缓存是否可以呢？本篇文章就 Spring 是如何使用三级缓存解决循环依赖作为引子，验证两级缓存是否可以解决循环依赖。 循环依赖既然要解决循环依赖，那么就要知道循环依赖是什么。如下图所示： 通过上图，我们可以看出： A 依赖于 B B 依赖于 C C 依赖于 A 1234567891011public class A &#123; private B b;&#125;public class B &#123; private C c;&#125;public class C &#123; private A a;&#125; 这种依赖关系形成了一种闭环，从而造成了循环依赖的局面。 下面是未解决循环依赖的常规步骤： 实例化 A，此时 A 还未完成属性填充和初始化方法（@PostConstruct）的执行。 A 对象发现需要注入 B 对象，但是容器中并没有 B 对象（如果对象创建完成并且属性注入完成和执行完初始化方法就会放入容器中）。 实例化 B，此时 B 还未完成属性填充和初始化方法（@PostConstruct）的执行。 B 对象发现需要注入 C 对象，但是容器中并没有 C 对象。 实例化 C，此时 C 还未完成属性填充和初始化方法（@PostConstruct）的执行。 C 对象发现需要注入 A 对象，但是容器中并没有 A 对象。 重复步骤 1。 三级缓存Spring 解决循环依赖的核心就是提前暴露对象，而提前暴露的对象就是放置于第二级缓存中。下表是三级缓存的说明： singletonObjects: 一级缓存，存放完整的 Bean。 earlySingletonObjects: 二级缓存，存放提前暴露的Bean，Bean 是不完整的，未完成属性注入和执行 init 方法。 singletonFactories: 三级缓存，存放的是 Bean 工厂，主要是生产 Bean，存放到二级缓存中。 所有被 Spring 管理的 Bean，最终都会存放在 singletonObjects 中，这里面存放的 Bean 是经历了所有生命周期的（除了销毁的生命周期），完整的，可以给用户使用的。 earlySingletonObjects 存放的是已经被实例化，但是还没有注入属性和执行 init 方法的 Bean。 singletonFactories 存放的是生产 Bean 的工厂。 Bean 都已经实例化了，为什么还需要一个生产 Bean 的工厂呢？这里实际上是跟 AOP 有关，如果项目中不需要为 Bean 进行代理，那么这个 Bean 工厂就会直接返回一开始实例化的对象，如果需要使用 AOP 进行代理，那么这个工厂就会发挥重要的作用了，这也是本文需要重点关注的问题之一。 解决循环依赖Spring 是如何通过上面介绍的三级缓存来解决循环依赖的呢？这里只用 A，B 形成的循环依赖来举例： 实例化 A，此时 A 还未完成属性填充和初始化方法（@PostConstruct）的执行，A 只是一个半成品。 为 A 创建一个 Bean 工厂，并放入到 singletonFactories 中。 发现 A 需要注入 B 对象，但是一级、二级、三级缓存均为发现对象 B。 实例化 B，此时 B 还未完成属性填充和初始化方法（@PostConstruct）的执行，B 只是一个半成品。 为 B 创建一个 Bean 工厂，并放入到 singletonFactories 中。 发现 B 需要注入 A 对象，此时在一级、二级未发现对象 A，但是在三级缓存中发现了对象 A，从三级缓存中得到对象 A，并将对象 A 放入二级缓存中，同时删除三级缓存中的对象 A。（注意，此时的 A 还是一个半成品，并没有完成属性填充和执行初始化方法） 将对象 A 注入到对象 B 中。 对象 B 完成属性填充，执行初始化方法，并放入到一级缓存中，同时删除二级缓存中的对象 B。（此时对象 B 已经是一个成品） 对象 A 得到对象 B，将对象 B 注入到对象 A 中。（对象 A 得到的是一个完整的对象 B） 对象 A 完成属性填充，执行初始化方法，并放入到一级缓存中，同时删除二级缓存中的对象 A。 我们从源码中来分析整个过程： 创建 Bean 的方法在 AbstractAutowireCapableBeanFactory::doCreateBean() 1234567891011121314151617181920212223242526protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, Object[] args) throws BeanCreationException &#123; BeanWrapper instanceWrapper = null; if (instanceWrapper == null) &#123; // ① 实例化对象 instanceWrapper = this.createBeanInstance(beanName, mbd, args); &#125; final Object bean = instanceWrapper != null ? instanceWrapper.getWrappedInstance() : null; Class&lt;?&gt; beanType = instanceWrapper != null ? instanceWrapper.getWrappedClass() : null; // ② 判断是否允许提前暴露对象，如果允许，则直接添加一个 ObjectFactory 到三级缓存 boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) &#123; // 添加三级缓存的方法详情在下方 addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean)); &#125; // ③ 填充属性 this.populateBean(beanName, mbd, instanceWrapper); // ④ 执行初始化方法，并创建代理 exposedObject = initializeBean(beanName, exposedObject, mbd); return exposedObject;&#125; 添加三级缓存的方法如下： 123456789101112131415protected void addSingletonFactory(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; Assert.notNull(singletonFactory, &quot;Singleton factory must not be null&quot;); synchronized (this.singletonObjects) &#123; if (!this.singletonObjects.containsKey(beanName)) &#123; // 判断一级缓存中不存在此对象 this.singletonFactories.put(beanName, singletonFactory); // 添加至三级缓存 this.earlySingletonObjects.remove(beanName); // 确保二级缓存没有此对象 this.registeredSingletons.add(beanName); &#125; &#125;&#125;@FunctionalInterfacepublic interface ObjectFactory&lt;T&gt; &#123; T getObject() throws BeansException;&#125; 通过这段代码，我们可以知道 Spring 在实例化对象的之后，就会为其创建一个 Bean 工厂，并将此工厂加入到三级缓存中。 因此，Spring 一开始提前暴露的并不是实例化的 Bean，而是将 Bean 包装起来的 ObjectFactory。为什么要这么做呢？ 这实际上涉及到 AOP，如果创建的 Bean 是有代理的，那么注入的就应该是代理 Bean，而不是原始的 Bean。但是 Spring 一开始并不知道 Bean 是否会有循环依赖，通常情况下（没有循环依赖的情况下），Spring 都会在完成填充属性，并且执行完初始化方法之后再为其创建代理。但是，如果出现了循环依赖的话，Spring 就不得不为其提前创建代理对象，否则注入的就是一个原始对象，而不是代理对象。因此，这里就涉及到应该在哪里提前创建代理对象？ Spring 的做法就是在 ObjectFactory 中去提前创建代理对象。它会执行 getObject() 方法来获取到 Bean。实际上，它真正执行的方法如下： 12345678910111213protected Object getEarlyBeanReference(String beanName, RootBeanDefinition mbd, Object bean) &#123; Object exposedObject = bean; if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof SmartInstantiationAwareBeanPostProcessor) &#123; SmartInstantiationAwareBeanPostProcessor ibp = (SmartInstantiationAwareBeanPostProcessor) bp; // 如果需要代理，这里会返回代理对象；否则返回原始对象 exposedObject = ibp.getEarlyBeanReference(exposedObject, beanName); &#125; &#125; &#125; return exposedObject;&#125; 因为提前进行了代理，避免对后面重复创建代理对象，会在 earlyProxyReferences 中记录已被代理的对象。 12345678910public abstract class AbstractAutoProxyCreator extends ProxyProcessorSupport implements SmartInstantiationAwareBeanPostProcessor, BeanFactoryAware &#123; @Override public Object getEarlyBeanReference(Object bean, String beanName) &#123; Object cacheKey = getCacheKey(bean.getClass(), beanName); // 记录已被代理的对象 this.earlyProxyReferences.put(cacheKey, bean); return wrapIfNecessary(bean, beanName, cacheKey); &#125;&#125; 通过上面的解析，我们可以知道 Spring 需要三级缓存的目的是为了在没有循环依赖的情况下，延迟代理对象的创建，使 Bean 的创建符合 Spring 的设计原则。 如何获取依赖我们目前已经知道了 Spring 的三级依赖的作用，但是 Spring 在注入属性的时候是如何去获取依赖的呢？ 他是通过一个 getSingleton() 方法去获取所需要的 Bean 的。 12345678910111213141516171819202122protected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; // 一级缓存 Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123; synchronized (this.singletonObjects) &#123; // 二级缓存 singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null &amp;&amp; allowEarlyReference) &#123; // 三级缓存 ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) &#123; // Bean 工厂中获取 Bean singletonObject = singletonFactory.getObject(); // 放入到二级缓存中 this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); &#125; &#125; &#125; &#125; return singletonObject;&#125; 当 Spring 为某个 Bean 填充属性的时候，它首先会寻找需要注入对象的名称，然后依次执行 getSingleton() 方法得到所需注入的对象，而获取对象的过程就是先从一级缓存中获取，一级缓存中没有就从二级缓存中获取，二级缓存中没有就从三级缓存中获取，如果三级缓存中也没有，那么就会去执行 doCreateBean() 方法创建这个 Bean。 二级缓存我们现在已经知道，第三级缓存的目的是为了延迟代理对象的创建，因为如果没有依赖循环的话，那么就不需要为其提前创建代理，可以将它延迟到初始化完成之后再创建。 既然目的只是延迟的话，那么我们是不是可以不延迟创建，而是在实例化完成之后，就为其创建代理对象，这样我们就不需要第三级缓存了。因此，我们可以将addSingletonFactory() 方法进行改造。 12345678910protected void addSingletonFactory(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; Assert.notNull(singletonFactory, &quot;Singleton factory must not be null&quot;); synchronized (this.singletonObjects) &#123; if (!this.singletonObjects.containsKey(beanName)) &#123; // 判断一级缓存中不存在此对象 object o = singletonFactory.getObject(); // 直接从工厂中获取 Bean this.earlySingletonObjects.put(beanName, o); // 添加至二级缓存中 this.registeredSingletons.add(beanName); &#125; &#125;&#125; 这样的话，每次实例化完 Bean 之后就直接去创建代理对象，并添加到二级缓存中。测试结果是完全正常的，Spring 的初始化时间应该也是不会有太大的影响，因为如果 Bean 本身不需要代理的话，是直接返回原始 Bean 的，并不需要走复杂的创建代理 Bean 的流程。 问题一级缓存不行我们看看一级缓存行不行，如果只留第一级缓存，那么单例的Bean都存在singletonObjects 中，Spring循环依赖主要基于Java引用传递，当获取到对象时，对象的field或者属性可以延后设置，理论上可以，但是如果延后设置出了问题，就会导致完整的Bean和不完整的Bean都在一级缓存中，这个引用时就有空指针的可能，所以一级缓存不行，至少要有singletonObjects 和earlySingletonObjects 两级。 二级缓存不行那么我们再看看两级缓存行不行 现在有A的field或者setter依赖B的实例对象，同时B的field或者setter依赖了A的实例，A首先开始创建，并将自己暴露到 earlySingletonObjects 中，开始填充属性，此时发现自己依赖B的属性，尝试去get(B)，发现B还没有被创建，所以开始创建B，在进行属性填充时初始化A，就从earlySingletonObjects 中获取到了实例化但没有任何属性的A，B拿到A后完成了初始化阶段，将自己放到singletonObjects中,此时返回A，A拿到B的对象继续完成初始化，完成后将自己放到singletonObjects中，由A与B中所表示的A的属性地址是一样的，所以A的属性填充完后，B也获取了A的属性，这样就解决了循环的问题。 似乎完美解决，如果就这么使用的话也没什么问题，但是再加上AOP情况就不同了，被AOP增强的Bean会在初始化后代理成为一个新的对象，也就是说： 如果有AOP，A依赖于B，B依赖于A，A实例化完成暴露出去，开始注入属性，发现引用B，B开始实例化，使用A暴露的对象，初始化完成后封装成代理对象，A再将代理后的B注入，再做代理，那么代理A中的B就是代理后的B，但是代理后的B中的A是没用代理的A。 显然这是不对的，所以在Spring中存在第三级缓存，在创建对象时判断是否是单例，允许循环依赖，正在创建中，就将其从earlySingletonObjects中移除掉，并在singletonFactories放入新的对象，这样后续再查询beanName时会走到singletonFactory.getObject()，其中就会去调用各个beanPostProcessor的getEarlyBeanReference方法，返回的对象就是代理后的对象。 123456789101112131415161718192021222324252627282930313233public abstract class AbstractAutowireCapableBeanFactory extends AbstractBeanFactory implements AutowireCapableBeanFactory &#123; // Eagerly cache singletons to be able to resolve circular references // even when triggered by lifecycle interfaces like BeanFactoryAware. boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Eagerly caching bean &#x27;&quot; + beanName + &quot;&#x27; to allow for resolving potential circular references&quot;); &#125; addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean)); &#125; /** * Add the given singleton factory for building the specified singleton * if necessary. * &lt;p&gt;To be called for eager registration of singletons, e.g. to be able to * resolve circular references. * @param beanName the name of the bean * @param singletonFactory the factory for the singleton object */ protected void addSingletonFactory(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; Assert.notNull(singletonFactory, &quot;Singleton factory must not be null&quot;); synchronized (this.singletonObjects) &#123; if (!this.singletonObjects.containsKey(beanName)) &#123; this.singletonFactories.put(beanName, singletonFactory); this.earlySingletonObjects.remove(beanName); this.registeredSingletons.add(beanName); &#125; &#125; &#125; 结论测试证明，二级缓存也是可以解决循环依赖的。为什么 Spring 不选择二级缓存，而要额外多添加一层缓存呢？如果 Spring 选择二级缓存来解决循环依赖的话，那么就意味着所有 Bean 都需要在实例化完成之后就立马为其创建代理，而 Spring 的设计原则是在 Bean 初始化完成之后才为其创建代理。所以，Spring 选择了三级缓存。但是因为循环依赖的出现，导致了 Spring 不得不提前去创建代理，因为如果不提前创建代理对象，那么注入的就是原始对象，这样就会产生错误。 参考Spring为什么要使用三级缓存解决循环依赖问题 Spring 解决循环依赖必须要三级缓存吗？","categories":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/tags/Spring/"},{"name":"缓存","slug":"缓存","permalink":"https://xmmarlowe.github.io/tags/%E7%BC%93%E5%AD%98/"}],"author":"Marlowe"},{"title":"AQS详解","slug":"并发/AQS详解","date":"2021-08-19T14:36:36.000Z","updated":"2021-08-21T03:29:42.488Z","comments":true,"path":"2021/08/19/并发/AQS详解/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/19/%E5%B9%B6%E5%8F%91/AQS%E8%AF%A6%E8%A7%A3/","excerpt":"AQS是一个用来构建锁和同步器的框架，使用AQS能简单且高效地构造出应用广泛的大量的同步器，比如我们提到的ReentrantLock，Semaphore，其他的诸如ReentrantReadWriteLock，SynchronousQueue，FutureTask等等皆是基于AQS的。当然，我们自己也能利用AQS非常轻松容易地构造出符合我们自己需求的同步器。","text":"AQS是一个用来构建锁和同步器的框架，使用AQS能简单且高效地构造出应用广泛的大量的同步器，比如我们提到的ReentrantLock，Semaphore，其他的诸如ReentrantReadWriteLock，SynchronousQueue，FutureTask等等皆是基于AQS的。当然，我们自己也能利用AQS非常轻松容易地构造出符合我们自己需求的同步器。 AbstractQueuedSynchronizer简介AQS 核心思想AQS核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制AQS是用CLH队列锁实现的，即将暂时获取不到锁的线程加入到队列中。 CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列(虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系)。AQS是将每条请求共享资源的线程封装成一个CLH锁队列的一个结点(Node)来实现锁的分配。 AQS使用一个int成员变量来表示同步状态，通过内置的FIFO队列来完成获取资源线程的排队工作。AQS使用CAS对该同步状态进行原子操作实现对其值的修改。 1private volatile int state;//共享变量，使用volatile修饰保证线程可见性 状态信息通过procted类型的getState，setState，compareAndSetState进行操作 123456789101112//返回同步状态的当前值protected final int getState() &#123; return state;&#125; // 设置同步状态的值protected final void setState(int newState) &#123; state = newState;&#125;//原子地(CAS操作)将同步状态值设置为给定值update如果当前同步状态的值等于expect(期望值)protected final boolean compareAndSetState(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, stateOffset, expect, update);&#125; AQS 对资源的共享方式AQS定义两种资源共享方式 Exclusive(独占)：只有一个线程能执行，如ReentrantLock。又可分为公平锁和非公平锁： 公平锁：按照线程在队列中的排队顺序，先到者先拿到锁 非公平锁：当线程要获取锁时，无视队列顺序直接去抢锁，谁抢到就是谁的 Share(共享)：多个线程可同时执行，如Semaphore/CountDownLatch。Semaphore、CountDownLatCh、 CyclicBarrier、ReadWriteLock 我们都会在后面讲到。 ReentrantReadWriteLock 可以看成是组合式，因为ReentrantReadWriteLock也就是读写锁允许多个线程同时对某一资源进行读。 不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源 state 的获取与释放方式即可，至于具体线程等待队列的维护(如获取资源失败入队/唤醒出队等)，AQS已经在上层已经帮我们实现好了。 AQS底层使用了模板方法模式 同步器的设计是基于模板方法模式的，如果需要自定义同步器一般的方式是这样(模板方法模式很经典的一个应用)： 使用者继承AbstractQueuedSynchronizer并重写指定的方法。(这些重写方法很简单，无非是对于共享资源state的获取和释放)将AQS组合在自定义同步组件的实现中，并调用其模板方法，而这些模板方法会调用使用者重写的方法。 AQS使用了模板方法模式，自定义同步器时需要重写下面几个AQS提供的模板方法： 12345isHeldExclusively()//该线程是否正在独占资源。只有用到condition才需要去实现它。tryAcquire(int)//独占方式。尝试获取资源，成功则返回true，失败则返回false。tryRelease(int)//独占方式。尝试释放资源，成功则返回true，失败则返回false。tryAcquireShared(int)//共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。tryReleaseShared(int)//共享方式。尝试释放资源，成功则返回true，失败则返回false。 默认情况下，每个方法都抛出 UnsupportedOperationException。 这些方法的实现必须是内部线程安全的，并且通常应该简短而不是阻塞。AQS类中的其他方法都是final ，所以无法被其他类使用，只有这几个方法可以被其他类使用。 以ReentrantLock为例，state初始化为0，表示未锁定状态。A线程lock()时，会调用tryAcquire()独占该锁并将state+1。此后，其他线程再tryAcquire()时就会失败，直到A线程unlock()到state=0(即释放锁)为止，其它线程才有机会获取该锁。当然，释放锁之前，A线程自己是可以重复获取此锁的(state会累加)，这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证state是能回到零态的。 AbstractQueuedSynchronizer数据结构AbstractQueuedSynchronizer类底层的数据结构是使用CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列(虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系)。AQS是将每条请求共享资源的线程封装成一个CLH锁队列的一个结点(Node)来实现锁的分配。其中Sync queue，即同步队列，是双向链表，包括head结点和tail结点，head结点主要用作后续的调度。而Condition queue不是必须的，其是一个单向链表，只有当使用Condition时，才会存在此单向链表。并且可能会有多个Condition queue。 AbstractQueuedSynchronizer源码分析类的继承关系AbstractQueuedSynchronizer继承自AbstractOwnableSynchronizer抽象类，并且实现了Serializable接口，可以进行序列化。 1public abstract class AbstractQueuedSynchronizer extends AbstractOwnableSynchronizer implements java.io.Serializable 其中AbstractOwnableSynchronizer抽象类的源码如下: 12345678910111213141516171819public abstract class AbstractOwnableSynchronizer implements java.io.Serializable &#123; // 版本序列号 private static final long serialVersionUID = 3737899427754241961L; // 构造方法 protected AbstractOwnableSynchronizer() &#123; &#125; // 独占模式下的线程 private transient Thread exclusiveOwnerThread; // 设置独占线程 protected final void setExclusiveOwnerThread(Thread thread) &#123; exclusiveOwnerThread = thread; &#125; // 获取独占线程 protected final Thread getExclusiveOwnerThread() &#123; return exclusiveOwnerThread; &#125;&#125; AbstractOwnableSynchronizer抽象类中，可以设置独占资源线程和获取独占资源线程。分别为setExclusiveOwnerThread与getExclusiveOwnerThread方法，这两个方法会被子类调用。 AbstractQueuedSynchronizer类有两个内部类，分别为Node类与ConditionObject类。下面分别做介绍。 类的内部类 - Node类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859static final class Node &#123; // 模式，分为共享与独占 // 共享模式 static final Node SHARED = new Node(); // 独占模式 static final Node EXCLUSIVE = null; // 结点状态 // CANCELLED，值为1，表示当前的线程被取消 // SIGNAL，值为-1，表示当前节点的后继节点包含的线程需要运行，也就是unpark // CONDITION，值为-2，表示当前节点在等待condition，也就是在condition队列中 // PROPAGATE，值为-3，表示当前场景下后续的acquireShared能够得以执行 // 值为0，表示当前节点在sync队列中，等待着获取锁 static final int CANCELLED = 1; static final int SIGNAL = -1; static final int CONDITION = -2; static final int PROPAGATE = -3; // 结点状态 volatile int waitStatus; // 前驱结点 volatile Node prev; // 后继结点 volatile Node next; // 结点所对应的线程 volatile Thread thread; // 下一个等待者 Node nextWaiter; // 结点是否在共享模式下等待 final boolean isShared() &#123; return nextWaiter == SHARED; &#125; // 获取前驱结点，若前驱结点为空，抛出异常 final Node predecessor() throws NullPointerException &#123; // 保存前驱结点 Node p = prev; if (p == null) // 前驱结点为空，抛出异常 throw new NullPointerException(); else // 前驱结点不为空，返回 return p; &#125; // 无参构造方法 Node() &#123; // Used to establish initial head or SHARED marker &#125; // 构造方法 Node(Thread thread, Node mode) &#123; // Used by addWaiter this.nextWaiter = mode; this.thread = thread; &#125; // 构造方法 Node(Thread thread, int waitStatus) &#123; // Used by Condition this.waitStatus = waitStatus; this.thread = thread; &#125;&#125; 每个线程被阻塞的线程都会被封装成一个Node结点，放入队列。每个节点包含了一个Thread类型的引用，并且每个节点都存在一个状态，具体状态如下。 CANCELLED：值为1，表示当前的线程被取消。 SIGNAL：值为-1，表示当前节点的后继节点包含的线程需要运行，需要进行unpark操作。 CONDITION：值为-2，表示当前节点在等待condition，也就是在condition queue中。 PROPAGATE：值为-3，表示当前场景下后续的acquireShared能够得以执行。 值为0：表示当前节点在sync queue中，等待着获取锁。 类的内部类 - ConditionObject类这个类有点长，耐心看下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472// 内部类public class ConditionObject implements Condition, java.io.Serializable &#123; // 版本号 private static final long serialVersionUID = 1173984872572414699L; /** First node of condition queue. */ // condition队列的头结点 private transient Node firstWaiter; /** Last node of condition queue. */ // condition队列的尾结点 private transient Node lastWaiter; /** * Creates a new &#123;@code ConditionObject&#125; instance. */ // 构造方法 public ConditionObject() &#123; &#125; // Internal methods /** * Adds a new waiter to wait queue. * @return its new wait node */ // 添加新的waiter到wait队列 private Node addConditionWaiter() &#123; // 保存尾结点 Node t = lastWaiter; // If lastWaiter is cancelled, clean out. if (t != null &amp;&amp; t.waitStatus != Node.CONDITION) &#123; // 尾结点不为空，并且尾结点的状态不为CONDITION // 清除状态为CONDITION的结点 unlinkCancelledWaiters(); // 将最后一个结点重新赋值给t t = lastWaiter; &#125; // 新建一个结点 Node node = new Node(Thread.currentThread(), Node.CONDITION); if (t == null) // 尾结点为空 // 设置condition队列的头结点 firstWaiter = node; else // 尾结点不为空 // 设置为节点的nextWaiter域为node结点 t.nextWaiter = node; // 更新condition队列的尾结点 lastWaiter = node; return node; &#125; /** * Removes and transfers nodes until hit non-cancelled one or * null. Split out from signal in part to encourage compilers * to inline the case of no waiters. * @param first (non-null) the first node on condition queue */ private void doSignal(Node first) &#123; // 循环 do &#123; if ( (firstWaiter = first.nextWaiter) == null) // 该节点的nextWaiter为空 // 设置尾结点为空 lastWaiter = null; // 设置first结点的nextWaiter域 first.nextWaiter = null; &#125; while (!transferForSignal(first) &amp;&amp; (first = firstWaiter) != null); // 将结点从condition队列转移到sync队列失败并且condition队列中的头结点不为空，一直循环 &#125; /** * Removes and transfers all nodes. * @param first (non-null) the first node on condition queue */ private void doSignalAll(Node first) &#123; // condition队列的头结点尾结点都设置为空 lastWaiter = firstWaiter = null; // 循环 do &#123; // 获取first结点的nextWaiter域结点 Node next = first.nextWaiter; // 设置first结点的nextWaiter域为空 first.nextWaiter = null; // 将first结点从condition队列转移到sync队列 transferForSignal(first); // 重新设置first first = next; &#125; while (first != null); &#125; /** * Unlinks cancelled waiter nodes from condition queue. * Called only while holding lock. This is called when * cancellation occurred during condition wait, and upon * insertion of a new waiter when lastWaiter is seen to have * been cancelled. This method is needed to avoid garbage * retention in the absence of signals. So even though it may * require a full traversal, it comes into play only when * timeouts or cancellations occur in the absence of * signals. It traverses all nodes rather than stopping at a * particular target to unlink all pointers to garbage nodes * without requiring many re-traversals during cancellation * storms. */ // 从condition队列中清除状态为CANCEL的结点 private void unlinkCancelledWaiters() &#123; // 保存condition队列头结点 Node t = firstWaiter; Node trail = null; while (t != null) &#123; // t不为空 // 下一个结点 Node next = t.nextWaiter; if (t.waitStatus != Node.CONDITION) &#123; // t结点的状态不为CONDTION状态 // 设置t节点的额nextWaiter域为空 t.nextWaiter = null; if (trail == null) // trail为空 // 重新设置condition队列的头结点 firstWaiter = next; else // trail不为空 // 设置trail结点的nextWaiter域为next结点 trail.nextWaiter = next; if (next == null) // next结点为空 // 设置condition队列的尾结点 lastWaiter = trail; &#125; else // t结点的状态为CONDTION状态 // 设置trail结点 trail = t; // 设置t结点 t = next; &#125; &#125; // public methods /** * Moves the longest-waiting thread, if one exists, from the * wait queue for this condition to the wait queue for the * owning lock. * * @throws IllegalMonitorStateException if &#123;@link #isHeldExclusively&#125; * returns &#123;@code false&#125; */ // 唤醒一个等待线程。如果所有的线程都在等待此条件，则选择其中的一个唤醒。在从 await 返回之前，该线程必须重新获取锁。 public final void signal() &#123; if (!isHeldExclusively()) // 不被当前线程独占，抛出异常 throw new IllegalMonitorStateException(); // 保存condition队列头结点 Node first = firstWaiter; if (first != null) // 头结点不为空 // 唤醒一个等待线程 doSignal(first); &#125; /** * Moves all threads from the wait queue for this condition to * the wait queue for the owning lock. * * @throws IllegalMonitorStateException if &#123;@link #isHeldExclusively&#125; * returns &#123;@code false&#125; */ // 唤醒所有等待线程。如果所有的线程都在等待此条件，则唤醒所有线程。在从 await 返回之前，每个线程都必须重新获取锁。 public final void signalAll() &#123; if (!isHeldExclusively()) // 不被当前线程独占，抛出异常 throw new IllegalMonitorStateException(); // 保存condition队列头结点 Node first = firstWaiter; if (first != null) // 头结点不为空 // 唤醒所有等待线程 doSignalAll(first); &#125; /** * Implements uninterruptible condition wait. * &lt;ol&gt; * &lt;li&gt; Save lock state returned by &#123;@link #getState&#125;. * &lt;li&gt; Invoke &#123;@link #release&#125; with saved state as argument, * throwing IllegalMonitorStateException if it fails. * &lt;li&gt; Block until signalled. * &lt;li&gt; Reacquire by invoking specialized version of * &#123;@link #acquire&#125; with saved state as argument. * &lt;/ol&gt; */ // 等待，当前线程在接到信号之前一直处于等待状态，不响应中断 public final void awaitUninterruptibly() &#123; // 添加一个结点到等待队列 Node node = addConditionWaiter(); // 获取释放的状态 int savedState = fullyRelease(node); boolean interrupted = false; while (!isOnSyncQueue(node)) &#123; // // 阻塞当前线程 LockSupport.park(this); if (Thread.interrupted()) // 当前线程被中断 // 设置interrupted状态 interrupted = true; &#125; if (acquireQueued(node, savedState) || interrupted) // selfInterrupt(); &#125; /* * For interruptible waits, we need to track whether to throw * InterruptedException, if interrupted while blocked on * condition, versus reinterrupt current thread, if * interrupted while blocked waiting to re-acquire. */ /** Mode meaning to reinterrupt on exit from wait */ private static final int REINTERRUPT = 1; /** Mode meaning to throw InterruptedException on exit from wait */ private static final int THROW_IE = -1; /** * Checks for interrupt, returning THROW_IE if interrupted * before signalled, REINTERRUPT if after signalled, or * 0 if not interrupted. */ private int checkInterruptWhileWaiting(Node node) &#123; return Thread.interrupted() ? (transferAfterCancelledWait(node) ? THROW_IE : REINTERRUPT) : 0; &#125; /** * Throws InterruptedException, reinterrupts current thread, or * does nothing, depending on mode. */ private void reportInterruptAfterWait(int interruptMode) throws InterruptedException &#123; if (interruptMode == THROW_IE) throw new InterruptedException(); else if (interruptMode == REINTERRUPT) selfInterrupt(); &#125; /** * Implements interruptible condition wait. * &lt;ol&gt; * &lt;li&gt; If current thread is interrupted, throw InterruptedException. * &lt;li&gt; Save lock state returned by &#123;@link #getState&#125;. * &lt;li&gt; Invoke &#123;@link #release&#125; with saved state as argument, * throwing IllegalMonitorStateException if it fails. * &lt;li&gt; Block until signalled or interrupted. * &lt;li&gt; Reacquire by invoking specialized version of * &#123;@link #acquire&#125; with saved state as argument. * &lt;li&gt; If interrupted while blocked in step 4, throw InterruptedException. * &lt;/ol&gt; */ // // 等待，当前线程在接到信号或被中断之前一直处于等待状态 public final void await() throws InterruptedException &#123; if (Thread.interrupted()) // 当前线程被中断，抛出异常 throw new InterruptedException(); // 在wait队列上添加一个结点 Node node = addConditionWaiter(); // int savedState = fullyRelease(node); int interruptMode = 0; while (!isOnSyncQueue(node)) &#123; // 阻塞当前线程 LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) // 检查结点等待时的中断类型 break; &#125; if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode); &#125; /** * Implements timed condition wait. * &lt;ol&gt; * &lt;li&gt; If current thread is interrupted, throw InterruptedException. * &lt;li&gt; Save lock state returned by &#123;@link #getState&#125;. * &lt;li&gt; Invoke &#123;@link #release&#125; with saved state as argument, * throwing IllegalMonitorStateException if it fails. * &lt;li&gt; Block until signalled, interrupted, or timed out. * &lt;li&gt; Reacquire by invoking specialized version of * &#123;@link #acquire&#125; with saved state as argument. * &lt;li&gt; If interrupted while blocked in step 4, throw InterruptedException. * &lt;/ol&gt; */ // 等待，当前线程在接到信号、被中断或到达指定等待时间之前一直处于等待状态 public final long awaitNanos(long nanosTimeout) throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); Node node = addConditionWaiter(); int savedState = fullyRelease(node); final long deadline = System.nanoTime() + nanosTimeout; int interruptMode = 0; while (!isOnSyncQueue(node)) &#123; if (nanosTimeout &lt;= 0L) &#123; transferAfterCancelledWait(node); break; &#125; if (nanosTimeout &gt;= spinForTimeoutThreshold) LockSupport.parkNanos(this, nanosTimeout); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; nanosTimeout = deadline - System.nanoTime(); &#125; if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode); return deadline - System.nanoTime(); &#125; /** * Implements absolute timed condition wait. * &lt;ol&gt; * &lt;li&gt; If current thread is interrupted, throw InterruptedException. * &lt;li&gt; Save lock state returned by &#123;@link #getState&#125;. * &lt;li&gt; Invoke &#123;@link #release&#125; with saved state as argument, * throwing IllegalMonitorStateException if it fails. * &lt;li&gt; Block until signalled, interrupted, or timed out. * &lt;li&gt; Reacquire by invoking specialized version of * &#123;@link #acquire&#125; with saved state as argument. * &lt;li&gt; If interrupted while blocked in step 4, throw InterruptedException. * &lt;li&gt; If timed out while blocked in step 4, return false, else true. * &lt;/ol&gt; */ // 等待，当前线程在接到信号、被中断或到达指定最后期限之前一直处于等待状态 public final boolean awaitUntil(Date deadline) throws InterruptedException &#123; long abstime = deadline.getTime(); if (Thread.interrupted()) throw new InterruptedException(); Node node = addConditionWaiter(); int savedState = fullyRelease(node); boolean timedout = false; int interruptMode = 0; while (!isOnSyncQueue(node)) &#123; if (System.currentTimeMillis() &gt; abstime) &#123; timedout = transferAfterCancelledWait(node); break; &#125; LockSupport.parkUntil(this, abstime); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode); return !timedout; &#125; /** * Implements timed condition wait. * &lt;ol&gt; * &lt;li&gt; If current thread is interrupted, throw InterruptedException. * &lt;li&gt; Save lock state returned by &#123;@link #getState&#125;. * &lt;li&gt; Invoke &#123;@link #release&#125; with saved state as argument, * throwing IllegalMonitorStateException if it fails. * &lt;li&gt; Block until signalled, interrupted, or timed out. * &lt;li&gt; Reacquire by invoking specialized version of * &#123;@link #acquire&#125; with saved state as argument. * &lt;li&gt; If interrupted while blocked in step 4, throw InterruptedException. * &lt;li&gt; If timed out while blocked in step 4, return false, else true. * &lt;/ol&gt; */ // 等待，当前线程在接到信号、被中断或到达指定等待时间之前一直处于等待状态。此方法在行为上等效于: awaitNanos(unit.toNanos(time)) &gt; 0 public final boolean await(long time, TimeUnit unit) throws InterruptedException &#123; long nanosTimeout = unit.toNanos(time); if (Thread.interrupted()) throw new InterruptedException(); Node node = addConditionWaiter(); int savedState = fullyRelease(node); final long deadline = System.nanoTime() + nanosTimeout; boolean timedout = false; int interruptMode = 0; while (!isOnSyncQueue(node)) &#123; if (nanosTimeout &lt;= 0L) &#123; timedout = transferAfterCancelledWait(node); break; &#125; if (nanosTimeout &gt;= spinForTimeoutThreshold) LockSupport.parkNanos(this, nanosTimeout); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; nanosTimeout = deadline - System.nanoTime(); &#125; if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode); return !timedout; &#125; // support for instrumentation /** * Returns true if this condition was created by the given * synchronization object. * * @return &#123;@code true&#125; if owned */ final boolean isOwnedBy(AbstractQueuedSynchronizer sync) &#123; return sync == AbstractQueuedSynchronizer.this; &#125; /** * Queries whether any threads are waiting on this condition. * Implements &#123;@link AbstractQueuedSynchronizer#hasWaiters(ConditionObject)&#125;. * * @return &#123;@code true&#125; if there are any waiting threads * @throws IllegalMonitorStateException if &#123;@link #isHeldExclusively&#125; * returns &#123;@code false&#125; */ // 查询是否有正在等待此条件的任何线程 protected final boolean hasWaiters() &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); for (Node w = firstWaiter; w != null; w = w.nextWaiter) &#123; if (w.waitStatus == Node.CONDITION) return true; &#125; return false; &#125; /** * Returns an estimate of the number of threads waiting on * this condition. * Implements &#123;@link AbstractQueuedSynchronizer#getWaitQueueLength(ConditionObject)&#125;. * * @return the estimated number of waiting threads * @throws IllegalMonitorStateException if &#123;@link #isHeldExclusively&#125; * returns &#123;@code false&#125; */ // 返回正在等待此条件的线程数估计值 protected final int getWaitQueueLength() &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); int n = 0; for (Node w = firstWaiter; w != null; w = w.nextWaiter) &#123; if (w.waitStatus == Node.CONDITION) ++n; &#125; return n; &#125; /** * Returns a collection containing those threads that may be * waiting on this Condition. * Implements &#123;@link AbstractQueuedSynchronizer#getWaitingThreads(ConditionObject)&#125;. * * @return the collection of threads * @throws IllegalMonitorStateException if &#123;@link #isHeldExclusively&#125; * returns &#123;@code false&#125; */ // 返回包含那些可能正在等待此条件的线程集合 protected final Collection&lt;Thread&gt; getWaitingThreads() &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); ArrayList&lt;Thread&gt; list = new ArrayList&lt;Thread&gt;(); for (Node w = firstWaiter; w != null; w = w.nextWaiter) &#123; if (w.waitStatus == Node.CONDITION) &#123; Thread t = w.thread; if (t != null) list.add(t); &#125; &#125; return list; &#125;&#125; 此类实现了Condition接口，Condition接口定义了条件操作规范，具体如下 1234567891011121314151617181920212223public interface Condition &#123; // 等待，当前线程在接到信号或被中断之前一直处于等待状态 void await() throws InterruptedException; // 等待，当前线程在接到信号之前一直处于等待状态，不响应中断 void awaitUninterruptibly(); //等待，当前线程在接到信号、被中断或到达指定等待时间之前一直处于等待状态 long awaitNanos(long nanosTimeout) throws InterruptedException; // 等待，当前线程在接到信号、被中断或到达指定等待时间之前一直处于等待状态。此方法在行为上等效于: awaitNanos(unit.toNanos(time)) &gt; 0 boolean await(long time, TimeUnit unit) throws InterruptedException; // 等待，当前线程在接到信号、被中断或到达指定最后期限之前一直处于等待状态 boolean awaitUntil(Date deadline) throws InterruptedException; // 唤醒一个等待线程。如果所有的线程都在等待此条件，则选择其中的一个唤醒。在从 await 返回之前，该线程必须重新获取锁。 void signal(); // 唤醒所有等待线程。如果所有的线程都在等待此条件，则唤醒所有线程。在从 await 返回之前，每个线程都必须重新获取锁。 void signalAll();&#125; Condition接口中定义了await、signal方法，用来等待条件、释放条件。之后会详细分析CondtionObject的源码。 类的属性属性中包含了头结点head，尾结点tail，状态state、自旋时间spinForTimeoutThreshold，还有AbstractQueuedSynchronizer抽象的属性在内存中的偏移地址，通过该偏移地址，可以获取和设置该属性的值，同时还包括一个静态初始化块，用于加载内存偏移地址。 123456789101112131415161718192021222324252627282930313233343536373839404142public abstract class AbstractQueuedSynchronizer extends AbstractOwnableSynchronizer implements java.io.Serializable &#123; // 版本号 private static final long serialVersionUID = 7373984972572414691L; // 头结点 private transient volatile Node head; // 尾结点 private transient volatile Node tail; // 状态 private volatile int state; // 自旋时间 static final long spinForTimeoutThreshold = 1000L; // Unsafe类实例 private static final Unsafe unsafe = Unsafe.getUnsafe(); // state内存偏移地址 private static final long stateOffset; // head内存偏移地址 private static final long headOffset; // state内存偏移地址 private static final long tailOffset; // tail内存偏移地址 private static final long waitStatusOffset; // next内存偏移地址 private static final long nextOffset; // 静态初始化块 static &#123; try &#123; stateOffset = unsafe.objectFieldOffset (AbstractQueuedSynchronizer.class.getDeclaredField(&quot;state&quot;)); headOffset = unsafe.objectFieldOffset (AbstractQueuedSynchronizer.class.getDeclaredField(&quot;head&quot;)); tailOffset = unsafe.objectFieldOffset (AbstractQueuedSynchronizer.class.getDeclaredField(&quot;tail&quot;)); waitStatusOffset = unsafe.objectFieldOffset (Node.class.getDeclaredField(&quot;waitStatus&quot;)); nextOffset = unsafe.objectFieldOffset (Node.class.getDeclaredField(&quot;next&quot;)); &#125; catch (Exception ex) &#123; throw new Error(ex); &#125; &#125;&#125; 类的构造方法此类构造方法为从抽象构造方法，供子类调用。 1protected AbstractQueuedSynchronizer() &#123; &#125; 类的核心方法 - acquire方法该方法以独占模式获取(资源)，忽略中断，即线程在aquire过程中，中断此线程是无效的。源码如下: 1234public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; 由上述源码可以知道，当一个线程调用acquire时，调用方法流程如下: 首先调用tryAcquire方法，调用此方法的线程会试图在独占模式下获取对象状态。此方法应该查询是否允许它在独占模式下获取对象状态，如果允许，则获取它。在AbstractQueuedSynchronizer源码中默认会抛出一个异常，即需要子类去重写此方法完成自己的逻辑。之后会进行分析。 若tryAcquire失败，则调用addWaiter方法，addWaiter方法完成的功能是将调用此方法的线程封装成为一个结点并放入Sync queue。 调用acquireQueued方法，此方法完成的功能是Sync queue中的结点不断尝试获取资源，若成功，则返回true，否则，返回false。 由于tryAcquire默认实现是抛出异常，所以此时，不进行分析，之后会结合一个例子进行分析。 首先分析addWaiter方法: 12345678910111213141516171819// 添加等待者private Node addWaiter(Node mode) &#123; // 新生成一个结点，默认为独占模式 Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure // 保存尾结点 Node pred = tail; if (pred != null) &#123; // 尾结点不为空，即已经被初始化 // 将node结点的prev域连接到尾结点 node.prev = pred; if (compareAndSetTail(pred, node)) &#123; // 比较pred是否为尾结点，是则将尾结点设置为node // 设置尾结点的next域为node pred.next = node; return node; // 返回新生成的结点 &#125; &#125; enq(node); // 尾结点为空(即还没有被初始化过)，或者是compareAndSetTail操作失败，则入队列 return node;&#125; addWaiter方法使用快速添加的方式往sync queue尾部添加结点，如果sync queue队列还没有初始化，则会使用enq插入队列中，enq方法源码如下: 123456789101112131415161718private Node enq(final Node node) &#123; for (;;) &#123; // 无限循环，确保结点能够成功入队列 // 保存尾结点 Node t = tail; if (t == null) &#123; // 尾结点为空，即还没被初始化 if (compareAndSetHead(new Node())) // 头结点为空，并设置头结点为新生成的结点 tail = head; // 头结点与尾结点都指向同一个新生结点 &#125; else &#123; // 尾结点不为空，即已经被初始化过 // 将node结点的prev域连接到尾结点 node.prev = t; if (compareAndSetTail(t, node)) &#123; // 比较结点t是否为尾结点，若是则将尾结点设置为node // 设置尾结点的next域为node t.next = node; return t; // 返回尾结点 &#125; &#125; &#125;&#125; enq方法会使用无限循环来确保节点的成功插入。 现在，分析acquireQueue方法。其源码如下: 12345678910111213141516171819202122232425// sync队列中的结点在独占且忽略中断的模式下获取(资源)final boolean acquireQueued(final Node node, int arg) &#123; // 标志 boolean failed = true; try &#123; // 中断标志 boolean interrupted = false; for (;;) &#123; // 无限循环 // 获取node节点的前驱结点 final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; // 前驱为头结点并且成功获得锁 setHead(node); // 设置头结点 p.next = null; // help GC failed = false; // 设置标志 return interrupted; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 首先获取当前节点的前驱节点，如果前驱节点是头结点并且能够获取(资源)，代表该当前节点能够占有锁，设置头结点为当前节点，返回。否则，调用shouldParkAfterFailedAcquire和parkAndCheckInterrupt方法，首先，我们看shouldParkAfterFailedAcquire方法，代码如下: 123456789101112131415161718192021222324252627282930313233// 当获取(资源)失败后，检查并且更新结点状态private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; // 获取前驱结点的状态 int ws = pred.waitStatus; if (ws == Node.SIGNAL) // 状态为SIGNAL，为-1 /* * This node has already set status asking a release * to signal it, so it can safely park. */ // 可以进行park操作 return true; if (ws &gt; 0) &#123; // 表示状态为CANCELLED，为1 /* * Predecessor was cancelled. Skip over predecessors and * indicate retry. */ do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); // 找到pred结点前面最近的一个状态不为CANCELLED的结点 // 赋值pred结点的next域 pred.next = node; &#125; else &#123; // 为PROPAGATE -3 或者是0 表示无状态,(为CONDITION -2时，表示此节点在condition queue中) /* * waitStatus must be 0 or PROPAGATE. Indicate that we * need a signal, but don&#x27;t park yet. Caller will need to * retry to make sure it cannot acquire before parking. */ // 比较并设置前驱结点的状态为SIGNAL compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; // 不能进行park操作 return false;&#125; 只有当该节点的前驱结点的状态为SIGNAL时，才可以对该结点所封装的线程进行park操作。否则，将不能进行park操作。再看parkAndCheckInterrupt方法，源码如下: 123456// 进行park操作并且返回该线程是否被中断private final boolean parkAndCheckInterrupt() &#123; // 在许可可用之前禁用当前线程，并且设置了blocker LockSupport.park(this); return Thread.interrupted(); // 当前线程是否已被中断，并清除中断标记位&#125; parkAndCheckInterrupt方法里的逻辑是首先执行park操作，即禁用当前线程，然后返回该线程是否已经被中断。再看final块中的cancelAcquire方法，其源码如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 取消继续获取(资源)private void cancelAcquire(Node node) &#123; // Ignore if node doesn&#x27;t exist // node为空，返回 if (node == null) return; // 设置node结点的thread为空 node.thread = null; // Skip cancelled predecessors // 保存node的前驱结点 Node pred = node.prev; while (pred.waitStatus &gt; 0) // 找到node前驱结点中第一个状态小于0的结点，即不为CANCELLED状态的结点 node.prev = pred = pred.prev; // predNext is the apparent node to unsplice. CASes below will // fail if not, in which case, we lost race vs another cancel // or signal, so no further action is necessary. // 获取pred结点的下一个结点 Node predNext = pred.next; // Can use unconditional write instead of CAS here. // After this atomic step, other Nodes can skip past us. // Before, we are free of interference from other threads. // 设置node结点的状态为CANCELLED node.waitStatus = Node.CANCELLED; // If we are the tail, remove ourselves. if (node == tail &amp;&amp; compareAndSetTail(node, pred)) &#123; // node结点为尾结点，则设置尾结点为pred结点 // 比较并设置pred结点的next节点为null compareAndSetNext(pred, predNext, null); &#125; else &#123; // node结点不为尾结点，或者比较设置不成功 // If successor needs signal, try to set pred&#x27;s next-link // so it will get one. Otherwise wake it up to propagate. int ws; if (pred != head &amp;&amp; ((ws = pred.waitStatus) == Node.SIGNAL || (ws &lt;= 0 &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp; pred.thread != null) &#123; // (pred结点不为头结点，并且pred结点的状态为SIGNAL)或者 // pred结点状态小于等于0，并且比较并设置等待状态为SIGNAL成功，并且pred结点所封装的线程不为空 // 保存结点的后继 Node next = node.next; if (next != null &amp;&amp; next.waitStatus &lt;= 0) // 后继不为空并且后继的状态小于等于0 compareAndSetNext(pred, predNext, next); // 比较并设置pred.next = next; &#125; else &#123; unparkSuccessor(node); // 释放node的前一个结点 &#125; node.next = node; // help GC &#125;&#125; 该方法完成的功能就是取消当前线程对资源的获取，即设置该结点的状态为CANCELLED，接着我们再看unparkSuccessor方法，源码如下: 123456789101112131415161718192021222324252627282930313233// 释放后继结点private void unparkSuccessor(Node node) &#123; /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ // 获取node结点的等待状态 int ws = node.waitStatus; if (ws &lt; 0) // 状态值小于0，为SIGNAL -1 或 CONDITION -2 或 PROPAGATE -3 // 比较并且设置结点等待状态，设置为0 compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ // 获取node节点的下一个结点 Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123; // 下一个结点为空或者下一个节点的等待状态大于0，即为CANCELLED // s赋值为空 s = null; // 从尾结点开始从后往前开始遍历 for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) // 找到等待状态小于等于0的结点，找到最前的状态小于等于0的结点 // 保存结点 s = t; &#125; if (s != null) // 该结点不为为空，释放许可 LockSupport.unpark(s.thread);&#125; 该方法的作用就是为了释放node节点的后继结点。 对于cancelAcquire与unparkSuccessor方法，如下示意图可以清晰的表示: 其中node为参数，在执行完cancelAcquire方法后的效果就是unpark了s结点所包含的t4线程。 现在，再来看acquireQueued方法的整个的逻辑。逻辑如下: 判断结点的前驱是否为head并且是否成功获取(资源)。 若步骤1均满足，则设置结点为head，之后会判断是否finally模块，然后返回。 若步骤2不满足，则判断是否需要park当前线程，是否需要park当前线程的逻辑是判断结点的前驱结点的状态是否为SIGNAL，若是，则park当前结点，否则，不进行park操作。 若park了当前线程，之后某个线程对本线程unpark后，并且本线程也获得机会运行。那么，将会继续进行步骤①的判断。 类的核心方法 - release方法以独占模式释放对象，其源码如下: 12345678910public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; // 释放成功 // 保存头结点 Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) // 头结点不为空并且头结点状态不为0 unparkSuccessor(h); //释放头结点的后继结点 return true; &#125; return false;&#125; 其中，tryRelease的默认实现是抛出异常，需要具体的子类实现，如果tryRelease成功，那么如果头结点不为空并且头结点的状态不为0，则释放头结点的后继结点，unparkSuccessor方法已经分析过，不再累赘。 对于其他方法我们也可以分析，与前面分析的方法大同小异，所以，不再累赘。 AbstractQueuedSynchronizer示例详解一借助下面示例来分析AbstractQueuedSyncrhonizer内部的工作机制。示例源码如下: 1234567891011121314151617181920212223242526272829import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;class MyThread extends Thread &#123; private Lock lock; public MyThread(String name, Lock lock) &#123; super(name); this.lock = lock; &#125; public void run () &#123; lock.lock(); try &#123; System.out.println(Thread.currentThread() + &quot; running&quot;); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125;public class AbstractQueuedSynchonizerDemo &#123; public static void main(String[] args) &#123; Lock lock = new ReentrantLock(); MyThread t1 = new MyThread(&quot;t1&quot;, lock); MyThread t2 = new MyThread(&quot;t2&quot;, lock); t1.start(); t2.start(); &#125;&#125; 运行结果(可能的一种): 12Thread[t1,5,main] runningThread[t2,5,main] running 结果分析: 从示例可知，线程t1与t2共用了一把锁，即同一个lock。可能会存在如下一种时序。 说明: 首先线程t1先执行lock.lock操作，然后t2执行lock.lock操作，然后t1执行lock.unlock操作，最后t2执行lock.unlock操作。基于这样的时序，分析AbstractQueuedSynchronizer内部的工作机制。 t1线程调用lock.lock方法，其方法调用顺序如下，只给出了主要的方法调用。 说明: 其中，前面的部分表示哪个类，后面是具体的类中的哪个方法，AQS表示AbstractQueuedSynchronizer类，AOS表示AbstractOwnableSynchronizer类。 t2线程调用lock.lock方法，其方法调用顺序如下，只给出了主要的方法调用。 说明: 经过一系列的方法调用，最后达到的状态是禁用t2线程，因为调用了LockSupport.lock。 t1线程调用lock.unlock，其方法调用顺序如下，只给出了主要的方法调用。 说明: t1线程中调用lock.unlock后，经过一系列的调用，最终的状态是释放了许可，因为调用了LockSupport.unpark。这时，t2线程就可以继续运行了。此时，会继续恢复t2线程运行环境，继续执行LockSupport.park后面的语句，即进一步调用如下。 说明: 在上一步调用了LockSupport.unpark后，t2线程恢复运行，则运行parkAndCheckInterrupt，之后，继续运行acquireQueued方法，最后达到的状态是头结点head与尾结点tail均指向了t2线程所在的结点，并且之前的头结点已经从sync队列中断开了。 t2线程调用lock.unlock，其方法调用顺序如下，只给出了主要的方法调用。 说明: t2线程执行lock.unlock后，最终达到的状态还是与之前的状态一样。 AbstractQueuedSynchronizer示例详解二下面我们结合Condition实现生产者与消费者，来进一步分析AbstractQueuedSynchronizer的内部工作机制。 Depot(仓库)类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class Depot &#123; private int size; private int capacity; private Lock lock; private Condition fullCondition; private Condition emptyCondition; public Depot(int capacity) &#123; this.capacity = capacity; lock = new ReentrantLock(); fullCondition = lock.newCondition(); emptyCondition = lock.newCondition(); &#125; public void produce(int no) &#123; lock.lock(); int left = no; try &#123; while (left &gt; 0) &#123; while (size &gt;= capacity) &#123; System.out.println(Thread.currentThread() + &quot; before await&quot;); fullCondition.await(); System.out.println(Thread.currentThread() + &quot; after await&quot;); &#125; int inc = (left + size) &gt; capacity ? (capacity - size) : left; left -= inc; size += inc; System.out.println(&quot;produce = &quot; + inc + &quot;, size = &quot; + size); emptyCondition.signal(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void consume(int no) &#123; lock.lock(); int left = no; try &#123; while (left &gt; 0) &#123; while (size &lt;= 0) &#123; System.out.println(Thread.currentThread() + &quot; before await&quot;); emptyCondition.await(); System.out.println(Thread.currentThread() + &quot; after await&quot;); &#125; int dec = (size - left) &gt; 0 ? left : size; left -= dec; size -= dec; System.out.println(&quot;consume = &quot; + dec + &quot;, size = &quot; + size); fullCondition.signal(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; 测试类 123456789101112131415161718192021222324252627282930313233343536373839404142class Consumer &#123; private Depot depot; public Consumer(Depot depot) &#123; this.depot = depot; &#125; public void consume(int no) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; depot.consume(no); &#125; &#125;, no + &quot; consume thread&quot;).start(); &#125;&#125;class Producer &#123; private Depot depot; public Producer(Depot depot) &#123; this.depot = depot; &#125; public void produce(int no) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; depot.produce(no); &#125; &#125;, no + &quot; produce thread&quot;).start(); &#125;&#125;public class ReentrantLockDemo &#123; public static void main(String[] args) throws InterruptedException &#123; Depot depot = new Depot(500); new Producer(depot).produce(500); new Producer(depot).produce(200); new Consumer(depot).consume(500); new Consumer(depot).consume(200); &#125;&#125; 运行结果(可能的一种): 12345678produce = 500, size = 500Thread[200 produce thread,5,main] before awaitconsume = 500, size = 0Thread[200 consume thread,5,main] before awaitThread[200 produce thread,5,main] after awaitproduce = 200, size = 200Thread[200 consume thread,5,main] after awaitconsume = 200, size = 0 说明: 根据结果，我们猜测一种可能的时序如下 说明: p1代表produce 500的那个线程，p2代表produce 200的那个线程，c1代表consume 500的那个线程，c2代表consume 200的那个线程。 p1线程调用lock.lock，获得锁，继续运行，方法调用顺序在前面已经给出。 p2线程调用lock.lock，由前面的分析可得到如下的最终状态。 说明: p2线程调用lock.lock后，会禁止p2线程的继续运行，因为执行了LockSupport.park操作。 c1线程调用lock.lock，由前面的分析得到如下的最终状态。 说明: 最终c1线程会在sync queue队列的尾部，并且其结点的前驱结点(包含p2的结点)的waitStatus变为了SIGNAL。 c2线程调用lock.lock，由前面的分析得到如下的最终状态。 说明: 最终c1线程会在sync queue队列的尾部，并且其结点的前驱结点(包含c1的结点)的waitStatus变为了SIGNAL。 p1线程执行emptyCondition.signal，其方法调用顺序如下，只给出了主要的方法调用。 说明: AQS.CO表示AbstractQueuedSynchronizer.ConditionObject类。此时调用signal方法不会产生任何其他效果。 p1线程执行lock.unlock，根据前面的分析可知，最终的状态如下。 说明: 此时，p2线程所在的结点为头结点，并且其他两个线程(c1、c2)依旧被禁止，所以，此时p2线程继续运行，执行用户逻辑。 p2线程执行fullCondition.await，其方法调用顺序如下，只给出了主要的方法调用。 说明: 最终到达的状态是新生成了一个结点，包含了p2线程，此结点在condition queue中；并且sync queue中p2线程被禁止了，因为在执行了LockSupport.park操作。从方法一些调用可知，在await操作中线程会释放锁资源，供其他线程获取。同时，head结点后继结点的包含的线程的许可被释放了，故其可以继续运行。由于此时，只有c1线程可以运行，故运行c1。 继续运行c1线程，c1线程由于之前被park了，所以此时恢复，继续之前的步骤，即还是执行前面提到的acquireQueued方法，之后，c1判断自己的前驱结点为head，并且可以获取锁资源，最终到达的状态如下。 说明: 其中，head设置为包含c1线程的结点，c1继续运行。 c1线程执行fullCondtion.signal，其方法调用顺序如下，只给出了主要的方法调用。 说明: signal方法达到的最终结果是将包含p2线程的结点从condition queue中转移到sync queue中，之后condition queue为null，之前的尾结点的状态变为SIGNAL。 c1线程执行lock.unlock操作，根据之前的分析，经历的状态变化如下。 说明: 最终c2线程会获取锁资源，继续运行用户逻辑。 c2线程执行emptyCondition.await，由前面的第七步分析，可知最终的状态如下。 著作权归https://pdai.tech所有。链接：https://www.pdai.tech/md/java/thread/java-thread-x-lock-AbstractQueuedSynchronizer.html 说明: await操作将会生成一个结点放入condition queue中与之前的一个condition queue是不相同的，并且unpark头结点后面的结点，即包含线程p2的结点。 p2线程被unpark，故可以继续运行，经过CPU调度后，p2继续运行，之后p2线程在AQS:await方法中被park，继续AQS.CO:await方法的运行，其方法调用顺序如下，只给出了主要的方法调用。 p2继续运行，执行emptyCondition.signal，根据第九步分析可知，最终到达的状态如下。 说明: 最终，将condition queue中的结点转移到sync queue中，并添加至尾部，condition queue会为空，并且将head的状态设置为SIGNAL。 p2线程执行lock.unlock操作，根据前面的分析可知，最后的到达的状态如下。 说明: unlock操作会释放c2线程的许可，并且将头结点设置为c2线程所在的结点。 c2线程继续运行，执行fullCondition. signal，由于此时fullCondition的condition queue已经不存在任何结点了，故其不会产生作用。 c2执行lock.unlock，由于c2是sync队列中最后一个结点，故其不会再调用unparkSuccessor了，直接返回true。即整个流程就完成了。 AbstractQueuedSynchronizer总结对于AbstractQueuedSynchronizer的分析，最核心的就是sync queue的分析。 每一个结点都是由前一个结点唤醒 当结点发现前驱结点是head并且尝试获取成功，则会轮到该线程运行。 condition queue中的结点向sync queue中转移是通过signal操作完成的。 当结点的状态为SIGNAL时，表示后面的结点需要运行。 参考JUC锁: 锁核心类AQS详解","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"AQS","slug":"AQS","permalink":"https://xmmarlowe.github.io/tags/AQS/"}],"author":"Marlowe"},{"title":"try-catch与throw的区别","slug":"Java/try-catch与throw的区别","date":"2021-08-19T14:21:34.000Z","updated":"2021-08-21T03:29:42.338Z","comments":true,"path":"2021/08/19/Java/try-catch与throw的区别/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/19/Java/try-catch%E4%B8%8Ethrow%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"简述try-catch与throw的区别。","text":"简述try-catch与throw的区别。 在 java 中，捕获处理一般有2种方式，throws 和 try-catch。 区别在于： 要么声明异常，也就是在方法名后面加上throws exception_name,…, 方法本身只是抛出异常，由函数调用者来捕获异常。 若产生异常，异常会沿着调用栈下移，一直找到与之匹配的处理方法，若到达调用栈底仍未找到，程序终止； 要么捕获异常。通过try-catch方法，catch子句中放置处理异常的语句； 对于会觉得会有异常抛出的程序块，用try{}包住，然后用catch来抓住这个异常，在catch中对异常做处理， 在try中如果有异常的话，程序会转到catch而不会中断，通常这两个是配合使用的,如果你不想因为程序有错，而抛出一大堆异常的话，你就把该程序try起来，try和catch只能获取程序运行时引发的异常，而throw语句可以引发明确的异常，程序到了throw语句这后就立即停止，不会执行后面的程序； 部分常见异常如下： 算术异常类：ArithmeticExecption 空指针异常类：NullPointerException 类型强制转换异常：ClassCastException 数组负下标异常：NegativeArrayException 数组下标越界异常：ArrayIndexOutOfBoundsException 违背安全原则异常：SecturityException 文件已结束异常：EOFException 文件未找到异常：FileNotFoundException 字符串转换为数字异常：NumberFormatException 操作数据库异常：SQLException 输入输出异常：IOException 方法未找到异常：NoSuchMethodException","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"异常","slug":"异常","permalink":"https://xmmarlowe.github.io/tags/%E5%BC%82%E5%B8%B8/"}],"author":"Marlowe"},{"title":"Maven打包之Fat/Shade/Shadow Jar","slug":"个人项目/Maven打包之Fat-Shade-Shadow-Jar","date":"2021-08-17T15:40:32.000Z","updated":"2021-08-21T03:29:42.392Z","comments":true,"path":"2021/08/17/个人项目/Maven打包之Fat-Shade-Shadow-Jar/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/17/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/Maven%E6%89%93%E5%8C%85%E4%B9%8BFat-Shade-Shadow-Jar/","excerpt":"","text":"问题 java -jar xxx.jar 执行时找不到主类 或 ClassNotFoundException 引入的 jar 包中依赖冲突了怎么办，有多个版本的依赖类 正文有没有想过 Spring Boot 项目可以直接打包成一个 jar 包还能直接通过java -jar运行，而当我们自己去写一个小项目去打成 jar 包的时候，要么就是打成的 jar 包运行的时候报 “找不到主类”， 要么就是报一个依赖的Class找不到，这是为什么呢？ Spring Boot 的项目里面，如果是 maven 项目的话里面会有一个 spring-boot-maven-plugin，起作用的就是这个maven 插件了，原因就是它做了2件很重要的事： 生成 Manifest 文件并配置了项目的启动类，也就是 Main-Class 将项目依赖的类库一起打包进最后的 jar 里面去 这样构建出来的 jar 包就可以直接通过 java -jar 直接运行了。 说完 spring-boot-maven-plugin 这个插件再说什么是 shade jar? 我第一次看到 shade 这个词的时候一脸懵逼，机器翻译出来叫”(为避免强光照射而)遮挡,遮住(眼睛)”，shade jar 是指将 jar 包和它的依赖包一起打包到一起，并能够将依赖包重命名（relocate - 重定位）. 如何理解这里的重定位\\重命名（relocate）呢？ 这里要随便提一下 Java 的类加载机制，Class loader 查找在用户的 classpath 下的类文件，在 classpath 下可能有不同的文件夹以及 jars, zip 文件等，包含了 class 文件和一些资源文件。 而 Class loader 要加载 classpath 的类文件的时候是通过这个类的唯一限定名（fully qualified name,FQN）来标识它的， 比如 com.kay.mylib.CoreClass. 这样就会有另外一个问题了，如果在 classpath 下的2个jar包内都有同一个类，并且这个类的唯一限定名是一样的，但是2个类的版本可能不一样，class loader 只会使用找到的第一个类（按照classpath 内的顺序）！ 这在 Java 中叫做 shadowing，由于存在2个不同版本的同一个类，其中一个被另外一个给覆盖了。 依赖解析的问题 在很多项目中都会依赖一些第三方的jar包，然而这些第三方的 jar 包很有可能就使用了另外一个相同的库，最要命的是这个库的版本还不一致，新的版本与老版本提供的方法还不兼容，这个时候我们使用的 maven 也好，gradle 也好就会发现依赖冲突了，它会说比如org.example:some-lib:1.0.0使用的org.example:core-lib:1.0.0 与 另外一个库 org.example:other-lib:1.0.0 使用的 org.example:core-lib:1.0.0 冲突了，此时我们的依赖树可能是这样的： 最简单的方式当然就是直接 exclude 一个了，但是如果这2个版本不兼容的话，比如 some-lib 使用的是一个在 core-lib:2.0.0 中已经废弃删除了的方法，那这种方式就不可行了。 理想的情况下，core-lib 的开发者应该会保证他开发的库应该是后向兼容的，也就是说即使升级了，那老的版本应该也是能工作的，这时候他可能会： 升级他的包的 ArtifactId, 比如从 org.example:core-lib 到org.example:core-lib2, 修改包名，比如 org.example.corelib 修改成 org.example.corelib2 这样的话新旧版本的库就可以同时存在了，依赖冲突也就解决了。 但是吧，这样开发者的工作就大了，虽然有一些成功的例子，比如 log4j -&gt; log4j2 , 但是大多数开发者并没有这样做。 那么有没有其他的办法呢？那就是Shade Shade Jar上文说到，shade jar 是指将 jar 包和它的依赖包一起打包到一起，并能够将依赖包重命名（relocate - 重定位）， 说到这里，大家应该都明白重命名是怎么回事了，就是修改依赖的包名，这样就不会出现依赖冲突的情况了，这个时候上面的依赖树可能就变成下面这样了： some-lib 和 other-lib 分别将自己所依赖的库打包到自己的 jar 里面，并且通过重命名之后2个 jar 没有依赖冲突了，这个时候他们各自依赖的库如果在 my-app要引用的话会是什么样子呢？ 比如在原始的org.example:core-lib中有一个类是org.example.corelib.CoreClass ,此时它在 some-lib 的包名可能已经被重命名为somelib.shading.org.example.corelib.CoreClass, 在other-lib 中的同一个类可能命名为otherlib.shading.org.example.corelib.CoreClass(重命名规则取决于我们配置的规则)，这样2个类的唯一限定名是完全不一样的，也就没有冲突一说了。 上面这种方式是 some-lib 和other-lib 的开发者将自己的依赖shade到自己的 jar 包中, 还有一种方式是我们作为 jar 的使用者将 some-lib 和other-lib 分别打包成 shade jar : 这两种方式的区别是，我们站在my-app的角度来 shade jar 的话，就相当于把 some-lib 和 other-lib 里面的 class 重新复制和重命名了一份（包括它们的依赖，有些依赖可能并不需要封装，因为它们比较稳定，不会发生大的变化），如果 jar 包的开发者来做 shade jar 的话，他们就能选择性的进行 shade ，把一些可能会导致兼容问题的依赖库给作为 shade jar 的一部分。 有一些工具能够帮助我们完成 shade jar 的工作，比如 maven 和 gradle 都有这样的工具: Apache Maven Shade Plugin – Introduction Apache Maven Assembly Plugin – Usage Gradle Shadow Plugin 具体使用方法可以参考相关的文档。 Shading Best Practices - 最佳实践 使用一个单独的模块来做 shade, 封装jar 和其传递依赖。可以使用一个单独的子模块与其他模块最后分开 使用一个特定于项目和模块的前缀名来做 shade，这样避免与其他 jar 包冲突。比如对于 some-lib 和它的传递依赖，可以使用myapp.shading.somelib 作为前缀，其中的类可能是这样的:myapp.shading.somelib.org.example.CoreClass, 这样就能与本项目的另外一个jar 包（比如other-lib的myapp.shading.otherlib.org.example.CoreClass）的类所区别。 想清楚哪些依赖要做 shade 封装起来，哪些依赖可以不做，尽量减少最后打包出来的依赖比较大. 确保 shade jar 中只包含重写了 package 的类，不要把未重写 package 的类也打包进去了，不然以后遇到这些也要做 shade 的时候会导致问题. 不要将 shaded classes 暴露在编译的 classpath下（compile classpath），即不要让 jar 的使用者使用你封装起来的依赖类，一旦别人用了你重命名只会的类，你以后想更新起来就比较困难了。 为你 shade jar 里面的依赖库选定好指定的版本（包括其传递依赖），不然后面更新版本的话里面的类和接口会有变化。 Shading Drawbacks - 缺点 每一个 shade 的依赖都会增加你最终构建出的 jar 的大小，同时 classpath 的类数量也会更多，在多个版本都存在 classpath 的情况下，也会导致开发者使用的时候感到疑惑 Debug 的时候比较困难，IDE 不知道从哪下载 shaded jar 里面依赖对应的源码, 只会有反编译的代码而没有注释和文档 shading 插件可以在字节码的层面帮忙把对象指向重新命名后对应的包去，但是无法使用反射来动态的加载类信息了。 参考什么是 Fat/Shade/Shadow Jar","categories":[{"name":"个人项目","slug":"个人项目","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://xmmarlowe.github.io/tags/SpringBoot/"},{"name":"Maven","slug":"Maven","permalink":"https://xmmarlowe.github.io/tags/Maven/"}],"author":"Marlowe"},{"title":"多线程之间按顺序调用，实现 A-> B -> C 三个线程启动,AA打印5次，BB打印10次，CC打印15次","slug":"并发/多线程之间按顺序调用，实现-A-B-C-三个线程启动-AA打印5次，BB打印10次，CC打印15次","date":"2021-08-17T14:14:36.000Z","updated":"2021-08-21T03:29:42.401Z","comments":true,"path":"2021/08/17/并发/多线程之间按顺序调用，实现-A-B-C-三个线程启动-AA打印5次，BB打印10次，CC打印15次/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/17/%E5%B9%B6%E5%8F%91/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B9%8B%E9%97%B4%E6%8C%89%E9%A1%BA%E5%BA%8F%E8%B0%83%E7%94%A8%EF%BC%8C%E5%AE%9E%E7%8E%B0-A-B-C-%E4%B8%89%E4%B8%AA%E7%BA%BF%E7%A8%8B%E5%90%AF%E5%8A%A8-AA%E6%89%93%E5%8D%B05%E6%AC%A1%EF%BC%8CBB%E6%89%93%E5%8D%B010%E6%AC%A1%EF%BC%8CCC%E6%89%93%E5%8D%B015%E6%AC%A1/","excerpt":"","text":"实现场景多线程之间按顺序调用，实现 A-&gt; B -&gt; C 三个线程启动，要求如下：AA打印5次，BB打印10次，CC打印15次紧接着AA打印5次，BB打印10次，CC打印15次…来10轮 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129package com.marlowe.demos;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * @program: JavaThreadDemo * @description: 锁绑定等多个条件Condition * 多线程之间按顺序调用，实现 A-&gt; B -&gt; C 三个线程启动，要求如下： * AA打印5次，BB打印10次，CC打印15次 * 紧接着 * AA打印5次，BB打印10次，CC打印15次 * … * 来3轮 * @author: Marlowe * @create: 2021-08-13 14:11 **/public class SyncAndReentrantLockDemo &#123; public static void main(String[] args) &#123; ShareResource shareResource = new ShareResource(); int num = 3; new Thread(() -&gt; &#123; for (int i = 0; i &lt; num; i++) &#123; shareResource.print5(); &#125; &#125;, &quot;A&quot;).start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; num; i++) &#123; shareResource.print10(); &#125; &#125;, &quot;B&quot;).start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; num; i++) &#123; shareResource.print15(); &#125; &#125;, &quot;C&quot;).start(); &#125;&#125;class ShareResource &#123; /** * A:1 B:2 C:3 */ private int number = 1; /** * 创建一个可重入如锁 */ private Lock lock = new ReentrantLock(); private Condition condition1 = lock.newCondition(); private Condition condition2 = lock.newCondition(); private Condition condition3 = lock.newCondition(); public void print5() &#123; lock.lock(); try &#123; // 判断 while (number != 1) &#123; // 不等于1，需要等待 condition1.await(); &#125; // 干活 for (int i = 0; i &lt; 5; i++) &#123; System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + number + &quot;\\t&quot; + i); &#125; // 唤醒 （干完活后，需要通知C线程执行） number = 2; // 通知2号去干活了 condition2.signal(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void print10() &#123; lock.lock(); try &#123; // 判断 while (number != 2) &#123; // 不等于2，需要等待 condition2.await(); &#125; // 干活 for (int i = 0; i &lt; 10; i++) &#123; System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + number + &quot;\\t&quot; + i); &#125; // 唤醒 （干完活后，需要通知C线程执行） number = 3; // 通知3号去干活了 condition3.signal(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void print15() &#123; lock.lock(); try &#123; // 判断 while (number != 3) &#123; // 不等于3，需要等待 condition3.await(); &#125; // 干活 for (int i = 0; i &lt; 15; i++) &#123; System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + number + &quot;\\t&quot; + i); &#125; // 唤醒 （干完活后，需要通知C线程执行） number = 1; // 通知1号去干活了 condition1.signal(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; 结果： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394&quot;D:\\Program Files\\Java\\jdk1.8\\bin\\java.exe&quot; &quot;-javaagent:D:\\Program Files\\IntelliJ IDEA 2021.1.3\\lib\\idea_rt.jar&#x3D;56930:D:\\Program Files\\IntelliJ IDEA 2021.1.3\\bin&quot; -Dfile.encoding&#x3D;UTF-8 -classpath &quot;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\charsets.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\deploy.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\ext\\access-bridge-64.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\ext\\cldrdata.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\ext\\dnsns.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\ext\\jaccess.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\ext\\jfxrt.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\ext\\localedata.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\ext\\nashorn.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\ext\\sunec.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\ext\\sunjce_provider.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\ext\\sunmscapi.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\ext\\sunpkcs11.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\ext\\zipfs.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\javaws.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\jce.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\jfr.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\jfxswt.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\jsse.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\management-agent.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\plugin.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\resources.jar;D:\\Program Files\\Java\\jdk1.8\\jre\\lib\\rt.jar;D:\\IDE_Project\\JavaLearning\\JavaThreadDemo\\target\\classes&quot; com.marlowe.demos.SyncAndReentrantLockDemoA 1 0A 1 1A 1 2A 1 3A 1 4B 2 0B 2 1B 2 2B 2 3B 2 4B 2 5B 2 6B 2 7B 2 8B 2 9C 3 0C 3 1C 3 2C 3 3C 3 4C 3 5C 3 6C 3 7C 3 8C 3 9C 3 10C 3 11C 3 12C 3 13C 3 14A 1 0A 1 1A 1 2A 1 3A 1 4B 2 0B 2 1B 2 2B 2 3B 2 4B 2 5B 2 6B 2 7B 2 8B 2 9C 3 0C 3 1C 3 2C 3 3C 3 4C 3 5C 3 6C 3 7C 3 8C 3 9C 3 10C 3 11C 3 12C 3 13C 3 14A 1 0A 1 1A 1 2A 1 3A 1 4B 2 0B 2 1B 2 2B 2 3B 2 4B 2 5B 2 6B 2 7B 2 8B 2 9C 3 0C 3 1C 3 2C 3 3C 3 4C 3 5C 3 6C 3 7C 3 8C 3 9C 3 10C 3 11C 3 12C 3 13C 3 14Process finished with exit code 0","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"线程","slug":"线程","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B/"},{"name":"Condition","slug":"Condition","permalink":"https://xmmarlowe.github.io/tags/Condition/"}],"author":"Marlowe"},{"title":"CPU占用过高定位分析思路","slug":"操作系统/CPU占用过高定位分析思路","date":"2021-08-17T13:59:34.000Z","updated":"2021-08-21T03:29:42.405Z","comments":true,"path":"2021/08/17/操作系统/CPU占用过高定位分析思路/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/17/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E5%8D%A0%E7%94%A8%E8%BF%87%E9%AB%98%E5%AE%9A%E4%BD%8D%E5%88%86%E6%9E%90%E6%80%9D%E8%B7%AF/","excerpt":"CPU占用过高分析思路","text":"CPU占用过高分析思路 1.先用top命令找出CPU占比最高的 2.ps -ef或者jps进一步定位，得知是一个怎么样的一个后台程序（ps -ef|grep java|grep -v grep） 3.定位到具体线程或者代码 3.1 ps -mp 进程编号 -o THREAD,tid,time -m显示所有的线程 -p pid进程使用cpu的时间 -o 该参数后是用户自定义格式 4.将需要的线程ID转换为16进制格式（英文小写格式） 4.1、printf “%x\\n” 有问题的线程ID 5.jstack 进程id | grep tid(16进制线程ID小写英文) -A60 前60行","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"CPU","slug":"CPU","permalink":"https://xmmarlowe.github.io/tags/CPU/"}],"author":"Marlowe"},{"title":"JVM调优(一)","slug":"Java/JVM调优(一)","date":"2021-08-16T14:00:15.000Z","updated":"2021-08-21T07:52:37.559Z","comments":true,"path":"2021/08/16/Java/JVM调优(一)/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/16/Java/JVM%E8%B0%83%E4%BC%98(%E4%B8%80)/","excerpt":"JVM调优相关步骤","text":"JVM调优相关步骤 性能调优性能调优包含多个层次，比如：架构调优、代码调优、JVM调优、数据库调优、操作系统调优等。 架构调优和代码调优是JVM调优的基础，其中架构调优是对系统影响最大的。 性能调优基本上按照以下步骤进行：明确优化目标、发现性能瓶颈、性能调优、通过监控及数据统计工具获得数据、确认是否达到目标。 何时进行JVM调优?遇到以下情况，就需要考虑进行JVM调优了： Heap内存（老年代）持续上涨达到设置的最大内存值； Full GC 次数频繁； GC 停顿时间过长（超过1秒）； 应用出现OutOfMemory 等内存异常； 应用中有使用本地缓存且占用大量内存空间； 系统吞吐量与响应性能不高或下降。 JVM调优的基本原则JVM调优是一个手段，但并不一定所有问题都可以通过JVM进行调优解决，因此，在进行JVM调优时，我们要遵循一些原则： 大多数的Java应用不需要进行JVM优化； 大多数导致GC问题的原因是代码层面的问题导致的（代码层面）； 上线之前，应先考虑将机器的JVM参数设置到最优； 减少创建对象的数量（代码层面）； 减少使用全局变量和大对象（代码层面）； 优先架构调优和代码调优，JVM优化是不得已的手段（代码、架构层面）； 分析GC情况优化代码比优化JVM参数更好（代码层面）； 通过以上原则，我们发现，其实最有效的优化手段是架构和代码层面的优化，而JVM优化则是最后不得已的手段，也可以说是对服务器配置的最后一次“压榨”。 JVM调优目标调优的最终目的都是为了令应用程序使用最小的硬件消耗来承载更大的吞吐。jvm调优主要是针对垃圾收集器的收集性能优化，令运行在虚拟机上的应用能够使用更少的内存以及延迟获取更大的吞吐量。 延迟：GC低停顿和GC低频率； 低内存占用； 高吞吐量; 其中，任何一个属性性能的提高，几乎都是以牺牲其他属性性能的损为代价的，不可兼得。具体根据在业务中的重要性确定。 JVM调优量化目标下面展示了一些JVM调优的量化目标参考实例： Heap 内存使用率 &lt;= 70%; Old generation内存使用率&lt;= 70%; avgpause &lt;= 1秒; Full gc 次数0 或 avg pause interval &gt;= 24小时 ; 注意：不同应用的JVM调优量化目标是不一样的。 JVM调优的步骤一般情况下，JVM调优可通过以下步骤进行： 分析GC日志及dump文件，判断是否需要优化，确定瓶颈问题点； 确定JVM调优量化目标； 确定JVM调优参数（根据历史JVM参数来调整）； 依次调优内存、延迟、吞吐量等指标； 对比观察调优前后的差异； 不断的分析和调整，直到找到合适的JVM参数配置； 找到最合适的参数，将这些参数应用到所有服务器，并进行后续跟踪。 以上操作步骤中，某些步骤是需要多次不断迭代完成的。一般是从满足程序的内存使用需求开始的，之后是时间延迟的要求，最后才是吞吐量的要求，要基于这个步骤来不断优化，每一个步骤都是进行下一步的基础，不可逆行之。 JVM参数JVM调优最重要的工具就是JVM参数了。先来了解一下JVM参数相关内容。-XX 参数被称为不稳定参数，此类参数的设置很容易引起JVM 性能上的差异，使JVM存在极大的不稳定性。如果此类参数设置合理将大大提高JVM的性能及稳定性。不稳定参数语法规则包含以下内容。 布尔类型参数值： -XX:+ ‘+’表示启用该选项 -XX:- ‘-‘表示关闭该选项 数字类型参数值： -XX:=给选项设置一个数字类型值，可跟随单位，例如：’m’或’M’表示兆字节;’k’或’K’千字节;’g’或’G’千兆字节。32K与32768是相同大小的。 字符串类型参数值： -XX:=给选项设置一个字符串类型值，通常用于指定一个文件、路径或一系列命令列表。例如：-XX:HeapDumpPath=./dump.core JVM参数解析及调优比如以下参数示例： 1-Xmx4g –Xms4g –Xmn1200m –Xss512k -XX:NewRatio=4 -XX:SurvivorRatio=8 -XX:PermSize=100m -XX:MaxPermSize=256m -XX:MaxTenuringThreshold=15 上面为Java7及以前版本的示例，在Java8中永久代的参数-XX:PermSize和-XX：MaxPermSize已经失效。 参数解析： -Xmx4g：堆内存最大值为4GB。 -Xms4g：初始化堆内存大小为4GB。 -Xmn1200m：设置年轻代大小为1200MB。增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8。 -Xss512k：设置每个线程的堆栈大小。JDK5.0以后每个线程堆栈大小为1MB，以前每个线程堆栈大小为256K。应根据应用线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右。 -XX:NewRatio=4：设置年轻代（包括Eden和两个Survivor区）与年老代的比值（除去持久代）。设置为4，则年轻代与年老代所占比值为1：4，年轻代占整个堆栈的1/5 -XX:SurvivorRatio=8：设置年轻代中Eden区与Survivor区的大小比值。设置为8，则两个Survivor区与一个Eden区的比值为2:8，一个Survivor区占整个年轻代的1/10 -XX:PermSize=100m：初始化永久代大小为100MB。 -XX:MaxPermSize=256m：设置持久代大小为256MB。 -XX:MaxTenuringThreshold=15：设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概论。 新生代、老生代、永久代的参数，如果不进行指定，虚拟机会自动选择合适的值，同时也会基于系统的开销自动调整。 可调优参数： -Xms：初始化堆内存大小，默认为物理内存的1/64(小于1GB)。 -Xmx：堆内存最大值。默认(MaxHeapFreeRatio参数可以调整)空余堆内存大于70%时，JVM会减少堆直到-Xms的最小限制。 -Xmn：新生代大小，包括Eden区与2个Survivor区。 -XX:SurvivorRatio=1：Eden区与一个Survivor区比值为1:1。 -XX:MaxDirectMemorySize=1G：直接内存。报java.lang.OutOfMemoryError: Direct buffer memory异常可以上调这个值。 -XX:+DisableExplicitGC：禁止运行期显式地调用System.gc()来触发fulll GC。 注意: Java RMI的定时GC触发机制可通过配置-Dsun.rmi.dgc.server.gcInterval=86400来控制触发的时间。 -XX:CMSInitiatingOccupancyFraction=60：老年代内存回收阈值，默认值为68。 -XX:ConcGCThreads=4：CMS垃圾回收器并行线程线，推荐值为CPU核心数。 -XX:ParallelGCThreads=8：新生代并行收集器的线程数。 -XX:MaxTenuringThreshold=10：设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概论。 -XX:CMSFullGCsBeforeCompaction=4：指定进行多少次fullGC之后，进行tenured区 内存空间压缩。 -XX:CMSMaxAbortablePrecleanTime=500：当abortable-preclean预清理阶段执行达到这个时间时就会结束。 在设置的时候，如果关注性能开销的话，应尽量把永久代的初始值与最大值设置为同一值，因为永久代的大小调整需要进行FullGC才能实现。 内存优化示例当JVM运行稳定之后，触发了FullGC我们一般会拿到如下信息: 以上gc日志中，在发生fullGC之时，整个应用的堆占用以及GC时间。为了更加精确需多次收集，计算平均值。或者是采用耗时最长的一次FullGC来进行估算。上图中，老年代空间占用在93168kb（约93MB），以此定为老年代空间的活跃数据。则其他堆空间的分配，基于以下规则来进行。 java heap：参数-Xms和-Xmx，建议扩大至3-4倍FullGC后的老年代空间占用。 永久代：-XX:PermSize和-XX:MaxPermSize，建议扩大至1.2-1.5倍FullGc后的永久带空间占用。 新生代：-Xmn，建议扩大至1-1.5倍FullGC之后的老年代空间占用。 老年代：2-3倍FullGC后的老年代空间占用。 基于以上规则，则对参数定义如下： 1java -Xms373m -Xmx373m -Xmn140m -XX:PermSize=5m -XX:MaxPermSize=5m 延迟优化示例对延迟性优化，首先需要了解延迟性需求及可调优的指标有哪些。 应用程序可接受的平均停滞时间: 此时间与测量的Minor GC持续时间进行比较。可接受的Minor GC频率：Minor GC的频率与可容忍的值进行比较。 可接受的最大停顿时间:最大停顿时间与最差情况下FullGC的持续时间进行比较。 可接受的最大停顿发生的频率：基本就是FullGC的频率。 其中，平均停滞时间和最大停顿时间，对用户体验最为重要。对于上面的指标，相关数据采集包括：MinorGC的持续时间、统计MinorGC的次数、FullGC的最差持续时间、最差情况下，FullGC的频率。 如上图，Minor GC的平均持续时间0.069秒，MinorGC的频率为0.389秒一次。 新生代空间越大，Minor GC的GC时间越长，频率越低。如果想减少其持续时长，就需要减少其空间大小。如果想减小其频率，就需要加大其空间大小。 这里以减少了新生代空间10%的大小，来减小延迟时间。在此过程中，应该保持老年代和持代的大小不变化。调优后的参数如下变化: 1java -Xms359m -Xmx359m -Xmn126m -XX:PermSize=5m -XX:MaxPermSize=5m 吞吐量调优吞吐量调优主要是基于应用程序的吞吐量要求而来的，应用程序应该有一个综合的吞吐指标，这个指标基于整个应用的需求和测试而衍生出来的。 评估当前吞吐量和目标差距是否巨大，如果在20%左右，可以修改参数，加大内存，再次从头调试，如果巨大就需要从整个应用层面来考虑，设计以及目标是否一致了，重新评估吞吐目标。 对于垃圾收集器来说，提升吞吐量的性能调优的目标就是尽可能避免或者很少发生FullGC或者Stop-The-World压缩式垃圾收集（CMS），因为这两种方式都会造成应用程序吞吐降低。尽量在MinorGC 阶段回收更多的对象，避免对象提升过快到老年代。 调优工具借助GCViewer日志分析工具，可以非常直观地分析出待调优点。可从以下几方面来分析： Memory,分析Totalheap、Tenuredheap、Youngheap内存占用率及其他指标，理论上内存占用率越小越好； Pause，分析Gc pause、Fullgc pause、Total pause三个大项中各指标，理论上GC次数越少越好，GC时长越小越好； 参考JVM性能调优详解","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://xmmarlowe.github.io/tags/JVM/"}],"author":"Marlowe"},{"title":"GitHub搜索新体验","slug":"常用工具/GitHub搜索新体验","date":"2021-08-15T13:59:53.000Z","updated":"2021-08-21T03:29:42.398Z","comments":true,"path":"2021/08/15/常用工具/GitHub搜索新体验/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/15/%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7/GitHub%E6%90%9C%E7%B4%A2%E6%96%B0%E4%BD%93%E9%AA%8C/","excerpt":"简单介绍GitHub一些常用操作","text":"简单介绍GitHub一些常用操作 GitHub操作之in限制搜索in关键词限制搜索范围： 公式 ：xxx(关键词) in:name或description或readme xxx in:name 项目名包含xxx的 xxx in:description 项目描述包含xxx的 xxx in:readme 项目的readme文件中包含xxx的组合使用 组合使用 搜索项目名或者readme中包含秒杀的项目 xxx in:name,readme GitHub作之star和fork范围搜索 公式： xxx关键字 stars 通配符 :&gt; 或者 :&gt;= 区间范围数字： stars:数字1…数字2 案例 查找stars数大于等于5000的springboot项目：springboot stars:&gt;=5000 查找forks数在1000~2000之间的springboot项目：springboot forks:1000…5000 组合使用 查找star大于1000，fork数在500到1000的springboot项目：springboot stars:&gt;1000 forks:500…1000 GitHub操作之awesome搜索 公式：awesome 关键字：awesome系列，一般用来收集学习、工具、书籍类相关的项目 搜索优秀的redis相关的项目，包括框架，教程等 awesome redis GitHub操作之#L数字 一行：地址后面紧跟 #L10 https://github.com/abc/abc/pom.xml#L13 多行：地址后面紧跟 #Lx - #Ln https://github.com/moxi624/abc/abc/pom.xml#L13-L30 GitHub操作之T搜索在项目仓库下按键盘T，进行项目内搜索 更多github快捷键 GitHub操作之搜索区域活跃用户 location：地区 language：语言 例如：location:beijing language:java","categories":[{"name":"常用工具","slug":"常用工具","permalink":"https://xmmarlowe.github.io/categories/%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"GitHub","slug":"GitHub","permalink":"https://xmmarlowe.github.io/tags/GitHub/"}],"author":"Marlowe"},{"title":"Spring Gateway、Zuul、Nginx的区别","slug":"Spring/Spring Gateway、Zuul、Nginx的区别","date":"2021-08-06T12:25:04.000Z","updated":"2021-08-21T06:38:10.020Z","comments":true,"path":"2021/08/06/Spring/Spring Gateway、Zuul、Nginx的区别/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/06/Spring/Spring%20Gateway%E3%80%81Zuul%E3%80%81Nginx%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"Spring Cloud GateWay学习之微服务网关Zuul、Gateway、nginx的区别。","text":"Spring Cloud GateWay学习之微服务网关Zuul、Gateway、nginx的区别。 Spring Cloud GateWay 是什么网关，Spring Cloud Gateway是Spring官方基于Spring 5.0，Spring Boot 2.0和Project Reactor等技术开发的网关，Spring Cloud Gateway旨在为微服务架构提供一种简单而有效的统一的API路由管理方式。Spring Cloud Gateway作为Spring Cloud生态系中的网关，目标是替代ZUUL，其不仅提供统一的路由方式，并且基于Filter链的方式提供了网关基本的功能，例如：安全，监控/埋点，和限流等。 GateWay整体流程 Spring Cloud GateWay 作用 协议转换，路由转发 流量聚合，对流量进行监控，日志输出 作为整个系统的前端工程，对流量进行控制，有限流的作用 作为系统的前端边界，外部流量只能通过网关才能访问系统 可以在网关层做权限判断 可以在网关层做缓存 微服务网关Zuul、Gateway、nginx的区别zuul 是Netflix的，早期在微服务中使用较广泛，是基于servlet实现的，阻塞式的api，不支持长连接。 只能同步，不支持异步。 不依赖spring-webflux，可以扩展至其他微服务框架。 内部没有实现限流、负载均衡，其负载均衡的实现是采用 Ribbon + Eureka 来实现本地负载均衡。 代码简单，注释多，易理解。 Gateway 是springcloud自己研制的微服务网关，是基于Spring5构建，，能够实现响应式非阻塞式的Api，支持长连接。 支持异步。 功能更强大，内部实现了限流、负载均衡等，扩展性也更强。Spring Cloud Gateway明确的区分了 Router 和 Filter，并且一个很大的特点是内置了非常多的开箱即用功能，并且都可以通过 SpringBoot 配置或者手工编码链式调用来使用。 依赖于spring-webflux，仅适合于Spring Cloud套件。 代码复杂，注释少。 nginx C语言编写，采用服务器实现负载均衡，高性能的HTTP和反向代理web服务器。 Nginx适合于服务器端负载均衡,Zuul和gateway 是本地负载均衡，适合微服务中实现网关。Spring Cloud Gateway 天然适合Spring Cloud 生态。 Nginx在微服务中的地位最后简单聊一下nginx，在过去几年微服务架构还没有流行的日子里，nginx已经得到了广大开发者的认可，其性能高、扩展性强、可以灵活利用lua脚本构建插件的特点让人没有抵抗力。 有一个能满足我所有需求还很方便我扩展的东西，还免费，凭啥不用？？ 但是，如今很多微服务架构的项目中不会选择nginx，我认为原因有以下几点：微服务框架一般来说是配套的，集成起来更容易如今微服务架构中，仅有很少的公司会面对无法解决的性能瓶颈，而他们也不会因此使用nginx，而是选择开发一套适合自己的微服务框架spring boot对于一些模板引擎如FreeMarker、themleaf的支持是非常好的，很多应用还没有达到动、静态文件分离的地步，对nginx的需求程度并不大。无论如何，nginx作为一个好用的组件，最终使不使用它都是由业务来驱动的，只要它能为我们方便的解决问题，那用它又有何不可呢？ 小结通过总结发现，在微服务架构中网关上的选择，最好的方式是使用现在比较成熟的Spring Cloud套件，其提供了Spring Cloud Gateway网关，或是结合公司情况来开发一套适合自己的微服务套件，至少从网关上可以看出来其内部实现并不难，同时也比较期待开源项目Nacos、Spring Cloud Alibaba 建设情况，期待它能构建一个高活跃社区的、稳定的、适合中国特色（大流量、高并发）的微服务基础架构。 竞争是发展的催化剂。在这个网关服务层出不穷的年代，各公司都铆足力气打造自己的网关产品，尽量让自己的产品成为用户的第一选择。而广大开发者也在享受这样的红利，使用高性能的网关来开发自己的应用。作为广大开发者的一员，我们欣然接受这样良性竞争的出现，并且也乐于尝试市面上出现的任何新产品，谁也说不准某一个产品以后就会成为优选的代名词。虽然从现在网关的性能差距看来，后发优势明显，但在可预见的将来，各网关迟早会到达性能瓶颈，在性能差距不大并且产品稳定之后，就会有各种差异化特性出现。而等到网关产品进入百舸争流的时代之后，用户就可以不再根据性能，而是根据自己的需求选择适合的网关服务了。 参考Spring Cloud GateWay学习之微服务网关Zuul、Gateway、nginx的区别和Spring Cloud GateWay的使用。","categories":[{"name":"个人项目","slug":"个人项目","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/tags/Spring/"},{"name":"Gateway","slug":"Gateway","permalink":"https://xmmarlowe.github.io/tags/Gateway/"},{"name":"Nginx","slug":"Nginx","permalink":"https://xmmarlowe.github.io/tags/Nginx/"}],"author":"Marlowe"},{"title":"分布式系统唯一ID生成方案","slug":"分布式/分布式系统唯一ID生成方案","date":"2021-08-05T15:23:18.000Z","updated":"2021-08-21T03:49:46.455Z","comments":true,"path":"2021/08/05/分布式/分布式系统唯一ID生成方案/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/05/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%94%AF%E4%B8%80ID%E7%94%9F%E6%88%90%E6%96%B9%E6%A1%88/","excerpt":"在互联网的业务系统中，涉及到各种各样的ID，如在支付系统中就会有支付ID、退款ID等。那一般生成ID都有哪些解决方案呢？特别是在复杂的分布式系统业务场景中，我们应该采用哪种适合自己的解决方案是十分重要的。下面我们一一来列举一下，不一定全部适合，这些解决方案仅供你参考，或许对你有用。","text":"在互联网的业务系统中，涉及到各种各样的ID，如在支付系统中就会有支付ID、退款ID等。那一般生成ID都有哪些解决方案呢？特别是在复杂的分布式系统业务场景中，我们应该采用哪种适合自己的解决方案是十分重要的。下面我们一一来列举一下，不一定全部适合，这些解决方案仅供你参考，或许对你有用。 分布式ID的特性 唯一性：确保生成的ID是全网唯一的。 有序递增性：确保生成的ID是对于某个用户或者业务是按一定的数字有序递增的。 高可用性：确保任何时候都能正确的生成ID。 带时间：ID里面包含时间，一眼扫过去就知道哪天的交易。 分布式ID的生成方案1. UUID核心思想：结合机器的网卡（基于名字空间/名字的散列值MD5/SHA1）、当地时间（基于时间戳&amp;时钟序列）、一个随记数来生成UUID。 其结构如下：aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee（即包含32个16进制数字，以连字号-分为五段，最终形成“8-4-4-4-12”的36个字符的字符串，即32个英数字母+4个连字号）。例如：550e8400-e29b-41d4-a716-446655440000 优点：① 本地生成，没有网络消耗，生成简单，没有高可用风险。 缺点：① 不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用。② 信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露，这个漏洞曾被用于寻找梅丽莎病毒的制作者位置。③ 无序查询效率低：由于生成的UUID是无序不可读的字符串，所以其查询效率低。 2. 数据库自增ID核心思想：使用数据库的id自增策略（如: Mysql的auto_increment）。 优点：① 简单，天然有序。 缺点：① 并发性不好。② 数据库写压力大。③ 数据库故障后不可使用。④ 存在数量泄露风险。 针对以上缺点，有以下几种优化方案： 数据库水平拆分，设置不同的初始值和相同的自增步长 核心思想：将数据库进行水平拆分，每个数据库设置不同的初始值和相同的自增步长。 如图所示，可保证每台数据库生成的ID是不冲突的，但这种固定步长的方式也会带来扩容的问题，很容易想到当扩容时会出现无ID初始值可分的窘境，解决方案有：① 根据扩容考虑决定步长。② 增加其他位标记区分扩容。这其实都是在需求与方案间的权衡，根据需求来选择最适合的方式。 3. 批量缓存自增ID核心思想：如果使用单台机器做ID生成，可以避免固定步长带来的扩容问题（方案1的缺点）。具体做法是：每次批量生成一批ID给不同的机器去慢慢消费，这样数据库的压力也会减小到N分之一，且故障后可坚持一段时间。 如图所示，但这种做法的缺点是服务器重启、单点故障会造成ID不连续。 4. Redis生成IDRedis的所有命令操作都是单线程的，本身提供像 incr 和 increby 这样的自增原子命令，所以能保证生成的 ID 肯定是唯一有序的。 优点：不依赖于数据库，灵活方便，且性能优于数据库；数字ID天然排序，对分页或者需要排序的结果很有帮助。 缺点：如果系统中没有Redis，还需要引入新的组件，增加系统复杂度；需要编码和配置的工作量比较大。 考虑到单节点的性能瓶颈，可以使用 Redis 集群来获取更高的吞吐量。假如一个集群中有5台 Redis。可以初始化每台 Redis 的值分别是1, 2, 3, 4, 5，然后步长都是 5。各个 Redis 生成的 ID 为： 12345A：1, 6, 11, 16, 21B：2, 7, 12, 17, 22C：3, 8, 13, 18, 23D：4, 9, 14, 19, 24E：5, 10, 15, 20, 25 随便负载到哪个机确定好，未来很难做修改。步长和初始值一定需要事先确定。使用 Redis 集群也可以方式单点故障的问题。 另外，比较适合使用 Redis 来生成每天从0开始的流水号。比如订单号 = 日期 + 当日自增长号。可以每天在 Redis 中生成一个 Key ，使用 INCR 进行累加。 5. Twitter的snowflake算法Twitter 利用 zookeeper 实现了一个全局ID生成的服务 Snowflake：https://github.com/twitter/snowflake 如上图的所示，Twitter 的 Snowflake 算法由下面几部分组成： 1位符号位： 由于 long 类型在 java 中带符号的，最高位为符号位，正数为 0，负数为 1，且实际系统中所使用的ID一般都是正数，所以最高位为 0。 41位时间戳（毫秒级）： 需要注意的是此处的 41 位时间戳并非存储当前时间的时间戳，而是存储时间戳的差值（当前时间戳 - 起始时间戳），这里的起始时间戳一般是ID生成器开始使用的时间戳，由程序来指定，所以41位毫秒时间戳最多可以使用 (1 &lt;&lt; 41) / (1000x60x60x24x365) = 69年。 10位数据机器位： 包括5位数据标识位和5位机器标识位，这10位决定了分布式系统中最多可以部署 1 &lt;&lt; 10 = 1024 s个节点。超过这个数量，生成的ID就有可能会冲突。 12位毫秒内的序列： 这 12 位计数支持每个节点每毫秒（同一台机器，同一时刻）最多生成 1 &lt;&lt; 12 = 4096个ID 加起来刚好64位，为一个Long型。 优点：高性能，低延迟，按时间有序，一般不会造成ID碰撞 缺点：需要独立的开发和部署，依赖于机器的时钟 简单实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119public class IdWorker &#123; /** * 起始时间戳 2017-04-01 */ private final long epoch = 1491004800000L; /** * 机器ID所占的位数 */ private final long workerIdBits = 5L; /** * 数据标识ID所占的位数 */ private final long dataCenterIdBits = 5L; /** * 支持的最大机器ID,结果是31 */ private final long maxWorkerId = ~(-1L &lt;&lt; workerIdBits); /** * 支持的最大数据标识ID,结果是31 */ private final long maxDataCenterId = ~(-1 &lt;&lt; dataCenterIdBits); /** * 毫秒内序列在id中所占的位数 */ private final long sequenceBits = 12L; /** * 机器ID向左移12位 */ private final long workerIdShift = sequenceBits; /** * 数据标识ID向左移17(12+5)位 */ private final long dataCenterIdShift = sequenceBits + workerIdBits; /** * 时间戳向左移22(12+5+5)位 */ private final long timestampShift = sequenceBits + workerIdBits + dataCenterIdBits; /** * 生成序列的掩码，这里为4095 (0b111111111111=0xfff=4095) */ private final long sequenceMask = ~(-1L &lt;&lt; sequenceBits); /** * 数据标识ID（0～31） */ private long dataCenterId; /** * 机器ID（0～31） */ private long workerId; /** * 毫秒内序列（0～4095） */ private long sequence; /** * 上次生成ID的时间戳 */ private long lastTimestamp = -1L; public IdWorker(long dataCenterId, long workerId) &#123; if (dataCenterId &gt; maxDataCenterId || dataCenterId &lt; 0) &#123; throw new IllegalArgumentException(String.format(&quot;dataCenterId can&#x27;t be greater than %d or less than 0&quot;, maxDataCenterId)); &#125; if (workerId &gt; maxWorkerId || workerId &lt; 0) &#123; throw new IllegalArgumentException(String.format(&quot;worker Id can&#x27;t be greater than %d or less than 0&quot;, maxWorkerId)); &#125; this.dataCenterId = dataCenterId; this.workerId = workerId; &#125; /** * 获得下一个ID (该方法是线程安全的) * @return snowflakeId */ public synchronized long nextId() &#123; long timestamp = timeGen(); //如果当前时间小于上一次ID生成的时间戳,说明系统时钟回退过,这个时候应当抛出异常 if (timestamp &lt; lastTimestamp) &#123; throw new RuntimeException(String.format(&quot;Clock moved backwards. Refusing to generate id for %d milliseconds&quot;, lastTimestamp - timestamp)); &#125; //如果是同一时间生成的，则进行毫秒内序列 if (timestamp == lastTimestamp) &#123; sequence = (sequence + 1) &amp; sequenceMask; //毫秒内序列溢出 if (sequence == 0) &#123; //阻塞到下一个毫秒,获得新的时间戳 timestamp = nextMillis(lastTimestamp); &#125; &#125; else &#123;//时间戳改变，毫秒内序列重置 sequence = 0L; &#125; lastTimestamp = timestamp; //移位并通过按位或运算拼到一起组成64位的ID return ((timestamp - epoch) &lt;&lt; timestampShift) | (dataCenterId &lt;&lt; dataCenterIdShift) | (workerId &lt;&lt; workerIdShift) | sequence; &#125; /** * 返回以毫秒为单位的当前时间 * @return 当前时间(毫秒) */ protected long timeGen() &#123; return System.currentTimeMillis(); &#125; /** * 阻塞到下一个毫秒，直到获得新的时间戳 * @param lastTimestamp 上次生成ID的时间截 * @return 当前时间戳 */ protected long nextMillis(long lastTimestamp) &#123; long timestamp = timeGen(); while (timestamp &lt;= lastTimestamp) &#123; timestamp = lastTimestamp; &#125; return timestamp; &#125; &#125; 6. 百度UidGeneratorUidGenerator是百度开源的分布式ID生成器，基于于snowflake算法的实现，看起来感觉还行。不过，国内开源的项目维护性真是担忧。 具体可以参考官网说明：https://github.com/baidu/uid-generator/blob/master/README.zh_cn.md 7. 美团LeafLeaf 是美团开源的分布式ID生成器，能保证全局唯一性、趋势递增、单调递增、信息安全，里面也提到了几种分布式方案的对比，但也需要依赖关系数据库、Zookeeper等中间件。 具体可以参考官网说明：https://tech.meituan.com/MT_Leaf.html 参考Leaf——美团点评分布式ID生成系统分布式唯一ID的几种生成方案一口气说出9种分布式ID生成方式，面试官有点懵了","categories":[{"name":"分布式","slug":"分布式","permalink":"https://xmmarlowe.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"ID","slug":"ID","permalink":"https://xmmarlowe.github.io/tags/ID/"},{"name":"分布式","slug":"分布式","permalink":"https://xmmarlowe.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"author":"Marlowe"},{"title":"SpringBoot整合微信支付","slug":"个人项目/SpringBoot整合微信支付","date":"2021-08-05T00:30:44.000Z","updated":"2021-08-24T15:24:38.046Z","comments":true,"path":"2021/08/05/个人项目/SpringBoot整合微信支付/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/05/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/SpringBoot%E6%95%B4%E5%90%88%E5%BE%AE%E4%BF%A1%E6%94%AF%E4%BB%98/","excerpt":"//TODO","text":"//TODO","categories":[{"name":"个人项目","slug":"个人项目","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://xmmarlowe.github.io/tags/SpringBoot/"},{"name":"WeChat","slug":"WeChat","permalink":"https://xmmarlowe.github.io/tags/WeChat/"},{"name":"Payment","slug":"Payment","permalink":"https://xmmarlowe.github.io/tags/Payment/"}],"author":"Marlowe"},{"title":"SpringBoot整合微信登陆","slug":"个人项目/SpringBoot整合微信登陆","date":"2021-08-05T00:27:34.000Z","updated":"2021-08-24T15:24:32.688Z","comments":true,"path":"2021/08/05/个人项目/SpringBoot整合微信登陆/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/05/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/SpringBoot%E6%95%B4%E5%90%88%E5%BE%AE%E4%BF%A1%E7%99%BB%E9%99%86/","excerpt":"//TODO","text":"//TODO","categories":[{"name":"个人项目","slug":"个人项目","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://xmmarlowe.github.io/tags/SpringBoot/"},{"name":"WeChat","slug":"WeChat","permalink":"https://xmmarlowe.github.io/tags/WeChat/"},{"name":"Login","slug":"Login","permalink":"https://xmmarlowe.github.io/tags/Login/"}],"author":"Marlowe"},{"title":"Shiro原理及执行流程","slug":"权限/Shiro原理及执行流程","date":"2021-08-03T11:48:05.000Z","updated":"2021-08-21T03:29:42.444Z","comments":true,"path":"2021/08/03/权限/Shiro原理及执行流程/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/03/%E6%9D%83%E9%99%90/Shiro%E5%8E%9F%E7%90%86%E5%8F%8A%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/","excerpt":"Shiro 是一个功能强大且易于使用的Java安全框架，它执行身份验证、授权、加密和会话管理。使用Shiro易于理解的API，您可以快速轻松地保护任何应用程序—从最小的移动应用程序到最大的web和企业应用程序。","text":"Shiro 是一个功能强大且易于使用的Java安全框架，它执行身份验证、授权、加密和会话管理。使用Shiro易于理解的API，您可以快速轻松地保护任何应用程序—从最小的移动应用程序到最大的web和企业应用程序。 Shiro的核心架构 1、SubjectSubject即主体，外部应用与subject进行交互，subject记录了当前操作用户，将用户的概念理解为当前操作的主体，可能是一个通过浏览器请求的用户，也可能是一个运行的程序。 Subject在shiro中是一个接口，接口中定义了很多认证授相关的方法，外部程序通过subject进行认证授，而subject是通过SecurityManager安全管理器进行认证授权 2、SecurityManagerSecurityManager即安全管理器，对全部的subject进行安全管理，它是shiro的核心，负责对所有的subject进行安全管理。通过SecurityManager可以完成subject的认证、授权等，实质上SecurityManager是通过Authenticator进行认证，通过Authorizer进行授权，通过SessionManager进行会话管理等。 SecurityManager是一个接口，继承了Authenticator, Authorizer, SessionManager这三个接口。 3、AuthenticatorAuthenticator即认证器，对用户身份进行认证，Authenticator是一个接口，shiro提供ModularRealmAuthenticator实现类，通过ModularRealmAuthenticator基本上可以满足大多数需求，也可以自定义认证器。 4、AuthorizerAuthorizer即授权器，用户通过认证器认证通过，在访问功能时需要通过授权器判断用户是否有此功能的操作权限。 5、RealmRealm即领域，相当于datasource数据源，securityManager进行安全认证需要通过Realm获取用户权限数据，比如：如果用户身份数据在数据库那么realm就需要从数据库获取用户身份信息。 ​ 注意：不要把realm理解成只是从数据源取数据，在realm中还有认证授权校验的相关的代码。 6、SessionManagersessionManager即会话管理，shiro框架定义了一套会话管理，它不依赖web容器的session，所以shiro可以使用在非web应用上，也可以将分布式应用的会话集中在一点管理，此特性可使它实现单点登录。 7、SessionDAOSessionDAO即会话dao，是对session会话操作的一套接口，比如要将session存储到数据库，可以通过jdbc将会话存储到数据库。 8、CacheManagerCacheManager即缓存管理，将用户权限数据存储在缓存，这样可以提高性能。 9、Cryptography​ Cryptography即密码管理，shiro提供了一套加密/解密的组件，方便开发。比如提供常用的散列、加/解密等功能。 Shiro中的认证1、认证身份认证，就是判断一个用户是否为合法用户的处理过程。最常用的简单身份认证方式是系统通过核对用户输入的用户名和口令，看其是否与系统中存储的该用户的用户名和口令一致，来判断用户身份是否正确。 2、shiro中认证的关键对象 Subject：主体 访问系统的用户，主体可以是用户、程序等，进行认证的都称为主体； Principal：身份信息 是主体（subject）进行身份认证的标识，标识必须具有唯一性，如用户名、手机号、邮箱地址等，一个主体可以有多个身份，但是必须有一个主身份（Primary Principal）。 credential：凭证信息 是只有主体自己知道的安全信息，如密码、证书等。 3、认证流程 Shiro中的授权1、授权授权，即访问控制，控制谁能访问哪些资源。主体进行身份认证后需要分配权限方可访问系统的资源，对于某些资源没有权限是无法访问的。 2、关键对象授权可简单理解为who对what(which)进行How操作： Who，即主体（Subject），主体需要访问系统中的资源。 What，即资源（Resource)，如系统菜单、页面、按钮、类方法、系统商品信息等。资源包括资源类型和资源实例，比如商品信息为资源类型，类型为t01的商品为资源实例，编号为001的商品信息也属于资源实例。 How，权限/许可（Permission)，规定了主体对资源的操作许可，权限离开资源没有意义，如用户查询权限、用户添加权限、某个类方法的调用权限、编号为001用户的修改权限等，通过权限可知主体对哪些资源都有哪些操作许可。 3、授权流程","categories":[{"name":"权限","slug":"权限","permalink":"https://xmmarlowe.github.io/categories/%E6%9D%83%E9%99%90/"}],"tags":[{"name":"Shiro","slug":"Shiro","permalink":"https://xmmarlowe.github.io/tags/Shiro/"}],"author":"Marlowe"},{"title":"登录业务技术选型","slug":"个人项目/登录业务技术选型","date":"2021-08-03T11:47:21.000Z","updated":"2021-08-21T03:29:42.386Z","comments":true,"path":"2021/08/03/个人项目/登录业务技术选型/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/03/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/%E7%99%BB%E5%BD%95%E4%B8%9A%E5%8A%A1%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B/","excerpt":"","text":"单点登录三种常见方式一、session广播机制简单来说，就是session复制 二、cookie + redis 在项目任何一个模块进行登录，登录之后，把数据放到两个地方（1）redis：key：生成唯一随机值（ip、用户id等），value：用户数据。（2）cookie：把redis中生成的key值放到cookie里面 访问项目其他模块，发送请求带着cookie发送，获取cookie值，拿着cookie进行后续操作。（1）从cookie中获取值，到redis中进行查询，根据key进行查询，如果查到数据就是登录状态。 三、使用token 在项目某个模块进行登录，登录之后，按照规则生成字符串，把登录后的用户包含到生成字符串里面，把字符串返回。（1）可以把字符串通过cookie返回（2）把字符串通过地址栏返回 再去访问其他模块，每次访问前端在request header里面添加一个token字段，token则是登陆后返回的字符串。后端通过获取请求头的token字段，根据字符串获取用户信息，如果可以获取到，则就是登录状态。 Jwt生成的字符串包含三部分 Jwt头信息 有效载荷，包含主体信息(用户信息) 签名哈希，防伪标志","categories":[{"name":"个人项目","slug":"个人项目","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"Jwt","slug":"Jwt","permalink":"https://xmmarlowe.github.io/tags/Jwt/"},{"name":"Session","slug":"Session","permalink":"https://xmmarlowe.github.io/tags/Session/"}],"author":"Marlowe"},{"title":"在线教育-微服务项目总结","slug":"个人项目/在线教育-微服务项目总结","date":"2021-08-02T15:34:22.000Z","updated":"2021-08-24T15:19:39.672Z","comments":true,"path":"2021/08/02/个人项目/在线教育-微服务项目总结/","link":"","permalink":"https://xmmarlowe.github.io/2021/08/02/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/%E5%9C%A8%E7%BA%BF%E6%95%99%E8%82%B2-%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/","excerpt":"简单总结一下在线教育项目…","text":"简单总结一下在线教育项目… 一、项目描述在线教育系统，分为前台网站系统和后台运营平台，B2C模式。 前台用户系统包括首页、课程、讲师、订单四大部分，使用了微服务技术架构，前后端分离开发。 后端的主要技术架构是：SpringBoot + SpringCloud + Shiro + MyBatis-Plus + MySQL + Maven + EasyExcel 前端的架构是：Node.js + Vue.js +element-ui+NUXT+ECharts 其他涉及到的中间件包括Redis、阿里云OSS、阿里云视频点播，业务中使用了ECharts做图表展示，使用EasyExcel完成分类批量添加、注册分布式单点登录使用了Jwt。 项目前后端分离开发，后端采用SpringCloud微服务架构，持久层用的是MyBatis-Plus，使用Swagger生成接口文档，接入了阿里云视频点播、阿里云OSS。 系统分为前台用户系统和后台管理系统两部分。 前台用户系统包括：首页、课程、名师、订单。 后台管理系统包括：讲师管理、课程分类管理、课程管理、统计分析、Banner管理、订单管理、权限管理等功能。 二、后台管理系统功能1、登录功能Shiro + Jwt 2、权限管理模块菜单管理 列表，添加，删除，修改 角色管理 列表，添加，删除，修改，批量删除 为角色分配菜单 用户管理 列表，添加，删除，修改，批量删除 为用户分配角色 权限管理表和关系使用最基础的RBAC五张表结构：用户表、角色表、权限表、用户角色表、角色权限表 3、讲师管理模块 条件查询分页列表、添加、修改、删除 4、课程分类模块 添加课程分类 读取Excel里面课程分类数据，添加到数据库中 课程分类列表 使用树形结构显示课程分类列表 5、课程管理模块课程列表功能添加课程 课程发布流程 填写课程基本信息 添加课程大纲（章节和小节） 课程信息确认 如何判断课程是否发布？使用一个status字段，Draft-&gt; 未发布，Normal-&gt;已发布。 课程添加过程中，中途把课程停止添加，重新去添加另外的课程，如何找到之前没有发布的课程，继续进行发布？到课程列表中，根据课程状态找到未发布的课程，点击编辑按钮，可以选择编辑课程基本信息和课程大纲信息，编辑完成后，可以选择对课程是否发布。 添加小节上传课程视频阿里云视频点播服务，调用SDK，new 一个文件上传对象，传入id，secret，和视频的基本信息，然后调用上传方法，得到response。 业务代码入下： 1234567891011121314151617181920212223242526272829/** * 上传视频到阿里云 * * @param file * @return */ @Override public String uploadVideoAly(MultipartFile file) &#123; try &#123; String fileName = file.getOriginalFilename(); String title = fileName.substring(0, fileName.lastIndexOf(&quot;.&quot;)); InputStream inputStream = file.getInputStream(); UploadStreamRequest request = new UploadStreamRequest(ConstantVodUtils.ACCESS_KEY_ID, ConstantVodUtils.ACCESS_KEY_SECRET, title, fileName, inputStream); UploadVideoImpl uploader = new UploadVideoImpl(); UploadStreamResponse response = uploader.uploadStream(request); String videoId = null; if (response.isSuccess()) &#123; videoId = response.getVideoId(); &#125; else &#123; videoId = response.getVideoId(); &#125; return videoId; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; 视频播放功能调用阿里云的SDK，此处采用直接通过视频id获取视频链接的方式，不安全，视频可以直接被别人下载到本地，应该采用授权的方式，并整合阿里云的播放器 1234567891011121314151617181920212223242526272829/** * 根据视频id获取视频url * * @param id * @return */ @ApiOperation(&quot;根据视频id获取视频url&quot;) @GetMapping(&quot;getPlayUrl/&#123;id&#125;&quot;) public R getPlayAuth(@PathVariable String id) &#123; try &#123; // 创建初始化对象 DefaultAcsClient client = InitVodClient.initVodClient(ConstantVodUtils.ACCESS_KEY_ID, ConstantVodUtils.ACCESS_KEY_SECRET); // 创建获取在线url的request和response对象 GetPlayInfoRequest request = new GetPlayInfoRequest(); // 向request设置视频id request.setVideoId(id); // 调用方法得到url GetPlayInfoResponse response = client.getAcsResponse(request); List&lt;GetPlayInfoResponse.PlayInfo&gt; playInfoList = response.getPlayInfoList(); List&lt;String&gt; playUrl = new ArrayList&lt;&gt;(); for (GetPlayInfoResponse.PlayInfo playInfo : playInfoList) &#123; playUrl.add(playInfo.getPlayURL()); &#125; return R.ok().data(&quot;playUrl&quot;, playUrl); &#125; catch (Exception e) &#123; throw new GuliException(20001, &quot;获取视频Url失败&quot;); &#125; &#125; 6、统计分析模块生成统计数据 通过定时任务，每天凌晨12点采集前一天的数据，数据通过远程调用采集，获取前一天用户模块中的注册数，视频播放次数，课程数量，并插入到统计表中 统计数据图表显示 根据数据类型和时间段条件查询统计表，前台展示数据 三、前台用户系统功能1、前台数据显示由于主页数据不经常改变，但又是用户高频访问的地方，为了减少数据库查询压力，将首页数据放入redis中，每次返回首页，从redis中获取数据。 显示轮播图显示热门课程 根据观看次数选出前8个课程 显示名师 根据等级选出前4为名师 2、注册获取手机验证码 用户填写完个人信息后，点击注册按钮，将手机号和验证码发送给kafka，直接返回，短信模块从kafka里面消费消息，给指定手机号发送短信验证码，验证码验证成功，将用户数据存入数据库。 阿里云短信服务，首先直接从redis中根据电话号码拿验证码，如果拿到了，直接返回，提示验证码已存在，请稍后再发送，如果redis中没有验证码，生成4位随机短信验证码，放入redis中，5分钟后过期。 注册时，对用户输入的数据进行校验，然后从redis中拿取验证码，和用户输入的验证码对比，看是否相等，相等则执行后续操作，接着，查询该手机号是否注册过，如果已经注册，则提示直接登录，没有注册，执行后续操作，将密码按照MD5加密后，放到数据库中保存。 3、登录普通登录 SSO(Single sign-on：单点登录) 登录实现流程： 登录调用登录接口返回token字符串，把返回的token字符串放到cookie里面，创建前端拦截器进行判断，如果cookie里面包含token字符串，把token字符串放到header里面。调用接口，根据token获取用户信息，把用户信息放到cookie里面，进行显示。 微信扫码登录 OAuth2：是针对特定问题的解决方案主要有两个问题：1、开放系统间授权 2、分布式访问 如何获取扫描人信息过程？ 扫描之后，微信接口返回code(临时票据)，拿着code值请求微信固定地址，得到两个值：access_token(访问凭证)和openid(微信唯一标识)，你拿着这两个值再去请求微信固定的地址，得到微信扫描人信息(比如昵称、头像等等) 4、名师列表功能5、名师详情功能6、课程列表功能 条件查询分页列表功能 根据一级分类，再根据二级分类查询课程 根据销量查询 根据课程发布时间查询 根据课程价格查询 7、课程详情页 课程信息显示（包含课程基本信息、分类、讲师、课程大纲） 判断课程是否需要购买 8、课程视频在线播放9、课程支付功能(微信支付)订单流程 生成课程订单 通过远程调用，根据用户id获取用户信息 通过远程调用，根据课程id获取课程信息 将订单信息传给kafka，直接返回给用户，让用户创建订单是无感知的。 从kafka里面获取订单信息，存入订单表 根据订单号生成微信支付二维码，并在前端设置定时器，隔3s查询一次支付状态 微信最终支付 微信支付实现流程 如果课程是收费课程，点击立即购买，生成课程订单 点击订单页面去支付，生成微信支付二维码 使用微信扫描二维码实现支付 支付之后，每隔3s查询支付状态(是否支付成功)，如果没有成功，等待支付，如果支付成功，更新订单状态(已经支付状态)，向支付记录表添加支付成功记录。 四、项目后端技术点总结1、微服务架构2、SpringBoot(1) SpringBoot本质就是Spring，只是快速构建Spring工程的脚手架 (2) 细节： 启动类包扫描机制 设置扫描规则@ComponentScan(“packagePath”) 配置类 (3) SpringBoot配置文件 配置文件类型：yml，properties 配置文件加载顺序： bootstrap-&gt;application-&gt;application-dev 3、SpringCloud(1) 是很多框架总称，使用这些框架实现微服务架构，基于SpringBoot实现 (2) 组成框架有哪些？ 服务发现–Netflix Eureka (Nacos) 服务调用–Netflix Feign 熔断器–Netflix Hystrix 服务网关–SpringCloud Gateway 分布式配置–SpringCloud Config (Nacos) 消息总线–SpringCloud Bus (Nacos) (3) 项目中，使用阿里巴巴Nacos，替代SpringCloud的一些组件 (4) Nacos 使用Nacos作为注册中心 使用Nacos作为配置中心 (5) Feign 服务调用，一个微服务调用另外一个微服务，实现远程调用 (6) 熔断器 服务调用失败或者超时，有一个兜底的方法（在调用接口上添加注解：@FeignClient(name = “service-vod”, fallback = VodFileDegradeFeignClient.class)，然后用fallback中的类实现调用接口） (7) Gateway网关 SpringCloud之前用Zuul网关，目前用GateWay网关 (8) 版本对应SpringBoot 2.2.1.RELEASESpringCloud Hoxton.RELEASE 4、MyBatisPlus(1) MyBatisPlus就是对MyBatis的增强(2) 自动填充(3) 乐观锁(4) 逻辑删除(5) 代码生成器 5、EasyExcel(1) 阿里巴巴提供操作Excel工具，代码简洁，效率很高。(2) EasyExcel对poi进行封装，采用SAX方式解析(3) 项目应用在添加课程分类，读取excel数据 EasyExcel简介Java解析、生成Excel比较有名的框架有Apache poi、jxl。但他们都存在一个严重的问题就是非常的耗内存，poi有一套SAX模式的API可以一定程度的解决一些内存溢出的问题，但POI还是有一些缺陷，比如07版Excel解压缩以及解压后存储都是在内存中完成的，内存消耗依然很大。easyexcel重写了poi对07版Excel的解析，能够原本一个3M的excel用POI sax依然需要100M左右内存降低到几M，并且再大的excel不会出现内存溢出，03版依赖POI的sax模式。在上层做了模型转换的封装，让使用者更加简单方便. EasyExcel项目地址:EasyExcel 6、Spring Security7、Redis(1) 首页数据通过Redis进行缓存(2) Redis数据类型(3) 使用Redis作为缓存，不太重要获取不经常改变的数据适合放到Redis作为缓存 8、Nginx(1) 反向代理服务器(2) 请求转发，负载均衡，动静分离 9、OAuth2 + Jwt(1) OAuth2针对特定问题的解决方案(2) Jwt包含三部分 10、HttpClient(1) 发送请求返回响应的工具，不需要浏览器完成请求和响应的过程(2) 应用场景，微信登陆获取扫描人信息，微信支付查询支付状态 11、Cookie(1) Cookie特点 客户端技术 每次发送请求带着cookie值进行发送 cookie有默认会话级别，关闭浏览器cookie默认不存在了，但是可以设置cookie有效时长：setMaxAge 12、微信登录13、微信支付14、阿里云OSS(1) 文件存储服务器 (2) 添加讲师时候上传讲师头像 15、阿里云视频点播服务(1) 视频上传、删除、播放 (2) 整合阿里云视频播放器进行视频播放 使用视频播放凭证 16、阿里云短信服务(1) 用户注册时，发送手机验证码 17、Git18、Docker五、问题总结1、MyBatis生成19位id值分布式系统中分布式id生成器生成的id 长度过大（19个字符长度的整数），js无法解析（js只能解析16个长度：2的53次幂） id策略改成 ID_WORKER_STR,把id的类型在程序中设置成了字符串 2、跨域问题(1) 访问协议、ip地址、端口号，这三个有任何一个不一样，都会产生跨域。 (2) 跨域解决： 在Controller上加@CrossOrigin注解 通过网关，编写配置文件，编写配置类 3、413问题(1) 上传视频的时候，因为Nginx有上传文件大小限制，如果超过Nginx大小，会出现413 (2) 413错误:请求体过大 解决方案：在Nginx配置客户端大小 4、Maven加载问题(1) maven加载项目的时候，默认不会加载src-java文件夹里面xml类型的文件 解决方案： 直接复制xml文件到target目录 通过配置pom实现 5、分布式系统CAP原理CAP定理指的是在一个分布式系统中，Consistency（一致性）、Availability（可用性）、Partition tolerance（分区容错性），三者不可同时获得。 一致性（C）： 在分布式系统中的所有数据备份，在同一时刻是否同样的值。（所有节点在同一时间的数据完全一致，越多节点，数据同步越耗时） 可用性（A）： 负载过大后，集群整体是否还能响应客户端的读写请求。（服务一直可用，而且是正常响应时间） 分区容错性（P）： 分区容错性，就是高可用性，一个节点崩了，并不影响其它的节点（100个节点，挂了几个，不影响服务，越多机器越好） CA 满足的情况下，P不能满足的原因： 数据同步(C)需要时间，也要正常的时间内响应(A)，那么机器数量就要少，所以P就不满足 CP 满足的情况下，A不能满足的原因： 数据同步(C)需要时间, 机器数量也多(P)，但是同步数据需要时间，所以不能再正常时间内响应，所以A就不满足 AP 满足的情况下，C不能满足的原因： 机器数量也多(P)，正常的时间内响应(A)，那么数据就不能及时同步到其他节点，所以C不满足 注册中心选择的原则Zookeeper：CP设计，保证了一致性，集群搭建的时候，某个节点失效，则会进行选举行的leader，或者半数以上节点不可用，则无法提供服务，因此可用性没法满足。 Eureka：AP原则，无主从节点，一个节点挂了，自动切换其他节点可以使用，去中心化。 结论分布式系统中P,肯定要满足，所以我们只能在一致性和可用性之间进行权衡 如果要求一致性，则选择zookeeper，如金融行业 如果要求可用性，则Eureka，如教育、电商系统 没有最好的选择，最好的选择是根据业务场景来进行架构设计 6、前端渲染和后端渲染有什么区别前端渲染是返回json给前端，通过javascript将数据绑定到页面上 后端渲染是在服务器端将页面生成直接发送给服务器，有利于SEO的优化 7、系统架构图 项目地址onlineEducation","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://xmmarlowe.github.io/tags/Docker/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://xmmarlowe.github.io/tags/SpringCloud/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://xmmarlowe.github.io/tags/SpringBoot/"}],"author":"Marlowe"},{"title":"初识oAuth2","slug":"个人项目/初识oAuth2","date":"2021-07-30T14:56:11.000Z","updated":"2021-08-21T03:29:42.388Z","comments":true,"path":"2021/07/30/个人项目/初识oAuth2/","link":"","permalink":"https://xmmarlowe.github.io/2021/07/30/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/%E5%88%9D%E8%AF%86oAuth2/","excerpt":"OAuth全称为Open Authorization（开放授权）。OAuth协议为用户资源的授权提供了一个安全的、开放而又简易的标准。与以往的授权方式不同之处是OAuth的授权不会使第三方触及到用户的帐号信息（如用户名与密码），即第三方无需使用用户的用户名与密码就可以申请获得该用户资源的授权，因此OAUTH是安全的。","text":"OAuth全称为Open Authorization（开放授权）。OAuth协议为用户资源的授权提供了一个安全的、开放而又简易的标准。与以往的授权方式不同之处是OAuth的授权不会使第三方触及到用户的帐号信息（如用户名与密码），即第三方无需使用用户的用户名与密码就可以申请获得该用户资源的授权，因此OAUTH是安全的。 一、为什么需要OAuth2.0协议简单来说就是当第三方应用需要用户保存在其他应用上的资源时，比如网易云音乐的第三方登录功能，需要获取用户在其他应用上的用户名和头像等信息，这时通过OAuth开放协议以一种安全的方式授权第三方应用去获取这些资源。 详细了解：为什么需要OAuth2.0 二、OAuth2.0 角色 Client：第三方应用，比如上面的网易云音乐 Resource Owner：资源所有者，即用户 Authorization Server：授权服务器，即提供第三方登录服务的服务器，如QQ Resource Server：拥有资源信息的服务器，通常和授权服务器属于同一应用 三、Access Token 和 Refresh TokenAccess Token是客户端访问资源服务器的令牌。通过这个令牌，客户端可以访问第三方应用上受保护的资源。但是Access Token有效期一般较短（这可以降低Acces Token泄漏而带来的风险），当Access Token过期时用户就需要频繁的授权客户端访问资源，这非常影响用户体验，因此引入Refresh Token来获取新的Access Token。 下面看看OAuth2.0的基本流程。 客户端请求获取访问令牌，并向授权服务器提供授权许可（这里有四种认证方式） 授权服务器对客户端身份进行认证，并校验授权许可，如果校验通过，则发放访问令牌和刷新令牌 客户端通过访问令牌（Access Token）访问受保护的资源 资源服务器校验访问令牌（Access Token），如果校验通过，则提供服务，返回资源 重复（3）和（4）直到访问令牌过期。如果客户端访问令牌（Access Token）已经过期，则认证服务器会返回InvalidTokenError异常（6），此时不能继续访问受保护的资源 当访问令牌（Access Token）失效以后，资源服务器返回一个无效令牌错误 客户端通过刷新令牌（Refresh Token）请求获取一个新的访问令牌 授权服务器对客户端进行身份认证并校验刷新令牌，如果校验通过，则发放新的访问令牌（并且，可以选择发放新的刷新令牌） 四、4种授权类型为了获得访问令牌(Token)，客户端需要先从资源所有者(用户)那里获得授权。授权是以授权许可(Grant Type)的形式来表示的。OAuth定义了四种授权类型： 1、授权码模式（Authorization Code Grant） 当用户访问资源时，比如在网易云音乐中使用第三方登录功能，例如QQ登录，那么这里的资源就是用户的QQ昵称和头像等信息。此时第三方应用（网易云音乐）将发送请求到授权服务器（QQ）去获取授权，此时授权服务器（QQ）将返回一个界面给用户，用户需要登录到QQ，并同意授权网易云音乐获得某些信息（资源）。当用户同意授权后，授权服务器将返回一个授权码（Authorization Code）给第三方应用，此时第三方应用在通过client_id、client_secret（这是需要第三方应用在授权服务器去申请的）和授权码去获得Access Token和Refresh Token，此时授权码将失效。然后就是第三方应用通过Access Token去资源服务器请求资源了，资源服务器校验Access Token成功后将返回资源给第三方应用。 2、隐式授权（Implicit Grant）隐式授权又称简化授权模式，它和授权码模式类似，只不过少了获取授权码的步骤，是直接获取令牌token的，且没有Refresh Token，适用于公开的浏览器单页应用。因为令牌直接从授权服务器返回，所以没有安全保证，令牌容易因为被拦截窃听而泄露。 3、密码模式（Resource Owner Password Credentials Grant） 首先资源所有者（用户）提供自己的用户名和密码给客户端（Client），然后客户端（Client）携带从用户那里获取的凭证去授权服务器请求Token， 授权服务器对客户端进行身份认证，并校验资源所有者的凭证，如果都校验通过，则发放Token。 适用范围：只适用于应用是受信任的场景。 一个典型的例子是同一个企业内部的不同产品要使用本企业的 Oauth2.0 体系。在这种情况下，由于是同个企业，不需要向用户展示“xxx将获取以下权限”等字样并询问用户的授权意向，而只需进行用户的身份认证即可。这个时候，只需要用户输入凭证并直接传递给鉴权服务器进行授权即可。 4、客户端授权模式（Client Credentials Grant） 客户端（Client）通过Client_id和Client_secret去授权服务器请求Token，授权服务器认证Client_id和Client_secret是否正确，若正确则发放Token给客户端（Client）。最后客户端通过AccessToken请求资源。 适用范围：只适用于应用是受信任的场景。 参考OAuth2.0入门（一）—— 基本概念详解和图文并茂讲解四种授权类型","categories":[{"name":"个人项目","slug":"个人项目","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"oAuth2","slug":"oAuth2","permalink":"https://xmmarlowe.github.io/tags/oAuth2/"}],"author":"Marlowe"},{"title":"Linux相关操作","slug":"操作系统/Linux相关操作","date":"2021-07-20T13:27:24.000Z","updated":"2021-07-21T14:53:10.194Z","comments":true,"path":"2021/07/20/操作系统/Linux相关操作/","link":"","permalink":"https://xmmarlowe.github.io/2021/07/20/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Linux%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C/","excerpt":"","text":"一、Linux 基本操作 a. 将 系统 hostname 改为 个人名称，如：zhang.san； hostnamectl set-hostname Marlowe.chen # 设置新的hostname b. 创建用户，用户名为：个人名称，如： zhang.san，将该用户加入root组并更改账户密码； useradd Marlowe.chen # 添加新用户passwd Marlowe.chen # 修改账户密码usermod -g root Marlowe.chen # 将用户加入root组 c. 查看是否安装了 xfsprogs 包，如果已安装，则移除对应 xfsprogs 包； yum search xfsprogs # 查找指定软件包yum remove xfsprogs # 移除软件包 d. 找到 xfsprogs rpm 包文件并删除； rpm -qi xfsprogs # 找到xfsprogs的rpm包 e. 使用 yum 下载 xfsprogs rpm 包到本地然后使用 rpm 工具安装； `` f. 将终端的最近100条历史执行命令记录以文本文件形式保存到个人用户目录（如：/home/ zhang.san）下； history 100 &gt; /home/Marlowe.chen/history.txt # 将最近的100条历史记录输出到文件 二、文件系统创建及挂载 a. 查找系统上的一块空间为30GB的空闲块设备； df -h # 查看文件系统空间使用 fdisk -l b. 将空闲块设备格式化成两个15GB大小的xfs文件系统； c. 创建/data_01和/data_02目录； d. 将格式化好的两个xfs文件系统分别挂载到/data_01和/data_02目录； 三、文本文件处理 a 从远程服务器 192.168.184.242 （root/eisoo.com123）复制 /data_01/File_CDP_Driver_writeData.tar.gz 文件到本地 /data_01目录下； scp -r root@192.168.184.242:/data_01/File_CDP_Driver_writeData.tar.gz /root/data_01 b. 将 File_CDP_Driver_writeData.tar.gz 文件解压缩，找到里面的 File_CDP_Driver_writeData.LOG 文件； tar -zxvf File_CDP_Driver_writeData.tar.gz c. 统计 File_CDP_Driver_writeData.LOG 文件中包含 错误码 “C0000034”的行数； iconv -f utf16 -t utf8 File_CDP_Driver_writeData.LOG -o File_CDP_Driver_writeData.log # 转换文件编码 grep -c &quot;C0000034&quot; File_CDP_Driver_writeData.log # 统计包含的行数 d. 统计 File_CDP_Driver_writeData.LOG 文件中不包含 错误码 “C0000034”的行数； grep -cv &quot;C0000034&quot; File_CDP_Driver_writeData.log # 统计不包含的行数 e. 对 File_CDP_Driver_writeData.LOG 文件按照 10MB/个 进行切割，切割后文件按[个人邮箱名称]_自然数序号方式命名； split -b 10m File_CDP_Driver_writeData.LOG -d -a 2 Marlowe.chen@aishu.cn_ f. 将切割后文件进行整体打包，以个人邮箱名称命名，复制到服务器 192.168.184.242 /data_01目录下； tar -cvf Marlowe.chen@aishu.cn.tar.gz Marlowe.chen@aishu.cn* # 打包命令 scp Marlowe.chen@aishu.cn.tar.gz 192.168.184.242:/data_01 # 复制到服务器 四、Shell 脚本编程 a. 将 考题3 的整个处理流程写成Shell 脚本 bash: 五、Linux系统分析工具操作实践","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://xmmarlowe.github.io/tags/Linux/"}],"author":"Marlowe"},{"title":"如何解决消息队列的延时以及过期失效问题？","slug":"中间件/如何解决消息队列的延时以及过期失效问题？","date":"2021-07-19T14:29:24.000Z","updated":"2021-08-25T15:38:50.305Z","comments":true,"path":"2021/07/19/中间件/如何解决消息队列的延时以及过期失效问题？/","link":"","permalink":"https://xmmarlowe.github.io/2021/07/19/%E4%B8%AD%E9%97%B4%E4%BB%B6/%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E5%BB%B6%E6%97%B6%E4%BB%A5%E5%8F%8A%E8%BF%87%E6%9C%9F%E5%A4%B1%E6%95%88%E9%97%AE%E9%A2%98%EF%BC%9F/","excerpt":"","text":"参考如何解决消息队列的延时以及过期失效问题？","categories":[{"name":"中间件","slug":"中间件","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"https://xmmarlowe.github.io/tags/MQ/"}],"author":"Marlowe"},{"title":"如何保证消息的顺序性？","slug":"中间件/如何保证消息的顺序性？","date":"2021-07-17T14:27:16.000Z","updated":"2021-08-25T15:38:43.026Z","comments":true,"path":"2021/07/17/中间件/如何保证消息的顺序性？/","link":"","permalink":"https://xmmarlowe.github.io/2021/07/17/%E4%B8%AD%E9%97%B4%E4%BB%B6/%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E7%9A%84%E9%A1%BA%E5%BA%8F%E6%80%A7%EF%BC%9F/","excerpt":"","text":"参考如何保证消息的顺序性？","categories":[{"name":"中间件","slug":"中间件","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"https://xmmarlowe.github.io/tags/MQ/"}],"author":"Marlowe"},{"title":"Kafka常见使用场景","slug":"中间件/Kafka常见使用场景","date":"2021-07-16T14:12:11.000Z","updated":"2021-08-25T15:39:09.576Z","comments":true,"path":"2021/07/16/中间件/Kafka常见使用场景/","link":"","permalink":"https://xmmarlowe.github.io/2021/07/16/%E4%B8%AD%E9%97%B4%E4%BB%B6/Kafka%E5%B8%B8%E8%A7%81%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF/","excerpt":"","text":"消息队列使用场景队列，在数据结构中是一种先进先出的结构，消息队列可以看成是一个盛放消息的容器，这些消息等待着各种业务来处理。 消息队列是分布式系统中重要的组件，kafka就可以看做是一种消息队列，其大致使用场景： 解耦 异步通信 削峰填谷 来看一个用户注册业务，在传统的单体项目中，假如注册流程是： 如果用户注册相关处理耗费30ms，发送短信又耗时30ms，那么一个完整的注册业务就耗时60ms，这60ms期间，服务器资源是被这一个注册业务独占的。 如果发送短信的业务出现了故障，那么整个注册业务就不成功： 再来看分布式系统中的注册业务，注册和发短信拆分为两个业务，用户填写完注册信息，将“用户注册”的消息发送到消息队列，然后直接响应给客户端： 这样即使发送短信业务出现了故障，用户的注册业务是完成了的，只不过客户端收到成功通知的时间晚了一会而已。 这就是消息队列用到的解耦和异步通信的场景。 还有一种比较典型的场景就是分布式系统中各个业务产生的各种日志： 这里消息队列起到了一个缓冲的作用，辅助数据库，减少流计算给数据库造成的压力。 Kafka作为消息队列的一种，它也有这么多的使用场景。 参考Kafka常见使用场景与Kafka高性能之道","categories":[{"name":"中间件","slug":"中间件","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://xmmarlowe.github.io/tags/Kafka/"}],"author":"Marlowe"},{"title":"如何保证消息不被重复消费？","slug":"中间件/如何保证消息不被重复消费？","date":"2021-07-16T14:07:49.000Z","updated":"2021-08-25T15:38:36.343Z","comments":true,"path":"2021/07/16/中间件/如何保证消息不被重复消费？/","link":"","permalink":"https://xmmarlowe.github.io/2021/07/16/%E4%B8%AD%E9%97%B4%E4%BB%B6/%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E4%B8%8D%E8%A2%AB%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9%EF%BC%9F/","excerpt":"如何保证消息不被重复消费？或者说，如何保证消息消费的幂等性？","text":"如何保证消息不被重复消费？或者说，如何保证消息消费的幂等性？ 解析回答这个问题，首先你别听到重复消息这个事儿，就一无所知吧，你先大概说一说可能会有哪些重复消费的问题。 首先，比如 RabbitMQ、RocketMQ、Kafka，都有可能会出现消息重复消费的问题，正常。因为这问题通常不是 MQ 自己保证的，是由我们开发来保证的。挑一个 Kafka 来举个例子，说说怎么重复消费吧。 Kafka 实际上有个 offset 的概念，就是每个消息写进去，都有一个 offset，代表消息的序号，然后 consumer 消费了数据之后，每隔一段时间（定时定期），会把自己消费过的消息的 offset 提交一下，表示“我已经消费过了，下次我要是重启啥的，你就让我继续从上次消费到的 offset 来继续消费吧”。 但是凡事总有意外，比如我们之前生产经常遇到的，就是你有时候重启系统，看你怎么重启了，如果碰到点着急的，直接 kill 进程了，再重启。这会导致 consumer 有些消息处理了，但是没来得及提交 offset，尴尬了。重启之后，少数消息会再次消费一次。 举个栗子。 有这么个场景。数据 1/2/3 依次进入 Kafka，Kafka 会给这三条数据每条分配一个 offset，代表这条数据的序号，我们就假设分配的 offset 依次是 152/153/154。消费者从 Kafka 去消费的时候，也是按照这个顺序去消费。假如当消费者消费了 offset=153 的这条数据，刚准备去提交 offset 到 Zookeeper，此时消费者进程被重启了。那么此时消费过的数据 1/2 的 offset 并没有提交，Kafka 也就不知道你已经消费了 offset=153 这条数据。那么重启之后，消费者会找 Kafka 说，嘿，哥儿们，你给我接着把上次我消费到的那个地方后面的数据继续给我传递过来。由于之前的 offset 没有提交成功，那么数据 1/2 会再次传过来，如果此时消费者没有去重的话，那么就会导致重复消费。 注意：新版的 Kafka 已经将 offset 的存储从 Zookeeper 转移至 Kafka brokers，并使用内部位移主题 __consumer_offsets 进行存储。 如果消费者干的事儿是拿一条数据就往数据库里写一条，会导致说，你可能就把数据 1/2 在数据库里插入了 2 次，那么数据就错啦。 其实重复消费不可怕，可怕的是你没考虑到重复消费之后，怎么保证幂等性。 举个例子吧。假设你有个系统，消费一条消息就往数据库里插入一条数据，要是你一个消息重复两次，你不就插入了两条，这数据不就错了？但是你要是消费到第二次的时候，自己判断一下是否已经消费过了，若是就直接扔了，这样不就保留了一条数据，从而保证了数据的正确性。 一条数据重复出现两次，数据库里就只有一条数据，这就保证了系统的幂等性。 幂等性，通俗点说，就一个数据，或者一个请求，给你重复来多次，你得确保对应的数据是不会改变的，不能出错。 所以第二个问题来了，怎么保证消息队列消费的幂等性？ 其实还是得结合业务来思考，我这里给几个思路： 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。 比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。 比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。 比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。 当然，如何保证 MQ 的消费是幂等性的，需要结合具体的业务来看。 参考如何保证消息不被重复消费？","categories":[{"name":"中间件","slug":"中间件","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"https://xmmarlowe.github.io/tags/MQ/"}],"author":"Marlowe"},{"title":"消息中间件之Kafka学习","slug":"中间件/消息中间件之Kafka学习","date":"2021-07-15T14:47:41.000Z","updated":"2021-08-25T15:39:02.790Z","comments":true,"path":"2021/07/15/中间件/消息中间件之Kafka学习/","link":"","permalink":"https://xmmarlowe.github.io/2021/07/15/%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E4%B9%8BKafka%E5%AD%A6%E4%B9%A0/","excerpt":"总结一些Kafka常见问题","text":"总结一些Kafka常见问题 Kafka架构图 Kafka 是什么？主要应用场景有哪些？Kafka 是一个分布式流式处理平台。这到底是什么意思呢？ 流平台具有三个关键功能： 消息队列： 发布和订阅消息流，这个功能类似于消息队列，这也是 Kafka 也被归类为消息队列的原因。 容错的持久方式存储记录消息流： Kafka 会把消息持久化到磁盘，有效避免了消息丢失的风险。 流式处理平台： 在消息发布的时候进行处理，Kafka 提供了一个完整的流式处理类库。 Kafka 主要有两大应用场景： 消息队列： 建立实时流数据管道，以可靠地在系统或应用程序之间获取数据。 数据处理： 构建实时的流数据处理程序来转换或处理数据流。 和其他消息队列相比,Kafka的优势在哪里？我们现在经常提到 Kafka 的时候就已经默认它是一个非常优秀的消息队列了，我们也会经常拿它给 RocketMQ、RabbitMQ 对比。我觉得 Kafka 相比其他消息队列主要的优势如下： 极致的性能： 基于 Scala 和 Java 语言开发，设计中大量使用了批量处理和异步的思想，最高可以每秒处理千万级别的消息。 生态系统兼容性无可匹敌： Kafka 与周边生态系统的兼容性是最好的没有之一，尤其在大数据和流计算领域。 实际上在早期的时候 Kafka 并不是一个合格的消息队列，早期的 Kafka 在消息队列领域就像是一个衣衫褴褛的孩子一样，功能不完备并且有一些小问题比如丢失消息、不保证消息可靠性等等。当然，这也和 LinkedIn 最早开发 Kafka 用于处理海量的日志有很大关系，哈哈哈，人家本来最开始就不是为了作为消息队列滴，谁知道后面误打误撞在消息队列领域占据了一席之地。 随着后续的发展，这些短板都被 Kafka 逐步修复完善。所以，Kafka 作为消息队列不可靠这个说法已经过时！ 队列模型了解吗？Kafka 的消息模型知道吗？队列模型：早期的消息模型 使用队列（Queue）作为消息通信载体，满足生产者与消费者模式，一条消息只能被一个消费者使用，未被消费的消息在队列中保留直到被消费或超时。 比如：我们生产者发送 100 条消息的话，两个消费者来消费一般情况下两个消费者会按照消息发送的顺序各自消费一半（也就是你一个我一个的消费。） 队列模型存在的问题： 假如我们存在这样一种情况：我们需要将生产者产生的消息分发给多个消费者，并且每个消费者都能接收到完成的消息内容。 这种情况，队列模型就不好解决了。很多比较杠精的人就说：我们可以为每个消费者创建一个单独的队列，让生产者发送多份。这是一种非常愚蠢的做法，浪费资源不说，还违背了使用消息队列的目的。 发布-订阅模型:Kafka 消息模型发布-订阅模型主要是为了解决队列模型存在的问题。 发布订阅模型（Pub-Sub） 使用主题（Topic） 作为消息通信载体，类似于广播模式；发布者发布一条消息，该消息通过主题传递给所有的订阅者，在一条消息广播之后才订阅的用户则是收不到该条消息的。 在发布 - 订阅模型中，如果只有一个订阅者，那它和队列模型就基本是一样的了。所以说，发布 - 订阅模型在功能层面上是可以兼容队列模型的。 Kafka 采用的就是发布 - 订阅模型。 RocketMQ 的消息模型和 Kafka 基本是完全一样的。唯一的区别是 Kafka 中没有队列这个概念，与之对应的是 Partition（分区）。 什么是Producer、Consumer、Broker、Topic、Partition？Kafka 将生产者发布的消息发送到 Topic（主题） 中，需要这些消息的消费者可以订阅这些 Topic（主题），如下图所示： 上面这张图也为我们引出了，Kafka 比较重要的几个概念： Producer（生产者）: 产生消息的一方。 Consumer（消费者）: 消费消息的一方。 Broker（代理）: 可以看作是一个独立的 Kafka 实例。多个 Kafka Broker 组成一个 Kafka Cluster。 同时，你一定也注意到每个 Broker 中又包含了 Topic 以及 Partition 这两个重要的概念： Topic（主题）: Producer 将消息发送到特定的主题，Consumer 通过订阅特定的 Topic(主题) 来消费消息。 Partition（分区）: Partition 属于 Topic 的一部分。一个 Topic 可以有多个 Partition ，并且同一 Topic 下的 Partition 可以分布在不同的 Broker 上，这也就表明一个 Topic 可以横跨多个 Broker 。这正如我上面所画的图一样。 划重点：Kafka 中的 Partition（分区） 实际上可以对应成为消息队列中的队列。这样是不是更好理解一点？ 什么是消费者组？消费者组是Kafka独有的概念，如果面试官问这个，就说明他对此是有一定了解的。 胡大给的标准答案是：官网上的介绍言简意赅，即消费者组是Kafka提供的可扩展且具有容错性的消费者机制。 但实际上，消费者组（Consumer Group）其实包含两个概念，作为队列，消费者组允许你分割数据处理到一组进程集合上（即一个消费者组中可以包含多个消费者进程，他们共同消费该topic的数据），这有助于你的消费能力的动态调整；作为发布-订阅模型（publish-subscribe），Kafka允许你将同一份消息广播到多个消费者组里，以此来丰富多种数据使用场景。 需要注意的是：在消费者组中，多个实例共同订阅若干个主题，实现共同消费。同一个组下的每个实例都配置有相同的组ID，被分配不同的订阅分区。当某个实例挂掉的时候，其他实例会自动地承担起它负责消费的分区。 因此，消费者组在一定程度上也保证了消费者程序的高可用性。 注意：消费者组的题目，能够帮你在某种程度上掌控下面的面试方向。 如果你擅长位移值原理（Offset），就不妨再提一下消费者组的位移提交机制； 如果你擅长Kafka Broker，可以提一下消费者组与Broker之间的交互； 如果你擅长与消费者组完全不相关的Producer，那么就可以这么说：“消费者组要消费的数据完全来自于Producer端生产的消息，我对Producer还是比较熟悉的。” 总之，你总得对consumer group相关的方向有一定理解，然后才能像面试官表名你对某一块很理解。 Kafka中位移（offset）的作用？标准答案：在Kafka中，每个主题分区下的每条消息都被赋予了一个唯一的ID数值，用于标识它在分区中的位置。这个ID数值，就被称为位移，或者叫偏移量。一旦消息被写入到分区日志，它的位移值将不能被修改。 答完这些之后，你还可以把整个面试方向转移到你希望的地方： 如果你深谙Broker底层日志写入的逻辑，可以强调下消息在日志中的存放格式 如果你明白位移值一旦被确定不能修改，可以强调下“Log Cleaner组件都不能影响位移值”这件事情 如果你对消费者的概念还算熟悉，可以再详细说说位移值和消费者位移值之间的区别 Kafka 的多副本机制了解吗？带来了什么好处？还有一点我觉得比较重要的是 Kafka 为分区（Partition）引入了多副本（Replica）机制。分区（Partition）中的多个副本之间会有一个叫做 leader 的家伙，其他副本称为 follower。我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。 生产者和消费者只与 leader 副本交互。你可以理解为其他副本只是 leader 副本的拷贝，它们的存在只是为了保证消息存储的安全性。当 leader 副本发生故障时会从 follower 中选举出一个 leader,但是 follower 中如果有和 leader 同步程度达不到要求的参加不了 leader 的竞选。 阐述下 Kafka 中的领导者副本（Leader Replica）和追随者副本（Follower Replica）的区别？推荐的答案：Kafka副本当前分为领导者副本和追随者副本。只有Leader副本才能对外提供读写服务，响应Clients端的请求。Follower副本只是采用拉（PULL）的方式，被动地同步Leader副本中的数据，并且在Leader副本所在的Broker宕机后，随时准备应聘Leader副本。 加分点： 强调Follower副本也能对外提供读服务。自Kafka 2.4版本开始，社区通过引入新的Broker端参数，允许Follower副本有限度地提供读服务。 强调Leader和Follower的消息序列在实际场景中不一致。通常情况下，很多因素可能造成Leader和Follower之间的不同步，比如程序问题，网络问题，broker问题等，短暂的不同步我们可以关注（秒级别），但长时间的不同步可能就需要深入排查了，因为一旦Leader所在节点异常，可能直接影响可用性。 注意：之前确保一致性的主要手段是高水位机制（HW），但高水位值无法保证Leader连续变更场景下的数据一致性，因此，社区引入了Leader Epoch机制，来修复高水位值的弊端。 Kafka 的多分区（Partition）以及多副本（Replica）机制有什么好处呢？ Kafka 通过给特定 Topic 指定多个 Partition, 而各个 Partition 可以分布在不同的 Broker 上, 这样便能提供比较好的并发能力（负载均衡）。 Partition 可以指定对应的 Replica 数, 这也极大地提高了消息存储的安全性, 提高了容灾能力，不过也相应的增加了所需要的存储空间。 Zookeeper 在 Kafka 中的作用知道吗？详情见：Zookeeper 在 Kafka 中的作用 ZooKeeper 主要为 Kafka 提供元数据的管理的功能。 从图中我们可以看出，Zookeeper 主要为 Kafka 做了下面这些事情： Broker 注册： 在 Zookeeper 上会有一个专门用来进行 Broker 服务器列表记录的节点。每个 Broker 在启动时，都会到 Zookeeper 上进行注册，即到/brokers/ids 下创建属于自己的节点。每个 Broker 就会将自己的 IP 地址和端口等信息记录到该节点中去 Topic 注册： 在 Kafka 中，同一个Topic 的消息会被分成多个分区并将其分布在多个 Broker 上，这些分区信息及与 Broker 的对应关系也都是由 Zookeeper 在维护。比如我创建了一个名字为 my-topic 的主题并且它有两个分区，对应到 zookeeper 中会创建这些文件夹：/brokers/topics/my-topic/Partitions/0、/brokers/topics/my-topic/Partitions/1 负载均衡： 上面也说过了 Kafka 通过给特定 Topic 指定多个 Partition, 而各个 Partition 可以分布在不同的 Broker 上, 这样便能提供比较好的并发能力。 对于同一个 Topic 的不同 Partition，Kafka 会尽力将这些 Partition 分布到不同的 Broker 服务器上。当生产者产生消息后也会尽量投递到不同 Broker 的 Partition 里面。当 Consumer 消费的时候，Zookeeper 可以根据当前的 Partition 数量以及 Consumer 数量来实现动态负载均衡。 …… Kafka 如何保证消息的消费顺序？我们在使用消息队列的过程中经常有业务场景需要严格保证消息的消费顺序，比如我们同时发了 2 个消息，这 2 个消息对应的操作分别对应的数据库操作是：更改用户会员等级、根据会员等级计算订单价格。假如这两条消息的消费顺序不一样造成的最终结果就会截然不同。 我们知道 Kafka 中 Partition(分区)是真正保存消息的地方，我们发送的消息都被放在了这里。而我们的 Partition(分区) 又存在于 Topic(主题) 这个概念中，并且我们可以给特定 Topic 指定多个 Partition。 每次添加消息到 Partition(分区) 的时候都会采用尾加法，如上图所示。Kafka 只能为我们保证 Partition(分区) 中的消息有序，而不能保证 Topic(主题) 中的 Partition(分区) 的有序。 消息在被追加到 Partition(分区)的时候都会分配一个特定的偏移量（offset）。Kafka 通过偏移量（offset）来保证消息在分区内的顺序性。 所以，我们就有一种很简单的保证消息消费顺序的方法：1 个 Topic 只对应一个 Partition。这样当然可以解决问题，但是破坏了 Kafka 的设计初衷。 Kafka 中发送 1 条消息的时候，可以指定 topic, partition, key,data（数据） 4 个参数。如果你发送消息的时候指定了 Partition 的话，所有消息都会被发送到指定的 Partition。并且，同一个 key 的消息可以保证只发送到同一个 partition，这个我们可以采用表/对象的 id 来作为 key 。 总结一下，对于如何保证 Kafka 中消息消费的顺序，有了下面两种方法： 1 个 Topic 只对应一个 Partition。 （推荐）发送消息的时候指定 key/Partition。 当然不仅仅只有上面两种方法，上面两种方法是我觉得比较好理解的， Kafka 如何保证消息不丢失?生产者丢失消息的情况生产者(Producer) 调用send方法发送消息之后，消息可能因为网络问题并没有发送过去。 所以，我们不能默认在调用send方法发送消息之后消息消息发送成功了。为了确定消息是发送成功，我们要判断消息发送的结果。但是要注意的是 Kafka 生产者(Producer) 使用 send 方法发送消息实际上是异步的操作，我们可以通过 get()方法获取调用结果，但是这样也让它变为了同步操作，示例代码如下： 详细代码见我的这篇文章：Kafka系列第三篇！10 分钟学会如何在 Spring Boot 程序中使用 Kafka 作为消息队列? 12345SendResult&lt;String, Object&gt; sendResult = kafkaTemplate.send(topic, o).get();if (sendResult.getRecordMetadata() != null) &#123; logger.info(&quot;生产者成功发送消息到&quot; + sendResult.getProducerRecord().topic() + &quot;-&gt; &quot; + sendRe sult.getProducerRecord().value().toString());&#125; 但是一般不推荐这么做！可以采用为其添加回调函数的形式，示例代码如下： 123ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaTemplate.send(topic, o);future.addCallback(result -&gt; logger.info(&quot;生产者成功发送消息到topic:&#123;&#125; partition:&#123;&#125;的消息&quot;, result.getRecordMetadata().topic(), result.getRecordMetadata().partition()), ex -&gt; logger.error(&quot;生产者发送消失败，原因：&#123;&#125;&quot;, ex.getMessage())); 如果消息发送失败的话，我们检查失败的原因之后重新发送即可！ 另外这里推荐为 Producer 的retries （重试次数）设置一个比较合理的值，一般是 3 ，但是为了保证消息不丢失的话一般会设置比较大一点。设置完成之后，当出现网络问题之后能够自动重试消息发送，避免消息丢失。另外，建议还要设置重试间隔，因为间隔太小的话重试的效果就不明显了，网络波动一次你3次一下子就重试完了 消费者丢失消息的情况我们知道消息在被追加到 Partition(分区)的时候都会分配一个特定的偏移量（offset）。偏移量（offset)表示 Consumer 当前消费到的 Partition(分区)的所在的位置。Kafka 通过偏移量（offset）可以保证消息在分区内的顺序性。 当消费者拉取到了分区的某个消息之后，消费者会自动提交了 offset。自动提交的话会有一个问题，试想一下，当消费者刚拿到这个消息准备进行真正消费的时候，突然挂掉了，消息实际上并没有被消费，但是 offset 却被自动提交了。 解决办法也比较粗暴，我们手动关闭自动提交 offset，每次在真正消费完消息之后之后再自己手动提交 offset 。 但是，细心的朋友一定会发现，这样会带来消息被重新消费的问题。比如你刚刚消费完消息之后，还没提交 offset，结果自己挂掉了，那么这个消息理论上就会被消费两次。 Kafka 弄丢了消息我们知道 Kafka 为分区（Partition）引入了多副本（Replica）机制。分区（Partition）中的多个副本之间会有一个叫做 leader 的家伙，其他副本称为 follower。我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。生产者和消费者只与 leader 副本交互。你可以理解为其他副本只是 leader 副本的拷贝，它们的存在只是为了保证消息存储的安全性。 试想一种情况：假如 leader 副本所在的 broker 突然挂掉，那么就要从 follower 副本重新选出一个 leader ，但是 leader 的数据还有一些没有被 follower 副本的同步的话，就会造成消息丢失。 设置 acks = all 解决办法就是我们设置 acks = all。acks 是 Kafka 生产者(Producer) 很重要的一个参数。 acks 的默认值即为1，代表我们的消息被leader副本接收之后就算被成功发送。当我们配置 acks = all 代表则所有副本都要接收到该消息之后该消息才算真正成功被发送。 设置 replication.factor &gt;= 3 为了保证 leader 副本能有 follower 副本能同步消息，我们一般会为 topic 设置 replication.factor &gt;= 3。这样就可以保证每个 分区(partition) 至少有 3 个副本。虽然造成了数据冗余，但是带来了数据的安全性。 设置 min.insync.replicas &gt; 1 一般情况下我们还需要设置 min.insync.replicas&gt; 1 ，这样配置代表消息至少要被写入到 2 个副本才算是被成功发送。min.insync.replicas 的默认值为 1 ，在实际生产中应尽量避免默认值 1。 但是，为了保证整个 Kafka 服务的高可用性，你需要确保 replication.factor &gt; min.insync.replicas 。为什么呢？设想一下假如两者相等的话，只要是有一个副本挂掉，整个分区就无法正常工作了。这明显违反高可用性！一般推荐设置成 replication.factor = min.insync.replicas + 1。 设置 unclean.leader.election.enable = false Kafka 0.11.0.0版本开始 unclean.leader.election.enable 参数的默认值由原来的true 改为false 我们最开始也说了我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。多个 follower 副本之间的消息同步情况不一样，当我们配置了 unclean.leader.election.enable = false 的话，当 leader 副本发生故障时就不会从 follower 副本中和 leader 同步程度达不到要求的副本中选择出 leader ，这样降低了消息丢失的可能性。 监控Kafka的框架都有哪些？对于SRE来讲，依然是送分题。但基础的我们要知道，Kafka本身是提供了JMX（Java Management Extensions）的，我们可以通过它来获取到Kafka内部的一些基本数据。 Kafka Manager：更多是Kafka的管理，对于SRE非常友好，也提供了简单的瞬时指标监控 Kafka Monitor：LinkedIn开源的免费框架，支持对集群进行系统测试，并实时监控测试结果。 CruiseControl：也是LinkedIn公司开源的监控框架，用于实时监测资源使用率，以及提供常用运维操作等。无UI界面，只提供REST API，可以进行多集群管理。 JMX监控：由于Kafka提供的监控指标都是基于JMX的，因此，市面上任何能够集成JMX的框架都可以使用，比如Zabbix和Prometheus。 已有大数据平台自己的监控体系：像Cloudera提供的CDH这类大数据平台，天然就提供Kafka监控方案。 JMXTool：社区提供的命令行工具，能够实时监控JMX指标。可以使用kafka-run-class.sh kafka.tools.JmxTool来查看具体的用法。 LEO、LSO、AR、ISR、HW都表示什么含义？作为SRE来讲，对于一个开源软件的原理以及概念的理解，是非常重要的。 LEO（Log End Offset）：日志末端位移值或末端偏移量，表示日志下一条待插入消息的位移值。举个例子，如果日志有10条消息，位移值从0开始，那么，第10条消息的位移值就是9。此时，LEO = 10。 LSO（Log Stable Offset）：这是Kafka事务的概念。如果你没有使用到事务，那么这个值不存在（其实也不是不存在，只是设置成一个无意义的值）。该值控制了事务型消费者能够看到的消息范围。它经常与Log Start Offset，即日志起始位移值相混淆，因为有些人将后者缩写成LSO，这是不对的。在Kafka中，LSO就是指代Log Stable Offset。 AR（Assigned Replicas）：AR是主题被创建后，分区创建时被分配的副本集合，副本个数由副本因子决定。 ISR（In-Sync Replicas）：Kafka中特别重要的概念，指代的是AR中那些与Leader保持同步的副本集合。在AR中的副本可能不在ISR中，但Leader副本天然就包含在ISR中。 HW（High watermark）：高水位值，这是控制消费者可读取消息范围的重要字段。一个普通消费者只能“看到”Leader副本上介于Log Start Offset和HW（不含）之间的所有消息。水位以上的消息是对消费者不可见的。 需要注意的是，通常在ISR中，可能会有人问到为什么有时候副本不在ISR中，这其实也就是上面说的Leader和Follower不同步的情况，为什么我们前面说，短暂的不同步我们可以关注，但是长时间的不同步，我们需要介入排查了，因为ISR里的副本后面都是通过replica.lag.time.max.ms，即Follower副本的LEO落后Leader LEO的时间是否超过阈值来决定副本是否在ISR内部的。 Kafka能手动删除消息吗？Kafka不需要用户手动删除消息。它本身提供了留存策略，能够自动删除过期消息。当然，它是支持手动删除消息的。 对于设置了Key且参数cleanup.policy=compact的主题而言，我们可以构造一条 的消息发送给Broker，依靠Log Cleaner组件提供的功能删除掉该 Key 的消息。 对于普通主题而言，我们可以使用kafka-delete-records命令，或编写程序调用Admin.deleteRecords方法来删除消息。这两种方法殊途同归，底层都是调用Admin的deleteRecords方法，通过将分区Log Start Offset值抬高的方式间接删除消息。 Kafka为什么不支持读写分离？这其实是分布式场景下的通用问题，因为我们知道CAP理论下，我们只能保证C（可用性）和A（一致性）取其一，如果支持读写分离，那其实对于一致性的要求可能就会有一定折扣，因为通常的场景下，副本之间都是通过同步来实现副本数据一致的，那同步过程中肯定会有时间的消耗，如果支持了读写分离，就意味着可能的数据不一致，或数据滞后。 Leader/Follower模型并没有规定Follower副本不可以对外提供读服务。很多框架都是允许这么做的，只是 Kafka最初为了避免不一致性的问题，而采用了让Leader统一提供服务的方式。 不过，自Kafka 2.4之后，Kafka提供了有限度的读写分离，也就是说，Follower副本能够对外提供读服务。 Java Consumer 为什么采用单线程来获取消息？在回答之前，如果先把这句话说出来，一定会加分：Java Consumer是双线程的设计。一个线程是用户主线程，负责获取消息；另一个线程是心跳线程，负责向Kafka汇报消费者存活情况。将心跳单独放入专属的线程，能够有效地规避因消息处理速度慢而被视为下线的“假死”情况。 单线程获取消息的设计能够避免阻塞式的消息获取方式。单线程轮询方式容易实现异步非阻塞式，这样便于将消费者扩展成支持实时流处理的操作算子。因为很多实时流处理操作算子都不能是阻塞式的。另外一个可能的好处是，可以简化代码的开发。多线程交互的代码是非常容易出错的。 简述Follower副本消息同步的完整流程首先，Follower发送FETCH请求给Leader。 接着，Leader会读取底层日志文件中的消息数据，再更新它内存中的Follower副本的LEO值，更新为FETCH请求中的fetchOffset值。 最后，尝试更新分区高水位值。Follower接收到FETCH响应之后，会把消息写入到底层日志，接着更新LEO和HW值。 Leader和Follower的HW值更新时机是不同的，Follower的HW更新永远落后于Leader的HW。这种时间上的错配是造成各种不一致的原因。 因此，对于消费者而言，消费到的消息永远是所有副本中最小的那个HW。 参考Kafka面试题总结 Kafka经典面试题详解","categories":[{"name":"中间件","slug":"中间件","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://xmmarlowe.github.io/tags/Kafka/"},{"name":"MQ","slug":"MQ","permalink":"https://xmmarlowe.github.io/tags/MQ/"}],"author":"Marlowe"},{"title":"初识消息队列","slug":"中间件/初识消息队列","date":"2021-07-14T16:12:48.000Z","updated":"2021-08-25T15:38:26.044Z","comments":true,"path":"2021/07/15/中间件/初识消息队列/","link":"","permalink":"https://xmmarlowe.github.io/2021/07/15/%E4%B8%AD%E9%97%B4%E4%BB%B6/%E5%88%9D%E8%AF%86%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/","excerpt":"“RabbitMQ？”“Kafka？”“RocketMQ？”…在日常学习与开发过程中，我们常常听到消息队列这个关键词。我也在我的多篇文章中提到了这个概念。可能你是熟练使用消息队列的老手，又或者你是不懂消息队列的新手，不论你了不了解消息队列，本文都将带你搞懂消息队列的一些基本理论。如果你是老手，你可能从本文学到你之前不曾注意的一些关于消息队列的重要概念，如果你是新手，相信本文将是你打开消息队列大门的一板砖。","text":"“RabbitMQ？”“Kafka？”“RocketMQ？”…在日常学习与开发过程中，我们常常听到消息队列这个关键词。我也在我的多篇文章中提到了这个概念。可能你是熟练使用消息队列的老手，又或者你是不懂消息队列的新手，不论你了不了解消息队列，本文都将带你搞懂消息队列的一些基本理论。如果你是老手，你可能从本文学到你之前不曾注意的一些关于消息队列的重要概念，如果你是新手，相信本文将是你打开消息队列大门的一板砖。 一、什么是消息队列我们可以把消息队列看作是一个存放消息的容器，当我们需要使用消息的时候，直接从容器中取出消息供自己使用即可。 消息队列是分布式系统中重要的组件之一。使用消息队列主要是为了通过异步处理提高系统性能和削峰、降低系统耦合性。 我们知道队列 Queue 是一种先进先出的数据结构，所以消费消息时也是按照顺序来消费的。 二、为什么要用消息队列通常来说，使用消息队列能为我们的系统带来下面三点好处： 通过异步处理提高系统性能（减少响应所需时间）。 削峰/限流 降低系统耦合性。 《大型网站技术架构》第四章和第七章均有提到消息队列对应用性能及扩展性的提升。 2.1 通过异步处理提高系统性能（减少响应所需时间） 将用户的请求数据存储到消息队列之后就立即返回结果。随后，系统再对消息进行消费。 因为用户请求数据写入消息队列之后就立即返回给用户了，但是请求数据在后续的业务校验、写数据库等操作中可能失败。因此，使用消息队列进行异步处理之后，需要适当修改业务流程进行配合，比如用户在提交订单之后，订单数据写入消息队列，不能立即返回用户订单提交成功，需要在消息队列的订单消费者进程真正处理完该订单之后，甚至出库后，再通过电子邮件或短信通知用户订单成功，以免交易纠纷。这就类似我们平时手机订火车票和电影票。 2.2 削峰/限流先将短时间高并发产生的事务消息存储在消息队列中，然后后端服务再慢慢根据自己的能力去消费这些消息，这样就避免直接把后端服务打垮掉。 举例：在电子商务一些秒杀、促销活动中，合理使用消息队列可以有效抵御促销活动刚开始大量订单涌入对系统的冲击。如下图所示： 2.3 降低系统耦合性使用消息队列还可以降低系统耦合性。我们知道如果模块之间不存在直接调用，那么新增模块或者修改模块就对其他模块影响较小，这样系统的可扩展性无疑更好一些。还是直接上图吧： 生产者（客户端）发送消息到消息队列中去，接受者（服务端）处理消息，需要消费的系统直接去消息队列取消息进行消费即可而不需要和其他系统有耦合，这显然也提高了系统的扩展性。 消息队列使利用发布-订阅模式工作，消息发送者（生产者）发布消息，一个或多个消息接受者（消费者）订阅消息。 从上图可以看到消息发送者（生产者）和消息接受者（消费者）之间没有直接耦合， 消息发送者将消息发送至分布式消息队列即结束对消息的处理，消息接受者从分布式消息队列获取该消息后进行后续处理，并不需要知道该消息从何而来。对新增业务，只要对该类消息感兴趣，即可订阅该消息，对原有系统和业务没有任何影响，从而实现网站业务的可扩展性设计。 消息接受者对消息进行过滤、处理、包装后，构造成一个新的消息类型，将消息继续发送出去，等待其他消息接受者订阅该消息。因此基于事件（消息对象）驱动的业务架构可以是一系列流程。 另外，为了避免消息队列服务器宕机造成消息丢失，会将成功发送到消息队列的消息存储在消息生产者服务器上，等消息真正被消费者服务器处理后才删除消息。在消息队列服务器宕机后，生产者服务器会选择分布式消息队列服务器集群中的其他服务器发布消息。 备注： 不要认为消息队列只能利用发布-订阅模式工作，只不过在解耦这个特定业务环境下是使用发布-订阅模式的。除了发布-订阅模式，还有点对点订阅模式（一个消息只有一个消费者），我们比较常用的是发布-订阅模式。另外，这两种消息模型是 JMS 提供的，AMQP 协议还提供了 5 种消息模型。 三、使用消息队列带来的一些问题 系统可用性降低： 系统可用性在某种程度上降低，为什么这样说呢？在加入 MQ 之前，你不用考虑消息丢失或者说 MQ 挂掉等等的情况，但是，引入 MQ 之后你就需要去考虑了！ 系统复杂性提高： 加入 MQ 之后，你需要保证消息没有被重复消费、处理消息丢失的情况、保证消息传递的顺序性等等问题！ 一致性问题： 我上面讲了消息队列可以实现异步，消息队列带来的异步确实可以提高系统响应速度。但是，万一消息的真正消费者并没有正确消费消息怎么办？这样就会导致数据不一致的情况了! 四、常见的消息队列对比 总结： ActiveMQ 的社区算是比较成熟，但是较目前来说，ActiveMQ 的性能比较差，而且版本迭代很慢，不推荐使用。 RabbitMQ 在吞吐量方面虽然稍逊于 Kafka 和 RocketMQ ，但是由于它基于 erlang 开发，所以并发能力很强，性能极其好，延时很低，达到微秒级。但是也因为 RabbitMQ 基于 erlang 开发，所以国内很少有公司有实力做 erlang 源码级别的研究和定制。如果业务场景对并发量要求不是太高（十万级、百万级），那这四种消息队列中，RabbitMQ 一定是你的首选。如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。 RocketMQ 阿里出品，Java 系开源项目，源代码我们可以直接阅读，然后可以定制自己公司的 MQ，并且 RocketMQ 有阿里巴巴的实际业务场景的实战考验。RocketMQ 社区活跃度相对较为一般，不过也还可以，文档相对来说简单一些，然后接口这块不是按照标准 JMS 规范走的有些系统要迁移需要修改大量代码。还有就是阿里出台的技术，你得做好这个技术万一被抛弃，社区黄掉的风险，那如果你们公司有技术实力我觉得用 RocketMQ 挺好的 Kafka 的特点其实很明显，就是仅仅提供较少的核心功能，但是提供超高的吞吐量，ms 级的延迟，极高的可用性以及可靠性，而且分布式可以任意扩展。同时 kafka 最好是支撑较少的 topic 数量即可，保证其超高吞吐量。kafka 唯一的一点劣势是有可能消息重复消费，那么对数据准确性会造成极其轻微的影响，在大数据领域中以及日志采集中，这点轻微影响可以忽略这个特性天然适合大数据实时计算以及日志收集。 参考消息队列其实很简单","categories":[{"name":"中间件","slug":"中间件","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"https://xmmarlowe.github.io/tags/MQ/"}],"author":"Marlowe"},{"title":"SpringBoot整合Shiro报错:The dependencies of some of the beans in the application context form a cycle","slug":"个人项目/SpringBoot整合Shiro报错-The-dependencies-of-some-of-the-beans-in-the-application-context-form-a-cycle","date":"2021-06-30T07:51:03.000Z","updated":"2021-07-05T10:18:11.506Z","comments":true,"path":"2021/06/30/个人项目/SpringBoot整合Shiro报错-The-dependencies-of-some-of-the-beans-in-the-application-context-form-a-cycle/","link":"","permalink":"https://xmmarlowe.github.io/2021/06/30/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/SpringBoot%E6%95%B4%E5%90%88Shiro%E6%8A%A5%E9%94%99-The-dependencies-of-some-of-the-beans-in-the-application-context-form-a-cycle/","excerpt":"SpringBoot整合Shiro遇到循环依赖问题…","text":"SpringBoot整合Shiro遇到循环依赖问题… 问题代码：ShiroConfig.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778@Configurationpublic class ShiroConfig &#123; /** * SecurityManager * * @param userRealm * @return */ @Bean public DefaultSecurityManager securityManager(UserRealm userRealm) &#123; // 创建securityManager DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(); // 使用自定义Realm securityManager.setRealm(userRealm); // 关闭shiro自带的session DefaultSubjectDAO subjectDAO = new DefaultSubjectDAO(); DefaultSessionStorageEvaluator defaultSessionStorageEvaluator = new DefaultSessionStorageEvaluator(); // 禁止session defaultSessionStorageEvaluator.setSessionStorageEnabled(false); subjectDAO.setSessionStorageEvaluator(defaultSessionStorageEvaluator); securityManager.setSubjectDAO(subjectDAO); return securityManager; &#125; // ShiroFilter 注入 JWTFilter, 并设置为主过滤器 @Bean public ShiroFilterFactoryBean shiroFilterFactoryBean(SecurityManager securityManager, JWTFilter jwtFilter) &#123; ShiroFilterFactoryBean shiroFilterFactoryBean = new ShiroFilterFactoryBean(); // 添加我们自定义的过滤器并取名为jwt ***很重要*** Map&lt;String, Filter&gt; newFilters = new HashMap&lt;&gt;(); newFilters.put(&quot;jwt&quot;, jwtFilter); shiroFilterFactoryBean.setFilters(newFilters); // 注入securityManager shiroFilterFactoryBean.setSecurityManager(securityManager); // 定义规则 Map&lt;String, String&gt; map = new LinkedHashMap&lt;&gt;(); // anon(写在authc的前面): 不进行认证都能访问 map.put(&quot;/login&quot;, &quot;anon&quot;); map.put(&quot;/register&quot;, &quot;anon&quot;); // 放行swagger map.put(&quot;/swagger/**&quot;, &quot;anon&quot;); map.put(&quot;/v2/api-docs&quot;, &quot;anon&quot;); map.put(&quot;/swagger-ui.html&quot;, &quot;anon&quot;); map.put(&quot;/swagger-resources/**&quot;, &quot;anon&quot;); map.put(&quot;/webjars/**&quot;, &quot;anon&quot;); map.put(&quot;/favicon.ico&quot;, &quot;anon&quot;); map.put(&quot;/captcha.jpg&quot;, &quot;anon&quot;); map.put(&quot;/csrf&quot;, &quot;anon&quot;); // 其他所有请求都由JWTFilter处理 map.put(&quot;/**&quot;, &quot;jwt&quot;); shiroFilterFactoryBean.setFilterChainDefinitionMap(map); return shiroFilterFactoryBean; &#125; /** * 不向 Spring容器中注册 JWTFilter Bean，防止 Spring 将 JWTFilter 注册为全局过滤器 * 全局过滤器会对所有请求进行拦截，而本例中只需要拦截除 /login 和 /logout 外的请求 * 另一种简单做法是：不将 JWTFilter 放入 Spring 容器 * &lt;p&gt; * 不添加下面这个 Bean 的话, 会出现 /** jwt 会拦截所有请求的情况 */ @Bean public FilterRegistrationBean&lt;Filter&gt; registration(JWTFilter jwtFilter) &#123; FilterRegistrationBean&lt;Filter&gt; registration = new FilterRegistrationBean&lt;&gt;(jwtFilter); registration.setEnabled(false); return registration; &#125;&#125; 报错信息： 解决方案在ShiroConfig.java类中添加如下方法： 123456@Beanpublic AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor(@Qualifier(&quot;securityManager&quot;) SecurityManager securityManager) &#123; AuthorizationAttributeSourceAdvisor advisor = new AuthorizationAttributeSourceAdvisor(); advisor.setSecurityManager(securityManager); return advisor;&#125;","categories":[{"name":"个人项目","slug":"个人项目","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://xmmarlowe.github.io/tags/SpringBoot/"},{"name":"Shiro","slug":"Shiro","permalink":"https://xmmarlowe.github.io/tags/Shiro/"},{"name":"问题教程","slug":"问题教程","permalink":"https://xmmarlowe.github.io/tags/%E9%97%AE%E9%A2%98%E6%95%99%E7%A8%8B/"}],"author":"Marlowe"},{"title":"git原理以及常用命令","slug":"常用工具/git原理以及常用命令","date":"2021-06-25T00:57:16.000Z","updated":"2021-08-21T02:03:22.208Z","comments":true,"path":"2021/06/25/常用工具/git原理以及常用命令/","link":"","permalink":"https://xmmarlowe.github.io/2021/06/25/%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7/git%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","excerpt":"","text":"","categories":[{"name":"常用工具","slug":"常用工具","permalink":"https://xmmarlowe.github.io/categories/%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"git","slug":"git","permalink":"https://xmmarlowe.github.io/tags/git/"}],"author":"Marlowe"},{"title":"RBAC权限详解","slug":"个人项目/RBAC权限详解","date":"2021-06-23T12:10:49.000Z","updated":"2021-08-03T12:32:53.343Z","comments":true,"path":"2021/06/23/个人项目/RBAC权限详解/","link":"","permalink":"https://xmmarlowe.github.io/2021/06/23/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/RBAC%E6%9D%83%E9%99%90%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"1、什么是系统权限 定义：对某个功能进行操作时，需要先获得相关的‘允许’，才可以操作，否则操作会被拒绝。这里的允许即为权限 系统权限是一个复杂过程： 一个功能有多种权限操作，比如新增权限、修改权限、查看权限、删除权限等 一个系统存在多种权限，比如功能权限、菜单权限、数据权限等 一个管理员能拥有多种权限和多种操作 2、什么是RBACRBAC模型（Role-Based Access Control：基于角色的访问控制）模型是20世纪90年代研究出来的一种新模型，但其实在20世纪70年代的多用户计算时期，这种思想就已经被提出来，直到20世纪90年代中后期，RBAC才在研究团体中得到一些重视，并先后提出了许多类型的RBAC模型。其中以美国George Mason大学信息安全技术实验室（LIST）提出的RBAC96模型最具有代表，并得到了普遍的公认。 RBAC认为权限授权的过程可以抽象地概括为：Who是否可以对What进行How的访问操作，并对这个逻辑表达式进行判断是否为True的求解过程，也即是将权限问题转换为What、How的问题，Who、What、How构成了访问权限三元组，具体的理论可以参考RBAC96的论文，这里我们就不做详细的展开介绍，大家有个印象即可。 3、RBAC的组成部分 User（用户）：每个用户都有唯一的UID识别，并被授予不同的角色 Role（角色）：不同角色具有不同的权限 Permission（权限）：访问权限 用户-角色映射：用户和角色之间的映射关系 角色-权限映射：角色和权限之间的映射 4、RBAC96模型 RBAC96是一个家族模型，包括了rbac0、rbac1、rbac2、rbac3模型，其中rbac3最为复杂。 5、RBAC0模型 每个角色至少具备一个权限，每个用户至少扮演一个角色； 用户可以在会话中更改激活角色 6、RBAC1模型 RBAC1模型是在RBAC0模型的基础上增加了角色可以存在继承关系 继承关系实现分类 多继承：可以存在多个父角色 单继承：只能有一个父角色 7、RBAC2模型 RBAC2模型是在RBAC0模型的基础上增加了角色访问控制 角色访问控制规则： 角色互斥：同一用户只能分配到一组互斥角色集合中的一个角色；例子：在审计活动中，一个角色不能同时被指派给会计角色和审计员角色。 基数约束：一个角色被分配的数量是有限制的；比如一个功能高层领导人只有一个，因此角色需要限制为1个 先决条件：指获得某个角色之前需要先获得其它角色 方式一：先决角色只有一种（必要） 方式二：先决角色有多种（必要） 方式二：先决角色有多种（选其一） 运行时互斥：例如一个用户具备两个角色，但在运行时不能同时激活两种角色 8、RABC3模型 RABC3=RABC1+RABC2 9、RBAC3+Resources模型 该模型是工作中对RBAC3模型的一种扩展模型，权限是用于对资源的控制； 一个权限可以控制多种资源，比如一个新增用户权限包括对用户列表页面、新增按钮两种资源的访问 常见的Resources包括： 菜单 页面操作按钮 文件 数据：数据范围 系统：oa系统、财务系统 终端：电脑、手机 10、RBAC3+Resources+Group模型 RBAC3+Resources的基础上增加了分组信息，用户分类管理便于操作。 分组实现分类： 允许授权的分组【实现】 非授权的分组 11、如何设计RBAC3+Resources+Group模型的数据库 12、sql附件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275/* Navicat Premium Data Transfer Source Server : localhost Source Server Type : MySQL Source Server Version : 80016 Source Host : localhost:3306 Source Schema : rbac Target Server Type : MySQL Target Server Version : 80016 File Encoding : 65001 Date: 23/06/2021 20:20:53*/SET NAMES utf8mb4;SET FOREIGN_KEY_CHECKS = 0;-- ------------------------------ Table structure for button-- ----------------------------DROP TABLE IF EXISTS `button`;CREATE TABLE `button` ( `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, `name` varchar(30) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;按钮名称&#x27;, `page_id` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;页面id&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci COMMENT = &#x27;页面操作按钮信息表&#x27; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for menu-- ----------------------------DROP TABLE IF EXISTS `menu`;CREATE TABLE `menu` ( `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, `name` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;菜单名称&#x27;, `path` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;请求路径&#x27;, `link_type` tinyint(1) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;0本网页打开1新网页打开&#x27;, `description` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;描述&#x27;, `parent_id` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;父菜单&#x27;, `page_id` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;页面id&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci COMMENT = &#x27;菜单信息表&#x27; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for page-- ----------------------------DROP TABLE IF EXISTS `page`;CREATE TABLE `page` ( `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, `name` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL, `description` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci COMMENT = &#x27;页面信息表&#x27; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for permission-- ----------------------------DROP TABLE IF EXISTS `permission`;CREATE TABLE `permission` ( `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, `name` varchar(20) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;权限名称&#x27;, `code` varchar(20) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;权限代码&#x27;, `created_time` datetime(0) NULL DEFAULT NULL COMMENT &#x27;创建时间&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for permission_buttion-- ----------------------------DROP TABLE IF EXISTS `permission_buttion`;CREATE TABLE `permission_buttion` ( `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, `permission_id` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;权限id&#x27;, `buttion_id` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;按钮id&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for permission_group-- ----------------------------DROP TABLE IF EXISTS `permission_group`;CREATE TABLE `permission_group` ( `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, `name` varchar(30) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;分组名称&#x27;, `description` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;分组描述&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci COMMENT = &#x27;权限组信息表&#x27; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for permission_menu-- ----------------------------DROP TABLE IF EXISTS `permission_menu`;CREATE TABLE `permission_menu` ( `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, `permission_id` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;权限id&#x27;, `menu_id` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;菜单id&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci COMMENT = &#x27;权限菜单信息表&#x27; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for role-- ----------------------------DROP TABLE IF EXISTS `role`;CREATE TABLE `role` ( `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, `name` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;角色名称，比如管理员&#x27;, `code` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;角色代码，比如admin&#x27;, `created_time` datetime(0) NULL DEFAULT NULL COMMENT &#x27;创建时间&#x27;, `max_count` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;该角色最多使用的人数&#x27;, `use_count` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;已使用的人数&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci COMMENT = &#x27;角色信息表&#x27; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for role_exclusion_group-- ----------------------------DROP TABLE IF EXISTS `role_exclusion_group`;CREATE TABLE `role_exclusion_group` ( `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, `name` varchar(20) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;分组名称&#x27;, `description` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;描述互斥的规则&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci COMMENT = &#x27;互斥角色分组表&#x27; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for role_exclusion_group_item-- ----------------------------DROP TABLE IF EXISTS `role_exclusion_group_item`;CREATE TABLE `role_exclusion_group_item` ( `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, `group_id` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;互斥分组id&#x27;, `role_id` int(11) NULL DEFAULT NULL COMMENT &#x27;角色ID&#x27;, `created_time` datetime(0) NULL DEFAULT NULL COMMENT &#x27;创建时间&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for role_extend-- ----------------------------DROP TABLE IF EXISTS `role_extend`;CREATE TABLE `role_extend` ( `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, `role_id` int(11) NULL DEFAULT NULL COMMENT &#x27;角色Id&#x27;, `parent_id` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;父角色Id&#x27;, `created_time` datetime(0) NULL DEFAULT NULL COMMENT &#x27;创建时间&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci COMMENT = &#x27;角色父类继承关系表&#x27; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for role_include_group-- ----------------------------DROP TABLE IF EXISTS `role_include_group`;CREATE TABLE `role_include_group` ( `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, `role_id` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;角色Id（该组是该角色的先决条件）&#x27;, `name` varchar(20) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;分组名称&#x27;, `type` tinyint(1) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;先决项之间的逻辑关系0and ，1or&#x27;, `description` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;描述&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci COMMENT = &#x27;角色先决条件信息表&#x27; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for role_include_group_item-- ----------------------------DROP TABLE IF EXISTS `role_include_group_item`;CREATE TABLE `role_include_group_item` ( `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, `group_id` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;互斥分组id&#x27;, `role_id` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;角色ID&#x27;, `created_time` datetime(0) NULL DEFAULT NULL COMMENT &#x27;创建时间&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci COMMENT = &#x27;先决条件项信息表&#x27; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for role_merge_group-- ----------------------------DROP TABLE IF EXISTS `role_merge_group`;CREATE TABLE `role_merge_group` ( `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, `name` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;合并后的权限名称&#x27;, `code` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;合并后的权限代码&#x27;, `created_time` datetime(0) NULL DEFAULT NULL COMMENT &#x27;创建时间&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for role_merge_group_item-- ----------------------------DROP TABLE IF EXISTS `role_merge_group_item`;CREATE TABLE `role_merge_group_item` ( `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, `role_id` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;角色ID&#x27;, `group_id` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;合并分组id&#x27;, `created_time` datetime(0) NULL DEFAULT NULL, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci COMMENT = &#x27;角色合并项信息表&#x27; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for role_permission-- ----------------------------DROP TABLE IF EXISTS `role_permission`;CREATE TABLE `role_permission` ( `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, `role_id` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;角色id&#x27;, `permission_id` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;权限id&#x27;, `created_time` datetime(0) NULL DEFAULT NULL, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci COMMENT = &#x27;角色权限关系信息表&#x27; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for roloe_group-- ----------------------------DROP TABLE IF EXISTS `roloe_group`;CREATE TABLE `roloe_group` ( `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, `name` varchar(30) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;分组名称&#x27;, `description` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;分组描述&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci COMMENT = &#x27;角色组信息表&#x27; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for user-- ----------------------------DROP TABLE IF EXISTS `user`;CREATE TABLE `user` ( `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, `username` varchar(20) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;登录名&#x27;, `password` varchar(40) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;登录密码&#x27;, `name` varchar(10) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;真实姓名&#x27;, `sex` tinyint(1) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;0未知 1男 2女&#x27;, `old` int(2) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;年龄&#x27;, `created_time` datetime(0) NULL DEFAULT NULL COMMENT &#x27;创建时间&#x27;, `last_login_time` datetime(0) NULL DEFAULT NULL COMMENT &#x27;最后一次登录时间&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci COMMENT = &#x27;用户信息表&#x27; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for user_group-- ----------------------------DROP TABLE IF EXISTS `user_group`;CREATE TABLE `user_group` ( `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, `name` varchar(30) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;分组名称&#x27;, `parent_id` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;父ID&#x27;, `description` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT &#x27;分组描述&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci COMMENT = &#x27;部门信息表&#x27; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for user_role-- ----------------------------DROP TABLE IF EXISTS `user_role`;CREATE TABLE `user_role` ( `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, `user_id` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;用户id&#x27;, `role_id` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;角色id&#x27;, `created_time` datetime(0) NULL DEFAULT NULL COMMENT &#x27;创建时间&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci COMMENT = &#x27;用户角色关系表&#x27; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for user_role_group-- ----------------------------DROP TABLE IF EXISTS `user_role_group`;CREATE TABLE `user_role_group` ( `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, `user_id` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;用户id&#x27;, `role_group_id` int(10) UNSIGNED NULL DEFAULT NULL COMMENT &#x27;角色id&#x27;, `created_time` datetime(0) NULL DEFAULT NULL COMMENT &#x27;创建时间&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci COMMENT = &#x27;用户角色组关系表&#x27; ROW_FORMAT = Dynamic;SET FOREIGN_KEY_CHECKS = 1;","categories":[{"name":"个人项目","slug":"个人项目","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"RBAC","slug":"RBAC","permalink":"https://xmmarlowe.github.io/tags/RBAC/"}],"author":"Marlowe"},{"title":"SpringCloud之Eureka学习","slug":"Spring/SpringCloud之Eureka学习","date":"2021-06-18T05:50:53.000Z","updated":"2021-06-23T12:22:18.755Z","comments":true,"path":"2021/06/18/Spring/SpringCloud之Eureka学习/","link":"","permalink":"https://xmmarlowe.github.io/2021/06/18/Spring/SpringCloud%E4%B9%8BEureka%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"","categories":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/categories/Spring/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://xmmarlowe.github.io/tags/SpringCloud/"},{"name":"Eureka","slug":"Eureka","permalink":"https://xmmarlowe.github.io/tags/Eureka/"}],"author":"Marlowe"},{"title":"SpringBoot整合Shiro+Jwt","slug":"个人项目/SpringBoot整合Shiro+Jwt","date":"2021-06-12T00:55:50.000Z","updated":"2021-08-17T15:25:57.369Z","comments":true,"path":"2021/06/12/个人项目/SpringBoot整合Shiro+Jwt/","link":"","permalink":"https://xmmarlowe.github.io/2021/06/12/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/SpringBoot%E6%95%B4%E5%90%88Shiro+Jwt/","excerpt":"RBAC项目中用到的Shiro、Jwt相关配置以及工具类","text":"RBAC项目中用到的Shiro、Jwt相关配置以及工具类 Maven 依赖12345678910111213&lt;!--shiro--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;&#x2F;groupId&gt; &lt;artifactId&gt;shiro-spring-boot-web-starter&lt;&#x2F;artifactId&gt; &lt;version&gt;1.5.3&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt;&lt;!--JWT--&gt;&lt;dependency&gt; &lt;groupId&gt;com.auth0&lt;&#x2F;groupId&gt; &lt;artifactId&gt;java-jwt&lt;&#x2F;artifactId&gt; &lt;version&gt;3.3.0&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt; Jwt相关JWTUtils.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128package com.marlowe.rbac.utils;import com.auth0.jwt.JWT;import com.auth0.jwt.JWTVerifier;import com.auth0.jwt.algorithms.Algorithm;import com.auth0.jwt.exceptions.JWTDecodeException;import com.auth0.jwt.interfaces.DecodedJWT;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.stereotype.Component;import java.io.UnsupportedEncodingException;import java.util.Date;/** * 要有属性的setter, 不然不能从application配置文件中注入自定义的值 * * @auther yincaiTA * @date 2021/3/16 11:27 * @description jwt生成和校验的工具类 */@Component@ConfigurationProperties(prefix = &quot;self.jwt&quot;)public class JWTUtils &#123; /** * 密钥 */ private String secret; /** * 过期时间(ms) */ private long expire; /** * 前端请求头部 */ private String header; /** * 校验token是否正确并对应一个用户 * * @param token 令牌 * @param username 用户名 * @param secret 加密密钥 * @return 校验是否通过, 看token中是否包含 username并且 = real_database_username 的信息 */ public boolean verify(String token, String username, String secret) &#123; try &#123; Algorithm algorithm = Algorithm.HMAC256(secret); JWTVerifier verifier = JWT.require(algorithm) .withClaim(&quot;username&quot;, username) .build(); verifier.verify(token); return true; &#125; catch (Exception exception) &#123; return false; &#125; &#125; /** * 获得token中的信息无需secret解密也能获得 * * @return token中包含的用户名 */ public String getUsername(String token) &#123; try &#123; DecodedJWT jwt = JWT.decode(token); return jwt.getClaim(&quot;username&quot;).asString(); &#125; catch (JWTDecodeException e) &#123; return null; &#125; &#125; /** * 生成签名, x min后过期x * * @param username 用户名 * @param secret 加密密钥 * @return 加密的token */ public String sign(String username, String secret) &#123; try &#123; Date date = new Date(System.currentTimeMillis() + this.expire); Algorithm algorithm = Algorithm.HMAC256(secret); // 附带username信息 return JWT.create() .withClaim(&quot;username&quot;, username) .withExpiresAt(date) .sign(algorithm); &#125; catch (UnsupportedEncodingException e) &#123; return null; &#125; &#125; /** * 判断过期 * * @param token 令牌 * @return token 是否过期 */ public boolean isExpire(String token) &#123; DecodedJWT jwt = JWT.decode(token); return System.currentTimeMillis() &gt; jwt.getExpiresAt().getTime(); &#125; public String getSecret() &#123; return secret; &#125; public long getExpire() &#123; return expire; &#125; public String getHeader() &#123; return header; &#125; public void setSecret(String secret) &#123; this.secret = secret; &#125; public void setExpire(long expire) &#123; this.expire = expire; &#125; public void setHeader(String header) &#123; this.header = header; &#125;&#125; JWTToken.java12345678910111213141516171819202122232425262728293031package com.marlowe.rbac.config.shiro.jwt;import org.apache.shiro.authc.AuthenticationToken;/** * @auther yincaiTA * @date 2021/3/17 08:29 * @description 证书或令牌 定义为 token */public class JWTToken implements AuthenticationToken &#123; // 证书/密钥 private String token; public JWTToken() &#123; &#125; public JWTToken(String token) &#123; this.token = token; &#125; @Override public Object getPrincipal() &#123; return token; &#125; @Override public Object getCredentials() &#123; return token; &#125;&#125; JWTFilter.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115package com.marlowe.rbac.config.shiro.jwt;import com.auth0.jwt.exceptions.JWTDecodeException;import com.fasterxml.jackson.databind.ObjectMapper;import com.marlowe.rbac.commons.result.Result;import com.marlowe.rbac.utils.JWTUtils;import org.apache.shiro.SecurityUtils;import org.apache.shiro.authc.AuthenticationException;import org.apache.shiro.web.filter.authc.BasicHttpAuthenticationFilter;import org.apache.shiro.web.util.WebUtils;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.http.HttpStatus;import org.springframework.stereotype.Component;import org.springframework.web.bind.annotation.RequestMethod;import javax.servlet.ServletRequest;import javax.servlet.ServletResponse;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.io.IOException;import java.io.PrintWriter;import java.util.HashMap;/** * @auther yincaiTA * @date 2021/3/17 10:06 * @description JWTToken检验过滤器 前端请求带token时进行处理 */@Componentpublic class JWTFilter extends BasicHttpAuthenticationFilter &#123; private final ObjectMapper objectMapper = new ObjectMapper(); @Autowired private JWTUtils jwtUtils; /* @Override isAccessAllowed() 时, 该方法是 在认证之前执行 如果已经认证的用户, 直接放行(即 subject.isAuthenticated() 为 true) */ /** * 因为是前后端分离, 所以除了ShiroConfig中定义了放行的路径外, 其余路径皆会访问到这个方法, 因为 subject.isAuthenticated() 总是false * 拦截校验 */ @Override protected boolean onAccessDenied(ServletRequest request, ServletResponse response) &#123; // 完成token登入 // 1. 检查请求头中是否含有token HttpServletRequest httpServletRequest = (HttpServletRequest) request; String token = httpServletRequest.getHeader(this.jwtUtils.getHeader()); // 2. 如果客户端没有携带token，拦下请求 if (null == token || &quot;&quot;.equals(token)) &#123; this.responseTokenError(response, &quot;请求头异常或token为空值!&quot;); return false; &#125; // 3. 如果有，对进行进行token验证 JWTToken jwtToken = new JWTToken(token); try &#123; SecurityUtils.getSubject().login(jwtToken); &#125; catch (AuthenticationException e) &#123; responseTokenError(response, e.getMessage()); return false; &#125; catch (JWTDecodeException e) &#123; responseTokenError(response, e.getCause().getMessage()); return false; &#125; // 放行 访问controller return true; &#125; /** * 拦截器的前置拦截, 前后端分离, 项目中除了需要跨域全局配置之外, 我们再拦截器中也需要提供跨域支持. 这样, 拦截器才不会在进入Controller之前就被限制了 * 对跨域提供支持 */ @Override protected boolean preHandle(ServletRequest request, ServletResponse response) throws Exception &#123; HttpServletRequest httpServletRequest = (HttpServletRequest) request; HttpServletResponse httpServletResponse = (HttpServletResponse) response; httpServletResponse.setHeader(&quot;Access-control-Allow-Origin&quot;, httpServletRequest.getHeader(&quot;Origin&quot;)); httpServletResponse.setHeader(&quot;Access-Control-Allow-Methods&quot;, &quot;GET,POST,OPTIONS,PUT,DELETE&quot;); httpServletResponse.setHeader(&quot;Access-Control-Allow-Headers&quot;, httpServletRequest.getHeader(&quot;Access-Control-Request-Headers&quot;)); // 跨域时会首先发送一个option请求，这里我们给option请求直接返回正常状态 if (httpServletRequest.getMethod().equals(RequestMethod.OPTIONS.name())) &#123; httpServletResponse.setStatus(HttpStatus.OK.value()); return false; &#125; return super.preHandle(request, response); &#125; /** * 无需转发，直接返回Response信息 Token认证错误 */ private void responseTokenError(ServletResponse response, String msg) &#123; HttpServletResponse httpServletResponse = WebUtils.toHttp(response); httpServletResponse.setStatus(HttpStatus.OK.value()); httpServletResponse.setCharacterEncoding(&quot;UTF-8&quot;); httpServletResponse.setContentType(&quot;application/json; charset=utf-8&quot;); try &#123; PrintWriter out = httpServletResponse.getWriter(); HashMap&lt;String, Object&gt; errorData = new HashMap&lt;&gt;(); errorData.put(&quot;errorCode&quot;, &quot;-1&quot;); errorData.put(&quot;errorMsg&quot;, msg); Result result = Result.ok(errorData); // 序列化响应信息 String data = this.objectMapper.writeValueAsString(result); out.append(data); &#125; catch (IOException e) &#123; e.printStackTrace(); System.out.println(e.getMessage()); &#125; &#125;&#125; Shiro相关UserRealm.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133package com.marlowe.rbac.config.shiro.realms;import cn.hutool.core.collection.CollectionUtil;import com.auth0.jwt.exceptions.JWTDecodeException;import com.marlowe.rbac.config.shiro.jwt.JWTToken;import com.marlowe.rbac.entity.Permission;import com.marlowe.rbac.entity.User;import com.marlowe.rbac.service.IRoleService;import com.marlowe.rbac.service.IUserService;import com.marlowe.rbac.utils.JWTUtils;import org.apache.shiro.authc.*;import org.apache.shiro.authz.AuthorizationInfo;import org.apache.shiro.authz.SimpleAuthorizationInfo;import org.apache.shiro.realm.AuthorizingRealm;import org.apache.shiro.subject.PrincipalCollection;import org.apache.shiro.util.ByteSource;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import java.util.List;/** * @auther yincaiTA * @date 2021/3/16 11:27 * @description 用户Realm */@Componentpublic class UserRealm extends AuthorizingRealm &#123; @Autowired private IUserService userService; @Autowired private IRoleService roleService; @Autowired private JWTUtils jwtUtils; /** * shiro默认采用 UsernamePasswordToken进行处理 * 而现在我们只处理 JWTToken类型的 token */ @Override public boolean supports(AuthenticationToken token) &#123; return token instanceof JWTToken; &#125; /** * 授权(为了方便 直接在一张表中描述了角色和权限) * * @param principals 身份信息 * @return AuthorizationInfo授权信息 */ @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) &#123; //获取身份信息 User user = (User) principals.getPrimaryPrincipal(); System.out.println(&quot;调用授权验证：&quot; + user.getUsername()); // 根据主身份信息获取角色和权限信息 User realUser = userService.findRolesByUserName(user.getUsername()); // 授权角色信息 if (!CollectionUtil.isEmpty(realUser.getRoles())) &#123; //权限信息对象info,用来存放查出的用户的所有的角色（Role）及权限（permission） SimpleAuthorizationInfo simpleAuthorizationInfo = new SimpleAuthorizationInfo(); System.out.println(realUser.getRoles()+&quot;-------------------------&quot;); realUser.getRoles().forEach(role -&gt; &#123; // 添加角色信息 simpleAuthorizationInfo.addRole(role.getName()); System.out.println(role.getName()+&quot;================================&quot;); //添加权限信息 List&lt;Permission&gt; permissions = roleService.findPermissionsByRoleId(role.getId()); if (!CollectionUtil.isEmpty(permissions)) &#123; permissions.forEach(permission -&gt; &#123; simpleAuthorizationInfo.addStringPermission(permission.getName()); System.out.println(permission.getName()+&quot;==============AAAAAAAAAA==================&quot;); &#125;); &#125; &#125;); return simpleAuthorizationInfo; &#125; return null; &#125; /** * 认证(因为是已经登陆的用户才会有token 所以不用进行密码验证 并且 shiro也是跳过login的) * * @param auth 认证信息 * @return AuthenticationInfo 认证信息 * @throws AuthenticationException 认证异常 */ @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken auth) throws AuthenticationException &#123; // 处理的是 JWTToken, .getPrincipal() 和 .getCredentials() 都是返回token字符串 String token = (String) auth.getPrincipal(); String username; try &#123; // 从令牌中获取username值 这里可能发生解码异常 username = this.jwtUtils.getUsername(token); // token不包含username信息 || token值中的密钥 是胡乱编造 if (username == null || !this.jwtUtils.verify(token, username, this.jwtUtils.getSecret())) &#123; // 因为token过期了 verify会直接判断false, 所以不能在之后判断是否过期 isExpire也有可能发生解码异常 if (this.jwtUtils.isExpire(token)) &#123; throw new ExpiredCredentialsException(&quot;token过期，请重新登入！&quot;); &#125; // 校验未通过 throw new IncorrectCredentialsException(&quot;token值异常(2)!!!&quot;); &#125; &#125; catch (JWTDecodeException | IllegalArgumentException e) &#123; // token的3部分缺失 / 根本解不了码 e.printStackTrace(); throw new IncorrectCredentialsException(&quot;token值异常(1)!!!!&quot;); &#125; catch (AuthenticationException e) &#123; // 过期/值异常 等 e.printStackTrace(); throw new IncorrectCredentialsException(e.getMessage()); &#125; // 数据库查询用户并返回 User user = this.userService.findUserByUsername(username); if (user == null) &#123; throw new UnknownAccountException(&quot;账号不存在!&quot;); &#125; // 通过认证 直接将user传递给授权过程 反正都通过了认证了 就不需要再在授权过程再走一遍 token-&gt;username-&gt;user 的过程了 return new SimpleAuthenticationInfo(user, token, this.getName()); &#125;&#125; ShiroConfig.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134package com.marlowe.rbac.config.shiro;import com.marlowe.rbac.config.shiro.jwt.JWTFilter;import com.marlowe.rbac.config.shiro.realms.UserRealm;import org.apache.shiro.mgt.DefaultSecurityManager;import org.apache.shiro.mgt.DefaultSessionStorageEvaluator;import org.apache.shiro.mgt.DefaultSubjectDAO;import org.apache.shiro.mgt.SecurityManager;import org.apache.shiro.spring.security.interceptor.AuthorizationAttributeSourceAdvisor;import org.apache.shiro.spring.web.ShiroFilterFactoryBean;import org.apache.shiro.web.mgt.DefaultWebSecurityManager;import org.springframework.beans.factory.annotation.Qualifier;import org.springframework.boot.web.servlet.FilterRegistrationBean;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import javax.servlet.Filter;import java.util.HashMap;import java.util.LinkedHashMap;import java.util.Map;/** * @auther yincaiTA * @date 2021/3/17 10:25 * @description shiro配置类 */@Configurationpublic class ShiroConfig &#123; /** * SecurityManager * * @param userRealm * @return */ @Bean public DefaultSecurityManager securityManager(UserRealm userRealm) &#123; // 创建securityManager DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(); // 使用自定义Realm securityManager.setRealm(userRealm); // 关闭shiro自带的session DefaultSubjectDAO subjectDAO = new DefaultSubjectDAO(); DefaultSessionStorageEvaluator defaultSessionStorageEvaluator = new DefaultSessionStorageEvaluator(); // 禁止session defaultSessionStorageEvaluator.setSessionStorageEnabled(false); subjectDAO.setSessionStorageEvaluator(defaultSessionStorageEvaluator); securityManager.setSubjectDAO(subjectDAO); // UnavailableSecurityManagerException: No SecurityManager accessible to the calling code, // either bound to the org.apache.shiro.util.ThreadContext or as a vm static singleton. This is an invalid application configuration. // 加上其中一个即可 但是加上了就会出现 /login 也会被拦截的情况, 但是不加又会出现 JWTFilter中注入不了JWTUtils的情况// SecurityUtils.setSecurityManager(securityManager);// ThreadContext.bind(securityManager); // 就是导包的问题！！！！ shiro-spring-boot-starter -&gt; 换成 shiro-spring-boot-web-starter return securityManager; &#125; // ShiroFilter 注入 JWTFilter, 并设置为主过滤器 @Bean public ShiroFilterFactoryBean shiroFilterFactoryBean(SecurityManager securityManager, JWTFilter jwtFilter) &#123; ShiroFilterFactoryBean shiroFilterFactoryBean = new ShiroFilterFactoryBean(); // 添加我们自定义的过滤器并取名为jwt ***很重要*** Map&lt;String, Filter&gt; newFilters = new HashMap&lt;&gt;(); newFilters.put(&quot;jwt&quot;, jwtFilter); shiroFilterFactoryBean.setFilters(newFilters); // 注入securityManager shiroFilterFactoryBean.setSecurityManager(securityManager); // 定义规则 Map&lt;String, String&gt; map = new LinkedHashMap&lt;&gt;(); // anon(写在authc的前面): 不进行认证都能访问 map.put(&quot;/login&quot;, &quot;anon&quot;); map.put(&quot;/register&quot;, &quot;anon&quot;);// map.put(&quot;/logout&quot;, &quot;anon&quot;); // 放行swagger map.put(&quot;/swagger/**&quot;, &quot;anon&quot;); map.put(&quot;/v2/api-docs&quot;, &quot;anon&quot;); map.put(&quot;/swagger-ui.html&quot;, &quot;anon&quot;); map.put(&quot;/swagger-resources/**&quot;, &quot;anon&quot;); map.put(&quot;/webjars/**&quot;, &quot;anon&quot;); map.put(&quot;/favicon.ico&quot;, &quot;anon&quot;); map.put(&quot;/captcha.jpg&quot;, &quot;anon&quot;); map.put(&quot;/csrf&quot;, &quot;anon&quot;); // 其他所有请求都由JWTFilter处理 map.put(&quot;/**&quot;, &quot;jwt&quot;); shiroFilterFactoryBean.setFilterChainDefinitionMap(map); return shiroFilterFactoryBean; &#125; /** * 不向 Spring容器中注册 JWTFilter Bean，防止 Spring 将 JWTFilter 注册为全局过滤器 * 全局过滤器会对所有请求进行拦截，而本例中只需要拦截除 /login 和 /logout 外的请求 * 另一种简单做法是：不将 JWTFilter 放入 Spring 容器 * &lt;p&gt; * 不添加下面这个 Bean 的话, 会出现 /** jwt 会拦截所有请求的情况 */ @Bean public FilterRegistrationBean&lt;Filter&gt; registration(JWTFilter jwtFilter) &#123; FilterRegistrationBean&lt;Filter&gt; registration = new FilterRegistrationBean&lt;&gt;(jwtFilter); registration.setEnabled(false); return registration; &#125;//// /**// * 下面的代码是添加注解支持// */// @Bean// @DependsOn(&quot;lifecycleBeanPostProcessor&quot;)// public DefaultAdvisorAutoProxyCreator defaultAdvisorAutoProxyCreator() &#123;// DefaultAdvisorAutoProxyCreator defaultAdvisorAutoProxyCreator = new DefaultAdvisorAutoProxyCreator();// // 强制使用cglib，防止重复代理和可能引起代理出错的问题，https://zhuanlan.zhihu.com/p/29161098// defaultAdvisorAutoProxyCreator.setProxyTargetClass(true);// return defaultAdvisorAutoProxyCreator;// &#125;//// @Bean// public LifecycleBeanPostProcessor lifecycleBeanPostProcessor() &#123;// return new LifecycleBeanPostProcessor();// &#125; @Bean public AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor(@Qualifier(&quot;securityManager&quot;) SecurityManager securityManager) &#123; AuthorizationAttributeSourceAdvisor advisor = new AuthorizationAttributeSourceAdvisor(); advisor.setSecurityManager(securityManager); return advisor; &#125;&#125;","categories":[{"name":"个人项目","slug":"个人项目","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/tags/Spring/"},{"name":"Shiro","slug":"Shiro","permalink":"https://xmmarlowe.github.io/tags/Shiro/"},{"name":"Jwt","slug":"Jwt","permalink":"https://xmmarlowe.github.io/tags/Jwt/"}],"author":"Marlowe"},{"title":"SpringBoot + MyBatis-Plus + Swagger快速创建后台项目","slug":"个人项目/SpringBoot-MyBatis-Plus-Swagger快速创建后台项目","date":"2021-06-08T12:07:21.000Z","updated":"2021-06-24T07:30:43.027Z","comments":true,"path":"2021/06/08/个人项目/SpringBoot-MyBatis-Plus-Swagger快速创建后台项目/","link":"","permalink":"https://xmmarlowe.github.io/2021/06/08/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/SpringBoot-MyBatis-Plus-Swagger%E5%BF%AB%E9%80%9F%E5%88%9B%E5%BB%BA%E5%90%8E%E5%8F%B0%E9%A1%B9%E7%9B%AE/","excerpt":"本文主要用于记录综合课程设计Ⅲ中的学习的后台相关插件使用…","text":"本文主要用于记录综合课程设计Ⅲ中的学习的后台相关插件使用… 项目简介快速开始1、GitHub创建项目我的GitHub地址 新建一个代码仓库，用于管理项目代码 2、使用idea将代码下载到本地并新建Springboot模块 新建SpringBoot项目模块 勾选相关依赖 项目创建成功 3、创建数据库 在navcat中新建数据库 新建employee表 4、开始编写代码1. 将application.properties 文件删除并新建application.yml、application-dev.yml、application-prod.yml。2. 分别编写对应的配置文件application.yml 1234567891011121314151617server: port: 8080 servlet: context-path: /spring: profiles: active: prod datasource: druid: url: jdbc:mysql://localhost:3306/weixin?useUnicode=true&amp;characterEncoding=UTF-8&amp;useJDBCCompliantTimezoneShift=true&amp;useLegacyDatetimeCode=false&amp;serverTimezone=GMT%2b8 username: root password: root initial-size: 1 min-idle: 1 max-active: 20 test-on-borrow: true driver-class-name: com.mysql.cj.jdbc.Driver application-dev.yml 1234567891011121314151617181920server: port: 8081 servlet: context-path: /spring: datasource: url: jdbc:mysql://localhost:3306/weixin?useUnicode=true&amp;characterEncoding=UTF-8&amp;useJDBCCompliantTimezoneShift=true&amp;useLegacyDatetimeCode=false&amp;serverTimezone=GMT%2b8 username: root password: root driver-class-name: com.mysql.cj.jdbc.Drivermybatis-plus: # xml地址 mapper-locations: classpath:mapper/*Mapper.xml # 实体扫描，多个package用逗号或者分号分隔 type-aliases-package: com.marlowe.music.entity #自己的实体类地址 configuration: # 这个配置会将执行的sql打印出来，在开发或测试的时候可以用 log-impl: org.apache.ibatis.logging.stdout.StdOutImpl application-prod.yml 1234567891011121314151617181920server: port: 8081 servlet: context-path: /spring: datasource: url: jdbc:mysql://localhost:3306/weixin?useUnicode=true&amp;characterEncoding=UTF-8&amp;useJDBCCompliantTimezoneShift=true&amp;useLegacyDatetimeCode=false&amp;serverTimezone=GMT%2b8 username: root password: root driver-class-name: com.mysql.cj.jdbc.Drivermybatis-plus: # xml地址 mapper-locations: classpath:mapper/*Mapper.xml # 实体扫描，多个package用逗号或者分号分隔 type-aliases-package: com.marlowe.music.entity #自己的实体类地址 configuration: # 这个配置会将执行的sql打印出来，在开发或测试的时候可以用 log-impl: org.apache.ibatis.logging.stdout.StdOutImpl 3. 引入相关依赖12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;!--MySQL--&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.25&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.2.6&lt;/version&gt;&lt;/dependency&gt;&lt;!--fastjson--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.62&lt;/version&gt;&lt;/dependency&gt;&lt;!--swagger在线文档--&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.hankcs&lt;/groupId&gt; &lt;artifactId&gt;hanlp&lt;/artifactId&gt; &lt;version&gt;portable-1.1.5&lt;/version&gt;&lt;/dependency&gt;&lt;!--hutool--&gt;&lt;dependency&gt; &lt;groupId&gt;cn.hutool&lt;/groupId&gt; &lt;artifactId&gt;hutool-all&lt;/artifactId&gt; &lt;version&gt;5.1.0&lt;/version&gt;&lt;/dependency&gt;&lt;!--mybatis-plus--&gt;&lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.0.5&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-generator&lt;/artifactId&gt; &lt;version&gt;3.4.1&lt;/version&gt;&lt;/dependency&gt;&lt;!--MyBatis 分页插件: MyBatis PageHelper--&gt;&lt;!-- https://mvnrepository.com/artifact/com.github.pagehelper/pagehelper --&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- velocity 模板引擎, Mybatis Plus 代码生成器需要 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.velocity&lt;/groupId&gt; &lt;artifactId&gt;velocity-engine-core&lt;/artifactId&gt; &lt;version&gt;2.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.freemarker&lt;/groupId&gt; &lt;artifactId&gt;freemarker&lt;/artifactId&gt; &lt;version&gt;2.3.31&lt;/version&gt;&lt;/dependency&gt; 4. 创建公共包新建commons包，用于统一结果处理，统一异常处理，以及代码生成器配置 统一结果处理 编写错误代码类 ErrorCode.java 123456789101112131415161718192021222324252627public enum ErrorCode &#123; /** * 请求成功 */ SUCCESS(&quot;200&quot;, &quot;请求成功&quot;), /** * 请求失败 */ ERROR(&quot;500&quot;, &quot;请求失败&quot;); private String code; private String message; public String getCode() &#123; return this.code; &#125; public String getMessage() &#123; return this.message; &#125; ErrorCode(String _code, String _message) &#123; this.code = _code; this.message = _message; &#125;&#125; 编写统一结果接口 IResult.java 1234567891011121314151617181920212223public interface IResult&lt;T&gt; &#123; /** * 获得信息 * * @return */ String getMsg(); /** * 获得状态码 * * @return */ String getCode(); /** * 获得数据 * * @return */ T getData();&#125; 编写统一结果实现类 Result.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889@Data@AllArgsConstructor@NoArgsConstructorpublic class Result&lt;T&gt; implements Serializable, IResult&lt;T&gt; &#123; /** * 200:成功 其他：失败 */ private String code; private String msg; private T data; public static Result ok() &#123; return new Result(ErrorCode.SUCCESS.getCode(), ErrorCode.SUCCESS.getMessage(), null); &#125; public static Result ok(String msg) &#123; return new Result(ErrorCode.SUCCESS.getCode(), msg, null); &#125; public static Result ok(String msg, Object data) &#123; return new Result(ErrorCode.SUCCESS.getCode(), msg, data); &#125; public static Result ok(Object data) &#123; return new Result(ErrorCode.SUCCESS.getCode(), ErrorCode.SUCCESS.getMessage(), data); &#125; public static Result error() &#123; return new Result(ErrorCode.ERROR.getCode(), ErrorCode.ERROR.getMessage(), null); &#125; public static Result error(ErrorCode errorCode) &#123; return new Result(errorCode.getCode(), errorCode.getMessage(), null); &#125; public static Result error(String code, String msg) &#123; return new Result(code, msg, null); &#125; public static Result error(String code, String msg, Object data) &#123; return new Result(code, ErrorCode.ERROR.getMessage(), data); &#125; public static Result error(ErrorCode errorCode, String msg) &#123; return new Result(ErrorCode.ERROR.getCode(), msg, null); &#125; /** * 获得信息 * * @return */ @Override public String getMsg() &#123; return this.msg; &#125; /** * 获得状态码 * * @return */ @Override public String getCode() &#123; return this.code; &#125; /** * 获得数据 * * @return */ @Override public T getData() &#123; return this.data; &#125; @Override public String toString() &#123; return &quot;Result&#123;&quot; + &quot;code=&#x27;&quot; + code + &#x27;\\&#x27;&#x27; + &quot;, msg=&#x27;&quot; + msg + &#x27;\\&#x27;&#x27; + &quot;, data=&quot; + data + &#x27;&#125;&#x27;; &#125;&#125; 编写代码自动生成类，并修改配置 CodeGenerator.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293public class CodeGenerator &#123; /** * &lt;p&gt; * 读取控制台内容 * &lt;/p&gt; */ public static String scanner(String tip) &#123; Scanner scanner = new Scanner(System.in); StringBuilder help = new StringBuilder(); help.append(&quot;请输入&quot; + tip + &quot;：&quot;); System.out.println(help.toString()); if (scanner.hasNext()) &#123; String ipt = scanner.next(); if (StringUtils.isNotEmpty(ipt)) &#123; return ipt; &#125; &#125; throw new MybatisPlusException(&quot;请输入正确的&quot; + tip + &quot;！&quot;); &#125; public static void main(String[] args) &#123; // 代码生成器 AutoGenerator mpg = new AutoGenerator(); // 全局配置 GlobalConfig gc = new GlobalConfig(); String projectPath = System.getProperty(&quot;user.dir&quot;); gc.setOutputDir(projectPath + &quot;/src/main/java&quot;); //生成文件输出目录 gc.setAuthor(&quot;marlowe&quot;); gc.setOpen(false); // gc.setSwagger2(true); 实体属性 Swagger2 注解 mpg.setGlobalConfig(gc); // 数据源配置 DataSourceConfig dsc = new DataSourceConfig(); dsc.setUrl(&quot;jdbc:mysql://localhost:3306/music?useUnicode=true&amp;useSSL=false&amp;characterEncoding=utf8&amp;serverTimezone=UTC&quot;); dsc.setDriverName(&quot;com.mysql.cj.jdbc.Driver&quot;); dsc.setUsername(&quot;root&quot;); dsc.setPassword(&quot;root&quot;); mpg.setDataSource(dsc); // 包配置 PackageConfig pc = new PackageConfig();// pc.setModuleName(scanner(&quot;模块名&quot;)); pc.setParent(&quot;com.marlowe.music&quot;); pc.setController(&quot;controller&quot;); pc.setEntity(&quot;entity&quot;); pc.setService(&quot;service&quot;); pc.setMapper(&quot;mapper&quot;); mpg.setPackageInfo(pc); // 自定义配置 InjectionConfig cfg = new InjectionConfig() &#123; @Override public void initMap() &#123; // to do nothing &#125; &#125;; //如果模板引擎是 freemarker String templatePath = &quot;/templates/mapper.xml.ftl&quot;; // 自定义输出配置 List&lt;FileOutConfig&gt; focList = new ArrayList&lt;&gt;(); // 自定义配置会被优先输出// focList.add(new FileOutConfig(templatePath) &#123;// @Override// public String outputFile(TableInfo tableInfo) &#123;// // 自定义输出文件名 ， 如果你 Entity 设置了前后缀、此处注意 xml 的名称会跟着发生变化！！// return projectPath + &quot;/src/main/resources/mapper/&quot; + pc.getModuleName()// + &quot;/&quot; + tableInfo.getEntityName() + &quot;Mapper&quot; + StringPool.DOT_XML;// &#125;// &#125;); cfg.setFileOutConfigList(focList); mpg.setCfg(cfg); //这个必须要,需要提供一个默认的 // 策略配置 StrategyConfig strategy = new StrategyConfig(); strategy.setNaming(NamingStrategy.underline_to_camel);//表名生成策略 strategy.setColumnNaming(NamingStrategy.underline_to_camel);//实体字段生成策略 strategy.setInclude(scanner(&quot;表名&quot;).split(&quot;,&quot;)); //需要生成的表 //strategy.setSuperEntityClass(&quot;com.baomidou.ant.common.BaseEntity&quot;); strategy.setEntityLombokModel(true);//使用lombook strategy.setRestControllerStyle(true); // 公共父类 //strategy.setSuperControllerClass(&quot;com.baomidou.ant.common.BaseController&quot;); // 写于父类中的公共字段// strategy.setSuperEntityColumns(&quot;id&quot;);// strategy.setInclude(scanner(&quot;表名，多个英文逗号分割&quot;).split(&quot;,&quot;));// strategy.setControllerMappingHyphenStyle(true);// strategy.setTablePrefix(pc.getModuleName() + &quot;_&quot;); mpg.setStrategy(strategy); mpg.setTemplateEngine(new FreemarkerTemplateEngine()); mpg.execute(); &#125;&#125; 5. 创建配置类包 编写Swagger配置类 SwaggerConfig.java 1234567891011121314151617181920212223242526272829303132@Configuration@EnableSwagger2public class SwaggerConfig &#123; /** * 配置了Swagger的Docket的bean实例 * * @return */ @Bean public Docket docket() &#123; return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .groupName(&quot;Marlowe&quot;) // enable是否启动Swagger，如果为false，则swagger不能在浏览器中访问 .select() .apis(RequestHandlerSelectors.basePackage(&quot;com.marlowe.music.controller&quot;)) .build(); &#125; public ApiInfo apiInfo() &#123; // 作者信息 Contact contact = new Contact(&quot;Marlowe&quot;, &quot;https://xmmarlowe.github.io&quot;, &quot;marlowe246@qq.com&quot;); return new ApiInfo(&quot;SpringBoot-VUE-Music API Documentation&quot;, &quot;Api Documentation&quot;, &quot;v1.0&quot;, &quot;urn:tos&quot;, contact, &quot;Apache 2.0&quot;, &quot;http://www.apache.org/licenses/LICENSE-2.0&quot;, new ArrayList()); &#125;&#125; 编写Pagehelper配置类 PageHelperConfig.java 12345678910111213141516@Configurationpublic class PageHelperConfig &#123; @Bean public PageHelper pageHelper() &#123; PageHelper pageHelper = new PageHelper(); Properties properties = new Properties(); //把这个设置为true，会带RowBounds第一个参数offset当成PageNum使用 properties.setProperty(&quot;offsetAsPageNum&quot;, &quot;true&quot;); //设置为true时，使用RowBounds分页会进行count查询 properties.setProperty(&quot;rowBoundsWithCount&quot;, &quot;true&quot;); properties.setProperty(&quot;reasonable&quot;, &quot;true&quot;); pageHelper.setProperties(properties); return pageHelper; &#125;&#125; 6. 运行CodeGenerator得到相关代码 编写service接口 IEmployeeService.java 1234567891011121314151617181920212223242526public interface IEmployeeService extends IService&lt;Employee&gt; &#123; /** * 添加员工 * @param employee * @return */ boolean addEmployee(Employee employee); /** * 通过id删除员工 * @param id * @return */ boolean delete(int id); /** * 通过员工姓名分页查询员工 * @param name * @param pageNo * @param pageSize * @return */ PageInfo&lt;Employee&gt; findByName(String name,int pageNo,int pageSize);&#125; 编写service实现类 EmployeeServiceImpl.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Servicepublic class EmployeeServiceImpl extends ServiceImpl&lt;EmployeeMapper, Employee&gt; implements IEmployeeService &#123; @Autowired private EmployeeMapper employeeMapper; /** * 添加员工 * * @param employee * @return */ @Override public boolean addEmployee(Employee employee) &#123; return employeeMapper.insert(employee) &gt; 0; &#125; /** * 通过id删除员工 * * @param id * @return */ @Override public boolean delete(int id) &#123; return employeeMapper.deleteById(id) &gt; 0; &#125; /** * 通过员工姓名分页查询员工 * * @param name * @param pageNo * @param pageSize * @return */ @Override public PageInfo&lt;Employee&gt; findByName(String name, int pageNo, int pageSize) &#123; // 设置分页查询参数 PageHelper.startPage(pageNo, pageSize); QueryWrapper&lt;Employee&gt; queryWrapper = new QueryWrapper&lt;&gt;(); queryWrapper.like(&quot;name&quot;, name); List&lt;Employee&gt; employees = employeeMapper.selectList(queryWrapper); PageInfo&lt;Employee&gt; pageInfo = new PageInfo(employees); return pageInfo; &#125;&#125; 编写controller EmployeeController.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@RestController@Api(tags = &quot;员工管理控制器&quot;)@RequestMapping(&quot;/employee&quot;)public class EmployeeController &#123; @Autowired private EmployeeServiceImpl employeeService; @ApiOperation(value = &quot;添加员工&quot;) @PostMapping(&quot;add&quot;) public String addEmployee(@RequestBody Employee employee) &#123; boolean b = employeeService.addEmployee(employee); if (b) &#123; return &quot;添加成功&quot;; &#125; else &#123; return &quot;添加失败&quot;; &#125; &#125; /** * 根据id删除员工 * * @param id * @return */ @ApiOperation(value = &quot;根据id删除员工&quot;) @PostMapping(&quot;delete/&#123;id&#125;&quot;) public Result deleteEmployee(@PathVariable int id) &#123; boolean delete = employeeService.delete(id); if (delete) &#123; return Result.ok(&quot;删除成功&quot;); &#125; else &#123; return Result.ok(&quot;删除失败&quot;); &#125; &#125; /** * 通过姓名分页查找员工 * * @param name * @return */ @ApiOperation(value = &quot;通过姓名分页查找员工&quot;) @PostMapping(&quot;find/&#123;name&#125;/&#123;pageNo&#125;/&#123;pageSize&#125;&quot;) public Result&lt;List&lt;Employee&gt;&gt; findEmployee(@PathVariable String name, @PathVariable int pageNo, @PathVariable int pageSize) &#123; PageInfo&lt;Employee&gt; pageInfo = employeeService.findByName(name, pageNo, pageSize); List&lt;Employee&gt; employees = pageInfo.getList(); return Result.ok(employees); &#125;&#125; 在启动类加@MapperScan(“mapper的包名”) 1@MapperScan(&quot;com.marlowe.whell.mapper&quot;) 启动代码 访问swagger在线文档 swagger在线文档 分别测试所有接口即可。 参考资料Marlowe’s的个人博客 项目地址","categories":[{"name":"个人项目","slug":"个人项目","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://xmmarlowe.github.io/tags/SpringBoot/"},{"name":"Swagger","slug":"Swagger","permalink":"https://xmmarlowe.github.io/tags/Swagger/"},{"name":"MyBatis-Plus","slug":"MyBatis-Plus","permalink":"https://xmmarlowe.github.io/tags/MyBatis-Plus/"}],"author":"Marlowe"},{"title":"项目实践之MySQL创建存储过程","slug":"个人项目/项目实践之MySQL创建存储过程","date":"2021-06-03T06:59:42.000Z","updated":"2021-06-03T13:52:06.610Z","comments":true,"path":"2021/06/03/个人项目/项目实践之MySQL创建存储过程/","link":"","permalink":"https://xmmarlowe.github.io/2021/06/03/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5%E4%B9%8BMySQL%E5%88%9B%E5%BB%BA%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/","excerpt":"","text":"问题来源歌曲表： 歌手表： 起初，我将歌手表和歌曲表分离，并没有在歌曲表中存储歌手的名字，需要利用歌曲表中的singer_id在歌曲表中进行连接查询才能获得歌手的姓名。 但是，后面发现连接查询的效率过低，因此选择将歌手的名字singer_name 冗余在歌曲表中，提升查询效率。 解决方案方案一 修改歌曲表，添加singer_name字段 清空数据库，修改实体类，修改爬虫，直接将歌手名插入到数据库中 实现方式： 123456789101112// 将数据封装，插入数据库Song song = new Song() .setSongId(songId) .setSingerId(singerId) .setLyric(lyric) .setUrl(songRealUrl) .setName(title) // 设置歌手姓名 .setSingerName(singerName) .setIsDownload(0);// 将歌曲信息插入数据库int insert1 = songMapper.insert(song); 方案二(推荐) 修改歌曲表，添加singer_name字段 在数据库层，通过歌曲表的singer_id在歌手表中查询歌手的姓名 将歌手姓名插入到歌曲表的singer_name字段中 实现方式： 123456789101112131415161718192021-- 创建存储过程 create PROCEDURE test()BEGIN -- 定义变量 DECLARE index_id int DEFAULT 1 ; DECLARE singerName VARCHAR(255); DECLARE singerID VARCHAR(255); -- 遍历歌手表 while index_id &lt; 1499 do -- 提取出歌手id和歌手姓名存储在变量中 SELECT name,singer_id into singerName,singerID from singer where id = index_id; -- 根据歌手id更新歌曲表的歌手姓名 update song set singer_name = singerName where singer_id=singerID; -- 下标自增 set index_id = index_id + 1; end while;END-- 调用存储过程CALL test() 结果：singer表：1498song表：63304 数据量大，执行速度有点缓慢 总结在系统开发前，尽量设计好数据库，不然改数据库结构有时候对开发很不友好。","categories":[{"name":"个人项目","slug":"个人项目","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://xmmarlowe.github.io/tags/MySQL/"},{"name":"存储过程","slug":"存储过程","permalink":"https://xmmarlowe.github.io/tags/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/"}],"author":"Marlowe"},{"title":"MySQL索引详解","slug":"数据库/MySQL索引详解","date":"2021-06-02T14:41:36.000Z","updated":"2021-06-03T07:01:03.772Z","comments":true,"path":"2021/06/02/数据库/MySQL索引详解/","link":"","permalink":"https://xmmarlowe.github.io/2021/06/02/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL%E7%B4%A2%E5%BC%95%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"何为索引？有什么作用？索引是一种用于快速查询和检索数据的数据结构。常见的索引结构有: B 树， B+树和 Hash。 索引的作用就相当于目录的作用。打个比方: 我们在查字典的时候，如果没有目录，那我们就只能一页一页的去找我们需要查的那个字，速度很慢。如果有目录了，我们只需要先去目录里查找字的位置，然后直接翻到那一页就行了。 索引的优缺点优点 ： 使用索引可以大大加快 数据的检索速度（大大减少的检索的数据量）, 这也是创建索引的最主要的原因。 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。 缺点 ： 创建索引和维护索引需要耗费许多时间。当对表中的数据进行增删改的时候，如果数据有索引，那么索引也需要动态的修改，会降低 SQL 执行效率。 索引需要使用物理文件存储，也会耗费一定空间。 但是，使用索引一定能提高查询性能吗? 大多数情况下，索引查询都是比全表扫描要快的。但是如果数据库的数据量不大，那么使用索引也不一定能够带来很大提升。（到底多大，走索引才比全表扫描慢） 索引的底层数据结构Hash表 &amp; B+树哈希表是键值对的集合，通过键(key)即可快速取出对应的值(value)，因此哈希表可以快速检索数据（接近 O（1））。 为何能够通过 key 快速取出 value呢？ 原因在于 哈希算法（也叫散列算法）。通过哈希算法，我们可以快速找到 value 对应的 index，找到了 index 也就找到了对应的 value。 12hash = hashfunc(key)index = hash % array_size 但是！哈希算法有个 Hash 冲突 问题，也就是说多个不同的 key 最后得到的 index 相同。通常情况下，我们常用的解决办法是 链地址法。链地址法就是将哈希冲突数据存放在链表中。就比如 JDK1.8 之前 HashMap 就是通过链地址法来解决哈希冲突的。不过，JDK1.8 以后HashMap为了减少链表过长的时候搜索时间过长引入了红黑树。 为了减少 Hash 冲突的发生，一个好的哈希函数应该“均匀地”将数据分布在整个可能的哈希值集合中。 既然哈希表这么快，为什么MySQL 没有使用其作为索引的数据结构呢？ 1.Hash 冲突问题 ：我们上面也提到过Hash 冲突了，不过对于数据库来说这还不算最大的缺点。 2.Hash 索引不支持顺序和范围查询(Hash 索引不支持顺序和范围查询是它最大的缺点： 假如我们要对表中的数据进行排序或者进行范围查询，那 Hash 索引可就不行了。 试想一种情况: 1SELECT * FROM tb1 WHERE id &lt; 500;Copy to clipboardErrorCopied 在这种范围查询中，优势非常大，直接遍历比 500 小的叶子节点就够了。而 Hash 索引是根据 hash 算法来定位的，难不成还要把 1 - 499 的数据，每个都进行一次 hash 计算来定位吗?这就是 Hash 最大的缺点了。 B 树&amp; B+树B 树也称 B-树,全称为 多路平衡查找树 ，B+ 树是 B 树的一种变体。B 树和 B+树中的 B 是 Balanced （平衡）的意思。 目前大部分数据库系统及文件系统都采用 B-Tree 或其变种 B+Tree 作为索引结构。 B 树&amp; B+树两者有何异同呢？ B 树的所有节点既存放键(key) 也存放 数据(data)，而 B+树只有叶子节点存放 key 和 data，其他内节点只存放 key。 B 树的叶子节点都是独立的;B+树的叶子节点有一条引用链指向与它相邻的叶子节点。 B 树的检索的过程相当于对范围内的每个节点的关键字做二分查找，可能还没有到达叶子节点，检索就结束了。而 B+树的检索效率就很稳定了，任何查找都是从根节点到叶子节点的过程，叶子节点的顺序检索很明显。 在 MySQL 中，MyISAM 引擎和 InnoDB 引擎都是使用 B+Tree 作为索引结构，但是，两者的实现方式不太一样。（下面的内容整理自《Java 工程师修炼之道》） MyISAM 引擎中，B+Tree 叶节点的 data 域存放的是数据记录的地址。在索引检索的时候，首先按照 B+Tree 搜索算法搜索索引，如果指定的 Key 存在，则取出其 data 域的值，然后以 data 域的值为地址读取相应的数据记录。这被称为“非聚簇索引”。 InnoDB 引擎中，其数据文件本身就是索引文件。相比 MyISAM，索引文件和数据文件是分离的，其表数据文件本身就是按 B+Tree 组织的一个索引结构，树的叶节点 data 域保存了完整的数据记录。这个索引的 key 是数据表的主键，因此 InnoDB 表数据文件本身就是主索引。这被称为“聚簇索引（或聚集索引）”，而其余的索引都作为辅助索引，辅助索引的 data 域存储相应记录主键的值而不是地址，这也是和 MyISAM 不同的地方。在根据主索引搜索时，直接找到 key 所在的节点即可取出数据；在根据辅助索引查找时，则需要先取出主键的值，在走一遍主索引。 因此，在设计表的时候，不建议使用过长的字段作为主键，也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂。 索引类型主键索引(Primary Key)数据表的主键列使用的就是主键索引。 一张数据表有只能有一个主键，并且主键不能为 null，不能重复。 在 MySQL 的 InnoDB 的表中，当没有显示的指定表的主键时，InnoDB 会自动先检查表中是否有唯一索引的字段，如果有，则选择该字段为默认的主键，否则 InnoDB 将会自动创建一个 6Byte 的自增主键。 二级索引(辅助索引)二级索引又称为辅助索引，是因为二级索引的叶子节点存储的数据是主键。也就是说，通过二级索引，可以定位主键的位置。 唯一索引，普通索引，前缀索引等索引属于二级索引。 PS:不懂的同学可以暂存疑，慢慢往下看，后面会有答案的，也可以自行搜索。 唯一索引(Unique Key) ：唯一索引也是一种约束。唯一索引的属性列不能出现重复的数据，但是允许数据为 NULL，一张表允许创建多个唯一索引。 建立唯一索引的目的大部分时候都是为了该属性列的数据的唯一性，而不是为了查询效率。 普通索引(Index) ：普通索引的唯一作用就是为了快速查询数据，一张表允许创建多个普通索引，并允许数据重复和 NULL。 前缀索引(Prefix) ：前缀索引只适用于字符串类型的数据。前缀索引是对文本的前几个字符创建索引，相比普通索引建立的数据更小， 因为只取前几个字符。 全文索引(Full Text) ：全文索引主要是为了检索大文本数据中的关键字的信息，是目前搜索引擎数据库使用的一种技术。Mysql5.6 之前只有 MYISAM 引擎支持全文索引，5.6 之后 InnoDB 也支持了全文索引。 二级索引: 聚集索引与非聚集索引聚集索引聚集索引即索引结构和数据一起存放的索引。主键索引属于聚集索引。 在 Mysql 中，InnoDB 引擎的表的 .ibd文件就包含了该表的索引和数据，对于 InnoDB 引擎表来说，该表的索引(B+树)的每个非叶子节点存储索引，叶子节点存储索引和索引对应的数据。 聚集索引的优点聚集索引的查询速度非常的快，因为整个 B+树本身就是一颗多叉平衡树，叶子节点也都是有序的，定位到索引的节点，就相当于定位到了数据。 聚集索引的缺点 依赖于有序的数据 ：因为 B+树是多路平衡树，如果索引的数据不是有序的，那么就需要在插入时排序，如果数据是整型还好，否则类似于字符串或 UUID 这种又长又难比较的数据，插入或查找的速度肯定比较慢。 更新代价大 ： 如果对索引列的数据被修改时，那么对应的索引也将会被修改， 而且况聚集索引的叶子节点还存放着数据，修改代价肯定是较大的， 所以对于主键索引来说，主键一般都是不可被修改的。 非聚集索引非聚集索引即索引结构和数据分开存放的索引。 二级索引属于非聚集索引。 MYISAM 引擎的表的.MYI 文件包含了表的索引， 该表的索引(B+树)的每个叶子非叶子节点存储索引， 叶子节点存储索引和索引对应数据的指针，指向.MYD 文件的数据。非聚集索引的叶子节点并不一定存放数据的指针， 因为二级索引的叶子节点就存放的是主键，根据主键再回表查数据。 非聚集索引的优点更新代价比聚集索引要小。 非聚集索引的更新代价就没有聚集索引那么大了，非聚集索引的叶子节点是不存放数据的 非聚集索引的缺点 跟聚集索引一样，非聚集索引也依赖于有序的数据 可能会二次查询(回表) :这应该是非聚集索引最大的缺点了。 当查到索引对应的指针或主键后，可能还需要根据指针或主键再到数据文件或表中查询。 这是 MySQL 的表的文件截图: 聚集索引和非聚集索引: 非聚集索引一定回表查询吗(覆盖索引)?非聚集索引不一定回表查询。 试想一种情况，用户准备使用 SQL 查询用户名，而用户名字段正好建立了索引。 1SELECT name FROM table WHERE name=&#x27;guang19&#x27;; 那么这个索引的 key 本身就是 name，查到对应的 name 直接返回就行了，无需回表查询。 即使是 MYISAM 也是这样，虽然 MYISAM 的主键索引确实需要回表， 因为它的主键索引的叶子节点存放的是指针。但是如果 SQL 查的就是主键呢? 1SELECT id FROM table WHERE id=1; 主键索引本身的 key 就是主键，查到返回就行了。这种情况就称之为覆盖索引了。 覆盖索引如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为“覆盖索引”。我们知道在 InnoDB 存储引擎中，如果不是主键索引，叶子节点存储的是主键+列值。最终还是要“回表”，也就是要通过主键再查找一次。这样就会比较慢覆盖索引就是把要查询出的列和索引是对应的，不做回表操作！ 覆盖索引即需要查询的字段正好是索引的字段，那么直接根据该索引，就可以查到数据了， 而无需回表查询。 如主键索引，如果一条 SQL 需要查询主键，那么正好根据主键索引就可以查到主键。再如普通索引，如果一条 SQL 需要查询 name，name 字段正好有索引， 那么直接根据这个索引就可以查到数据，也无需回表。 覆盖索引: 创建索引的注意事项1.选择合适的字段创建索引： 不为 NULL 的字段 ：索引字段的数据应该尽量不为 NULL，因为对于数据为 NULL 的字段，数据库较难优化。如果字段频繁被查询，但又避免不了为 NULL，建议使用 0,1,true,false 这样语义较为清晰的短值或短字符作为替代。 被频繁查询的字段 ：我们创建索引的字段应该是查询操作非常频繁的字段。 被作为条件查询的字段 ：被作为 WHERE 条件查询的字段，应该被考虑建立索引。 频繁需要排序的字段 ：索引已经排序，这样查询可以利用索引的排序，加快排序查询时间。 被经常频繁用于连接的字段 ：经常用于连接的字段可能是一些外键列，对于外键列并不一定要建立外键，只是说该列涉及到表与表的关系。对于频繁被连接查询的字段，可以考虑建立索引，提高多表连接查询的效率。 2.被频繁更新的字段应该慎重建立索引。 虽然索引能带来查询上的效率，但是维护索引的成本也是不小的。 如果一个字段不被经常查询，反而被经常修改，那么就更不应该在这种字段上建立索引了。 3.尽可能的考虑建立联合索引而不是单列索引。 因为索引是需要占用磁盘空间的，可以简单理解为每个索引都对应着一颗 B+树。如果一个表的字段过多，索引过多，那么当这个表的数据达到一个体量后，索引占用的空间也是很多的，且修改索引时，耗费的时间也是较多的。如果是联合索引，多个字段在一个索引上，那么将会节约很大磁盘空间，且修改数据的操作效率也会提升。 4.注意避免冗余索引 。 冗余索引指的是索引的功能相同，能够命中索引(a, b)就肯定能命中索引(a) ，那么索引(a)就是冗余索引。如（name,city ）和（name ）这两个索引就是冗余索引，能够命中前者的查询肯定是能够命中后者的 在大多数情况下，都应该尽量扩展已有的索引而不是创建新索引。 5.考虑在字符串类型的字段上使用前缀索引代替普通索引。 前缀索引仅限于字符串类型，较普通索引会占用更小的空间，所以可以考虑使用前缀索引带替普通索引。 使用索引的一些建议 对于中到大型表索引都是非常有效的，但是特大型表的话维护开销会很大，不适合建索引 避免 where 子句中对字段施加函数，这会造成无法命中索引。 在使用 InnoDB 时使用与业务无关的自增主键作为主键，即使用逻辑主键，而不要使用业务主键。 删除长期未使用的索引，不用的索引的存在会造成不必要的性能损耗 MySQL 5.7 可以通过查询 sys 库的 schema_unused_indexes 视图来查询哪些索引从未被使用 在使用 limit offset 查询缓慢时，可以借助索引来提高性能 MySQL 如何为表字段添加索引？1.添加 PRIMARY KEY（主键索引） 1ALTER TABLE `table_name` ADD PRIMARY KEY ( `column` ) 2.添加 UNIQUE(唯一索引) 1ALTER TABLE `table_name` ADD UNIQUE ( `column` ) 3.添加 INDEX(普通索引) 1ALTER TABLE `table_name` ADD INDEX index_name ( `column` ) 4.添加 FULLTEXT(全文索引) 1ALTER TABLE `table_name` ADD FULLTEXT ( `column`) 5.添加多列索引 1ALTER TABLE `table_name` ADD INDEX index_name ( `column1`, `column2`, `column3` ) 参考MySQL数据库索引总结","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"索引","slug":"索引","permalink":"https://xmmarlowe.github.io/tags/%E7%B4%A2%E5%BC%95/"}],"author":"Marlowe"},{"title":"MySQL之主从复制与读写分离","slug":"数据库/MySQL之主从复制与读写分离","date":"2021-06-02T14:41:36.000Z","updated":"2021-08-26T10:09:44.595Z","comments":true,"path":"2021/06/02/数据库/MySQL之主从复制与读写分离/","link":"","permalink":"https://xmmarlowe.github.io/2021/06/02/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL%E4%B9%8B%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E4%B8%8E%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/","excerpt":"MySQL主从复制是其最重要的功能之一。主从复制是指一台服务器充当主数据库服务器，另一台或多台服务器充当从数据库服务器，主服务器中的数据自动复制到从服务器之中。对于多级复制，数据库服务器即可充当主机，也可充当从机。MySQL主从复制的基础是主服务器对数据库修改记录二进制日志，从服务器通过主服务器的二进制日志自动执行更新。","text":"MySQL主从复制是其最重要的功能之一。主从复制是指一台服务器充当主数据库服务器，另一台或多台服务器充当从数据库服务器，主服务器中的数据自动复制到从服务器之中。对于多级复制，数据库服务器即可充当主机，也可充当从机。MySQL主从复制的基础是主服务器对数据库修改记录二进制日志，从服务器通过主服务器的二进制日志自动执行更新。 主从复制主要涉及三个线程: binlog 线程、I/O 线程和 SQL 线程。 binlog 线程: 负责将主服务器上的数据更改写入二进制日志中。 I/O 线程: 负责从主服务器上读取二进制日志，并写入从服务器的中继日志中。 SQL 线程: 负责读取中继日志并重放其中的 SQL 语句。 一、简介MySQL主从复制是其最重要的功能之一。主从复制是指一台服务器充当主数据库服务器，另一台或多台服务器充当从数据库服务器，主服务器中的数据自动复制到从服务器之中。对于多级复制，数据库服务器即可充当主机，也可充当从机。MySQL主从复制的基础是主服务器对数据库修改记录二进制日志，从服务器通过主服务器的二进制日志自动执行更新。 二、主从复制的类型1.基于语句的复制：主服务器上面执行的语句在从服务器上面再执行一遍，在MySQL-3.23版本以后支持。 存在的问题：时间上可能不完全同步造成偏差，执行语句的用户也可能是不同一个用户。 2.基于行的复制：把主服务器上面改编后的内容直接复制过去，而不关心到底改变该内容是由哪条语句引发的，在MySQL-5.0版本以后引入。 存在的问题：比如一个工资表中有一万个用户，我们把每个用户的工资+1000，那么基于行的复制则要复制一万行的内容，由此造成的开销比较大，而基于语句的复制仅仅一条语句就可以了。 3.混合类型的复制：MySQL默认使用基于语句的复制，当基于语句的复制会引发问题的时候就会使用基于行的复制，MySQL会自动进行选择。 在MySQL主从复制架构中，读操作可以在所有的服务器上面进行，而写操作只能在主服务器上面进行。主从复制架构虽然给读操作提供了扩展，可如果写操作也比较多的话（多台从服务器还要从主服务器上面同步数据），单主模型的复制中主服务器势必会成为性能瓶颈。 三、主从复制的工作原理1、基于语句的复制： 主服务器上面执行的语句在从服务器上面再执行一遍，在MySQL-3.23版本以后支持。 存在的问题： 时间上可能不完全同步造成偏差，执行语句的用户也可能是不同一个用户。 2、基于行的复制： 把主服务器上面改编后的内容直接复制过去，而不关心到底改变该内容是由哪条语句引发的，在MySQL-5.0版本以后引入。 存在的问题： 比如一个工资表中有一万个用户，我们把每个用户的工资+1000，那么基于行的复制则要复制一万行的内容，由此造成的开销比较大，而基于语句的复制仅仅一条语句就可以了。 3、混合类型的复制： MySQL默认使用基于语句的复制，当基于语句的复制会引发问题的时候就会使用基于行的复制，MySQL会自动进行选择。 在MySQL主从复制架构中，读操作可以在所有的服务器上面进行，而写操作只能在主服务器上面进行。主从复制架构虽然给读操作提供了扩展，可如果写操作也比较多的话（多台从服务器还要从主服务器上面同步数据），单主模型的复制中主服务器势必会成为性能瓶颈。 如下图所示： 主服务器上面的任何修改都会保存在二进制日志Binary log里面，从服务器上面启动一个I/O thread（实际上就是一个主服务器的客户端进程），连接到主服务器上面请求读取二进制日志，然后把读取到的二进制日志写到本地的一个Realy log里面。从服务器上面开启一个SQL thread定时检查Realy log，如果发现有更改立即把更改的内容在本机上面执行一遍。 如果一主多从的话，这时主库既要负责写又要负责为几个从库提供二进制日志。此时可以稍做调整，将二进制日志只给某一从，这一从再开启二进制日志并将自己的二进制日志再发给其它从。或者是干脆这个从不记录只负责将二进制日志转发给其它从，这样架构起来性能可能要好得多，而且数据之间的延时应该也稍微要好一些。 工作原理图如下： 实际上在老版本的MySQL主从复制中Slave端并不是两个进程完成的，而是由一个进程完成。但是后来发现这样做存在较大的风险和性能问题，主要如下： 首先，一个进程会使复制bin-log日志和解析日志并在自身执行的过程成为一个串行的过程，性能受到了一定的限制，异步复制的延迟也会比较长。 另外，Slave端从Master端获取bin-log过来之后，需要接着解析日志内容，然后在自身执行。在这个过程中，Master端可能又产生了大量变化并新增了大量的日志。如果在这个阶段Master端的存储出现了无法修复的错误，那么在这个阶段所产生的所有变更都将永远无法找回。如果在Slave端的压力比较大的时候，这个过程的时间可能会比较长。 为了提高复制的性能并解决存在的风险，后面版本的MySQL将Slave端的复制动作交由两个进程来完成。提出这个改进方案的人是Yahoo!的一位工程师“Jeremy Zawodny”。这样既解决了性能问题，又缩短了异步的延时时间，同时也减少了可能存在的数据丢失量。 当然，即使是换成了现在这样两个线程处理以后，同样也还是存在slave数据延时以及数据丢失的可能性的，毕竟这个复制是异步的。只要数据的更改不是在一个事物中，这些问题都是会存在的。如果要完全避免这些问题，就只能用MySQL的cluster来解决了。不过MySQL的cluster是内存数据库的解决方案，需要将所有数据都load到内存中，这样就对内存的要求就非常大了，对于一般的应用来说可实施性不是太大。 还有一点要提的是MySQL的复制过滤(Replication Filters)，复制过滤可以让你只复制服务器中的一部分数据。有两种复制过滤：在Master上过滤二进制日志中的事件；在Slave上过滤中继日志中的事件。如下： 配置Master的my.cnf文件(关键性的配置)/etc/my.cnf 123456789101112131415161718192021222324252627282930313233log-bin=mysql-binserver-id = 1binlog-do-db=icingabinlog-do-db=DB2 //如果备份多个数据库，重复设置这个选项即可binlog-do-db=DB3 //需要同步的数据库，如果没有本行，即表示同步所有的数据库binlog-ignore-db=mysql //被忽略的数据库配置Slave的my.cnf文件(关键性的配置)/etc/my.cnflog-bin=mysql-binserver-id=2master-host=10.1.68.110master-user=backupmaster-password=1234qwermaster-port=3306replicate-do-db=icingareplicate-do-db=DB2replicate-do-db=DB3 //需要同步的数据库，如果没有本行，即表示同步所有的数据库replicate-ignore-db=mysql //被忽略的数据库 猜想binlog-do-db参数用于主服务器中，通过过滤Binary Log来过滤掉配置文件中不允许复制的数据库，也就是不向Binary Log中写入不允许复制数据的操作日志；而replicate-do-db用于从服务器中，通过过滤Relay Log来过滤掉不允许复制的数据库或表，也就是执行Relay Log中的动作时不执行那些不被允许的修改动作。这样的话，多个从数据库服务器的情况：有的从服务器既从主服务器中复制数据，又做为主服务器向另外的从服务器复制数据，那它的配置文件中应该可以同时存在binlog-do-db、replicate-do-db这两个参数才对。一切都是自己的预测，关于binlog-do-db、replicate-do-db的具体使用方法还得在实际开发中一点点摸索才可以。 网上有说，复制时忽略某些数据库或者表的操作最好不要在主服务器上面进行，因为主服务器忽略之后就不会再往二进制文件中写了，但是在从服务器上面虽然忽略了某些数据库但是主服务器上面的这些操作信息依然会被复制到从服务器上面的relay log里面，只是不会在从服务器上面执行而已。我想这个意思应该是建议在从服务器中设置replicate-do-db，而不要在主服务器上设置binlog-do-db。 另外，不管是黑名单（binlog-ignore-db、replicate-ignore-db）还是白名单（binlog-do-db、replicate-do-db）只写一个就行了，如果同时使用那么只有白名单生效。 四、主从复制的过程MySQL主从复制的两种情况：同步复制和异步复制，实际复制架构中大部分为异步复制。 复制的基本过程如下： Slave上面的IO进程连接上Master，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容。 Master接收到来自Slave的IO进程的请求后，负责复制的IO进程会根据请求信息读取日志指定位置之后的日志信息，返回给Slave的IO进程。返回信息中除了日志所包含的信息之外，还包括本次返回的信息已经到Master端的bin-log文件的名称以及bin-log的位置。 Slave的IO进程接收到信息后，将接收到的日志内容依次添加到Slave端的relay-log文件的最末端，并将读取到的Master端的 bin-log的文件名和位置记录到master-info文件中，以便在下一次读取的时候能够清楚的告诉Master“我需要从某个bin-log的哪个位置开始往后的日志内容，请发给我”。 Slave的Sql进程检测到relay-log中新增加了内容后，会马上解析relay-log的内容成为在Master端真实执行时候的那些可执行的内容，并在自身执行。 五、主从复制的具体配置复制通常用来创建主节点的副本，通过添加冗余节点来保证高可用性，当然复制也可以用于其他用途，例如在从节点上进行数据读、分析等等。在横向扩展的业务中，复制很容易实施，主要表现在在利用主节点进行写操作，多个从节点进行读操作，MySQL复制的异步性是指：事物首先在主节点上提交，然后复制给从节点并在从节点上应用，这样意味着在同一个时间点主从上的数据可能不一致。异步复制的好处在于它比同步复制要快，如果对数据的一致性要求很高，还是采用同步复制较好。 最简单的复制模式就是一主一从的复制模式了，这样一个简单的架构只需要三个步骤即可完成： （1）建立一个主节点，开启binlog，设置服务器id； （2）建立一个从节点，设置服务器id； （3）将从节点连接到主节点上。 六、深入了解MySQL主从配置1. 一主多从由一个master和一个slave组成复制系统是最简单的情况。Slave之间并不相互通信，只能与master进行通信。在实际应用场景中，MySQL复制90%以上都是一个Master复制到一个或者多个Slave的架构模式，主要用于读压力比较大的应用的数据库端廉价扩展解决方案。 在上图中，是我们开始时提到的一主多从的情况，这时主库既要负责写又要负责为几个从库提供二进制日志。这种情况将二进制日志只给某一从，这一从再开启二进制日志并将自己的二进制日志再发给其它从，或者是干脆这个从不记录只负责将二进制日志转发给其它从，这样架构起来性能可能要好得多，而且数据之间的延时应该也稍微要好一些。PS：这些前面都写过了，又复制了一遍。 2. 主主复制 上图中，Master-Master复制的两台服务器，既是master，又是另一台服务器的slave。这样，任何一方所做的变更，都会通过复制应用到另外一方的数据库中。在这种复制架构中，各自上运行的不是同一db，比如左边的是db1,右边的是db2，db1的从在右边反之db2的从在左边，两者互为主从，再辅助一些监控的服务还可以实现一定程度上的高可以用。 3.主动—被动模式的Master-Master(Master-Master in Active-Passive Mode) 上图中，这是由master-master结构变化而来的，它避免了M-M的缺点，实际上，这是一种具有容错和高可用性的系统。它的不同点在于其中只有一个节点在提供读写服务，另外一个节点时刻准备着，当主节点一旦故障马上接替服务。比如通过corosync+pacemaker+drbd+MySQL就可以提供这样一组高可用服务，主备模式下再跟着slave服务器，也可以实现读写分离。 4.带从服务器的Master-Master结构(Master-Master with Slaves) 这种结构的优点就是提供了冗余。在地理上分布的复制结构，它不存在单一节点故障问题，而且还可以将读密集型的请求放到slave上。 5.MySQL-5.5支持半同步复制早前的MySQL复制只能是基于异步来实现，从MySQL-5.5开始，支持半自动复制。在以前的异步（asynchronous）复制中，主库在执行完一些事务后，是不会管备库的进度的。如果备库处于落后，而更不幸的是主库此时又出现Crash（例如宕机），这时备库中的数据就是不完整的。简而言之，在主库发生故障的时候，我们无法使用备库来继续提供数据一致的服务了。Semisynchronous Replication(半同步复制)则一定程度上保证提交的事务已经传给了至少一个备库。Semi synchronous中，仅仅保证事务的已经传递到备库上，但是并不确保已经在备库上执行完成了。 此外，还有一种情况会导致主备数据不一致。在某个session中，主库上提交一个事务后，会等待事务传递给至少一个备库，如果在这个等待过程中主库Crash，那么也可能备库和主库不一致，这是很致命的。如果主备网络故障或者备库挂了，主库在事务提交后等待10秒（rpl_semi_sync_master_timeout的默认值）后，就会继续。这时，主库就会变回原来的异步状态。 MySQL在加载并开启Semi-sync插件后，每一个事务需等待备库接收日志后才返回给客户端。如果做的是小事务，两台主机的延迟又较小，则Semi-sync可以实现在性能很小损失的情况下的零数据丢失。 读写分离主服务器处理写操作以及实时性要求比较高的读操作，而从服务器处理读操作。 读写分离能提高性能的原因在于: 主从服务器负责各自的读和写，极大程度缓解了锁的争用； 从服务器可以使用 MyISAM，提升查询性能以及节约系统开销； 增加冗余，提高可用性。 读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。 参考MySQL - 主从复制与读写分离 MySql 主从复制及配置实现","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://xmmarlowe.github.io/tags/MySQL/"}],"author":"Marlowe"},{"title":"动态规划和贪心算法","slug":"算法与数据结构/动态规划和贪心算法","date":"2021-06-01T14:36:02.000Z","updated":"2021-06-01T15:14:48.407Z","comments":true,"path":"2021/06/01/算法与数据结构/动态规划和贪心算法/","link":"","permalink":"https://xmmarlowe.github.io/2021/06/01/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E5%92%8C%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/","excerpt":"","text":"动态规划动态规划）与分治法相似，都是通过组合子问题的解来求解原问题。分治法将问题划分为互不相交的子问题，递归求解子问题，再将它们的解组合起来，求出原问题的解。与之相反，动态规划应用于子问题重叠的情况，即不同的子问题具有公共的子子问题（子问题的求解释递归进行的，将其划分为更小的子子问题）。这种情况下，动态规划对公共子子问题只求一次解，而分治法会反复求解公共子子问题。 贪心算法从问题的某一个初始解出发逐步逼近给定的目标，以尽可能快的地求得更好的解。当达到某算法中的某一步不能再继续前进时，算法停止。 该算法存在问题： 不能保证求得的最后解是最佳的； 不能用来求最大或最小解问题； 只能求满足某些约束条件的可行解的范围。 区别动态规划算法： 1.全局最优解中一定包含某个局部最优解，但不一定包含前一个局部最优解，因此需要记录之前的所有最优解。 2.动态规划的关键是状态转移方程，即如何由以求出的局部最优解来推导全局最优解 3.边界条件：即最简单的，可以直接得出的局部最优解 贪心算法： 1.贪心算法中，作出的每步贪心决策都无法改变，因为贪心策略是由上一步的最优解推导下一步的最优解，而上一部之前的最优解则不作保留。 2.由（1）中的介绍，可以知道贪心法正确的条件是：每一步的最优解一定包含上一步的最优解。 区别: 1) 动态规划算法中，每步所做的选择往往依赖于相关子问题的解，因而只有在解出相关子问题时才能做出选择。而贪心算法，仅在当前状态下做出最好选择，即局部最优选择，然后再去解做出这个选择后产生的相应的子问题。 2) 动态规划算法通常以自底向上的方式解各子问题，而贪心算法则通常自顶向下的方式进行。","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"https://xmmarlowe.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"DP","slug":"DP","permalink":"https://xmmarlowe.github.io/tags/DP/"},{"name":"贪心","slug":"贪心","permalink":"https://xmmarlowe.github.io/tags/%E8%B4%AA%E5%BF%83/"}],"author":"Marlowe"},{"title":"Mybatis传参的几种方式","slug":"Spring/Mybatis传参的几种方式","date":"2021-06-01T11:35:26.000Z","updated":"2021-06-01T15:14:48.388Z","comments":true,"path":"2021/06/01/Spring/Mybatis传参的几种方式/","link":"","permalink":"https://xmmarlowe.github.io/2021/06/01/Spring/Mybatis%E4%BC%A0%E5%8F%82%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F/","excerpt":"","text":"#｛｝第一种情形，传入单个参数 userIdservice层： 12345678@Overridepublic User getUserInfo(Integer userId) &#123; User user = userMapper.getUserInfo(userId); //省略 业务代码... return user;&#125; mapper层： 1User getUserInfo(Integer userId); mapper.xml： 123456&lt;!--查询--&gt;&lt;select id=&quot;getUserInfo&quot; resultType=&quot;com.demo.elegant.pojo.User&quot;&gt; select userId from users where userId=#&#123;userId&#125;;&lt;/select&gt; 第二种情况，传入多个参数 userId,sex 使用索引对应值按照顺序传参 注意mapper层和xml层！ service层： 123456@Overridepublic User getUserInfo(Integer userId,String sex) &#123; User user = userMapper.getUserInfo(userId,sex); //省略 业务代码... return user;&#125; mapper层： 1User getUserInfo(Integer userId,String sex); mapper.xml： 123456&lt;!--查询--&gt;&lt;select id=&quot;getUserInfo&quot; resultType=&quot;com.demo.elegant.pojo.User&quot;&gt; select userId from users where userId=#&#123;0&#125; and sex=#&#123;1&#125;;&lt;/select&gt; 第三种情形，传入多个参数 userId,sex 使用注解@Paramservice层： 123456@Overridepublic User getUserInfo(Integer userId,String sex) &#123; User user = userMapper.getUserInfo(userId,sex); //省略 业务代码... return user;&#125; mapper层： 1User getUserInfo(@Param(&quot;userId&quot;)Integer userId,@Param(&quot;sex&quot;)String sex); mapper.xml： 123456&lt;!--查询--&gt;&lt;select id=&quot;getUserInfo&quot; resultType=&quot;com.demo.elegant.pojo.User&quot;&gt; select userId from users where userId=#&#123;userId&#125; and sex=#&#123;sex&#125;;&lt;/select&gt; 第四种情形，传入多个参数 使用User实体类传入service层： 123456@Overridepublic User getUserInfo(User user) &#123; User userInfo = userMapper.getUserInfo(user); //省略 业务代码... return userInfo;&#125; mapper层： 1User getUserInfo(User User); mapper.xml： 123456&lt;!--查询--&gt;&lt;select id=&quot;getUserInfo&quot; parameterType=&quot;User&quot; resultType=&quot;com.demo.elegant.pojo.User&quot;&gt; select userId from users where userId=#&#123;userId&#125; and sex=#&#123;sex&#125;;&lt;/select&gt; 第五种情形，传入多个参数， 使用Map类传入service层： 123456@Overridepublic User getUserInfo(Map map) &#123; User user = userMapper.getUserInfo(map); //省略 业务代码... return user;&#125; mapper层： 1User getUserInfo(Map map); mapper.xml： 123456&lt;!--查询--&gt;&lt;select id=&quot;getUserInfo&quot; parameterType=&quot;Map&quot; resultType=&quot;com.demo.elegant.pojo.User&quot;&gt; select userId from users where userId=#&#123;userId&#125; and sex=#&#123;sex&#125;;&lt;/select&gt; 第六种情形，传入多个参，使用 map封装实体类传入这种情况其实使用场景比较少，因为上面的各种知识其实已经够用了 service层： 12345678910@Overridepublic User getUserInfo1(Integer userId,String sex) &#123; User userInfo = new User(userId,sex); Map&lt;String,Object&gt; map=new HashMap&lt;String,Object&gt;(); map.put(&quot;user&quot;,userInfo); User userResult= userMapper.getUserInfo(map); //省略 业务代码... return userResult;&#125; mapper层： 1User getUserInfo(Map map); mapper.xml： 123456&lt;!--查询--&gt;&lt;select id=&quot;getUserInfo&quot; parameterType=&quot;Map&quot; resultType=&quot;com.demo.elegant.pojo.User&quot;&gt; select userId from users where userId=#&#123;userInfo.userId&#125; and sex=#&#123;userInfo.sex&#125;;&lt;/select&gt; 第七种情形，即需要传入实体类，又需要传入多个单独参，使用注解@Paramservice层： 123456@Overridepublic User getUserInfo(User user,Integer age) &#123; User userResult = userMapper.getUserInfo(user,age); //省略 业务代码... return userResult;&#125; mapper层： 1User getUserInfo(@Param(&quot;userInfo&quot;) User user,@Param(&quot;age&quot;) Integer age); mapper.xml： 123456&lt;!--查询--&gt;&lt;select id=&quot;getUserInfo&quot; resultType=&quot;com.demo.elegant.pojo.User&quot;&gt; select userId from users where userId=#&#123;userInfo.userId&#125; and sex=#&#123;userInfo.sex&#125; and age=#&#123;age&#125;;&lt;/select&gt; List传参service层： 12345List&lt;Integer&gt;list= new ArrayList&gt;(); list. add(44); list. add(45); list. add(46);List&lt;SysUser&gt; sysUser= sysUserMapper. selectList(list); mapper层： 1List&lt;SysUser&gt; selectList(List&lt;Integer&gt; ids); mapper.xml： 12345678&lt;select id=&quot;selectList&quot;resultMap&quot;BaseResultMap&quot;&gt; select &lt;include refid=&quot;Base_Column_List&quot;/&gt; from sys_user where id in &lt;foreach item=&quot;item&quot; index=&quot;index&quot; collection=&quot;list&quot;open=&quot;(&quot;separator&quot;,&quot;close=&quot;)&quot;&gt; #&#123;item&#125; &lt;/foreach&gt; &lt;/select&gt; 数组传参service层： 1List&lt;SysUser&gt; sysuser= sysUserMapper. selectlist(new Integer[]&#123;44,45,46&#125;); mapper层： 1List&lt;SysUser&gt; selectList(Integer[]ids); mapper.xml： 12345678&lt;select id=&quot;selectList&quot;resultMap&quot;BaseResultMap&quot;&gt; select &lt;include refid=&quot;Base Column_List&quot;/&gt; from sys user where id in &lt;foreach item=&quot;item&quot; index=&quot;index collection=&quot;array&quot;open=&quot;(&quot;separator=&quot;,&quot; close=&quot;)&quot;&gt; #&#123;item&#125; &lt;/foreach&gt; &lt;/select&gt; $｛｝使用这个的时候，只需要注意，如果是传递字段名或者表名，是直接做参数传入即可， 但是如果作为sql’语句里面使用的值， 记得需要手动拼接 ‘ ‘ 号。 例如， 传入单个参数 sex： service层： 12345678@Overridepublic User getUserInfo(String sex) &#123; sex=&quot;&#x27;&quot;+sex+&quot;&#x27;&quot;; User user = userMapper.getUserInfo(sex); //省略 业务代码... return user;&#125; mapper层： 1User getUserInfo(String sex); mapper.xml： 123456&lt;!--查询--&gt;&lt;select id=&quot;getUserInfo&quot; resultType=&quot;com.demo.elegant.pojo.User&quot;&gt; select userId from users where sex=$&#123;sex&#125;;&lt;/select&gt; 多个参数，那也就是使用注解@Param取名字解决即可。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/categories/Spring/"}],"tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://xmmarlowe.github.io/tags/Mybatis/"}],"author":"Marlowe"},{"title":"Spring-Boot-的优点、启动流程、与Spring的区别","slug":"Spring/Spring-Boot-的优点、启动流程、与Spring的区别","date":"2021-06-01T09:05:09.000Z","updated":"2021-06-01T15:14:48.392Z","comments":true,"path":"2021/06/01/Spring/Spring-Boot-的优点、启动流程、与Spring的区别/","link":"","permalink":"https://xmmarlowe.github.io/2021/06/01/Spring/Spring-Boot-%E7%9A%84%E4%BC%98%E7%82%B9%E3%80%81%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B%E3%80%81%E4%B8%8ESpring%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"Spring Boot 特性一：更快速的构建能力Spring 和 Spring Boot 有什么区别？Spring Boot 的优点有哪些？ 作为 Java 开发人员对 Spring 框架都很熟悉，Spring 为 Java 程序提供了全面的基础架构支持，包含了很多非常实用的功能，如 Spring JDBC、Spring AOP、Spring ORM、Spring Test 等，这些模块的出现，大大的缩短了应用程序的开发时间，同时提高了应用开发的效率。 Spring Boot 本质上是 Spring 框架的延伸和扩展，它的诞生是为了简化 Spring 框架初始搭建以及开发的过程，使用它可以不再依赖 Spring 应用程序中的 XML 配置，为更快、更高效的开发 Spring 提供更加有力的支持。Spring Boot 具体的特性如下。 Spring Boot 提供了更多的 Starters 用于快速构建业务框架，Starters 可以理解为启动器，它包含了一系列可以集成到应用里面的依赖包，你可以一站式集成 Spring 及其他技术，而不需要到处找依赖包。 例如在 Spring 中如果要创建 Web 应用程序的最小依赖项为： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;xxx&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;xxx&lt;/version&gt;&lt;/dependency&gt; 而 Spring Boot 只需要一个依赖项就可以来启动和运行 Web 应用程序，如下所示： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 当我们添加了 Starter 模块支持之后，在项目的构建期，它就会把所有其他依赖项将自动添加到项目中。 这样的例子还有很多，比如测试库，如果是 Spring 项目我们通常要添加 Spring Test、JUnit、Hamcrest 和 Mockito 库；而如果是 Spring Boot 项目的话，只需要添加 spring-boot-starter-test 即可，它会自动帮我们把其他的依赖项添加到项目中。 常见的 Starters 有以下几个： spring-boot-starter-test spring-boot-starter-web spring-boot-starter-data-jpa spring-boot-starter-thymeleaf Spring Boot 特性二：起步依赖Spring Boot 提供了起步依赖，也就是在创建 Spring Boot 时可以直接勾选依赖模块，这样在项目初始化时就会把相关依赖直接添加到项目中，大大缩短了查询并添加依赖的时间，如下图所示： Spring Boot 特性三：内嵌容器支持Spring Boot 内嵌了 Tomcat、Jetty、Undertow 三种容器，其默认嵌入的容器是 Tomcat，这个在我们启动 Spring Boot 项目的时候，在控制台上就能看到，具体信息如下： o.s.b.w.embedded.tomcat.TomcatWebServer :Tomcat started on port(s): 8080 (http) with context path ‘’ 可以看出 Spring Boot 默认使用的是 Tomcat 容器启动的。 我们可以通过修改 pom.xml 来移除内嵌的 Tomcat 更换为其他的容器，比如更换为 Jetty 容器，配置如下： 12345678910111213141516&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;!-- 移处 Tomcat --&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;!-- 移处 jetty 容器 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jetty&lt;/artifactId&gt;&lt;/dependency&gt; 当我们添加完成之后，再重新生成 pom.xml 文件，然后再启动 Spring Boot 项目容器信息就变了，如下所示： o.e.jetty.server.AbstractConnector: Started ServerConnector@53f9009d{HTTP/1.1, (http/1.1)}{0.0.0.0:8080} o.s.b.web.embedded.jetty.JettyWebServer 可以看出 Spring Boot 使用了我们指定的 Jetty 容器启动了。 Spring Boot 特性四：Actuator 监控Spring Boot 自带了 Actuator 监控功能，主要用于提供对应用程序监控，以及控制的能力，比如监控应用程序的运行状况，或者内存、线程池、Http 请求统计等，同时还提供了关闭应用程序等功能。 Actuator 提供了 19 个接口，接口请求地址和代表含义如下表所示： Spring Boot 启动源码分析我们知道 Spring Boot 程序的入口是 SpringApplication.run(Application.class, args) 方法，那么就从 run() 方法开始分析吧，它的源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public ConfigurableApplicationContext run(String... args) &#123; // 1.创建并启动计时监控类 StopWatch stopWatch = new StopWatch(); stopWatch.start(); // 2.声明应用上下文对象和异常报告集合 ConfigurableApplicationContext context = null; Collection&lt;SpringBootExceptionReporter&gt; exceptionReporters = new ArrayList(); // 3.设置系统属性 headless 的值 this.configureHeadlessProperty(); // 4.创建所有 Spring 运行监听器并发布应用启动事件 SpringApplicationRunListeners listeners = this.getRunListeners(args); listeners.starting(); Collection exceptionReporters; try &#123; // 5.处理 args 参数 ApplicationArguments applicationArguments = new DefaultApplicationArguments(args); // 6.准备环境 ConfigurableEnvironment environment = this.prepareEnvironment(listeners, applicationArguments); this.configureIgnoreBeanInfo(environment); // 7.创建 Banner 的打印类 Banner printedBanner = this.printBanner(environment); // 8.创建应用上下文 context = this.createApplicationContext(); // 9.实例化异常报告器 exceptionReporters = this.getSpringFactoriesInstances(SpringBootExceptionReporter.class, new Class[]&#123;ConfigurableApplicationContext.class&#125;, context); // 10.准备应用上下文 this.prepareContext(context, environment, listeners, applicationArguments, printedBanner); // 11.刷新应用上下文 this.refreshContext(context); // 12.应用上下文刷新之后的事件的处理 this.afterRefresh(context, applicationArguments); // 13.停止计时监控类 stopWatch.stop(); // 14.输出日志记录执行主类名、时间信息 if (this.logStartupInfo) &#123; (new StartupInfoLogger(this.mainApplicationClass)).logStarted(this.getApplicationLog(), stopWatch); &#125; // 15.发布应用上下文启动完成事件 listeners.started(context); // 16.执行所有 Runner 运行器 this.callRunners(context, applicationArguments); &#125; catch (Throwable var10) &#123; this.handleRunFailure(context, var10, exceptionReporters, listeners); throw new IllegalStateException(var10); &#125; try &#123; // 17.发布应用上下文就绪事件 listeners.running(context); // 18.返回应用上下文对象 return context; &#125; catch (Throwable var9) &#123; this.handleRunFailure(context, var9, exceptionReporters, (SpringApplicationRunListeners)null); throw new IllegalStateException(var9); &#125;&#125; 从以上源码可以看出 Spring Boot 的启动总共分为以下 18 个步骤。 Spring Boot 的启动流程1.创建并启动计时监控类 此计时器是为了监控并记录 Spring Boot 应用启动的时间的，它会记录当前任务的名称，然后开启计时器。 2.声明应用上下文对象和异常报告集合 此过程声明了应用上下文对象和一个异常报告的 ArrayList 集合。 3.设置系统属性 headless 的值 设置 Java.awt.headless = true，其中 awt（Abstract Window Toolkit）的含义是抽象窗口工具集。设置为 true 表示运行一个 headless 服务器，可以用它来作一些简单的图像处理。 4.创建所有 Spring 运行监听器并发布应用启动事件 此过程用于获取配置的监听器名称并实例化所有的类。 5.初始化默认应用的参数类 也就是说声明并创建一个应用参数对象。 6.准备环境 创建配置并且绑定环境（通过 property sources 和 profiles 等配置文件）。 7.创建 Banner 的打印类 Spring Boot 启动时会打印 Banner 图片，如下图所示： 此 banner 信息是在 SpringBootBanner 类中定义的，我们可以通过实现 Banner 接口来自定义 banner 信息，然后通过代码 setBanner() 方法设置 Spring Boot 项目使用自己自定义 Banner 信息，或者是在 resources 下添加一个 banner.txt，把 banner 信息添加到此文件中，就可以实现自定义 banner 的功能了。 8.创建应用上下文 根据不同的应用类型来创建不同的 ApplicationContext 上下文对象。 9.实例化异常报告器 它调用的是 getSpringFactoriesInstances() 方法来获取配置异常类的名称，并实例化所有的异常处理类。 10.准备应用上下文 此方法的主要作用是把上面已经创建好的对象，传递给 prepareContext 来准备上下文，例如将环境变量 environment 对象绑定到上下文中、配置 bean 生成器以及资源加载器、记录启动日志等操作。 11.刷新应用上下文 此方法用于解析配置文件，加载 bean 对象，并且启动内置的 web 容器等操作。 12.应用上下文刷新之后的事件处理 这个方法的源码是空的，可以做一些自定义的后置处理操作。 13.停止计时监控类 停止此过程第一步中的程序计时器，并统计任务的执行信息。 14.输出日志信息 把相关的记录信息，如类名、时间等信息进行控制台输出。 15.发布应用上下文启动完成事件 触发所有 SpringApplicationRunListener 监听器的 started 事件方法。 16.执行所有 Runner 运行器 执行所有的 ApplicationRunner 和 CommandLineRunner 运行器。 17.发布应用上下文就绪事件 触发所有的 SpringApplicationRunListener 监听器的 running 事件。 18.返回应用上下文对象 到此为止 Spring Boot 的启动程序就结束了，我们就可以正常来使用 Spring Boot 框架了。 参考SpringBoot 有哪些优点？它和 Spring 有什么区别？","categories":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/tags/Spring/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://xmmarlowe.github.io/tags/SpringBoot/"}],"author":"Marlowe"},{"title":"图","slug":"算法与数据结构/图","date":"2021-06-01T08:31:34.000Z","updated":"2021-06-01T15:14:48.414Z","comments":true,"path":"2021/06/01/算法与数据结构/图/","link":"","permalink":"https://xmmarlowe.github.io/2021/06/01/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%9B%BE/","excerpt":"","text":"图是一种较为复杂的非线性结构。 为啥说其较为复杂呢？ 根据前面的内容，我们知道： 线性数据结构的元素满足唯一的线性关系，每个元素(除第一个和最后一个外)只有一个直接前趋和一个直接后继。 树形数据结构的元素之间有着明显的层次关系。 但是，树形结构的元素之间的关系是任意的。 何为图呢？ 简单来说，图就是由顶点的有穷非空集合和顶点之间的边组成的集合。通常表示为：**G(V,E)**，其中，G表示一个图，V表示顶点的集合，E表示边的集合。 下图所展示的就是图这种数据结构，并且还是一张有向图。 图在我们日常生活中的例子很多！比如我们在社交软件上好友关系就可以用图来表示。 图的基本概念顶点图中的数据元素，我们称之为顶点，图至少有一个顶点（非空有穷集合） 对应到好友关系图，每一个用户就代表一个顶点。 边顶点之间的关系用边表示。 对应到好友关系图，两个用户是好友的话，那两者之间就存在一条边。 度度表示一个顶点包含多少条边，在有向图中，还分为出度和入度，出度表示从该顶点出去的边的条数，入度表示进入该顶点的边的条数。 对应到好友关系图，度就代表了某个人的好友数量。 无向图和有向图边表示的是顶点之间的关系，有的关系是双向的，比如同学关系，A是B的同学，那么B也肯定是A的同学，那么在表示A和B的关系时，就不用关注方向，用不带箭头的边表示，这样的图就是无向图。 有的关系是有方向的，比如父子关系，师生关系，微博的关注关系，A是B的爸爸，但B肯定不是A的爸爸，A关注B，B不一定关注A。在这种情况下，我们就用带箭头的边表示二者的关系，这样的图就是有向图。 无权图和有权图对于一个关系，如果我们只关心关系的有无，而不关心关系有多强，那么就可以用无权图表示二者的关系。 对于一个关系，如果我们既关心关系的有无，也关心关系的强度，比如描述地图上两个城市的关系，需要用到距离，那么就用带权图来表示，带权图中的每一条边一个数值表示权值，代表关系的强度。 图的存储邻接矩阵存储邻接矩阵将图用二维矩阵存储，是一种较为直观的表示方式。 如果第i个顶点和第j个顶点之间有关系，且关系权值为n，则 A[i][j]=n 。 在无向图中，我们只关心关系的有无，所以当顶点i和顶点j有关系时，A[i][j]=1，当顶点i和顶点j没有关系时，A[i][j]=0。如下图所示： 值得注意的是：无向图的邻接矩阵是一个对称矩阵，因为在无向图中，顶点i和顶点j有关系，则顶点j和顶点i必有关系。 邻接矩阵存储的方式优点是简单直接（直接使用一个二维数组即可），并且，在获取两个定点之间的关系的时候也非常高效（直接获取指定位置的数组元素的值即可）。但是，这种存储方式的缺点也比较明显，那就是比较浪费空间. 邻接表存储针对上面邻接矩阵比较浪费内存空间的问题，诞生了图的另外一种存储方法—邻接表 。 邻接链表使用一个链表来存储某个顶点的所有后继相邻顶点。对于图中每个顶点Vi，把所有邻接于Vi的顶点Vj链成一个单链表，这个单链表称为顶点Vi的 邻接表。如下图所示： 大家可以数一数邻接表中所存储的元素的个数以及图中边的条数，你会发现： 在无向图中，邻接表元素个数等于边的条数的两倍，如左图所示的无向图中，边的条数为7，邻接表存储的元素个数为14。 在有向图中，邻接表元素个数等于边的条数，如右图所示的有向图中，边的条数为8，邻接表存储的元素个数为8。 图的搜索广度优先搜索广度优先搜索就像水面上的波纹一样一层一层向外扩展，如下图所示： 广度优先搜索的具体实现方式用到了之前所学过的线性数据结构——队列。具体过程如下图所示： 第1步： 第2步： 第3步： 第4步： 第5步： 第6步： 深度优先搜索深度优先搜索就是“一条路走到黑”，从源顶点开始，一直走到没有后继节点，才回溯到上一顶点，然后继续“一条路走到黑”，如下图所示： 和广度优先搜索类似，深度优先搜索的具体实现用到了另一种线性数据结构——栈。具体过程如下图所示： 第1步： 第2步： 第3步： 第4步： 第5步： 第6步： 参考图","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"https://xmmarlowe.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"图","slug":"图","permalink":"https://xmmarlowe.github.io/tags/%E5%9B%BE/"}],"author":"Marlowe"},{"title":"堆相关知识点详解","slug":"算法与数据结构/堆相关知识点详解","date":"2021-06-01T08:00:52.000Z","updated":"2021-06-01T15:14:48.418Z","comments":true,"path":"2021/06/01/算法与数据结构/堆相关知识点详解/","link":"","permalink":"https://xmmarlowe.github.io/2021/06/01/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%A0%86%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E7%82%B9%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"什么是堆?堆是一种满足以下条件的树： 堆中的每一个节点值都大于等于（或小于等于）子树中所有节点的值。或者说，任意一个节点的值都大于等于（或小于等于）所有子节点的值。 大家可以把堆(最大堆)理解为一个公司,这个公司很公平,谁能力强谁就当老大,不存在弱的人当老大,老大手底下的人一定不会比他强。这样有助于理解后续堆的操作。 !!!特别提示： 很多博客说堆是完全二叉树，其实并非如此，堆不一定是完全二叉树，只是为了方便存储和索引，我们通常用完全二叉树的形式来表示堆，事实上，广为人知的斐波那契堆和二项堆就不是完全二叉树,它们甚至都不是二叉树。 （二叉）堆是一个数组，它可以被看成是一个 近似的完全二叉树。——《算法导论》第三版大家可以尝试判断下面给出的图是否是二叉树？ 第1个和第2个是堆。第1个是最大堆，每个节点都比子树中所有节点大。第2个是最小堆，每个节点都比子树中所有节点小。 第3个不是，第三个中，根结点1比2和15小，而15却比3大，19比5大，不满足堆的性质。 堆的用途当我们只关心所有数据中的最大值或者最小值，存在多次获取最大值或者最小值，多次插入或删除数据时，就可以使用堆。 有小伙伴可能会想到用有序数组，初始化一个有序数组时间复杂度是 O(nlog(n))，查找最大值或者最小值时间复杂度都是 O(1)，但是，涉及到更新（插入或删除）数据时，时间复杂度为 O(n)，即使是使用复杂度为 O(log(n)) 的二分法找到要插入或者删除的数据，在移动数据时也需要 O(n) 的时间复杂度。 相对于有序数组而言，堆的主要优势在于更新数据效率较高。 堆的初始化时间复杂度为 O(nlog(n))，堆可以做到O(1)时间复杂度取出最大值或者最小值，O(log(n))时间复杂度插入或者删除数据，具体操作在后续章节详细介绍。 堆的分类堆分为 最大堆 和 最小堆。二者的区别在于节点的排序方式。 最大堆 ：堆中的每一个节点的值都大于等于子树中所有节点的值 最小堆：堆中的每一个节点的值都小于等于子树中所有节点的值如下图所示，图1是最大堆，图2是最小堆 堆的存储之前介绍树的时候说过，由于完全二叉树的优秀性质，利用数组存储二叉树即节省空间，又方便索引（若根结点的序号为1，那么对于树中任意节点i，其左子节点序号为 2*i，右子节点序号为 2*i+1）。 为了方便存储和索引，（二叉）堆可以用完全二叉树的形式进行存储。存储的方式如下图所示： 堆的操作堆的更新操作主要包括两种 : 插入元素 和 删除堆顶元素。操作过程需要着重掌握和理解。 在进入正题之前，再重申一遍，堆是一个公平的公司，有能力的人自然会走到与他能力所匹配的位置 插入元素 插入元素，作为一个新入职的员工，初来乍到，这个员工需要从基层做起 将要插入的元素放到最后 有能力的人会逐渐升职加薪，是金子总会发光的！！！ 从底向上，如果父结点比该元素大，则该节点和父结点交换，直到无法交换 删除堆顶元素根据堆的性质可知，最大堆的堆顶元素为所有元素中最大的，最小堆的堆顶元素是所有元素中最小的。当我们需要多次查找最大元素或者最小元素的时候，可以利用堆来实现。 删除堆顶元素后，为了保持堆的性质，需要对堆的结构进行调整，我们将这个过程称之为”堆化“，堆化的方法分为两种： 一种是自底向上的堆化，上述的插入元素所使用的就是自底向上的堆化，元素从最底部向上移动。 另一种是自顶向下堆化，元素由最顶部向下移动。在讲解删除堆顶元素的方法时，我将阐述这两种操作的过程，大家可以体会一下二者的不同。 自底向上堆化 在堆这个公司中，会出现老大离职的现象，老大离职之后，他的位置就空出来了 首先删除堆顶元素，使得数组中下标为1的位置空出。 那么他的位置由谁来接替呢，当然是他的直接下属了，谁能力强就让谁上呗 比较根结点的左子节点和右子节点，也就是下标为2,3的数组元素，将较大的元素填充到根结点(下标为1)的位置。 这个时候又空出一个位置了，老规矩，谁有能力谁上 一直循环比较空出位置的左右子节点，并将较大者移至空位，直到堆的最底部 这个时候已经完成了自底向上的堆化，没有元素可以填补空缺了，但是，我们可以看到数组中出现了“气泡”，这会导致存储空间的浪费。接下来我们试试自顶向下堆化。 自顶向下堆化自顶向下的堆化用一个词形容就是“石沉大海”，那么第一件事情，就是把石头抬起来，从海面扔下去。这个石头就是堆的最后一个元素，我们将最后一个元素移动到堆顶。 然后开始将这个石头沉入海底，不停与左右子节点的值进行比较，和较大的子节点交换位置，直到无法交换位置。 堆的操作总结 插入元素 ：先将元素放至数组末尾，再自底向上堆化，将末尾元素上浮 删除堆顶元素 ：删除堆顶元素，将末尾元素放至堆顶，再自顶向下堆化，将堆顶元素下沉。也可以自底向上堆化，只是会产生“气泡”，浪费存储空间。最好采用自顶向下堆化的方式。 堆排序堆排序的过程分为两步： 第一步是建堆，将一个无序的数组建立为一个堆 第二步是排序，将堆顶元素取出，然后对剩下的元素进行堆化，反复迭代，直到所有元素被取出为止。 建堆如果你已经足够了解堆化的过程，那么建堆的过程掌握起来就比较容易了。建堆的过程就是一个对所有非叶节点的自顶向下堆化过程。 首先要了解哪些是非叶节点，最后一个节点的父结点及它之前的元素，都是非叶节点。也就是说，如果节点个数为n，那么我们需要对n/2到1的节点进行自顶向下（沉底）堆化。 具体过程如下图： 将初始的无序数组抽象为一棵树，图中的节点个数为6，所以4,5,6节点为叶节点，1,2,3节点为非叶节点，所以要对1-3号节点进行自顶向下（沉底）堆化，注意，顺序是从后往前堆化，从3号节点开始，一直到1号节点。 3号节点堆化结果： 2号节点堆化结果： 1号节点堆化结果： 至此，数组所对应的树已经成为了一个最大堆，建堆完成！ 排序由于堆顶元素是所有元素中最大的，所以我们重复取出堆顶元素，将这个最大的堆顶元素放至数组末尾，并对剩下的元素进行堆化即可。 现在思考两个问题： 删除堆顶元素后需要执行自顶向下（沉底）堆化还是自底向上（上浮）堆化？ 取出的堆顶元素存在哪，新建一个数组存？ 先回答第一个问题，我们需要执行自顶向下（沉底）堆化，这个堆化一开始要将末尾元素移动至堆顶，这个时候末尾的位置就空出来了，由于堆中元素已经减小，这个位置不会再被使用，所以我们可以将取出的元素放在末尾。 机智的小伙伴已经发现了，这其实是做了一次交换操作，将堆顶和末尾元素调换位置，从而将取出堆顶元素和堆化的第一步(将末尾元素放至根结点位置)进行合并。 详细过程如下图所示： 取出第一个元素并堆化： 取出第二个元素并堆化： 取出第三个元素并堆化： 取出第四个元素并堆化： 取出第五个元素并堆化： 取出第六个元素并堆化： 堆排序完成！ 参考堆","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"https://xmmarlowe.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"堆","slug":"堆","permalink":"https://xmmarlowe.github.io/tags/%E5%A0%86/"}],"author":"Marlowe"},{"title":"优先队列和堆","slug":"算法与数据结构/优先队列和堆","date":"2021-06-01T08:00:24.000Z","updated":"2021-06-01T15:14:48.399Z","comments":true,"path":"2021/06/01/算法与数据结构/优先队列和堆/","link":"","permalink":"https://xmmarlowe.github.io/2021/06/01/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97%E5%92%8C%E5%A0%86/","excerpt":"","text":"什么是优先队列？听这个名字就能知道，优先队列也是一种队列，只不过不同的是，优先队列的出队顺序是按照优先级来的；在有些情况下，可能需要找到元素集合中的最小或者最大元素，可以利用优先队列ADT来完成操作，优先队列ADT是一种数据结构，它支持插入和删除最小值操作（返回并删除最小元素）或删除最大值操作（返回并删除最大元素）； 这些操作等价于队列的enQueue和deQueue操作，区别在于，对于优先队列，元素进入队列的顺序可能与其被操作的顺序不同，作业调度是优先队列的一个应用实例，它根据优先级的高低而不是先到先服务的方式来进行调度； 如果最小键值元素拥有最高的优先级，那么这种优先队列叫作升序优先队列（即总是先删除最小的元素），类似的，如果最大键值元素拥有最高的优先级，那么这种优先队列叫作降序优先队列（即总是先删除最大的元素）；由于这两种类型时对称的，所以只需要关注其中一种，如升序优先队列； 优先队列ADT下面操作组成了优先队列的一个ADT； 1.优先队列的主要操作 优先队列是元素的容器，每个元素有一个相关的键值； insert(key, data)：插入键值为key的数据到优先队列中，元素以其key进行排序； deleteMin/deleteMax：删除并返回最小/最大键值的元素； getMinimum/getMaximum：返回最小/最大剑指的元素，但不删除它； 2.优先队列的辅助操作 第k最小/第k最大：返回优先队列中键值为第k个最小/最大的元素； 大小（size）：返回优先队列中的元素个数； 堆排序（Heap Sort）：基于键值的优先级将优先队列中的元素进行排序； 优先队列的应用 数据压缩：赫夫曼编码算法； 最短路径算法：Dijkstra算法； 最小生成树算法：Prim算法； 事件驱动仿真：顾客排队算法； 选择问题：查找第k个最小元素； 等等等等…. 堆和二叉堆什么是堆堆是一颗具有特定性质的二叉树，堆的基本要求就是堆中所有结点的值必须大于或等于（或小于或等于）其孩子结点的值，这也称为堆的性质；堆还有另一个性质，就是当 h &gt; 0 时，所有叶子结点都处于第 h 或 h - 1 层（其中 h 为树的高度，完全二叉树），也就是说，堆应该是一颗完全二叉树； 在下面的例子中，左边的树为堆（每个元素都大于其孩子结点的值），而右边的树不是堆（因为5大于其孩子结点2） 二叉堆在二叉堆中，每个结点最多有两个孩子结点，在实际应用中，二叉堆已经足够满足需求，因此接下来主要讨论二叉最小堆和二叉最大堆； 堆的表示：在描述堆的操作前，首先来看堆是怎样表示的，一种可能的方法就是使用数组，因为堆在形式上是一颗完全二叉树，用数组来存储它不会浪费任何空间，例如下图： 用数组来表示堆不仅不会浪费空间还具有一定的优势： 每个结点的左孩子为下标i的2倍：left child(i) = i * 2；每个结点的右孩子为下标i的2倍加1：right child(i) = i * 2 + 1 每个结点的父亲结点为下标的二分之一：parent(i) = i / 2，注意这里是整数除，2和3除以2都为1，大家可以验证一下； 注意：这里是把下标为0的地方空出来了的，主要是为了方便理解，如果0不空出来只需要在计算的时候把i值往右偏移一个位置就行了（也就是加1，大家可以试试，下面的演示也采取这样的方式）； 二叉堆的相关操作堆的基本结构1234567891011121314public class MaxHeap&lt;E extends Comparable&lt;E&gt;&gt; &#123; private Array&lt;E&gt; data; public MaxHeap(int capacity)&#123; data = new Array&lt;&gt;(capacity); &#125; public MaxHeap()&#123; data = new Array&lt;&gt;(); &#125; // 返回堆中的元素个数 public int size()&#123; return data.getSize(); &#125; // 返回一个布尔值, 表示堆中是否为空 public boolean isEmpty()&#123; return data.isEmpty(); &#125; // 返回完全二叉树的数组表示中，一个索引所表示的元素的父亲节点的索引 private int parent(int index)&#123; if(index == 0) throw new IllegalArgumentException(&quot;index-0 doesn&#x27;t have parent.&quot;); return (index - 1) / 2; &#125; // 返回完全二叉树的数组表示中，一个索引所表示的元素的左孩子节点的索引 private int leftChild(int index)&#123; return index * 2 + 1; &#125; // 返回完全二叉树的数组表示中，一个索引所表示的元素的右孩子节点的索引 private int rightChild(int index)&#123; return index * 2 + 2; &#125;&#125; 向堆中添加元素和Sift Up当插入一个元素到堆中时，它可能不满足堆的性质，在这种情况下，需要调整堆中元素的位置使之重新变成堆，这个过程称为堆化（heapifying）；在最大堆中，要堆化一个元素，需要找到它的父亲结点，如果不满足堆的基本性质则交换两个元素的位置，重复该过程直到每个结点都满足堆的性质为止，下面我们来模拟一下这个过程： 下面我们在该堆中插入一个新的元素26： 我们通过索引（上面的公式）可以很容易地找到新插入元素的父亲结点，然后比较它们的大小，如果新元素更大则交换两个元素的位置，这个操作就相当于把该元素上浮了一下： 重复该操作直到26到了一个满足堆条件的位置，此时就完成了插入的操作： 对应的代码如下： 1234567891011// 向堆中添加元素public void add(E e)&#123; data.addLast(e); siftUp(data.getSize() - 1);&#125;private void siftUp(int k)&#123; while(k &gt; 0 &amp;&amp; data.get(parent(k)).compareTo(data.get(k)) &lt; 0 )&#123; data.swap(k, parent(k)); k = parent(k); &#125;&#125; 取出堆中的最大元素和Sift Down如果理解了上述的过程，那么取出堆中的最大元素（堆顶元素）将变得容易，不过这里运用到一个小技巧，就是用最后一个元素替换掉栈顶元素，然后把最后一个元素删除掉，这样一来元素的总个数也满足条件，然后只需要把栈顶元素依次往下调整就好了，这个操作就叫做Sift Down（下沉）： 用最后元素替换掉栈顶元素，然后删除最后一个元素： 然后比较其孩子结点的大小： 如果不满足堆的条件，那么就跟孩子结点中较大的一个交换位置： 重复该步骤，直到16到达合适的位置： 完成取出最大元素的操作： 对应的代码如下： 1234567891011121314151617181920212223242526272829// 看堆中的最大元素public E findMax()&#123; if(data.getSize() == 0) throw new IllegalArgumentException(&quot;Can not findMax when heap is empty.&quot;); return data.get(0);&#125;// 取出堆中最大元素public E extractMax()&#123; E ret = findMax(); data.swap(0, data.getSize() - 1); data.removeLast(); siftDown(0); return ret;&#125;private void siftDown(int k)&#123; while(leftChild(k) &lt; data.getSize())&#123; int j = leftChild(k); // 在此轮循环中,data[k]和data[j]交换位置 if( j + 1 &lt; data.getSize() &amp;&amp; data.get(j + 1).compareTo(data.get(j)) &gt; 0 ) j ++; // data[j] 是 leftChild 和 rightChild 中的最大值 if(data.get(k).compareTo(data.get(j)) &gt;= 0 ) break; data.swap(k, j); k = j; &#125;&#125; Replace 和 HeapifyReplace这个操作其实就是取出堆中最大的元素之后再新插入一个元素，常规的做法是取出最大元素之后，再利用上面的插入新元素的操作对堆进行Sift Up操作，但是这里有一个小技巧就是直接使用新元素替换掉堆顶元素，之后再进行Sift Down操作，这样就把两次O(logn）的操作变成了一次O(logn)： 1234567// 取出堆中的最大元素，并且替换成元素epublic E replace(E e)&#123; E ret = findMax(); data.set(0, e); siftDown(0); return ret;&#125; Heapify翻译过来就是堆化的意思，就是将任意数组整理成堆的形状，通常的做法是遍历数组从0开始添加创建一个新的堆，但是这里存在一个小技巧就是把当前数组就看做是一个完全二叉树，然后从最后一个非叶子结点开始进行Sift Down操作就可以了，最后一个非叶子结点也很好找，就是最后一个结点的父亲结点，大家可以验证一下： 从22这个节点开始，依次开始Sift Down操作： 重复该过程直到堆顶元素： 完成堆化操作： 将n个元素逐个插入到一个空堆中，算法复杂度是O(nlogn)，而heapify的过程，算法复杂度为O(n)，这是有一个质的飞跃的，下面是代码： 12345public MaxHeap(E[] arr)&#123; data = new Array&lt;&gt;(arr); for(int i = parent(arr.length - 1) ; i &gt;= 0 ; i --) siftDown(i);&#125; 基于堆的优先队列首先我们的队列仍然需要继承我们之前将队列时候声明的哪个接口Queue，然后实现这个接口中的方法就可以了，子类简单写一下： 1234567891011public class PriorityQueue&lt;E extends Comparable&lt;E&gt;&gt; implements Queue&lt;E&gt; &#123; private MaxHeap&lt;E&gt; maxHeap; public PriorityQueue()&#123; maxHeap = new MaxHeap&lt;&gt;(); &#125; @Override public int getSize()&#123; return maxHeap.size(); &#125; @Override public boolean isEmpty()&#123; return maxHeap.isEmpty(); &#125; @Override public E getFront()&#123; return maxHeap.findMax(); &#125; @Override public void enqueue(E e)&#123; maxHeap.add(e); &#125; @Override public E dequeue()&#123; return maxHeap.extractMax(); &#125;&#125; Java中的PriorityQueue在Java中也实现了自己的优先队列java.util.PriorityQueue，与我们自己写的不同之处在于，Java中内置的为最小堆，然后就是一些函数名不一样，底层还是维护了一个Object类型的数组，大家可以戳戳看有什么不同，另外如果想要把最小堆变成最大堆可以给PriorityQueue传入自己的比较器，例如： 12345678910111213141516171819202122// 默认为最小堆PriorityQueue&lt;Integer&gt; pq = new PriorityQueue&lt;&gt;();pq.add(5);pq.add(2);pq.add(1);pq.add(10);pq.add(3);while (!pq.isEmpty()) &#123; System.out.println(pq.poll() + &quot;, &quot;);&#125;System.out.println();System.out.println(&quot;————————————————————————&quot;);// 使用Lambda表达式传入自己的比较器转换成最大堆PriorityQueue&lt;Integer&gt; pq2 = new PriorityQueue&lt;&gt;((a, b) -&gt; b - a);pq2.add(5);pq2.add(2);pq2.add(1);pq2.add(10);pq2.add(3);while (!pq2.isEmpty()) &#123; System.out.println(pq2.poll() + &quot;, &quot;);&#125; 参考数据结构与算法(4)——优先队列和堆","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"https://xmmarlowe.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"队列","slug":"队列","permalink":"https://xmmarlowe.github.io/tags/%E9%98%9F%E5%88%97/"},{"name":"堆","slug":"堆","permalink":"https://xmmarlowe.github.io/tags/%E5%A0%86/"}],"author":"Marlowe"},{"title":"普里姆算法和克鲁斯卡尔算法","slug":"算法与数据结构/普里姆算法和克鲁斯卡尔算法","date":"2021-06-01T06:10:14.000Z","updated":"2021-06-01T15:14:48.436Z","comments":true,"path":"2021/06/01/算法与数据结构/普里姆算法和克鲁斯卡尔算法/","link":"","permalink":"https://xmmarlowe.github.io/2021/06/01/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%99%AE%E9%87%8C%E5%A7%86%E7%AE%97%E6%B3%95%E5%92%8C%E5%85%8B%E9%B2%81%E6%96%AF%E5%8D%A1%E5%B0%94%E7%AE%97%E6%B3%95/","excerpt":"『普里姆算法』和『克鲁斯卡尔算法』，它们的目的都是生成『最小生成树』，它们两者的实现原理是比较相似的，只不过一个通过边，而另一个主要是通过顶点来实现的，下面我们就一个一个来进行介绍。","text":"『普里姆算法』和『克鲁斯卡尔算法』，它们的目的都是生成『最小生成树』，它们两者的实现原理是比较相似的，只不过一个通过边，而另一个主要是通过顶点来实现的，下面我们就一个一个来进行介绍。 普里姆算法普里姆算法（Prim），是图论中的一种算法，可在加权连通图里搜索最小生成树，意即由此算法搜索到的边子集所构成的树中，不但包括了连通图里的所有顶点，且其所有边的权值之和亦为最小，我们通过一个简单的示例来了解一下为什么需要『普里姆算法』，如下 图中的顶点我们可以将其想象成一个一个的村庄，而我们的目标就是让所有的村庄都连通起来，并且消耗的资源最少，当然实现的方式有很多种，比如下面这样 通过计算可以发现，它的成本为 11 + 26 + 20 + 22 + 18 + 21 + 24 + 19 = 161，不过如果我们仔细观察的话，可以发现这种连通方式是十分消耗资源的，所以我们稍微调整一下，就有了下面这种方式 通过计算发现其成本为 8 + 12 + 10 + 11 + 17 + 19 + 16 + 7 = 100，这样看起来似乎成本小了不少，但是有没有消耗更少的连通方式呢，方法是有的 这一次的成本为 8 + 12 + 10 + 11 + 16 + 19 + 16 + 7 = 99，可以发现，这一次便是最优的解决方式，那么问题就来了，我们该如何从多种方式当中来选取最优的方案呢，所以这就有了普里姆算法，下面我们就通过一个示例来了解，到底什么是普里姆算法，如下 我们选择从 0 开始出发构造我们的 MST，我们约定，蓝色顶点为蓝点集合（表示暂时还未遍历的点），黑色顶点为黑点集合（表示已经遍历过的点），红色边为最短边，灰色边为淘汰边，下面我们就开始从 0 进行遍历，所以出发点由蓝色变成黑色，如下 下面我们把和顶点 0 与相邻顶点之间的连线改变成紫色 我们第一步就是找与它相邻的边当中权值最小的，可以很明显的发现，是顶点 2，所以我们的目标就是 2 号顶点 下面一步就比较复杂，因为和顶点 2 相邻的有 1，3，4，5，但是 1 和 0 相连，3 也和 0 相连，但是我们在这里约定，『若是一个蓝点与多个黑点有边相连，则取权值最小的边作为紫边』，所以这时我们就需要比较 (0, 1) 和 (1, 2) 之间的边的权值，可以发现 (1, 2) 的权值更小，同理 (0, 3) 的边比 (2, 3) 的边的权值也更小，所以就有了如下的情况 我们选择剔除掉 (0, 1) 和 (2, 3) 之间的边（因为它们的权值更大），接着我们将与 2 相连的边调整为了紫色，接下来同理，我们继续寻找紫边当中权值最小的，可以发现是 (2, 5)，所以我们的目标就是 5 号顶点，如下 接下来同理，我们比较 (0, 3)，(3, 5)，(2, 4) 和 (4, 5)，结果如下 继续寻找权值最小的边，为 (5, 3)，所以变成如下的情况 接下来同理，继续选择权值较小的边，因为两边一致，所以我们随便挑选一条 继续比较 (1, 4) 和 (2, 4)，可以发现 (1, 4) 的更小 所以最终的结果如下 下面我们看如何用代码来进行实现，大致思想是 设图 G 顶点集合为 U，首先任意选择图 G 中的一点作为起始点 a，将该点加入集合 V 再从集合 U - V 中找到另一点 b 使得点 b 到 V 中任意一点的权值最小，此时将 b 点也加入集合 V 以此类推，现在的集合 V = { a, b }，再从集合 U - V 中找到另一点 c 使得点 c 到 V 中任意一点的权值最小，此时将 c 点加入集合 V，直至所有顶点全部被加入 V 此时就构建出了一颗 MST，因为有 N 个顶点，所以该 MST 就有 N - 1 条边，每一次向集合 V 中加入一个点，就意味着找到一条 MST 的边 在此之前，我们可以将上面的图片当中的数据转化为图片的格式。 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334/** * Java: prim算法生成最小生成树(邻接矩阵) * * @author skywang * @date 2014/04/23 */import java.io.IOException;import java.util.Scanner;public class MatrixUDG &#123; private char[] mVexs; // 顶点集合 private int[][] mMatrix; // 邻接矩阵 private static final int INF = Integer.MAX_VALUE; // 最大值 /* * 创建图(自己输入数据) */ public MatrixUDG() &#123; // 输入&quot;顶点数&quot;和&quot;边数&quot; System.out.printf(&quot;input vertex number: &quot;); int vlen = readInt(); System.out.printf(&quot;input edge number: &quot;); int elen = readInt(); if ( vlen &lt; 1 || elen &lt; 1 || (elen &gt; (vlen*(vlen - 1)))) &#123; System.out.printf(&quot;input error: invalid parameters!\\n&quot;); return ; &#125; // 初始化&quot;顶点&quot; mVexs = new char[vlen]; for (int i = 0; i &lt; mVexs.length; i++) &#123; System.out.printf(&quot;vertex(%d): &quot;, i); mVexs[i] = readChar(); &#125; // 1. 初始化&quot;边&quot;的权值 mMatrix = new int[vlen][vlen]; for (int i = 0; i &lt; vlen; i++) &#123; for (int j = 0; j &lt; vlen; j++) &#123; if (i==j) mMatrix[i][j] = 0; else mMatrix[i][j] = INF; &#125; &#125; // 2. 初始化&quot;边&quot;的权值: 根据用户的输入进行初始化 for (int i = 0; i &lt; elen; i++) &#123; // 读取边的起始顶点,结束顶点,权值 System.out.printf(&quot;edge(%d):&quot;, i); char c1 = readChar(); // 读取&quot;起始顶点&quot; char c2 = readChar(); // 读取&quot;结束顶点&quot; int weight = readInt(); // 读取&quot;权值&quot; int p1 = getPosition(c1); int p2 = getPosition(c2); if (p1==-1 || p2==-1) &#123; System.out.printf(&quot;input error: invalid edge!\\n&quot;); return ; &#125; mMatrix[p1][p2] = weight; mMatrix[p2][p1] = weight; &#125; &#125; /* * 创建图(用已提供的矩阵) * * 参数说明： * vexs -- 顶点数组 * matrix-- 矩阵(数据) */ public MatrixUDG(char[] vexs, int[][] matrix) &#123; // 初始化&quot;顶点数&quot;和&quot;边数&quot; int vlen = vexs.length; // 初始化&quot;顶点&quot; mVexs = new char[vlen]; for (int i = 0; i &lt; mVexs.length; i++) mVexs[i] = vexs[i]; // 初始化&quot;边&quot; mMatrix = new int[vlen][vlen]; for (int i = 0; i &lt; vlen; i++) for (int j = 0; j &lt; vlen; j++) mMatrix[i][j] = matrix[i][j]; &#125; /* * 返回ch位置 */ private int getPosition(char ch) &#123; for(int i=0; i&lt;mVexs.length; i++) if(mVexs[i]==ch) return i; return -1; &#125; /* * 读取一个输入字符 */ private char readChar() &#123; char ch=&#x27;0&#x27;; do &#123; try &#123; ch = (char)System.in.read(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; while(!((ch&gt;=&#x27;a&#x27;&amp;&amp;ch&lt;=&#x27;z&#x27;) || (ch&gt;=&#x27;A&#x27;&amp;&amp;ch&lt;=&#x27;Z&#x27;))); return ch; &#125; /* * 读取一个输入字符 */ private int readInt() &#123; Scanner scanner = new Scanner(System.in); return scanner.nextInt(); &#125; /* * 返回顶点v的第一个邻接顶点的索引，失败则返回-1 */ private int firstVertex(int v) &#123; if (v&lt;0 || v&gt;(mVexs.length-1)) return -1; for (int i = 0; i &lt; mVexs.length; i++) if (mMatrix[v][i]!=0 &amp;&amp; mMatrix[v][i]!=INF) return i; return -1; &#125; /* * 返回顶点v相对于w的下一个邻接顶点的索引，失败则返回-1 */ private int nextVertex(int v, int w) &#123; if (v&lt;0 || v&gt;(mVexs.length-1) || w&lt;0 || w&gt;(mVexs.length-1)) return -1; for (int i = w + 1; i &lt; mVexs.length; i++) if (mMatrix[v][i]!=0 &amp;&amp; mMatrix[v][i]!=INF) return i; return -1; &#125; /* * 深度优先搜索遍历图的递归实现 */ private void DFS(int i, boolean[] visited) &#123; visited[i] = true; System.out.printf(&quot;%c &quot;, mVexs[i]); // 遍历该顶点的所有邻接顶点。若是没有访问过，那么继续往下走 for (int w = firstVertex(i); w &gt;= 0; w = nextVertex(i, w)) &#123; if (!visited[w]) DFS(w, visited); &#125; &#125; /* * 深度优先搜索遍历图 */ public void DFS() &#123; boolean[] visited = new boolean[mVexs.length]; // 顶点访问标记 // 初始化所有顶点都没有被访问 for (int i = 0; i &lt; mVexs.length; i++) visited[i] = false; System.out.printf(&quot;DFS: &quot;); for (int i = 0; i &lt; mVexs.length; i++) &#123; if (!visited[i]) DFS(i, visited); &#125; System.out.printf(&quot;\\n&quot;); &#125; /* * 广度优先搜索（类似于树的层次遍历） */ public void BFS() &#123; int head = 0; int rear = 0; int[] queue = new int[mVexs.length]; // 辅组队列 boolean[] visited = new boolean[mVexs.length]; // 顶点访问标记 for (int i = 0; i &lt; mVexs.length; i++) visited[i] = false; System.out.printf(&quot;BFS: &quot;); for (int i = 0; i &lt; mVexs.length; i++) &#123; if (!visited[i]) &#123; visited[i] = true; System.out.printf(&quot;%c &quot;, mVexs[i]); queue[rear++] = i; // 入队列 &#125; while (head != rear) &#123; int j = queue[head++]; // 出队列 for (int k = firstVertex(j); k &gt;= 0; k = nextVertex(j, k)) &#123; //k是为访问的邻接顶点 if (!visited[k]) &#123; visited[k] = true; System.out.printf(&quot;%c &quot;, mVexs[k]); queue[rear++] = k; &#125; &#125; &#125; &#125; System.out.printf(&quot;\\n&quot;); &#125; /* * 打印矩阵队列图 */ public void print() &#123; System.out.printf(&quot;Martix Graph:\\n&quot;); for (int i = 0; i &lt; mVexs.length; i++) &#123; for (int j = 0; j &lt; mVexs.length; j++) System.out.printf(&quot;%10d &quot;, mMatrix[i][j]); System.out.printf(&quot;\\n&quot;); &#125; &#125; /* * prim最小生成树 * * 参数说明： * start -- 从图中的第start个元素开始，生成最小树 */ public void prim(int start) &#123; int num = mVexs.length; // 顶点个数 int index=0; // prim最小树的索引，即prims数组的索引 char[] prims = new char[num]; // prim最小树的结果数组 int[] weights = new int[num]; // 顶点间边的权值 // prim最小生成树中第一个数是&quot;图中第start个顶点&quot;，因为是从start开始的。 prims[index++] = mVexs[start]; // 初始化&quot;顶点的权值数组&quot;， // 将每个顶点的权值初始化为&quot;第start个顶点&quot;到&quot;该顶点&quot;的权值。 for (int i = 0; i &lt; num; i++ ) weights[i] = mMatrix[start][i]; // 将第start个顶点的权值初始化为0。 // 可以理解为&quot;第start个顶点到它自身的距离为0&quot;。 weights[start] = 0; for (int i = 0; i &lt; num; i++) &#123; // 由于从start开始的，因此不需要再对第start个顶点进行处理。 if(start == i) continue; int j = 0; int k = 0; int min = INF; // 在未被加入到最小生成树的顶点中，找出权值最小的顶点。 while (j &lt; num) &#123; // 若weights[j]=0，意味着&quot;第j个节点已经被排序过&quot;(或者说已经加入了最小生成树中)。 if (weights[j] != 0 &amp;&amp; weights[j] &lt; min) &#123; min = weights[j]; k = j; &#125; j++; &#125; // 经过上面的处理后，在未被加入到最小生成树的顶点中，权值最小的顶点是第k个顶点。 // 将第k个顶点加入到最小生成树的结果数组中 prims[index++] = mVexs[k]; // 将&quot;第k个顶点的权值&quot;标记为0，意味着第k个顶点已经排序过了(或者说已经加入了最小树结果中)。 weights[k] = 0; // 当第k个顶点被加入到最小生成树的结果数组中之后，更新其它顶点的权值。 for (j = 0 ; j &lt; num; j++) &#123; // 当第j个节点没有被处理，并且需要更新时才被更新。 if (weights[j] != 0 &amp;&amp; mMatrix[k][j] &lt; weights[j]) weights[j] = mMatrix[k][j]; &#125; &#125; // 计算最小生成树的权值 int sum = 0; for (int i = 1; i &lt; index; i++) &#123; int min = INF; // 获取prims[i]在mMatrix中的位置 int n = getPosition(prims[i]); // 在vexs[0...i]中，找出到j的权值最小的顶点。 for (int j = 0; j &lt; i; j++) &#123; int m = getPosition(prims[j]); if (mMatrix[m][n]&lt;min) min = mMatrix[m][n]; &#125; sum += min; &#125; // 打印最小生成树 System.out.printf(&quot;PRIM(%c)=%d: &quot;, mVexs[start], sum); for (int i = 0; i &lt; index; i++) System.out.printf(&quot;%c &quot;, prims[i]); System.out.printf(&quot;\\n&quot;); &#125; public static void main(String[] args) &#123; char[] vexs = &#123;&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;, &#x27;E&#x27;, &#x27;F&#x27;, &#x27;G&#x27;&#125;; int matrix[][] = &#123; /*A*//*B*//*C*//*D*//*E*//*F*//*G*/ /*A*/ &#123; 0, 12, INF, INF, INF, 16, 14&#125;, /*B*/ &#123; 12, 0, 10, INF, INF, 7, INF&#125;, /*C*/ &#123; INF, 10, 0, 3, 5, 6, INF&#125;, /*D*/ &#123; INF, INF, 3, 0, 4, INF, INF&#125;, /*E*/ &#123; INF, INF, 5, 4, 0, 2, 8&#125;, /*F*/ &#123; 16, 7, 6, INF, 2, 0, 9&#125;, /*G*/ &#123; 14, INF, INF, INF, 8, 9, 0&#125;&#125;; MatrixUDG pG; // 自定义&quot;图&quot;(输入矩阵队列) //pG = new MatrixUDG(); // 采用已有的&quot;图&quot; pG = new MatrixUDG(vexs, matrix); //pG.print(); // 打印图 //pG.DFS(); // 深度优先遍历 //pG.BFS(); // 广度优先遍历 pG.prim(0); // prim算法生成最小生成树 &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409/** * Java prim算法生成最小生成树(邻接表) * * @author skywang * @date 2014/04/23 */import java.io.IOException;import java.util.Scanner;public class ListUDG &#123; private static int INF = Integer.MAX_VALUE; // 邻接表中表对应的链表的顶点 private class ENode &#123; int ivex; // 该边所指向的顶点的位置 int weight; // 该边的权 ENode nextEdge; // 指向下一条弧的指针 &#125; // 邻接表中表的顶点 private class VNode &#123; char data; // 顶点信息 ENode firstEdge; // 指向第一条依附该顶点的弧 &#125;; private VNode[] mVexs; // 顶点数组 /* * 创建图(自己输入数据) */ public ListUDG() &#123; // 输入&quot;顶点数&quot;和&quot;边数&quot; System.out.printf(&quot;input vertex number: &quot;); int vlen = readInt(); System.out.printf(&quot;input edge number: &quot;); int elen = readInt(); if ( vlen &lt; 1 || elen &lt; 1 || (elen &gt; (vlen*(vlen - 1)))) &#123; System.out.printf(&quot;input error: invalid parameters!\\n&quot;); return ; &#125; // 初始化&quot;顶点&quot; mVexs = new VNode[vlen]; for (int i = 0; i &lt; mVexs.length; i++) &#123; System.out.printf(&quot;vertex(%d): &quot;, i); mVexs[i] = new VNode(); mVexs[i].data = readChar(); mVexs[i].firstEdge = null; &#125; // 初始化&quot;边&quot; //mMatrix = new int[vlen][vlen]; for (int i = 0; i &lt; elen; i++) &#123; // 读取边的起始顶点和结束顶点 System.out.printf(&quot;edge(%d):&quot;, i); char c1 = readChar(); char c2 = readChar(); int weight = readInt(); int p1 = getPosition(c1); int p2 = getPosition(c2); // 初始化node1 ENode node1 = new ENode(); node1.ivex = p2; node1.weight = weight; // 将node1链接到&quot;p1所在链表的末尾&quot; if(mVexs[p1].firstEdge == null) mVexs[p1].firstEdge = node1; else linkLast(mVexs[p1].firstEdge, node1); // 初始化node2 ENode node2 = new ENode(); node2.ivex = p1; node2.weight = weight; // 将node2链接到&quot;p2所在链表的末尾&quot; if(mVexs[p2].firstEdge == null) mVexs[p2].firstEdge = node2; else linkLast(mVexs[p2].firstEdge, node2); &#125; &#125; /* * 创建图(用已提供的矩阵) * * 参数说明： * vexs -- 顶点数组 * edges -- 边 */ public ListUDG(char[] vexs, EData[] edges) &#123; // 初始化&quot;顶点数&quot;和&quot;边数&quot; int vlen = vexs.length; int elen = edges.length; // 初始化&quot;顶点&quot; mVexs = new VNode[vlen]; for (int i = 0; i &lt; mVexs.length; i++) &#123; mVexs[i] = new VNode(); mVexs[i].data = vexs[i]; mVexs[i].firstEdge = null; &#125; // 初始化&quot;边&quot; for (int i = 0; i &lt; elen; i++) &#123; // 读取边的起始顶点和结束顶点 char c1 = edges[i].start; char c2 = edges[i].end; int weight = edges[i].weight; // 读取边的起始顶点和结束顶点 int p1 = getPosition(c1); int p2 = getPosition(c2); // 初始化node1 ENode node1 = new ENode(); node1.ivex = p2; node1.weight = weight; // 将node1链接到&quot;p1所在链表的末尾&quot; if(mVexs[p1].firstEdge == null) mVexs[p1].firstEdge = node1; else linkLast(mVexs[p1].firstEdge, node1); // 初始化node2 ENode node2 = new ENode(); node2.ivex = p1; node2.weight = weight; // 将node2链接到&quot;p2所在链表的末尾&quot; if(mVexs[p2].firstEdge == null) mVexs[p2].firstEdge = node2; else linkLast(mVexs[p2].firstEdge, node2); &#125; &#125; /* * 将node节点链接到list的最后 */ private void linkLast(ENode list, ENode node) &#123; ENode p = list; while(p.nextEdge!=null) p = p.nextEdge; p.nextEdge = node; &#125; /* * 返回ch位置 */ private int getPosition(char ch) &#123; for(int i=0; i&lt;mVexs.length; i++) if(mVexs[i].data==ch) return i; return -1; &#125; /* * 读取一个输入字符 */ private char readChar() &#123; char ch=&#x27;0&#x27;; do &#123; try &#123; ch = (char)System.in.read(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; while(!((ch&gt;=&#x27;a&#x27;&amp;&amp;ch&lt;=&#x27;z&#x27;) || (ch&gt;=&#x27;A&#x27;&amp;&amp;ch&lt;=&#x27;Z&#x27;))); return ch; &#125; /* * 读取一个输入字符 */ private int readInt() &#123; Scanner scanner = new Scanner(System.in); return scanner.nextInt(); &#125; /* * 深度优先搜索遍历图的递归实现 */ private void DFS(int i, boolean[] visited) &#123; ENode node; visited[i] = true; System.out.printf(&quot;%c &quot;, mVexs[i].data); node = mVexs[i].firstEdge; while (node != null) &#123; if (!visited[node.ivex]) DFS(node.ivex, visited); node = node.nextEdge; &#125; &#125; /* * 深度优先搜索遍历图 */ public void DFS() &#123; boolean[] visited = new boolean[mVexs.length]; // 顶点访问标记 // 初始化所有顶点都没有被访问 for (int i = 0; i &lt; mVexs.length; i++) visited[i] = false; System.out.printf(&quot;DFS: &quot;); for (int i = 0; i &lt; mVexs.length; i++) &#123; if (!visited[i]) DFS(i, visited); &#125; System.out.printf(&quot;\\n&quot;); &#125; /* * 广度优先搜索（类似于树的层次遍历） */ public void BFS() &#123; int head = 0; int rear = 0; int[] queue = new int[mVexs.length]; // 辅组队列 boolean[] visited = new boolean[mVexs.length]; // 顶点访问标记 for (int i = 0; i &lt; mVexs.length; i++) visited[i] = false; System.out.printf(&quot;BFS: &quot;); for (int i = 0; i &lt; mVexs.length; i++) &#123; if (!visited[i]) &#123; visited[i] = true; System.out.printf(&quot;%c &quot;, mVexs[i].data); queue[rear++] = i; // 入队列 &#125; while (head != rear) &#123; int j = queue[head++]; // 出队列 ENode node = mVexs[j].firstEdge; while (node != null) &#123; int k = node.ivex; if (!visited[k]) &#123; visited[k] = true; System.out.printf(&quot;%c &quot;, mVexs[k].data); queue[rear++] = k; &#125; node = node.nextEdge; &#125; &#125; &#125; System.out.printf(&quot;\\n&quot;); &#125; /* * 打印矩阵队列图 */ public void print() &#123; System.out.printf(&quot;List Graph:\\n&quot;); for (int i = 0; i &lt; mVexs.length; i++) &#123; System.out.printf(&quot;%d(%c): &quot;, i, mVexs[i].data); ENode node = mVexs[i].firstEdge; while (node != null) &#123; System.out.printf(&quot;%d(%c) &quot;, node.ivex, mVexs[node.ivex].data); node = node.nextEdge; &#125; System.out.printf(&quot;\\n&quot;); &#125; &#125; /* * 获取边&lt;start, end&gt;的权值；若start和end不是连通的，则返回无穷大。 */ private int getWeight(int start, int end) &#123; if (start==end) return 0; ENode node = mVexs[start].firstEdge; while (node!=null) &#123; if (end==node.ivex) return node.weight; node = node.nextEdge; &#125; return INF; &#125; /* * prim最小生成树 * * 参数说明： * start -- 从图中的第start个元素开始，生成最小树 */ public void prim(int start) &#123; int min,i,j,k,m,n,tmp,sum; int num = mVexs.length; int index=0; // prim最小树的索引，即prims数组的索引 char[] prims = new char[num]; // prim最小树的结果数组 int[] weights = new int[num]; // 顶点间边的权值 // prim最小生成树中第一个数是&quot;图中第start个顶点&quot;，因为是从start开始的。 prims[index++] = mVexs[start].data; // 初始化&quot;顶点的权值数组&quot;， // 将每个顶点的权值初始化为&quot;第start个顶点&quot;到&quot;该顶点&quot;的权值。 for (i = 0; i &lt; num; i++ ) weights[i] = getWeight(start, i); for (i = 0; i &lt; num; i++) &#123; // 由于从start开始的，因此不需要再对第start个顶点进行处理。 if(start == i) continue; j = 0; k = 0; min = INF; // 在未被加入到最小生成树的顶点中，找出权值最小的顶点。 while (j &lt; num) &#123; // 若weights[j]=0，意味着&quot;第j个节点已经被排序过&quot;(或者说已经加入了最小生成树中)。 if (weights[j] != 0 &amp;&amp; weights[j] &lt; min) &#123; min = weights[j]; k = j; &#125; j++; &#125; // 经过上面的处理后，在未被加入到最小生成树的顶点中，权值最小的顶点是第k个顶点。 // 将第k个顶点加入到最小生成树的结果数组中 prims[index++] = mVexs[k].data; // 将&quot;第k个顶点的权值&quot;标记为0，意味着第k个顶点已经排序过了(或者说已经加入了最小树结果中)。 weights[k] = 0; // 当第k个顶点被加入到最小生成树的结果数组中之后，更新其它顶点的权值。 for (j = 0 ; j &lt; num; j++) &#123; // 获取第k个顶点到第j个顶点的权值 tmp = getWeight(k, j); // 当第j个节点没有被处理，并且需要更新时才被更新。 if (weights[j] != 0 &amp;&amp; tmp &lt; weights[j]) weights[j] = tmp; &#125; &#125; // 计算最小生成树的权值 sum = 0; for (i = 1; i &lt; index; i++) &#123; min = INF; // 获取prims[i]在矩阵表中的位置 n = getPosition(prims[i]); // 在vexs[0...i]中，找出到j的权值最小的顶点。 for (j = 0; j &lt; i; j++) &#123; m = getPosition(prims[j]); tmp = getWeight(m, n); if (tmp &lt; min) min = tmp; &#125; sum += min; &#125; // 打印最小生成树 System.out.printf(&quot;PRIM(%c)=%d: &quot;, mVexs[start].data, sum); for (i = 0; i &lt; index; i++) System.out.printf(&quot;%c &quot;, prims[i]); System.out.printf(&quot;\\n&quot;); &#125; // 示例类：边的结构体(用来演示) private static class EData &#123; char start; // 边的起点 char end; // 边的终点 int weight; // 边的权重 public EData(char start, char end, int weight) &#123; this.start = start; this.end = end; this.weight = weight; &#125; &#125;; public static void main(String[] args) &#123; char[] vexs = &#123;&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;, &#x27;E&#x27;, &#x27;F&#x27;, &#x27;G&#x27;&#125;; EData[] edges = &#123; // 起点 终点 权 new EData(&#x27;A&#x27;, &#x27;B&#x27;, 12), new EData(&#x27;A&#x27;, &#x27;F&#x27;, 16), new EData(&#x27;A&#x27;, &#x27;G&#x27;, 14), new EData(&#x27;B&#x27;, &#x27;C&#x27;, 10), new EData(&#x27;B&#x27;, &#x27;F&#x27;, 7), new EData(&#x27;C&#x27;, &#x27;D&#x27;, 3), new EData(&#x27;C&#x27;, &#x27;E&#x27;, 5), new EData(&#x27;C&#x27;, &#x27;F&#x27;, 6), new EData(&#x27;D&#x27;, &#x27;E&#x27;, 4), new EData(&#x27;E&#x27;, &#x27;F&#x27;, 2), new EData(&#x27;E&#x27;, &#x27;G&#x27;, 8), new EData(&#x27;F&#x27;, &#x27;G&#x27;, 9), &#125;; ListUDG pG; // 自定义&quot;图&quot;(输入矩阵队列) //pG = new ListUDG(); // 采用已有的&quot;图&quot; pG = new ListUDG(vexs, edges); //pG.print(); // 打印图 //pG.DFS(); // 深度优先遍历 //pG.BFS(); // 广度优先遍历 pG.prim(0); // prim算法生成最小生成树 &#125;&#125; 克鲁斯卡尔算法无论是普里姆算法（Prim）还是克鲁斯卡尔算法（Kruskal），他们考虑问题的出发点都是为使生成树上边的权值之和达到最小，则应使生成树中每一条边的权值尽可能的小，普里姆算法是以『某顶点为起点』，逐步找各个顶点上最小权值的边来构建最小生成树的 但是现在我们换一种思考方式，我们从边出发，直接去找『最小权值的边』来构建生成树，这也是克鲁斯卡尔算法的精髓，还是老规矩，我们通过图片来进行了解，如下，我们使用红点来表示顶点 然后按照权值递增的顺序依次连接 (0, 2)，(3， 5)，(1， 4) 和 (2, 5)，我们将其也标注为红色，如下 由于边 (0, 3) 的两个顶点在同一棵树上，所以舍去，而边 (2, 4) 和 (1, 2) 的长度相同，可以任选一条加入，最后结果如下 最后我们来看如何用代码进行实现，我们还是以上面的示例为例，先将图转换成一个边集数组，如下 但是这一次，我们需要借助于一个 parent 数组来进行实现，初始化如下 下面是完成后的 parent 数组的变化，如下 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441/** * Java: Kruskal算法生成最小生成树(邻接矩阵) * * @author skywang * @date 2014/04/24 */import java.io.IOException;import java.util.Scanner;public class MatrixUDG &#123; private int mEdgNum; // 边的数量 private char[] mVexs; // 顶点集合 private int[][] mMatrix; // 邻接矩阵 private static final int INF = Integer.MAX_VALUE; // 最大值 /* * 创建图(自己输入数据) */ public MatrixUDG() &#123; // 输入&quot;顶点数&quot;和&quot;边数&quot; System.out.printf(&quot;input vertex number: &quot;); int vlen = readInt(); System.out.printf(&quot;input edge number: &quot;); int elen = readInt(); if ( vlen &lt; 1 || elen &lt; 1 || (elen &gt; (vlen*(vlen - 1)))) &#123; System.out.printf(&quot;input error: invalid parameters!\\n&quot;); return ; &#125; // 初始化&quot;顶点&quot; mVexs = new char[vlen]; for (int i = 0; i &lt; mVexs.length; i++) &#123; System.out.printf(&quot;vertex(%d): &quot;, i); mVexs[i] = readChar(); &#125; // 1. 初始化&quot;边&quot;的权值 mEdgNum = elen; mMatrix = new int[vlen][vlen]; for (int i = 0; i &lt; vlen; i++) &#123; for (int j = 0; j &lt; vlen; j++) &#123; if (i==j) mMatrix[i][j] = 0; else mMatrix[i][j] = INF; &#125; &#125; // 2. 初始化&quot;边&quot;的权值: 根据用户的输入进行初始化 for (int i = 0; i &lt; elen; i++) &#123; // 读取边的起始顶点,结束顶点,权值 System.out.printf(&quot;edge(%d):&quot;, i); char c1 = readChar(); // 读取&quot;起始顶点&quot; char c2 = readChar(); // 读取&quot;结束顶点&quot; int weight = readInt(); // 读取&quot;权值&quot; int p1 = getPosition(c1); int p2 = getPosition(c2); if (p1==-1 || p2==-1) &#123; System.out.printf(&quot;input error: invalid edge!\\n&quot;); return ; &#125; mMatrix[p1][p2] = weight; mMatrix[p2][p1] = weight; &#125; &#125; /* * 创建图(用已提供的矩阵) * * 参数说明： * vexs -- 顶点数组 * matrix-- 矩阵(数据) */ public MatrixUDG(char[] vexs, int[][] matrix) &#123; // 初始化&quot;顶点数&quot;和&quot;边数&quot; int vlen = vexs.length; // 初始化&quot;顶点&quot; mVexs = new char[vlen]; for (int i = 0; i &lt; mVexs.length; i++) mVexs[i] = vexs[i]; // 初始化&quot;边&quot; mMatrix = new int[vlen][vlen]; for (int i = 0; i &lt; vlen; i++) for (int j = 0; j &lt; vlen; j++) mMatrix[i][j] = matrix[i][j]; // 统计&quot;边&quot; mEdgNum = 0; for (int i = 0; i &lt; vlen; i++) for (int j = i+1; j &lt; vlen; j++) if (mMatrix[i][j]!=INF) mEdgNum++; &#125; /* * 返回ch位置 */ private int getPosition(char ch) &#123; for(int i=0; i&lt;mVexs.length; i++) if(mVexs[i]==ch) return i; return -1; &#125; /* * 读取一个输入字符 */ private char readChar() &#123; char ch=&#x27;0&#x27;; do &#123; try &#123; ch = (char)System.in.read(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; while(!((ch&gt;=&#x27;a&#x27;&amp;&amp;ch&lt;=&#x27;z&#x27;) || (ch&gt;=&#x27;A&#x27;&amp;&amp;ch&lt;=&#x27;Z&#x27;))); return ch; &#125; /* * 读取一个输入字符 */ private int readInt() &#123; Scanner scanner = new Scanner(System.in); return scanner.nextInt(); &#125; /* * 返回顶点v的第一个邻接顶点的索引，失败则返回-1 */ private int firstVertex(int v) &#123; if (v&lt;0 || v&gt;(mVexs.length-1)) return -1; for (int i = 0; i &lt; mVexs.length; i++) if (mMatrix[v][i]!=0 &amp;&amp; mMatrix[v][i]!=INF) return i; return -1; &#125; /* * 返回顶点v相对于w的下一个邻接顶点的索引，失败则返回-1 */ private int nextVertex(int v, int w) &#123; if (v&lt;0 || v&gt;(mVexs.length-1) || w&lt;0 || w&gt;(mVexs.length-1)) return -1; for (int i = w + 1; i &lt; mVexs.length; i++) if (mMatrix[v][i]!=0 &amp;&amp; mMatrix[v][i]!=INF) return i; return -1; &#125; /* * 深度优先搜索遍历图的递归实现 */ private void DFS(int i, boolean[] visited) &#123; visited[i] = true; System.out.printf(&quot;%c &quot;, mVexs[i]); // 遍历该顶点的所有邻接顶点。若是没有访问过，那么继续往下走 for (int w = firstVertex(i); w &gt;= 0; w = nextVertex(i, w)) &#123; if (!visited[w]) DFS(w, visited); &#125; &#125; /* * 深度优先搜索遍历图 */ public void DFS() &#123; boolean[] visited = new boolean[mVexs.length]; // 顶点访问标记 // 初始化所有顶点都没有被访问 for (int i = 0; i &lt; mVexs.length; i++) visited[i] = false; System.out.printf(&quot;DFS: &quot;); for (int i = 0; i &lt; mVexs.length; i++) &#123; if (!visited[i]) DFS(i, visited); &#125; System.out.printf(&quot;\\n&quot;); &#125; /* * 广度优先搜索（类似于树的层次遍历） */ public void BFS() &#123; int head = 0; int rear = 0; int[] queue = new int[mVexs.length]; // 辅组队列 boolean[] visited = new boolean[mVexs.length]; // 顶点访问标记 for (int i = 0; i &lt; mVexs.length; i++) visited[i] = false; System.out.printf(&quot;BFS: &quot;); for (int i = 0; i &lt; mVexs.length; i++) &#123; if (!visited[i]) &#123; visited[i] = true; System.out.printf(&quot;%c &quot;, mVexs[i]); queue[rear++] = i; // 入队列 &#125; while (head != rear) &#123; int j = queue[head++]; // 出队列 for (int k = firstVertex(j); k &gt;= 0; k = nextVertex(j, k)) &#123; //k是为访问的邻接顶点 if (!visited[k]) &#123; visited[k] = true; System.out.printf(&quot;%c &quot;, mVexs[k]); queue[rear++] = k; &#125; &#125; &#125; &#125; System.out.printf(&quot;\\n&quot;); &#125; /* * 打印矩阵队列图 */ public void print() &#123; System.out.printf(&quot;Martix Graph:\\n&quot;); for (int i = 0; i &lt; mVexs.length; i++) &#123; for (int j = 0; j &lt; mVexs.length; j++) System.out.printf(&quot;%10d &quot;, mMatrix[i][j]); System.out.printf(&quot;\\n&quot;); &#125; &#125; /* * prim最小生成树 * * 参数说明： * start -- 从图中的第start个元素开始，生成最小树 */ public void prim(int start) &#123; int num = mVexs.length; // 顶点个数 int index=0; // prim最小树的索引，即prims数组的索引 char[] prims = new char[num]; // prim最小树的结果数组 int[] weights = new int[num]; // 顶点间边的权值 // prim最小生成树中第一个数是&quot;图中第start个顶点&quot;，因为是从start开始的。 prims[index++] = mVexs[start]; // 初始化&quot;顶点的权值数组&quot;， // 将每个顶点的权值初始化为&quot;第start个顶点&quot;到&quot;该顶点&quot;的权值。 for (int i = 0; i &lt; num; i++ ) weights[i] = mMatrix[start][i]; // 将第start个顶点的权值初始化为0。 // 可以理解为&quot;第start个顶点到它自身的距离为0&quot;。 weights[start] = 0; for (int i = 0; i &lt; num; i++) &#123; // 由于从start开始的，因此不需要再对第start个顶点进行处理。 if(start == i) continue; int j = 0; int k = 0; int min = INF; // 在未被加入到最小生成树的顶点中，找出权值最小的顶点。 while (j &lt; num) &#123; // 若weights[j]=0，意味着&quot;第j个节点已经被排序过&quot;(或者说已经加入了最小生成树中)。 if (weights[j] != 0 &amp;&amp; weights[j] &lt; min) &#123; min = weights[j]; k = j; &#125; j++; &#125; // 经过上面的处理后，在未被加入到最小生成树的顶点中，权值最小的顶点是第k个顶点。 // 将第k个顶点加入到最小生成树的结果数组中 prims[index++] = mVexs[k]; // 将&quot;第k个顶点的权值&quot;标记为0，意味着第k个顶点已经排序过了(或者说已经加入了最小树结果中)。 weights[k] = 0; // 当第k个顶点被加入到最小生成树的结果数组中之后，更新其它顶点的权值。 for (j = 0 ; j &lt; num; j++) &#123; // 当第j个节点没有被处理，并且需要更新时才被更新。 if (weights[j] != 0 &amp;&amp; mMatrix[k][j] &lt; weights[j]) weights[j] = mMatrix[k][j]; &#125; &#125; // 计算最小生成树的权值 int sum = 0; for (int i = 1; i &lt; index; i++) &#123; int min = INF; // 获取prims[i]在mMatrix中的位置 int n = getPosition(prims[i]); // 在vexs[0...i]中，找出到j的权值最小的顶点。 for (int j = 0; j &lt; i; j++) &#123; int m = getPosition(prims[j]); if (mMatrix[m][n]&lt;min) min = mMatrix[m][n]; &#125; sum += min; &#125; // 打印最小生成树 System.out.printf(&quot;PRIM(%c)=%d: &quot;, mVexs[start], sum); for (int i = 0; i &lt; index; i++) System.out.printf(&quot;%c &quot;, prims[i]); System.out.printf(&quot;\\n&quot;); &#125; /* * 克鲁斯卡尔（Kruskal)最小生成树 */ public void kruskal() &#123; int index = 0; // rets数组的索引 int[] vends = new int[mEdgNum]; // 用于保存&quot;已有最小生成树&quot;中每个顶点在该最小树中的终点。 EData[] rets = new EData[mEdgNum]; // 结果数组，保存kruskal最小生成树的边 EData[] edges; // 图对应的所有边 // 获取&quot;图中所有的边&quot; edges = getEdges(); // 将边按照&quot;权&quot;的大小进行排序(从小到大) sortEdges(edges, mEdgNum); for (int i=0; i&lt;mEdgNum; i++) &#123; int p1 = getPosition(edges[i].start); // 获取第i条边的&quot;起点&quot;的序号 int p2 = getPosition(edges[i].end); // 获取第i条边的&quot;终点&quot;的序号 int m = getEnd(vends, p1); // 获取p1在&quot;已有的最小生成树&quot;中的终点 int n = getEnd(vends, p2); // 获取p2在&quot;已有的最小生成树&quot;中的终点 // 如果m!=n，意味着&quot;边i&quot;与&quot;已经添加到最小生成树中的顶点&quot;没有形成环路 if (m != n) &#123; vends[m] = n; // 设置m在&quot;已有的最小生成树&quot;中的终点为n rets[index++] = edges[i]; // 保存结果 &#125; &#125; // 统计并打印&quot;kruskal最小生成树&quot;的信息 int length = 0; for (int i = 0; i &lt; index; i++) length += rets[i].weight; System.out.printf(&quot;Kruskal=%d: &quot;, length); for (int i = 0; i &lt; index; i++) System.out.printf(&quot;(%c,%c) &quot;, rets[i].start, rets[i].end); System.out.printf(&quot;\\n&quot;); &#125; /* * 获取图中的边 */ private EData[] getEdges() &#123; int index=0; EData[] edges; edges = new EData[mEdgNum]; for (int i=0; i &lt; mVexs.length; i++) &#123; for (int j=i+1; j &lt; mVexs.length; j++) &#123; if (mMatrix[i][j]!=INF) &#123; edges[index++] = new EData(mVexs[i], mVexs[j], mMatrix[i][j]); &#125; &#125; &#125; return edges; &#125; /* * 对边按照权值大小进行排序(由小到大) */ private void sortEdges(EData[] edges, int elen) &#123; for (int i=0; i&lt;elen; i++) &#123; for (int j=i+1; j&lt;elen; j++) &#123; if (edges[i].weight &gt; edges[j].weight) &#123; // 交换&quot;边i&quot;和&quot;边j&quot; EData tmp = edges[i]; edges[i] = edges[j]; edges[j] = tmp; &#125; &#125; &#125; &#125; /* * 获取i的终点 */ private int getEnd(int[] vends, int i) &#123; while (vends[i] != 0) i = vends[i]; return i; &#125; // 边的结构体 private static class EData &#123; char start; // 边的起点 char end; // 边的终点 int weight; // 边的权重 public EData(char start, char end, int weight) &#123; this.start = start; this.end = end; this.weight = weight; &#125; &#125;; public static void main(String[] args) &#123; char[] vexs = &#123;&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;, &#x27;E&#x27;, &#x27;F&#x27;, &#x27;G&#x27;&#125;; int matrix[][] = &#123; /*A*//*B*//*C*//*D*//*E*//*F*//*G*/ /*A*/ &#123; 0, 12, INF, INF, INF, 16, 14&#125;, /*B*/ &#123; 12, 0, 10, INF, INF, 7, INF&#125;, /*C*/ &#123; INF, 10, 0, 3, 5, 6, INF&#125;, /*D*/ &#123; INF, INF, 3, 0, 4, INF, INF&#125;, /*E*/ &#123; INF, INF, 5, 4, 0, 2, 8&#125;, /*F*/ &#123; 16, 7, 6, INF, 2, 0, 9&#125;, /*G*/ &#123; 14, INF, INF, INF, 8, 9, 0&#125;&#125;; MatrixUDG pG; // 自定义&quot;图&quot;(输入矩阵队列) //pG = new MatrixUDG(); // 采用已有的&quot;图&quot; pG = new MatrixUDG(vexs, matrix); //pG.print(); // 打印图 //pG.DFS(); // 深度优先遍历 //pG.BFS(); // 广度优先遍历 //pG.prim(0); // prim算法生成最小生成树 pG.kruskal(); // Kruskal算法生成最小生成树 &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498/** * Java prim算法生成最小生成树(邻接表) * * @author skywang * @date 2014/04/23 */import java.io.IOException;import java.util.Scanner;public class ListUDG &#123; private static int INF = Integer.MAX_VALUE; // 邻接表中表对应的链表的顶点 private class ENode &#123; int ivex; // 该边所指向的顶点的位置 int weight; // 该边的权 ENode nextEdge; // 指向下一条弧的指针 &#125; // 邻接表中表的顶点 private class VNode &#123; char data; // 顶点信息 ENode firstEdge; // 指向第一条依附该顶点的弧 &#125;; private int mEdgNum; // 边的数量 private VNode[] mVexs; // 顶点数组 /* * 创建图(自己输入数据) */ public ListUDG() &#123; // 输入&quot;顶点数&quot;和&quot;边数&quot; System.out.printf(&quot;input vertex number: &quot;); int vlen = readInt(); System.out.printf(&quot;input edge number: &quot;); int elen = readInt(); if ( vlen &lt; 1 || elen &lt; 1 || (elen &gt; (vlen*(vlen - 1)))) &#123; System.out.printf(&quot;input error: invalid parameters!\\n&quot;); return ; &#125; // 初始化&quot;顶点&quot; mVexs = new VNode[vlen]; for (int i = 0; i &lt; mVexs.length; i++) &#123; System.out.printf(&quot;vertex(%d): &quot;, i); mVexs[i] = new VNode(); mVexs[i].data = readChar(); mVexs[i].firstEdge = null; &#125; // 初始化&quot;边&quot; mEdgNum = elen; for (int i = 0; i &lt; elen; i++) &#123; // 读取边的起始顶点和结束顶点 System.out.printf(&quot;edge(%d):&quot;, i); char c1 = readChar(); char c2 = readChar(); int weight = readInt(); int p1 = getPosition(c1); int p2 = getPosition(c2); // 初始化node1 ENode node1 = new ENode(); node1.ivex = p2; node1.weight = weight; // 将node1链接到&quot;p1所在链表的末尾&quot; if(mVexs[p1].firstEdge == null) mVexs[p1].firstEdge = node1; else linkLast(mVexs[p1].firstEdge, node1); // 初始化node2 ENode node2 = new ENode(); node2.ivex = p1; node2.weight = weight; // 将node2链接到&quot;p2所在链表的末尾&quot; if(mVexs[p2].firstEdge == null) mVexs[p2].firstEdge = node2; else linkLast(mVexs[p2].firstEdge, node2); &#125; &#125; /* * 创建图(用已提供的矩阵) * * 参数说明： * vexs -- 顶点数组 * edges -- 边 */ public ListUDG(char[] vexs, EData[] edges) &#123; // 初始化&quot;顶点数&quot;和&quot;边数&quot; int vlen = vexs.length; int elen = edges.length; // 初始化&quot;顶点&quot; mVexs = new VNode[vlen]; for (int i = 0; i &lt; mVexs.length; i++) &#123; mVexs[i] = new VNode(); mVexs[i].data = vexs[i]; mVexs[i].firstEdge = null; &#125; // 初始化&quot;边&quot; mEdgNum = elen; for (int i = 0; i &lt; elen; i++) &#123; // 读取边的起始顶点和结束顶点 char c1 = edges[i].start; char c2 = edges[i].end; int weight = edges[i].weight; // 读取边的起始顶点和结束顶点 int p1 = getPosition(c1); int p2 = getPosition(c2); // 初始化node1 ENode node1 = new ENode(); node1.ivex = p2; node1.weight = weight; // 将node1链接到&quot;p1所在链表的末尾&quot; if(mVexs[p1].firstEdge == null) mVexs[p1].firstEdge = node1; else linkLast(mVexs[p1].firstEdge, node1); // 初始化node2 ENode node2 = new ENode(); node2.ivex = p1; node2.weight = weight; // 将node2链接到&quot;p2所在链表的末尾&quot; if(mVexs[p2].firstEdge == null) mVexs[p2].firstEdge = node2; else linkLast(mVexs[p2].firstEdge, node2); &#125; &#125; /* * 将node节点链接到list的最后 */ private void linkLast(ENode list, ENode node) &#123; ENode p = list; while(p.nextEdge!=null) p = p.nextEdge; p.nextEdge = node; &#125; /* * 返回ch位置 */ private int getPosition(char ch) &#123; for(int i=0; i&lt;mVexs.length; i++) if(mVexs[i].data==ch) return i; return -1; &#125; /* * 读取一个输入字符 */ private char readChar() &#123; char ch=&#x27;0&#x27;; do &#123; try &#123; ch = (char)System.in.read(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; while(!((ch&gt;=&#x27;a&#x27;&amp;&amp;ch&lt;=&#x27;z&#x27;) || (ch&gt;=&#x27;A&#x27;&amp;&amp;ch&lt;=&#x27;Z&#x27;))); return ch; &#125; /* * 读取一个输入字符 */ private int readInt() &#123; Scanner scanner = new Scanner(System.in); return scanner.nextInt(); &#125; /* * 深度优先搜索遍历图的递归实现 */ private void DFS(int i, boolean[] visited) &#123; ENode node; visited[i] = true; System.out.printf(&quot;%c &quot;, mVexs[i].data); node = mVexs[i].firstEdge; while (node != null) &#123; if (!visited[node.ivex]) DFS(node.ivex, visited); node = node.nextEdge; &#125; &#125; /* * 深度优先搜索遍历图 */ public void DFS() &#123; boolean[] visited = new boolean[mVexs.length]; // 顶点访问标记 // 初始化所有顶点都没有被访问 for (int i = 0; i &lt; mVexs.length; i++) visited[i] = false; System.out.printf(&quot;DFS: &quot;); for (int i = 0; i &lt; mVexs.length; i++) &#123; if (!visited[i]) DFS(i, visited); &#125; System.out.printf(&quot;\\n&quot;); &#125; /* * 广度优先搜索（类似于树的层次遍历） */ public void BFS() &#123; int head = 0; int rear = 0; int[] queue = new int[mVexs.length]; // 辅组队列 boolean[] visited = new boolean[mVexs.length]; // 顶点访问标记 for (int i = 0; i &lt; mVexs.length; i++) visited[i] = false; System.out.printf(&quot;BFS: &quot;); for (int i = 0; i &lt; mVexs.length; i++) &#123; if (!visited[i]) &#123; visited[i] = true; System.out.printf(&quot;%c &quot;, mVexs[i].data); queue[rear++] = i; // 入队列 &#125; while (head != rear) &#123; int j = queue[head++]; // 出队列 ENode node = mVexs[j].firstEdge; while (node != null) &#123; int k = node.ivex; if (!visited[k]) &#123; visited[k] = true; System.out.printf(&quot;%c &quot;, mVexs[k].data); queue[rear++] = k; &#125; node = node.nextEdge; &#125; &#125; &#125; System.out.printf(&quot;\\n&quot;); &#125; /* * 打印矩阵队列图 */ public void print() &#123; System.out.printf(&quot;List Graph:\\n&quot;); for (int i = 0; i &lt; mVexs.length; i++) &#123; System.out.printf(&quot;%d(%c): &quot;, i, mVexs[i].data); ENode node = mVexs[i].firstEdge; while (node != null) &#123; System.out.printf(&quot;%d(%c) &quot;, node.ivex, mVexs[node.ivex].data); node = node.nextEdge; &#125; System.out.printf(&quot;\\n&quot;); &#125; &#125; /* * 获取边&lt;start, end&gt;的权值；若start和end不是连通的，则返回无穷大。 */ private int getWeight(int start, int end) &#123; if (start==end) return 0; ENode node = mVexs[start].firstEdge; while (node!=null) &#123; if (end==node.ivex) return node.weight; node = node.nextEdge; &#125; return INF; &#125; /* * prim最小生成树 * * 参数说明： * start -- 从图中的第start个元素开始，生成最小树 */ public void prim(int start) &#123; int min,i,j,k,m,n,tmp,sum; int num = mVexs.length; int index=0; // prim最小树的索引，即prims数组的索引 char[] prims = new char[num]; // prim最小树的结果数组 int[] weights = new int[num]; // 顶点间边的权值 // prim最小生成树中第一个数是&quot;图中第start个顶点&quot;，因为是从start开始的。 prims[index++] = mVexs[start].data; // 初始化&quot;顶点的权值数组&quot;， // 将每个顶点的权值初始化为&quot;第start个顶点&quot;到&quot;该顶点&quot;的权值。 for (i = 0; i &lt; num; i++ ) weights[i] = getWeight(start, i); for (i = 0; i &lt; num; i++) &#123; // 由于从start开始的，因此不需要再对第start个顶点进行处理。 if(start == i) continue; j = 0; k = 0; min = INF; // 在未被加入到最小生成树的顶点中，找出权值最小的顶点。 while (j &lt; num) &#123; // 若weights[j]=0，意味着&quot;第j个节点已经被排序过&quot;(或者说已经加入了最小生成树中)。 if (weights[j] != 0 &amp;&amp; weights[j] &lt; min) &#123; min = weights[j]; k = j; &#125; j++; &#125; // 经过上面的处理后，在未被加入到最小生成树的顶点中，权值最小的顶点是第k个顶点。 // 将第k个顶点加入到最小生成树的结果数组中 prims[index++] = mVexs[k].data; // 将&quot;第k个顶点的权值&quot;标记为0，意味着第k个顶点已经排序过了(或者说已经加入了最小树结果中)。 weights[k] = 0; // 当第k个顶点被加入到最小生成树的结果数组中之后，更新其它顶点的权值。 for (j = 0 ; j &lt; num; j++) &#123; // 获取第k个顶点到第j个顶点的权值 tmp = getWeight(k, j); // 当第j个节点没有被处理，并且需要更新时才被更新。 if (weights[j] != 0 &amp;&amp; tmp &lt; weights[j]) weights[j] = tmp; &#125; &#125; // 计算最小生成树的权值 sum = 0; for (i = 1; i &lt; index; i++) &#123; min = INF; // 获取prims[i]在矩阵表中的位置 n = getPosition(prims[i]); // 在vexs[0...i]中，找出到j的权值最小的顶点。 for (j = 0; j &lt; i; j++) &#123; m = getPosition(prims[j]); tmp = getWeight(m, n); if (tmp &lt; min) min = tmp; &#125; sum += min; &#125; // 打印最小生成树 System.out.printf(&quot;PRIM(%c)=%d: &quot;, mVexs[start].data, sum); for (i = 0; i &lt; index; i++) System.out.printf(&quot;%c &quot;, prims[i]); System.out.printf(&quot;\\n&quot;); &#125; /* * 克鲁斯卡尔（Kruskal)最小生成树 */ public void kruskal() &#123; int index = 0; // rets数组的索引 int[] vends = new int[mEdgNum]; // 用于保存&quot;已有最小生成树&quot;中每个顶点在该最小树中的终点。 EData[] rets = new EData[mEdgNum]; // 结果数组，保存kruskal最小生成树的边 EData[] edges; // 图对应的所有边 // 获取&quot;图中所有的边&quot; edges = getEdges(); // 将边按照&quot;权&quot;的大小进行排序(从小到大) sortEdges(edges, mEdgNum); for (int i=0; i&lt;mEdgNum; i++) &#123; int p1 = getPosition(edges[i].start); // 获取第i条边的&quot;起点&quot;的序号 int p2 = getPosition(edges[i].end); // 获取第i条边的&quot;终点&quot;的序号 int m = getEnd(vends, p1); // 获取p1在&quot;已有的最小生成树&quot;中的终点 int n = getEnd(vends, p2); // 获取p2在&quot;已有的最小生成树&quot;中的终点 // 如果m!=n，意味着&quot;边i&quot;与&quot;已经添加到最小生成树中的顶点&quot;没有形成环路 if (m != n) &#123; vends[m] = n; // 设置m在&quot;已有的最小生成树&quot;中的终点为n rets[index++] = edges[i]; // 保存结果 &#125; &#125; // 统计并打印&quot;kruskal最小生成树&quot;的信息 int length = 0; for (int i = 0; i &lt; index; i++) length += rets[i].weight; System.out.printf(&quot;Kruskal=%d: &quot;, length); for (int i = 0; i &lt; index; i++) System.out.printf(&quot;(%c,%c) &quot;, rets[i].start, rets[i].end); System.out.printf(&quot;\\n&quot;); &#125; /* * 获取图中的边 */ private EData[] getEdges() &#123; int index=0; EData[] edges; edges = new EData[mEdgNum]; for (int i=0; i &lt; mVexs.length; i++) &#123; ENode node = mVexs[i].firstEdge; while (node != null) &#123; if (node.ivex &gt; i) &#123; edges[index++] = new EData(mVexs[i].data, mVexs[node.ivex].data, node.weight); &#125; node = node.nextEdge; &#125; &#125; return edges; &#125; /* * 对边按照权值大小进行排序(由小到大) */ private void sortEdges(EData[] edges, int elen) &#123; for (int i=0; i&lt;elen; i++) &#123; for (int j=i+1; j&lt;elen; j++) &#123; if (edges[i].weight &gt; edges[j].weight) &#123; // 交换&quot;边i&quot;和&quot;边j&quot; EData tmp = edges[i]; edges[i] = edges[j]; edges[j] = tmp; &#125; &#125; &#125; &#125; /* * 获取i的终点 */ private int getEnd(int[] vends, int i) &#123; while (vends[i] != 0) i = vends[i]; return i; &#125; // 边的结构体 private static class EData &#123; char start; // 边的起点 char end; // 边的终点 int weight; // 边的权重 public EData(char start, char end, int weight) &#123; this.start = start; this.end = end; this.weight = weight; &#125; &#125;; public static void main(String[] args) &#123; char[] vexs = &#123;&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;, &#x27;E&#x27;, &#x27;F&#x27;, &#x27;G&#x27;&#125;; EData[] edges = &#123; // 起点 终点 权 new EData(&#x27;A&#x27;, &#x27;B&#x27;, 12), new EData(&#x27;A&#x27;, &#x27;F&#x27;, 16), new EData(&#x27;A&#x27;, &#x27;G&#x27;, 14), new EData(&#x27;B&#x27;, &#x27;C&#x27;, 10), new EData(&#x27;B&#x27;, &#x27;F&#x27;, 7), new EData(&#x27;C&#x27;, &#x27;D&#x27;, 3), new EData(&#x27;C&#x27;, &#x27;E&#x27;, 5), new EData(&#x27;C&#x27;, &#x27;F&#x27;, 6), new EData(&#x27;D&#x27;, &#x27;E&#x27;, 4), new EData(&#x27;E&#x27;, &#x27;F&#x27;, 2), new EData(&#x27;E&#x27;, &#x27;G&#x27;, 8), new EData(&#x27;F&#x27;, &#x27;G&#x27;, 9), &#125;; ListUDG pG; // 自定义&quot;图&quot;(输入矩阵队列) //pG = new ListUDG(); // 采用已有的&quot;图&quot; pG = new ListUDG(vexs, edges); //pG.print(); // 打印图 //pG.DFS(); // 深度优先遍历 //pG.BFS(); // 广度优先遍历 //pG.prim(0); // prim算法生成最小生成树 pG.kruskal(); // Kruskal算法生成最小生成树 &#125;&#125; 参考普里姆算法和克鲁斯卡尔算法 Prim算法(三)之 Java详解 Kruskal算法(三)之 Java详解","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"https://xmmarlowe.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://xmmarlowe.github.io/tags/%E7%AE%97%E6%B3%95/"}],"author":"Marlowe"},{"title":"面向对象三大特性和六大原则","slug":"Java/面向对象三大特性和六大原则","date":"2021-06-01T01:51:59.000Z","updated":"2021-06-01T15:14:48.432Z","comments":true,"path":"2021/06/01/Java/面向对象三大特性和六大原则/","link":"","permalink":"https://xmmarlowe.github.io/2021/06/01/Java/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E4%B8%89%E5%A4%A7%E7%89%B9%E6%80%A7%E5%92%8C%E5%85%AD%E5%A4%A7%E5%8E%9F%E5%88%99/","excerpt":"","text":"对象的概念Java 是面向对象的编程语言，对象就是面向对象程序设计的核心。所谓对象就是真实世界中的实体，对象与实体是一一对应的，也就是说现实世界中每一个实体都是一个对象，它是一种具体的概念。对象有以下特点： 对象具有属性和行为。 对象具有变化的状态。 对象具有唯一性。 对象都是某个类别的实例。 一切皆为对象，真实世界中的所有事物都可以视为对象。 三大特性面向对象开发模式更有利于人们开拓思维，在具体的开发过程中便于程序的划分，方便程序员分工合作，提高开发效率。 该开发模式之所以使程序设计更加完善和强大，主要是因为面向对象具有继承、封装和多态 3 个核心特性。 封装在面向对象程式设计方法中，封装（英语：Encapsulation）是指一种将抽象性函式接口的实现细节部份包装、隐藏起来的方法。 封装可以被认为是一个保护屏障，防止该类的代码和数据被外部类定义的代码随机访问。 要访问该类的代码和数据，必须通过严格的接口控制。 封装最主要的功能在于我们能修改自己的实现代码，而不用修改那些调用我们代码的程序片段。 适当的封装可以让程式码更容易理解与维护，也加强了程式码的安全性。 封装的概念Java 语言的基本封装单位是类。由于类的用途是封装复杂性，所以类的内部有隐藏实现复杂性的机制。Java 提供了私有和公有的访问模式，类的公有接口代表外部的用户应该知道或可以知道的每件东西，私有的方法数据只能通过该类的成员代码来访问，这就可以确保不会发生不希望的事情。 封装的优点 良好的封装能够减少耦合。 类内部的结构可以自由修改。 可以对成员变量进行更精确的控制。 隐藏信息，实现细节。 Java 封装，说白了就是将一大坨公共通用的实现逻辑玩意，装到一个盒子里（class），出入口都在这个盒子上。你要用就将这个盒子拿来用，连接出入口，就能用了，不用就可以直接扔，对你代码没什么影响。 封装的目的 偷懒，辛苦一次，后面都能少敲很多代码，增强了代码得复用性 简化代码，看起来更容易懂 隐藏核心实现逻辑代码，简化外部逻辑，并且不让其他人修改，jar 都这么干 一对一，一个功能就只为这个功能服务；避免头发绳子一块用，导致最后一团糟 Java 中的内部类 内部类（ Inner Class ）就是定义在另外一个类里面的类。与之对应，包含内部类的类被称为外部类。 那么问题来了：那为什么要将一个类定义在另一个类里面呢？清清爽爽的独立的一个类多好啊！！ 答：内部类的主要作用如下： 内部类提供了更好的封装，可以把内部类隐藏在外部类之内，不允许同一个包中的其他类访问该类。 内部类的方法可以直接访问外部类的所有数据，包括私有的数据。 内部类所实现的功能使用外部类同样可以实现，只是有时使用内部类更方便。 内部类可分为以下几种： 成员内部类 静态内部类 方法内部类 匿名内部类 继承继承是java面向对象编程技术的一块基石，因为它允许创建分等级层次的类。 继承就是子类继承父类的特征和行为，使得子类对象（实例）具有父类的实例域和方法，或子类从父类继承方法，使得子类具有父类相同的行为。 兔子和羊属于食草动物类，狮子和豹属于食肉动物类。 食草动物和食肉动物又是属于动物类。 所以继承需要符合的关系是：is-a，父类更通用，子类更具体。 虽然食草动物和食肉动物都是属于动物，但是两者的属性和行为上有差别，所以子类会具有父类的一般特性也会具有自身的特性。 多态多态是同一个行为具有多个不同表现形式或形态的能力。 多态就是同一个接口，使用不同的实例而执行不同操作，如图所示： 多态性是对象多种表现形式的体现。 现实中，比如我们按下 F1 键这个动作： 如果当前在 Flash 界面下弹出的就是 AS 3 的帮助文档； 如果当前在 Word 下弹出的就是 Word 帮助； 在 Windows 下弹出的就是 Windows 帮助和支持。 同一个事件发生在不同的对象上会产生不同的结果。 多态的好处可替换性（substitutability）：多态对已存在代码具有可替换性。例如，多态对圆Circle类工作，对其他任何圆形几何体，如圆环，也同样工作。 可扩充性（extensibility）：多态对代码具有可扩充性。增加新的子类不影响已存在类的多态性、继承性，以及其他特性的运行和操作。实际上新加子类更容易获得多态功能。例如，在实现了圆锥、半圆锥以及半球体的多态基础上，很容易增添球体类的多态性。 接口性（interface-ability）：多态是超类通过方法签名，向子类提供了一个共同接口，由子类来完善或者覆盖它而实现的。 灵活性（flexibility）：它在应用中体现了灵活多样的操作，提高了使用效率。 简化性（simplicity）：多态简化对应用软件的代码编写和修改过程，尤其在处理大量对象的运算和操作时，这个特点尤为突出和重要。 子代父类实例化，然后就相当于一个父亲有很多儿子，送快递的给这个父亲的儿子送东西，他只需要送到父亲的家就行了，至于具体是那个儿子的，父亲还会分不清自己的儿子么，所以你就不用操心了。 使用多态是一种好习惯 多态方式声明是一种好的习惯。当我们创建的类，使用时，只用到它的超类或接口定义的方法时，我们可以将其索引声明为它的超类或接口类型。 它的好处是，如果某天我们对这个接口方法的实现方式变了，对这个接口又有一个新的实现类，我们的程序也需要使用最新的实现方式，此时只要将对象实现修改一下，索引无需变化。 比如Map&lt; String,String&gt; map = new HashMap &lt; String,String&gt;(); 想换成HashTable实现，可以Map&lt; String,String&gt; map = new HashTable &lt; String,String&gt;(); 比如写一个方法，参数要求传递List类型，你就可以用List list = new ArrayList()中的list传递，但是你写成ArrayList list = new ArrayList()是传递不进去的。尽管方法处理时都一样。另外，方法还可以根据你传递的不同list（ArrayList或者LinkList）进行不同处理。 六大原则面向对象的三大特性是”封装、”多态”、”继承”，五大原则是”单一职责原则”、”开放封闭原则”、”里氏替换原则”、”依赖倒置原则”、”接口分离原则”、”迪米特原则（高内聚低耦合）”。 单一职责原则SRP(Single Responsibility Principle)是指一个类的功能要单一，不能包罗万象。如同一个人一样，分配的工作不能太多，否则一天到晚虽然忙忙碌碌的，但效率却高不起来。 开放封闭原则OCP(Open－Close Principle)一个模块在扩展性方面应该是开放的而在更改性方面应该是封闭的。比如：一个网络模块，原来只服务端功能，而现在要加入客户端功能，那么应当在不用修改服务端功能代码的前提下，就能够增加客户端功能的实现代码，这要求在设计之初，就应当将服务端和客户端分开，公共部分抽象出来。 里式替换原则LSP(the Liskov Substitution Principle LSP)子类应当可以替换父类并出现在父类能够出现的任何地方。（比如父类public，子类一定是public）比如：公司搞年度晚会，所有员工可以参加抽奖，那么不管是老员工还是新员工，也不管是总部员工还是外派员工，都应当可以参加抽奖，否则这公司就不和谐了。 依赖倒置原则DIP(the Dependency Inversion Principle DIP)A.高层次的模块不应该依赖于低层次的模块，他们都应该依赖于抽象。 B.抽象不应该依赖于具体实现，具体实现应该依赖于抽象。 具体依赖抽象，上层依赖下层。高层模块就是调用端，底层模块就是具体实现类。（应该让底层模块定义抽象接口并且实现，让高层模块调用抽象接口，而不是直接调用实现类。） 通俗来讲：依赖倒置原则的本质就是通过抽象（接口或抽象类）使个各类或模块的实现彼此独立，互不影响，实现模块间的松耦合。 问题描述：类A直接依赖类B，假如要将类A改为依赖类C，则必须通过修改类A的代码来达成。这种场景下，类A一般是高层模块，负责复杂的业务逻辑；类B和类C是低层模块，负责基本的原子操作；假如修改类A，会给程序带来不必要的风险。 解决方案：将类A修改为依赖接口interface，类B和类C各自实现接口interface，类A通过接口interface间接与类B或者类C发生联系，则会大大降低修改类A的几率。（比如A依赖于车的轮胎，速度，牌子等接口，然后让B，C直接实现这些接口的方法，A间接通过接口与BC发生联系。） 好处：依赖倒置的好处在小型项目中很难体现出来。但在大中型项目中可以减少需求变化引起的工作量。使并行开发更友好。 接口分离原则ISP(the Interface Segregation Principle ISP)模块间要通过抽象接口隔离开，而不是通过具体的类强耦合起来，即面向接口编程。（提供接口，给其他模块调用） 核心思想：类间的依赖关系应该建立在最小的接口上通俗来讲：建立单一接口，不要建立庞大臃肿的接口，尽量细化接口，接口中的方法尽量少。 也就是说，我们要为各个类建立专用的接口，而不要试图去建立一个很庞大的接口供所有依赖它的类去调用。 问题描述：类A通过接口interface依赖类B，类C通过接口interface依赖类D，如果接口interface对于类A和类C来说不是最小接口，则类B和类D必须去实现他们不需要的方法。 需注意：接口尽量小，但是要有限度。对接口进行细化可以提高程序设计灵活性，但是如果过小，则会造成接口数量过多，使设计复杂化。所以一定要适度提高内聚，减少对外交互。使接口用最少的方法去完成最多的事情为依赖接口的类定制服务。只暴露给调用的类它需要的方法，它不需要的方法则隐藏起来。只有专注地为一个模块提供定制服务，才能建立最小的依赖关系。 迪米特法则（Law of Demeter,简称LoD）核心思想：类间解耦。 通俗来讲：一个类对自己依赖的类知道的越少越好。自从我们接触编程开始，就知道了软件编程的总的原则：低耦合，高内聚。无论是面向过程编程还是面向对象编程，只有使各个模块之间的耦合尽量的低，才能提高代码的复用率。 耦合是： 简单地说，软件工程中对象之间的耦合度就是对象之间的依赖性。指导使用和维护对象的主要问题是对象之间的多重依赖性。对象之间的耦合越高，维护成本越高。因此对象的设计应使类和构件之间的耦合最小。 有软硬件之间的耦合，还有软件各模块之间的耦合。耦合性是程序结构中各个模块之间相互关联的度量。它取决于各个模块之间的接口的复杂程度、调用模块的方式以及哪些信息通过接口。 耦合可以分为以下几种，它们之间的耦合度由高到低排列如下： 内容耦合：当一个模块直接修改或操作另一个模块的数据时，或一个模块不通过正常入口而转入另一个模块时，这样的耦合被称为内容耦合。内容耦合是最高程度的耦合，应该避免使用之。 公共耦合：两个或两个以上的模块共同引用一个全局数据项，这种耦合被称为公共耦合。在具有大量公共耦合的结构中，确定究竟是哪个模块给全局变量赋了一个特定的值是十分困难的。 外部耦合：一组模块都访问同一全局简单变量而不是同一全局数据结构，而且不是通过参数表传递该全局变量的信息，则称之为外部耦合。 控制耦合：一个模块通过接口向另一个模块传递一个控制信号，接受信号的模块根据信号值而进行适当的动作，这种耦合被称为控制耦合。 标记耦合：若一个模块A通过接口向两个模块B和C传递一个公共参数，那么称模块B和C之间存在一个标记耦合。 数据耦合：模块之间通过参数来传递数据，那么被称为数据耦合。数据耦合是最低的一种耦合形式，系统中一般都存在这种类型的耦合，因为为了完成一些有意义的功能，往往需要将某些模块的输出数据作为另一些模块的输入数据。 非直接耦合：两个模块之间没有直接关系，它们之间的联系完全是通过主模块的控制和调用来实现的。 总结耦合是影响软件复杂程度和设计质量的一个重要因素，在设计上我们应采用以下原则：如果模块间必须存在耦合，就尽量使用数据耦合，少用控制耦合，限制公共耦合的范围，尽量避免使用内容耦合。 同一个模块内的各个元素之间要高度紧密，但是各个模块之间的相互依存度却要不那么紧密。 一些问题面向对象和面向过程的区别？ 面向过程： 一种较早的编程思想，顾名思义就是该思想是站着过程的角度思考问题，强调的就是功能行为，功能的执行过程，即先后顺序，而每一个功能我们都使用函数（类似于方法）把这些步骤一步一步实现。使用的时候依次调用函数就可以了。 面向过程的设计： 最小的程序单元是函数，每个函数负责完成某一个功能，用于接受输入数据，函数对输入数据进行处理，然后输出结果数据，整个软件系统由一个个的函数组成，其中作为程序入口的函数称之为主函数，主函数依次调用其他函数，普通函数之间可以相互调用，从而实现整个系统功能。 面向过程最大的问题在于随着系统的膨胀，面向过程将无法应付，最终导致系统的崩溃。为了解决这一种软件危机，我们提出面向对象思想。 面向过程的缺陷： 是采用指定而下的设计模式，在设计阶段就需要考虑每一个模块应该分解成哪些子模块，每一个子模块又细分为更小的子模块，如此类推，直到将模块细化为一个个函数。 存在的问题​ 设计不够直观，与人类的思维习惯不一致 系统软件适应新差，可拓展性差，维护性低 面向对象：​ 一种基于面向过程的新编程思想，顾名思义就是该思想是站在对象的角度思考问题，我们把多个功能合理放到不同对象里，强调的是具备某些功能的对象。具备某种功能的实体，称为对象。面向对象最小的程序单元是：类。面向对象更加符合常规的思维方式，稳定性好，可重用性强，易于开发大型软件产品，有良好的可维护性。 在软件工程上，面向对象可以使工程更加模块化，实现更低的耦合和更高的内聚。 什么是方法重写子类如果对继承的父类的方法不满意（不适合），可以自己编写继承的方法，这种方式就称为方法的重写。当调用方法时会优先调用子类的方法。 重写要注意： a、返回值类型 b、方法名 c、参数类型及个数 都要与父类继承的方法相同，才叫方法的重写。 重载和重写的区别方法重载：在同一个类中处理不同数据的多个相同方法名的多态手段。 方法重写：相对继承而言，子类中对父类已经存在的方法进行区别化的修改。 继承的初始化顺序1、初始化父类再初始化子类 2、先执行初始化对象中属性，再执行构造方法中的初始化。 基于上面两点，我们就知道实例化一个子类，java程序的执行顺序是： 父类对象属性初始化—-&gt;父类对象构造方法—-&gt;子类对象属性初始化—&gt;子类对象构造方法","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"对象","slug":"对象","permalink":"https://xmmarlowe.github.io/tags/%E5%AF%B9%E8%B1%A1/"}],"author":"Marlowe"},{"title":"一致性哈希算法","slug":"算法与数据结构/一致性哈希算法","date":"2021-05-29T06:57:38.000Z","updated":"2021-06-01T15:14:48.402Z","comments":true,"path":"2021/05/29/算法与数据结构/一致性哈希算法/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/29/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95/","excerpt":"一致性哈希算法在1997年由麻省理工学院的Karger等人在解决分布式Cache中提出的，设计目标是为了解决因特网中的热点(Hot spot)问题，初衷和CARP十分类似。一致性哈希修正了CARP使用的简单哈希算法带来的问题，使得DHT可以在P2P环境中真正得到应用。","text":"一致性哈希算法在1997年由麻省理工学院的Karger等人在解决分布式Cache中提出的，设计目标是为了解决因特网中的热点(Hot spot)问题，初衷和CARP十分类似。一致性哈希修正了CARP使用的简单哈希算法带来的问题，使得DHT可以在P2P环境中真正得到应用。 简介常见的负载均衡方法有很多，但是它们的优缺点也都很明显： 随机访问策略：系统随机访问，缺点：可能造成服务器负载压力不均衡，俗话讲就是撑的撑死，饿的饿死。 轮询策略：请求均匀分配，如果服务器有性能差异，则无法实现性能好的服务器能够多承担一部分。 权重轮询策略：权值需要静态配置，无法自动调节，不适合对长连接和命中率有要求的场景。 Hash取模策略：不稳定，如果列表中某台服务器宕机，则会导致路由算法产生变化，由此导致命中率的急剧下降。一致性哈希策略。 以上几个策略，排除本篇介绍的一致性哈希，可能使用最多的就是 Hash取模策略了。Hash取模策略的缺点也是很明显的，这种缺点也许在负载均衡的时候不是很明显，但是在涉及数据访问的主从备份和分库分表中就体现明显了。 一致性Hash性质考虑到分布式系统每个节点都有可能失效，并且新的节点很可能动态的增加进来，如何保证当系统的节点数目发生变化时仍然能够对外提供良好的服务，这是值得考虑的，尤其实在设计分布式缓存系统时，如果某台服务器失效，对于整个系统来说如果不采用合适的算法来保证一致性，那么缓存于系统中的所有数据都可能会失效（即由于系统节点数目变少，客户端在请求某一对象时需要重新计算其hash值（通常与系统中的节点数目有关），由于hash值已经改变，所以很可能找不到保存该对象的服务器节点），因此一致性hash就显得至关重要，良好的分布式cahce系统中的一致性hash算法应该满足以下几个方面： 平衡性(Balance)平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件。 单调性(Monotonicity)单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲区加入到系统中，那么哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲区中去，而不会被映射到旧的缓冲集合中的其他缓冲区。简单的哈希算法往往不能满足单调性的要求，如最简单的线性哈希：x = (ax + b) mod (P)，在上式中，P表示全部缓冲的大小。不难看出，当缓冲大小发生变化时(从P1到P2)，原来所有的哈希结果均会发生变化，从而不满足单调性的要求。哈希结果的变化意味着当缓冲空间发生变化时，所有的映射关系需要在系统内全部更新。而在P2P系统内，缓冲的变化等价于Peer加入或退出系统，这一情况在P2P系统中会频繁发生，因此会带来极大计算和传输负荷。单调性就是要求哈希算法能够应对这种情况。 分散性(Spread)在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。 负载(Load)负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。 平滑性(Smoothness)平滑性是指缓存服务器的数目平滑改变和缓存对象的平滑改变是一致的。 算法思想 首先求出memcached服务器（节点）的哈希值，并将其配置到0～23^2的圆上。 然后采用同样的方法求出存储数据的键的哈希值，并映射到相同的圆上。 然后从数据映射到的位置开始顺时针查找，将数据保存到找到的第一个服务器上。如果超过23^2仍然找不到服务器，就会保存到第一台服务器上。 当我们添加一台机器的时候，只会将一部分节点分配到新加入的节点，如图所示，Node2-Node5之间的数据应该是存储在Node3上，加上新节点Node5之后，只需要将Node2-Node5之间的数据从Node3重分布到Node5就行了，不会全部重分布。 不过这个有一个问题，就是数据分布的不均匀了。 虚拟节点 为了解决节点过少数据分部不均匀的问题，引入了虚拟节点，增加hash分布的均匀性。hash环上不再存在物理节点，而是存虚拟节点，虚拟节点对应对应的物理机器。 总结一致性哈希解决了数据迁移的时候全量重新分布的问题，只用重新分布一部分。还有就是哈希冲突问题还是会有的，但是冲突的概率很低，而且就算冲突了也影响不大。 参考一致性哈希算法","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"https://xmmarlowe.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://xmmarlowe.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"哈希","slug":"哈希","permalink":"https://xmmarlowe.github.io/tags/%E5%93%88%E5%B8%8C/"}],"author":"Marlowe"},{"title":"迪杰斯特拉算法","slug":"算法与数据结构/迪杰斯特拉算法","date":"2021-05-29T06:03:04.000Z","updated":"2021-06-01T15:14:48.425Z","comments":true,"path":"2021/05/29/算法与数据结构/迪杰斯特拉算法/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/29/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E8%BF%AA%E6%9D%B0%E6%96%AF%E7%89%B9%E6%8B%89%E7%AE%97%E6%B3%95/","excerpt":"迪杰斯特拉(Dijkstra)算法是典型最短路径算法，用于计算一个节点到其他节点的最短路径。 它的主要特点是以起始点为中心向外层层扩展(广度优先搜索思想)，直到扩展到终点为止。","text":"迪杰斯特拉(Dijkstra)算法是典型最短路径算法，用于计算一个节点到其他节点的最短路径。 它的主要特点是以起始点为中心向外层层扩展(广度优先搜索思想)，直到扩展到终点为止。 介绍迪杰斯特拉算法是由荷兰计算机科学家狄克斯特拉于1959 年提出的，因此又叫狄克斯特拉算法。是从一个顶点到其余各顶点的最短路径算法，解决的是有权图中最短路径问题。迪杰斯特拉算法主要特点是以起始点为中心向外层层扩展，直到扩展到终点为止。迪杰斯特拉算法采用的是贪心策略，将Graph中的节点集分为最短路径计算完成的节点集S和未计算完成的节点集T，每次将从T中挑选V0-&gt;Vt最小的节点Vt加入S，并更新V0经由Vt到T中剩余节点的更短距离，直到T中的节点全部加入S中，它贪心就贪心在每次都选择一个距离源点最近的节点加入最短路径节点集合。迪杰斯特拉算法只支持非负权图，它计算的是单源最短路径，即单个源点到剩余节点的最短路径，时间复杂度为O(n²)。 算法流程本节将对算法流程进行模拟，设置Graph为包含7个顶点和9条边的有向无环图，源点为0，计算从源点0到剩余节点的最短路径，Graph如下： 每个节点将维护shortest和visited两个数据结构，shortest存储v0到该节点的最短路径，visited存储v0到该节点的最短路径是否求出。S为已求出最短路径的节点，T为未求出最短路径的节点。源节点只允许将S中的节点作为中间节点来计算到达其它节点的最短路径，不允许将T中的节点作为中间节点来计算到达其它节点的最短路径。随着S中节点的增加，源节点可达的节点才会增加。初始状态下，源节点只可达节点1和节点3。 算法步骤如下： 将源节点（即节点0）加入S中，对shortest和visited数组进行更新。 S中现有节点0，源节点可达T中的节点1和节点3，节点0-&gt;节点1距离为6，节点0-&gt;节点3距离为2，按距离从小到大排序，因此选择将节点3加入S中。更新源点将节点3作为中间节点到达其它节点的距离。 S中现有节点0和节点3，源节点可达T中的节点1和4，节点0-&gt;节点1距离为6，节点0-&gt;节点4距离为7，按距离从小到大排序，因此选择将节点1加入S中。更新源点将节点1作为中间节点到达其它节点的距离。 S中现有节点0、1、3，源节点可达T中的节点2、4、5，0-&gt;2距离为11，0-&gt;4距离为7，0-&gt;5距离为9，按距离从小到大排序，因此选择将节点4加入S中。更新源点将节点4作为中间节点到达其它节点的距离。 S中现有节点0、1、3、4，源节点可达T中的节点2、5、6，0-&gt;2距离为11，0-&gt;5距离为9，0-&gt;6距离为8，按距离从小到大排序，因此选择将节点6加入S中。更新源点将节点6作为中间节点到达其它节点的距离。 S中现有节点0、1、3、4、6，源节点可达T中的节点2、5，0-&gt;2距离为11，0-&gt;5距离为9，按距离从小到大排序，因此选择将节点5加入S中。更新源点将节点5作为中间节点到达其它节点的距离。 T中只剩下节点2，0-&gt;2距离为11，将节点2加入S中。 算法结束，源点到其它节点的最短路径都已依次求出。 Java实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public static void dijstra(int[][] matrix, int source) &#123; //最短路径长度 int[] shortest = new int[matrix.length]; //判断该点的最短路径是否求出 int[] visited = new int[matrix.length]; //存储输出路径 String[] path = new String[matrix.length]; //初始化输出路径 for (int i = 0; i &lt; matrix.length; i++) &#123; path[i] = new String(source + &quot;-&gt;&quot; + i); &#125; //初始化源节点 shortest[source] = 0; visited[source] = 1; for (int i = 1; i &lt; matrix.length; i++) &#123; int min = Integer.MAX_VALUE; int index = -1; for (int j = 0; j &lt; matrix.length; j++) &#123; //已经求出最短路径的节点不需要再加入计算并判断加入节点后是否存在更短路径 if (visited[j] == 0 &amp;&amp; matrix[source][j] &lt; min) &#123; min = matrix[source][j]; index = j; &#125; &#125; //更新最短路径 shortest[index] = min; visited[index] = 1; //更新从index跳到其它节点的较短路径 for (int m = 0; m &lt; matrix.length; m++) &#123; if (visited[m] == 0 &amp;&amp; matrix[source][index] + matrix[index][m] &lt; matrix[source][m]) &#123; matrix[source][m] = matrix[source][index] + matrix[index][m]; path[m] = path[index] + &quot;-&gt;&quot; + m; &#125; &#125; &#125; //打印最短路径 for (int i = 0; i &lt; matrix.length; i++) &#123; if (i != source) &#123; if (shortest[i] == MaxValue) &#123; System.out.println(source + &quot;到&quot; + i + &quot;不可达&quot;); &#125; else &#123; System.out.println(source + &quot;到&quot; + i + &quot;的最短路径为：&quot; + path[i] + &quot;，最短距离是：&quot; + shortest[i]); &#125; &#125; &#125;&#125; 参考迪杰斯特拉算法","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"https://xmmarlowe.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Dijkstra","slug":"Dijkstra","permalink":"https://xmmarlowe.github.io/tags/Dijkstra/"}],"author":"Marlowe"},{"title":"邻接表与邻接矩阵","slug":"算法与数据结构/邻接表与邻接矩阵","date":"2021-05-29T05:57:42.000Z","updated":"2021-06-01T15:14:48.428Z","comments":true,"path":"2021/05/29/算法与数据结构/邻接表与邻接矩阵/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/29/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E9%82%BB%E6%8E%A5%E8%A1%A8%E4%B8%8E%E9%82%BB%E6%8E%A5%E7%9F%A9%E9%98%B5/","excerpt":"图的存储结构主要分两种，一种是邻接矩阵，一种是邻接表。","text":"图的存储结构主要分两种，一种是邻接矩阵，一种是邻接表。 邻接矩阵图的邻接矩阵存储方式是用两个数组来表示图。一个一维数组存储图中顶点信息，一个二维数组（邻接矩阵）存储图中的边或弧的信息。设图G有n个顶点，则邻接矩阵是一个n*n的方阵，定义为： 看一个实例，下图左就是一个无向图。 从上面可以看出，无向图的边数组是一个对称矩阵。所谓对称矩阵就是n阶矩阵的元满足aij = aji。即从矩阵的左上角到右下角的主对角线为轴，右上角的元和左下角相对应的元全都是相等的。从上面可以看出，无向图的边数组是一个对称矩阵。所谓对称矩阵就是n阶矩阵的元满足aij = aji。即从矩阵的左上角到右下角的主对角线为轴，右上角的元和左下角相对应的元全都是相等的。从这个矩阵中，很容易知道图中的信息。（1）要判断任意两顶点是否有边无边就很容易了；（2）要知道某个顶点的度，其实就是这个顶点vi在邻接矩阵中第i行或（第i列）的元素之和；（3）求顶点vi的所有邻接点就是将矩阵中第i行元素扫描一遍，arc[i][j]为1就是邻接点；而有向图讲究入度和出度，顶点vi的入度为1，正好是第i列各数之和。顶点vi的出度为2，即第i行的各数之和。 若图G是网图，有n个顶点，则邻接矩阵是一个n*n的方阵，定义为： 邻接表邻接矩阵是不错的一种图存储结构，但是，对于边数相对顶点较少的图，这种结构存在对存储空间的极大浪费。因此，找到一种数组与链表相结合的存储方法称为邻接表。邻接表的处理方法是这样的：（1）图中顶点用一个一维数组存储，当然，顶点也可以用单链表来存储，不过，数组可以较容易的读取顶点的信息，更加方便。（2）图中每个顶点vi的所有邻接点构成一个线性表，由于邻接点的个数不定，所以，用单链表存储，无向图称为顶点vi的边表，有向图则称为顶点vi作为弧尾的出边表。例如，下图就是一个无向图的邻接表的结构。 从图中可以看出，顶点表的各个结点由data和firstedge两个域表示，data是数据域，存储顶点的信息，firstedge是指针域，指向边表的第一个结点，即此顶点的第一个邻接点。边表结点由adjvex和next两个域组成。adjvex是邻接点域，存储某顶点的邻接点在顶点表中的下标，next则存储指向边表中下一个结点的指针。对于带权值的网图，可以在边表结点定义中再增加一个weight的数据域，存储权值信息即可。如下图所示。 两者区别对于一个具有n个顶点e条边的无向图它的邻接表表示有n个顶点表结点2e个边表结点对于一个具有n个顶点e条边的有向图它的邻接表表示有n个顶点表结点e个边表结点 如果图中边的数目远远小于n2称作稀疏图，这是用邻接表表示比用邻接矩阵表示节省空间 如果图中边的数目接近于n2,对于无向图接近于n*(n-1)称作稠密图,考虑到邻接表中要附加链域，采用邻接矩阵表示法为宜。 参考图的邻接矩阵和邻接表的比较","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"https://xmmarlowe.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"邻接表","slug":"邻接表","permalink":"https://xmmarlowe.github.io/tags/%E9%82%BB%E6%8E%A5%E8%A1%A8/"},{"name":"领接矩阵","slug":"领接矩阵","permalink":"https://xmmarlowe.github.io/tags/%E9%A2%86%E6%8E%A5%E7%9F%A9%E9%98%B5/"}],"author":"Marlowe"},{"title":"树和图的区别","slug":"算法与数据结构/树和图的区别","date":"2021-05-29T05:26:16.000Z","updated":"2021-06-01T15:14:48.422Z","comments":true,"path":"2021/05/29/算法与数据结构/树和图的区别/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/29/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%A0%91%E5%92%8C%E5%9B%BE%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"树是一种“层次”关系，图是“网络”关系。","text":"树是一种“层次”关系，图是“网络”关系。 树和图的区别 树 图 树是一种特殊的图，它永远不会有多个路径。从A到B总是有一种方法。 图是一种具有多种方法来从任何点A到达任何其他点B的系统。 必须连接树。 可能未连接图形 由于它已连接，所以我们可以从一个特定结点到达所有其他结点。这种搜索称为遍历。 遍历可能不适用于图形。因为图形可能未连接。 树不包含回路，电路。 图可能包含自循环，循环。 树中必须有一个根节点。 图中没有这种根节点。 我们在树上遍历。这意味着从某个角度出发，我们进入树的每个节点。 我们在图上进行搜索。这意味着从任何节点开始尝试找到我们需要的特定结点。 树可以进行前序，中序，后序遍历。 图可以进行深度优先遍历和广度优先遍历。 树是有向无环图。 图是循环的或非循环的。 树是一个分层的模型结构。 图是网络模型。 所有的树都是图。 但是所有的图都不是树。 根据不同的属性，树可以分为二叉树，二叉搜索树，AVL树，堆。 图可以分为有向图和无向图。 如果树有n个顶点，则它必须仅有n-1条边。 在图中，变得数量不取决与顶点的数量。 与图形相比，复杂度更低。 由于循环，比树要复杂。 树的数据结构树，和图一样也是一系列点的集合。有一个根节点。这个根节点有一些子节点。子节点也有它们自己的孙子节点。不断重复直到所有的数据都被用树的数据结构表示。下面的图表示了一个树的数据结构 有向无环的图–树——-》并查集 树是没有环的图 图的数据结构图从数学领域进化而来，主要被用来描述一个从一个位置到另一个位置的路线的模型。一个图包含一系列的点和一系列的边。边用来把点连接起来。路线是用来描述共用一条边的点的轨迹的术语。这个图表示了一个三个点和三个边的图。 图最经典的实现是找两个点之间的路径。找到从一个点到另一个点到最短路径，和找访问所有节点的最短路径。 参考树和图的区别、联系（树是有向无环的特殊的图）","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"https://xmmarlowe.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"图","slug":"图","permalink":"https://xmmarlowe.github.io/tags/%E5%9B%BE/"},{"name":"树","slug":"树","permalink":"https://xmmarlowe.github.io/tags/%E6%A0%91/"}],"author":"Marlowe"},{"title":"Spring和SpringBoot常用注解","slug":"Spring/Spring和SpringBoot常用注解","date":"2021-05-28T05:34:35.000Z","updated":"2021-06-01T15:14:48.395Z","comments":true,"path":"2021/05/28/Spring/Spring和SpringBoot常用注解/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/28/Spring/Spring%E5%92%8CSpringBoot%E5%B8%B8%E7%94%A8%E6%B3%A8%E8%A7%A3/","excerpt":"总结Spring和SpringBoot常用注解…","text":"总结Spring和SpringBoot常用注解… Spring常用注解1、用于注册bean对象注解@Component作用： 调用无参构造创建一个bean对象，并把对象存入Spring的IoC容器，交由Spring容器进行管理。相当于在xml中配置一个Bean。 属性： value：指定Bean的id。如果不指定value属性，默认Bean的id是当前类的类名，首字母小写。 子注解： 以下三个注解是从@Component派生出来的，它们的作用与@Component是一样的，Spring增加这三个注解是为了在语义行区分MVC三层架构对象。 @Component：用于注册非表现层、业务层、持久层的对@Controller：用于注册表现层对象@Service：用于注册业务层对象@Reposity：用于注册持久层对象 @Bean作用： 用于把当前方法的返回值（对象）作为Bean对象存入Spring的IoC容器中（注册Bean） 属性： name/value：用于指定Bean的id。当不写时，默认值是当前方法的名称。注意：当我们使用注解配置方法时，如果方法有参数，Spring框架会去容器中查找有没有可用的Bean对象，查找的方式和Autowired注解的方式是一样的。 案例： 123456789101112131415161718192021222324252627282930@Configurationpublic class JdbcConfig &#123; /** * 用于创建QueryRunner对象 * @param dataSource * @return */ @Bean(value = &quot;queryRunner&quot;) public QueryRunner createQueryRunner(DataSource dataSource) &#123; return new QueryRunner(dataSource); &#125; /** * 创建数据源对象 * @return */ @Bean(value = &quot;dataSource&quot;) public DataSource createDataSource() &#123; try &#123; ComboPooledDataSource comboPooledDataSource = new ComboPooledDataSource(); comboPooledDataSource.setDriverClass(&quot;com.mysql.jdbc.Driver&quot;); comboPooledDataSource.setJdbcUrl(&quot;jdbc:mysql://localhost:3306/mydatabase&quot;); comboPooledDataSource.setUser(&quot;root&quot;); comboPooledDataSource.setPassword(&quot;root&quot;); return comboPooledDataSource; &#125;catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; 2、用于依赖注入的注解@Autowired作用： @Autowire和@Resource都是Spring支持的注解形式动态装配Bean的方式。Autowire默认按照类型(byType)装配，如果想要按照名称(byName)装配，需结合@Qualifier注解使用。 属性： required：@Autowire注解默认情况下要求依赖对象必须存在。如果不存在，则在注入的时候会抛出异常。如果允许依赖对象为null，需设置required属性为false。 案例： 1234@Autowired /* Autowire默认按照类型(byType)装配 */// @Autowired(required = false) /* @Autowire注解默认情况下要求依赖对象必须存在。如果不存在，则在注入的时候会抛出异常。如果允许依赖对象为null，需设置required属性为false。 */// @Qualifier(&quot;userService&quot;) /* 按照名称(byName)装配 */private UserService userService; @Qualifier作用： @Autowired是按照名称(byName)装配，使用了@Qualifier注解之后就变成了按照名称(byName)装配。它在给字段注入时不能独立使用，必须和@Autowire一起使用；但是给方法参数注入时，可以独立使用。 属性： value：用于指定要注入的bean的id，其中，该属性可以省略不写。 案例： 1234@Autowire@Qualifier(value=&quot;userService&quot;) //@Qualifier(&quot;userService&quot;) //value属性可以省略不写private UserService userService; @Resource作用： @Autowire和@Resource都是Spring支持的注解形式动态装配bean的方式。@Resource默认按照名称(byName)装配，名称可以通过name属性指定。如果没有指定name，则注解在字段上时，默认取（name=字段名称）装配。如果注解在setter方法上时，默认取（name=属性名称）装配。 属性： name：用于指定要注入的bean的idtype：用于指定要注入的bean的type 装配顺序: 如果同时指定name和type属性，则找到唯一匹配的bean装配，未找到则抛异常； 如果指定name属性，则按照名称(byName)装配，未找到则抛异常； 如果指定type属性，则按照类型(byType)装配，未找到或者找到多个则抛异常； 既未指定name属性，又未指定type属性，则按照名称(byName)装配；如果未找到，则按照类型(byType)装配。 案例： 1234@Resource(name=&quot;userService&quot;)//@Resource(type=&quot;userService&quot;)//@Resource(name=&quot;userService&quot;, type=&quot;UserService&quot;)private UserService userService; @Value作用： 通过@Value可以将外部的值动态注入到Bean中，可以为基本类型数据和String类型数据的变量注入数据 案例： 1234567891011121314151617181920212223242526// 1.基本类型数据和String类型数据的变量注入数据@Value(&quot;tom&quot;) private String name;@Value(&quot;18&quot;) private Integer age;// 2.从properties配置文件中获取数据并设置到成员变量中// 2.1jdbcConfig.properties配置文件定义如下jdbc.driver \\= com.mysql.jdbc.Driver jdbc.url \\= jdbc:mysql://localhost:3306/eesy jdbc.username \\= root jdbc.password \\= root// 2.2获取数据如下@Value(&quot;$&#123;jdbc.driver&#125;&quot;) private String driver;@Value(&quot;$&#123;jdbc.url&#125;&quot;) private String url; @Value(&quot;$&#123;jdbc.username&#125;&quot;) private String username; @Value(&quot;$&#123;jdbc.password&#125;&quot;) private String password; 3、生命周期相关的注解@PostConstruct作用： 指定初始化方法 案例： 1234@PostConstruct public void init() &#123; System.out.println(&quot;初始化方法执行&quot;); &#125; @PreDestroy作用： 指定销毁方法 案例： 1234@PreDestroy public void destroy() &#123; System.out.println(&quot;销毁方法执行&quot;); &#125; SpringBoot常用注解springboot中的常用注解有：@SpringBootApplication、@Repository、@Service、@RestController、@ResponseBody、@Component、@ComponentScan等等。 1、@SpringBootApplication这个注解是Spring Boot最核心的注解，用在 Spring Boot的主类上，标识这是一个 Spring Boot 应用，用来开启 Spring Boot 的各项能力。实际上这个注解是@Configuration,@EnableAutoConfiguration,@ComponentScan三个注解的组合。由于这些注解一般都是一起使用，所以Spring Boot提供了一个统一的注解@SpringBootApplication。 2、@EnableAutoConfiguration允许 Spring Boot 自动配置注解，开启这个注解之后，Spring Boot 就能根据当前类路径下的包或者类来配置 Spring Bean。 如：当前类路径下有 Mybatis 这个 JAR 包，MybatisAutoConfiguration 注解就能根据相关参数来配置 Mybatis 的各个 Spring Bean。 @EnableAutoConfiguration实现的关键在于引入了AutoConfigurationImportSelector，其核心逻辑为selectImports方法，逻辑大致如下： 从配置文件META-INF/spring.factories加载所有可能用到的自动配置类； 去重，并将exclude和excludeName属性携带的类排除； 过滤，将满足条件（@Conditional）的自动配置类返回； 3、@Configuration用于定义配置类，指出该类是 Bean 配置的信息源，相当于传统的xml配置文件，一般加在主类上。如果有些第三方库需要用到xml文件，建议仍然通过@Configuration类作为项目的配置主类——可以使用@ImportResource注解加载xml配置文件。 4、@ComponentScan组件扫描。让spring Boot扫描到Configuration类并把它加入到程序上下文。 @ComponentScan注解默认就会装配标识了@Controller，@Service，@Repository，@Component注解的类到spring容器中。 5、@Repository用于标注数据访问组件，即DAO组件。 使用@Repository注解可以确保DAO或者repositories提供异常转译，这个注解修饰的DAO或者repositories类会被ComponetScan发现并配置，同时也不需要为它们提供XML配置项。 6、@Service一般用于修饰service层的组件 7、@RestController用于标注控制层组件(如struts中的action)，表示这是个控制器bean,并且是将函数的返回值直 接填入HTTP响应体中,是REST风格的控制器；它是@Controller和@ResponseBody的合集。 8、@ResponseBody表示该方法的返回结果直接写入HTTP response body中 一般在异步获取数据时使用，在使用@RequestMapping后，返回值通常解析为跳转路径，加上@responsebody后返回结果不会被解析为跳转路径，而是直接写入HTTP response body中。比如异步获取json数据，加上@responsebody后，会直接返回json数据。 9、@Component泛指组件，当组件不好归类的时候，我们可以使用这个注解进行标注。 10、@Bean相当于XML中的&lt;bean&gt;&lt;/bean&gt;,放在方法的上面，而不是类，意思是产生一个bean,并交给spring管理。 11、@AutoWiredbyType方式。把配置好的Bean拿来用，完成属性、方法的组装，它可以对类成员变量、方法及构造函数进行标注，完成自动装配的工作。 当加上（required=false）时，就算找不到bean也不报错。 12、@Qualifier当有多个同一类型的Bean时，可以用@Qualifier(“name”)来指定。与@Autowired配合使用 13、@Resource(name=”name”,type=”type”)没有括号内内容的话，默认byName。与@Autowired干类似的事。 14、@RequestMappingRequestMapping是一个用来处理请求地址映射的注解；提供路由信息，负责URL到Controller中的具体函数的映射，可用于类或方法上。用于类上，表示类中的所有响应请求的方法都是以该地址作为父路径。 15、@RequestParam用在方法的参数前面。例： @RequestParam String a =request.getParameter(&quot;a&quot;) 路径变量。参数与大括号里的名字一样要相同。例： 16、@PathVariable1234567RequestMapping(&quot;user/get/mac/&#123;macAddress&#125;&quot;)public String getByMacAddress(@PathVariable String macAddress)&#123; //do something;&#125; Spring Profiles提供了一种隔离应用程序配置的方式，并让这些配置只能在特定的环境下生效。 17、@Profiles任何@Component或@Configuration都能被@Profile标记，从而限制加载它的时机。 123456789@Configuration@Profile(&quot;prod&quot;)public class ProductionConfiguration &#123; // ...&#125; Spring Boot可使用注解的方式将自定义的properties文件映射到实体bean中，比如config.properties文件。 18、@ConfigurationProperties1234567891011121314151617181920212223242526272829@Data@ConfigurationProperties(&quot;rocketmq.consumer&quot;)public class RocketMQConsumerProperties extends RocketMQProperties &#123; private boolean enabled = true; private String consumerGroup; private MessageModel messageModel = MessageModel.CLUSTERING; private ConsumeFromWhere consumeFromWhere = ConsumeFromWhere.CONSUME_FROM_LAST_OFFSET; private int consumeThreadMin = 20; private int consumeThreadMax = 64; private int consumeConcurrentlyMaxSpan = 2000; private int pullThresholdForQueue = 1000; private int pullInterval = 0; private int consumeMessageBatchMaxSize = 1; private int pullBatchSize = 32;&#125;","categories":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/categories/Spring/"}],"tags":[{"name":"注解","slug":"注解","permalink":"https://xmmarlowe.github.io/tags/%E6%B3%A8%E8%A7%A3/"}],"author":"Marlowe"},{"title":"IPv4地址","slug":"计算机网络/IPv4地址","date":"2021-05-25T13:06:27.000Z","updated":"2021-05-28T05:24:06.889Z","comments":true,"path":"2021/05/25/计算机网络/IPv4地址/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/25/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IPv4%E5%9C%B0%E5%9D%80/","excerpt":"OSI/RM 的网络层和 TCP/IP 协议体系结果的网际互连层最重要的一个协议就是 IP 协议，目前正处于 IPv4 和 IPv6 这两个版本的交替、过渡时间。这篇博客主要介绍 IPv4 地址。","text":"OSI/RM 的网络层和 TCP/IP 协议体系结果的网际互连层最重要的一个协议就是 IP 协议，目前正处于 IPv4 和 IPv6 这两个版本的交替、过渡时间。这篇博客主要介绍 IPv4 地址。 IPv4 地址基本格式IPv4 使用 32 位（4 字节）地址，因此整个地址空间有 4 294 967 296（2^32^）个地址，也就是近43亿个地址。不过，有一些地址是特殊用途而保留的，如局域网专用地址（约1800万个地址）和组播地址（约2700万个地址），这样一来可直接在广域网上使用、路由的公网 IP 地址数量就更加少了。 说明：公网 IP 地址是指可以子啊广域网上直接使用的，直接被路由（也就是可以被指路径查到），并需向 IP 地址管理机构申请、注册、购买，且全球唯一（不存在多个用户拥有、使用相同的公网 IP 地址的情况）的 IPv4 地址。打个比方，公网 IP 地址就像公民的身份证号码，每个身份证号都是全国唯一的，并且通过这个号码可以查到我们的基本信息，找到我们。公网 IP 地址直接分配给互联网上的主机、服务器或其他设备，可以通过它在全球范围内找到对应的主机、服务器和设备。如各大企业网站通常都是直接使用公网 IP 地址的。 与公网 IP 地址相对应的自然是私网 IP 地址，又称为专用网络 IP 地址或者局域网 IP 地址。私网 IP 地址是指仅可以在各用户自己的局域网内部使用，且不同用户可以重复使用，无须向 IP 地址管理机构申请、注册，也无须购买的 IPv4 地址。私网 IP 地址就相当于我们企业内部的员工编号，仅在内部使用，不能通过这个员工编号来在全国范围内找到我们。企业内部局域网使用的就是私网 IP 地址，具体有哪些地址属于私网 IP 地址我们将会在后面进行详细的介绍。 随着公网地址不断被分配给最终的用户，IPv4 地址枯竭问题也随之产生。虽然基于可变的子网掩码（VLSM）、无类别域间路由（CIDR）和网络地址转换（NAT）的地址结构重构显著地减少了地址的枯竭的速度，但在 2011 年 2 月 3 日，在最后 5 个地址块被分配给 5 个区域互联网注册管理机构之后，IANA 的主要地址池空了，所以现在正在积极推动 IPv6，我们将会在下一篇博客讲解 IPv6 地址。 IPv4 地址在计算机内部是以二进制形式表示的，每个地址都有 32 位，由数字 0 和 1 构成。在这 32 位的二进制数中，其实每个 8 位之间并没有我们所看到的那个用来分隔各段的一个小圆点，只是为了方便我们自己阅读，在每个字节之间用一个小圆点分隔。因为整个 IP 地址有 32 位，无论是书写还是记忆都很不方便，于是我们在日常的 IP 地址管理中把这个 32 位长的二进制 IP 地址分段转换成对应的十进制，在每个字节间用小圆点分隔。引用某个 IPv4地址时，可使用 W.X.Y.Z 的点分十进制表示形式，如 192.168.1.10 等。 由前面介绍的数据转换内容可以知道，每个 8 位二进制所能表示的最大数就是 2^8^ −1=256−1=255 （最小数为0），所以 IPv4地址转换成十进制数后，每段8位二进制组的取值范围是 0~255。因为 IP 地址在计算机是以二进制表示的，32位就相当于 4 字节，所以在 IPv4 协议数据报格式，无论是源 IP 字段，还是目的 IP 地址字段都占 4 字节。 子网掩码我们为设备配置 IP 地址时，通常是不能仅配置 IPv4 地址，而必须同时配置所谓的子网掩码，如下图所示。那么子网掩码是什么？它有什么用呢？ 要想理解什么是子网掩码，就不能不先了解 IPv4 地址的构成。互联网是由很多小型网络构成的，每个网络上有很多主机，这样便构成了一个有层次的结构。IPv4 地址在设计时就考虑到地址分配的层次特点，将每个 IP 地址都分割成网络 ID 和主机 ID 两部分，以便于 IPv4 地址的寻址操作。那么 IPv4 地址的网络 ID 和主机 ID 各是多少位呢？如果不指定，在寻址时就不知道对应 IPv4 地址中哪些位代表网络 ID、哪些位代表主机 ID，这就需要通过这里所说的子网掩码来实现了。 与二进制 IPv4 相同，子网掩码也有 1 和 0 组成，且长度也是 32 位，我们也可以把它分成网络 ID 和主机 ID 两部分，且各自长度与 IPv4 地址的网络 ID 和主机 ID 部分对应相等。但子网掩码的网络 ID 部分全是 1，1 的数目等于网络 ID 的长度；主机 ID 部分全是 0 表示，0 的数目等于主机 ID 的长度。下图所示是一个网络长度为 20 的子网掩码。这样做的目的是为了在寻址过程中使子网掩码与对应的 IPv4 地址做逻辑与运算时用 0 遮住 IPv4 地址中原主机 ID 部分（因为 0 与任何数相与的结果都是 0 ），而不改变原网络 ID 部分（因为 1 与任何数相与都不改变原来的值），这样就一来就可以很容易确定对应目的 IPv4 地址所在的网络了，确定了网络，也就确定了主机，因为在 IPv4 地址中除了网络 ID 部分就是主机 ID 部分。子网掩码不是一个地址，但是可以确定一个 IPv4地址中的哪一部分是网络 ID，哪一部分是主机 ID，连续 1 的部分就代表网络 ID，连续 0 的部分就代表主机 ID。子网掩码的作用就是获取主机通信不同情况，选择不同路由，子网掩码一旦设置，对应 IPv4地址中的网络 ID 和主机 ID 部分就固定了。 与 IPv4地址一样，子网掩码也可以转换成点分十进制形式。根据子网掩码格式可以发现，子网掩码有 0.0.0.0；255.0.0.0；255.255.0.0；255.255.255.0；255.255.255.255 五种，其中 0.0.0.0 代表任意网路的掩码，如我们在设置默认路由时，不仅 IP 地址为 0.0.0.0，子网掩码也为 0.0.0.0；A 类地址的默认子网掩码为 255.0.0.0；B 类地址的默认子网掩码为 255.255.0.0；C 类地址的默认子网掩码为 255.255.255.0；而 255.255.255.255 可以看作是单一主机网络，代表这个网络就这一个 IPv4 地址，在配置 ACL（访问控制列表）时，如果控制的是一台主机，则对应的子网掩码也为 255.255.255.255。有关 A、B、C 类地址的分类将在下面介绍。 IPv4 地址的基本分类IPv4 地址共有 2^32^ 个，最初把一个地址分成两部分：“网络识别码”在地址的最高的字节当中，”主机识别码“在剩下的部分中。这样划分的话，就使得最多只能分配给 256 个网络，显然这样是远远不够的。 为了克服这个限制，在随后出现的分类网络中，地址的最高位字节被重新定义为网路的类别（即网络 ID），共 5 个：A、B、C、D 和 E。A、B 和 C 类用于单播通信中设备 IP 地址分配；D 类属于组播地址，用于组播通信；E类是保留地址。他们均有不同的网络类别（也就是网络 ID） 长度，剩余部分用来识别网络内的主机（称为主机 ID）。网络 ID 用来确定每类网络中有的网络数，而主机 ID 用来确定每个网络中的 IP 地址数。下面分别介绍这五类地址的结构。 A 类 IPv4 地址A 类 IPv4 地址结构如下图所示，其中网络 ID 占用最高一个字节，也就是第一个二进制 8 位组，而主机 ID 则占用剩余三个字节，也就是后面的三个二进制 8 位组（一共 24 位）。 在分类中规定，A 类 IPv4 地址中网络 ID 的最高位固定为 0，后面 7 位可变。这样一来，A 类网络的总数从 256（2^8^）个减少到 128（2^7^)个。但实际可以使用的只有 126 个，即整个 IPv4 地址中可构建 126 个 A 类网络，因为网络 ID 为 0 和 127 的 A 类网络不可用的。网络 ID 全为 0 的地址为保留地址，不能被分配；而网络 ID 为 01111111（相当于十进制的 127）的地址专用本地环路测试（也就是通常所说的环路地址），也是不能分配的。也就是以 0 或者 127 开头的地址是不能分配给节点使用的。 又因为 A 类 IPv4 地址中主机 ID 又24 位，所有可以使用的主机 ID 数，也就是可以每个 A 类网络中拥有的 IPv4 地址数为 166 777 216（2^24^）。但主机 ID 全为 0 的地址为网路地址，而主机全为 1 的地址为广播地址，不能分配给主机使用，所以实际上可用的地址数为 166 777 214（2^24^−2）。A 类网络中可以构建的网络数最少，但每个网络中拥有的地址数是最多的，也就是可以构建的网络规模最大，适用于大型企业和运营商。 A 类 IPv4 地址的子网掩码固定为 255.0.0.0，因为子掩码就是网络 ID 部分全为 1，主机 ID 部分全为 0，而 A 类地址中网络 ID 部分就是最高的那个字节。 B 类 IPv4 地址B 类 IPv4 地址结构如下图所示，其网络 ID 占用最高的前两个字节，也就是第一个和第二个二进制 8 位组，而主机 ID 则占用剩余的两个字节，也就是后面两个二进制 8 位组。 B 类 IPv4 地址的网络 ID 的最高两位固定分别为 1、0，后面 14 位可变。由此可知 B 类网络的总数从 65536（2^16^）减少到 16384（2^14^）个；B 类 IPv4 地址中主机 ID 为 16 位，所以可用的主机数，也就是每个 B 类网络拥有的 IPv4 地址数为 65536（2^16^）个。同样因为主机 ID 全为 0 的地址是网络地址，而主机 ID 全为 1 的地址为广播地址，不能分配给主机使用，所以实际上可以使用的地址数为 65534 个。 B 类 IPv4 地址的子网掩码为固定的255.255.0.0，因为 B 类地址中网络 ID 部分是最高的两字节，每个字节均为 8 个连续的 1，转换成十进制后每个字节就是 255 了。 C 类 IPv4 地址C 类 IPv4 地址结构如下图所示，其网络 ID 占用最高的前三个字节，也就是第一个、第二个和第三个二进制 8 位组，而主机 ID 只占用最后的一个字节，也就是只有最后一个二进制 8 位组。 C 类 IPv4 地址的网络ID的最高三位固定分别为 1、1、0，后面的 21 位可变。由此得知 C 类网络总数从 166 777 216（2^24^）减少到 2 097 152（2^22^）个。C 类地址中主机 ID 仅为 8 位，所以可用的主机 ID 数，也就是每个 C 类网络拥有的 IPv4 地址数为 256（2^8^）个。同样因为主机 ID 全为 0的地址为网络地址，而主机 ID 全为1的地址为广播地址，不能分配给主机使用，所以实际上可用的地址数为 254（2^8^−2）。 C 类单播地址的子网掩码为固定的255.255.255.0，因为 C 类地址中网络 ID 部分是最高的前 3 个字节，每个字节均为 8 个连续的 1，转换成十进制后每个字节就是 255 了。 下表总结了A、B 和 C三类 IPv4 地址的主要特征 类别| w 的值| 网络 ID 部分| 主机 ID 部分| 网络 ID 数| 每个网络的主机 ID 数||:—:|:—:|:—:|:—:|:—:|:—:|A| 1-126| w| x.y.z| 126| 16 777 214|B| 128-191| w.x| y.z| 16 384| 65 534|C| 192-223| w.x.y| z| 2 097 152| 254| D 类 IPv4 地址D 类 IPv4 地址是组播地址，用于 IPv4 组播通信中。通过组播 IPv4 地址，组播时源主机（组播源）只需发送一份数据，就可以使对应组播组（组播组使用 D IPv4 地址标识）中的一个主机或者多个主机收到这份数据的副本的通信方式，但只有组播组内的主机可以接收到该数据。 IP 组播技术有效地解决了单点发送多点接受的问题，实现了 IP 网络中点到多点的高效数据传输，能够大量节约网络带宽、降低网络负载。还可以利用网络的组播特性方便地提供一些新的增值服务，包括在线直播、网络电视、远程教育、远程医疗、网络电台、实时视频会议等互联网的信息服务领域。 D 类 IPv4 地址结构如下图所示，规定在最高字节中前四位分别固定为 1、1、1、0，组播地址范围为 224.0.0.0 ~ 239.255.255.255。 整个组播 IPv4 地址根据不同的应用环境和用途又可以分为预留组播地址、公用组播地址、临时组播地址、本地管理组播地址四大类。 1）预留组播地址预留组播地址（又称永久组播地址）就是由 IANA 保留不分配给特定用户使用，仅为公用的组播路由协议分配使用的组播地址，地址范围为 224.0.0.0 ~ 224.0.0.255。使用这些预留组播地址的组播协议包括 IGMP（Internet 组管理协议）、CGMP（Cisco 组管理协议）、IGMP Snooping（IGMP 侦听）和 PIM（协议无关组播）等。使用这段组播地址的 IP 包不被路由器转发。 在这个地址组段中，224.0.0.0 不分配；224.0.0.1 分配给本地组播网络所有支持组播的主机；224.0.0.2 分配给本地组播网络中的所有组播路由器；224.0.0.4 分配给本地组播网络中的所有 SVMRP 路由器；224.0.0.5 分配给本地组播网络中的所有 OSPF 路由器；224.0.0.6 分配给本地组播网络中的所有 OSPF 指定路由器（DR）；224.0.0.9 分配给本地组播网络中的所有 RIPv2 路由器；224.0.0.10 分配给组播网络中所有 IGRP 路由器；224.0.0.13 分配给本地组播网络中的所有 PIMv2 路由器；224.0.0.22 分配给本地组播网络中的所有 IGMPv3 路由器。 2）公用组播地址公用组播地址就是在全球范围内可以直接在互联网上使用的组播地址，就像前面介绍的公网单播 IPv4 地址一样。公用组播地址范围为 224.0.1.0 ~ 224.0.1.255，也是有 IANA 为提出申请并付费的用户分配。 3）临时组播地址临时组播地址就是由企业用户在本企业局域网内部使用的组播地址，地址范围为 224.0.2.0 ~ 238.255.255.255，仅在本地局域网有效，就像前面介绍的局域网 IPv4 地址一样。 4）本地管理组播地址本地管理组播地址也是保留使用的，专用于局域网内部测试，地址范围为 239.0.0.0~238.255.255.255，仅在特定的本地网络范围有效。 当网络层收到组播报文时，根据组播目的查找组播转发表，对报文进行转发。在私网中，组播时不需要再工作站配置的，只需要在网络中的路由器或者支持组播协议的三层交换机上进行配置。私网工作站被分配的组播地址都是 224.0.0.1，就像环路地址 127.0.0.1 一样，无需另外配置。只要在路由器中启用了组播协议后就可以对加入到组播组中。公网中，工作站组播地址选择 224.0.1.0 ~ 238.255.255.255 范围中的一个就可以了。 另外，要注意的是，在进行组播通信时，在数据链路层目的 MAC 地址封装的也是组播 MAC 地址。IANA 把 0.：00：5E 开头的以太网 MAC 块作为组播地址对应的二层组播 MAC 地址。组播 MAC 地址的范围是 01:00:5E:00:00:00 ~ 01:00:5E:7F:FF:FF（前 24 位为 MAC 头，固定不变，第 25 位为 0），并要求将 IPv4 组播地址的后 28 位（因为最高的 4 位是固定不变的）映射到 48 位的 MAC 地址空间中。 具体的映射方法是将组播 IPv4 地址中的低 23 位放入 MAC 地址的低 23 位，如下图所示。至于为什么要映射后面的23位，原因在于根据 IANA 给出组播 MAC 地址段是前 3 字节（也就是 24 位）来标识单位或者厂商，只有后面的 24 位来和 IP 地址映射；而给定的地址空间后 3 字节的最高位相同，都为0，那么给定的 MAC 地址段内只有 23 位了，所以最终只能丢弃 28 位 IPv4 地址中的 5 位，剩下的 23 位和 MAC 的 23 位相映射。注意，这个映射无须手动进行，在路由器启动组播协议，站点加入到组播后就会自动生成。 由于 IPv4 多播地址的后 28 位中只有 23 位被映射到 MAC 地址，这样会有 32 个（2^5^，IPv4 多播地址中有 5 位可变）IP 多播地址映射到同一 MAC 地址上。 E 类 IPv4 地址E 类地址输入 IANA 保留地址，不分配给用户使用，地址段范围为 240.0.0.0 ~ 247.255.255.255，其特征是最高 5 位分别是 1、1、1、1、0，如下图所示，也就是有 27 位是可变的。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"IPv4","slug":"IPv4","permalink":"https://xmmarlowe.github.io/tags/IPv4/"}],"author":"Marlowe"},{"title":"数据库三大范式","slug":"数据库/数据库三大范式","date":"2021-05-25T09:52:14.000Z","updated":"2021-05-28T05:10:29.853Z","comments":true,"path":"2021/05/25/数据库/数据库三大范式/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/25/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%89%E5%A4%A7%E8%8C%83%E5%BC%8F/","excerpt":"","text":"三大范式简介 第一范式：第一范式（1NF）是指数据库表的每一列都是不可分割的基本数据项 第二范式：第二范式（2NF）是在第一范式（1NF）的基础上建立起来的，即满足第二范式（2NF）必须先满足第一范式（1NF）。第二范式要求每一行都要有唯一标识存在，这个唯一属性列被称为主关键字或主键、主码。实体的属性完全依赖于主关键字。 第三范式：第三范式是在第二范式的基础上建立起来的。第三范式指：属性不依赖于其他非主属性。 举例说明现有一张表： 存在的问题： 存在非常严重的冗余，姓名，系名，班主任这三列大大重复。 添加数据存在问题，要想单独添加系名和系主任，则无法办到 数据删除存在问题，删除一名同学对应的系名和系主任也没有了，数据依赖太强 解决办法使用三大范式进行数据表的拆分 使用第一范式，将系列拆分为系名和系主任两列，结果如下图： 使用第二范式，将表拆分为每列属性可依赖于首列的两张表，如图所示： 使用第三范式，继续进行拆分，将表分为仅依赖于首列主属性的表 这样就解决了上述存在的三个问题，表也拆分完毕。 简单来说以下三大范式：第一范式就是说每个列就是是一个不可拆分的词语，若还可以再细分，便不符合第一范式。第二范式就是告诉人们在建表的时候要确立主键，方便查找。第三范式就是说如果一张表可以拆分要继续拆分。 参考数据库设计的三大范式通俗讲解","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"author":"Marlowe"},{"title":"Java数组拷贝的几种方式","slug":"Java/Java数组拷贝的几种方式","date":"2021-05-23T08:48:25.000Z","updated":"2021-05-24T03:48:55.665Z","comments":true,"path":"2021/05/23/Java/Java数组拷贝的几种方式/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/23/Java/Java%E6%95%B0%E7%BB%84%E6%8B%B7%E8%B4%9D%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F/","excerpt":"目前在Java中数据拷贝提供了如下方式：1、clone 2、System.arraycopy 3、Arrays.copyOf 4、Arrays.copyOfRange。","text":"目前在Java中数据拷贝提供了如下方式：1、clone 2、System.arraycopy 3、Arrays.copyOf 4、Arrays.copyOfRange。 clone 方法clone方法是从Object类继承过来的，基本数据类型（int ，boolean，char，byte，short，float ，double，long）都可以直接使用clone方法进行克隆，注意String类型是因为其值不可变所以才可以使用。 int 类型示例123456int[] a1 = &#123;1, 3&#125;;int[] a2 = a1.clone();a1[0] = 666;System.out.println(Arrays.toString(a1)); //[666, 3]System.out.println(Arrays.toString(a2)); //[1, 3] String类型示例123456String[] a1 = &#123;&quot;a1&quot;, &quot;a2&quot;&#125;;String[] a2 = a1.clone();a1[0] = &quot;b1&quot;; //更改a1数组中元素的值System.out.println(Arrays.toString(a1)); //[b1, a2]System.out.println(Arrays.toString(a2)); //[a1, a2] System.arraycopy1System.arraycopy方法是一个本地的方法，源码里定义如下： 其参数含义为： 1（原数组， 原数组的开始位置， 目标数组， 目标数组的开始位置， 拷贝个数） 用法示例123456int[] a1 = &#123;1, 2, 3, 4, 5&#125;;int[] a2 = new int[10];System.arraycopy(a1, 1, a2, 3, 3);System.out.println(Arrays.toString(a1)); // [1, 2, 3, 4, 5]System.out.println(Arrays.toString(a2)); // [0, 0, 0, 2, 3, 4, 0, 0, 0, 0] 当使用这个方法的时候，需要复制到一个已经分配内存单元的数组。 Arrays.copyOfArrays.copyOf底层其实也是用的System.arraycopy 源码如下： 123456789public static &lt;T,U&gt; T[] copyOf(U[] original, int newLength, Class&lt;? extends T[]&gt; newType) &#123; @SuppressWarnings(&quot;unchecked&quot;) T[] copy = ((Object)newType == (Object)Object[].class) ? (T[]) new Object[newLength] : (T[]) Array.newInstance(newType.getComponentType(), newLength); System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength)); return copy;&#125; 参数含义： 1（原数组，拷贝的个数） 用法示例12345int[] a1 = &#123;1, 2, 3, 4, 5&#125;;int[] a2 = Arrays.copyOf(a1, 3);System.out.println(Arrays.toString(a1)) // [1, 2, 3, 4, 5]System.out.println(Arrays.toString(a2)) // [1, 2, 3] 使用该方法无需我们事先使用new关键字对对象进行内存单元的分配 Arrays.copyOfRangeArrays.copyOfRange底层其实也是用的System.arraycopy，只不过封装了一个方法 123456789101112public static &lt;T,U&gt; T[] copyOfRange(U[] original, int from, int to, Class&lt;? extends T[]&gt; newType) &#123; int newLength = to - from; if (newLength &lt; 0) throw new IllegalArgumentException(from + &quot; &gt; &quot; + to); @SuppressWarnings(&quot;unchecked&quot;) T[] copy = ((Object)newType == (Object)Object[].class) ? (T[]) new Object[newLength] : (T[]) Array.newInstance(newType.getComponentType(), newLength); System.arraycopy(original, from, copy, 0, Math.min(original.length - from, newLength)); return copy;&#125; 参数含义 1（原数组，开始位置，拷贝的个数） 用法示例： 12345int[] a1 = &#123;1, 2, 3, 4, 5&#125;;int[] a2 = Arrays.copyOfRange(a1, 0, 1);System.out.println(Arrays.toString(a1)) // [1, 2, 3, 4, 5]System.out.println(Arrays.toString(a2)) // [1] 最后需要注意的是基本类型的拷贝是不影响原数组的值的，如果是引用类型，就不能在这用了，因为数组的拷贝是浅拷贝，对于基本类型可以，对于引用类型是不适合的。 那么如何实现对象的深度拷贝呢？实现Cloneable接口实现Cloneable接口，并重写clone方法，注意一个类不实现这个接口，直接使用clone方法是编译通不过的。 123456789101112131415161718192021/** * Created by Joe on 2018/2/13. */public class Dog implements Cloneable &#123; private String id; private String name; public Dog(String id, String name) &#123; this.id = id; this.name = name; &#125; // 省略 getter 、 setter 以及 toString 方法 @Override public Dog clone() throws CloneNotSupportedException &#123; Dog dog = (Dog) super.clone(); return dog; &#125;&#125; 示例： 1234567Dog dog1 = new Dog(&quot;1&quot;, &quot;Dog1&quot;);Dog dog2 = dog1.clone();dog2.setName(&quot;Dog1 changed&quot;);System.out.println(dog1); // Dog&#123;id=&#x27;1&#x27;, name=&#x27;Dog1&#x27;&#125;System.out.println(dog2); // Dog&#123;id=&#x27;1&#x27;, name=&#x27;Dog1 changed&#x27;&#125; 组合类深拷贝如果一个类里面，又引用其他的类，其他的类又有引用别的类，那么想要深度拷贝必须所有的类及其引用的类都得实现Cloneable接口，重写clone方法，这样以来非常麻烦，简单的方法是让所有的对象实现序列化接口（Serializable），然后通过序列化反序列化的方法来深度拷贝对象。 1234567891011121314151617181920public Dog myClone() &#123; Dog dog = null; try &#123; //将对象序列化成为流，因为写在流是对象里的一个拷贝 //而原始对象扔在存在JVM中，所以利用这个特性可以实现深拷贝 ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); ObjectOutputStream objectOutputStream = new ObjectOutputStream(byteArrayOutputStream); objectOutputStream.writeObject(this); //将流序列化为对象 ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(byteArrayOutputStream.toByteArray()); ObjectInputStream objectInputStream = new ObjectInputStream(byteArrayInputStream); dog = (Dog) objectInputStream.readObject(); &#125; catch (IOException | ClassNotFoundException e) &#123; e.printStackTrace(); &#125; return dog;&#125; 参考Java - 数组拷贝的几种方式","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"数组","slug":"数组","permalink":"https://xmmarlowe.github.io/tags/%E6%95%B0%E7%BB%84/"}],"author":"Marlowe"},{"title":"Java四种引用-强、软、弱、虚","slug":"Java/Java四种引用-强、软、弱、虚","date":"2021-05-21T14:28:04.000Z","updated":"2021-05-21T14:46:24.145Z","comments":true,"path":"2021/05/21/Java/Java四种引用-强、软、弱、虚/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/21/Java/Java%E5%9B%9B%E7%A7%8D%E5%BC%95%E7%94%A8-%E5%BC%BA%E3%80%81%E8%BD%AF%E3%80%81%E5%BC%B1%E3%80%81%E8%99%9A/","excerpt":"","text":"Java的四种对象引用的基本概念从JDK1.2版本开始，把对象的引用分为四种级别，从而使程序更加灵活的控制对象的生命周期。这四种级别由高到低依次为：强引用、软引用、弱引用和虚引用。 1、强引用Object obj =new Object(); 上述Object这类对象就具有强引用，属于不可回收的资源，垃圾回收器绝不会回收它。当内存空间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不会靠回收具有强引用的对象，来解决内存不足的问题。 值得注意的是：如果想中断或者回收强引用对象，可以显式地将引用赋值为null，这样的话JVM就会在合适的时间，进行垃圾回收。 下图是堆区的内存示意图，分为新生代，老生代，而垃圾回收主要也是在这部分区域中进行。 2、软引用（SoftReference）如果一个对象只具有软引用，那么它的性质属于可有可无的那种。如果此时内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。 软引用可用来实现内存敏感的告诉缓存。软引用可以和一个引用队列联合使用，如果软件用所引用的对象被垃圾回收，Java虚拟机就会把这个软引用加入到与之关联的引用队列中。 12345Object obj = new Object();ReferenceQueue queue = new ReferenceQueue();SoftReference reference = new SoftReference(obj, queue);//强引用对象滞空，保留软引用obj = null; 当内存不足时，软引用对象被回收时，reference.get()为null，此时软引用对象的作用已经发挥完毕，这时将其添加进ReferenceQueue 队列中 如果要判断哪些软引用对象已经被清理： 1234SoftReference ref = null;while ((ref = (SoftReference) queue.poll()) != null) &#123; //清除软引用对象&#125; 3、弱引用(WeakReference)如果一个对象具有弱引用，那其的性质也是可有可无的状态。 而弱引用和软引用的区别在于：弱引用的对象拥有更短的生命周期，只要垃圾回收器扫描到它，不管内存空间充足与否，都会回收它的内存。 同样的弱引用也可以和引用队列一起使用。 12345Object obj = new Object();ReferenceQueue queue = new ReferenceQueue();WeakReference reference = new WeakReference(obj, queue);//强引用对象滞空，保留软引用obj = null; 4、虚引用（PhantomReference）虚引用和前面的软引用、弱引用不同，它并不影响对象的生命周期。如果一个对象与虚引用关联，则跟没有引用与之关联一样，在任何时候都可能被垃圾回收器回收。 注意：虚引用必须和引用队列关联使用，当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会把这个虚引用加入到与之关联的引用队列中。 程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。如果程序发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 12345Object obj = new Object();ReferenceQueue queue = new ReferenceQueue();PhantomReference reference = new PhantomReference(obj, queue);//强引用对象滞空，保留软引用obj = null; 引用总结 对于强引用，平时在编写代码时会经常使用。 而其他三种类型的引用，使用得最多就是软引用和弱引用，这两种既有相似之处又有区别，他们都来描述非必须对象。 被软引用关联的对象只有在内存不足时才会被回收，而被弱引用关联的对象在JVM进行垃圾回收时总会被回收。 四种对象引用的差异对比Java中4种引用的级别由高到低依次为： 强引用 &gt; 软引用 &gt; 弱引用 &gt; 虚引用 垃圾回收时对比： 对象可及性的判断在很多的时候，一个对象并不是从根集直接引用的，而是一个对象被其他对象引用，甚至同时被几个对象所引用，从而构成一个以根集为顶的树形结构。 在这个树形的引用链中，箭头的方向代表了引用的方向，所指向的对象是被引用对象。由图可以看出，从根集到一个对象可以由很多条路径。 比如到达对象5的路径就有① -&gt; ⑤，③ -&gt;⑦两条路径。由此带来了一个问题，那就是某个对象的可及性如何判断： （1）单条引用路径可及性判断： 在这条路径中，最弱的一个引用决定对象的可及性。 （2）多条引用路径可及性判断： 几条路径中，最强的一条的引用决定对象的可及性。 比如，我们假设图2中引用①和③为强引用，⑤为软引用，⑦为弱引用，对于对象5按照这两个判断原则，路径①-⑤取最弱的引用⑤，因此该路径对对象5的引用为软引用。同样，③-⑦为弱引用。在这两条路径之间取最强的引用，于是对象5是一个软可及对象。 比较容易理解的是Java垃圾回收器会优先清理可及强度低的对象 另外两个重要的点： 强可达的对象一定不会被清理 JVM保证抛出out of memory之前，清理所有的软引用对象 最后总结成一张表格： 引用类型 被垃圾回收时间 用途 生存时间 强引用 从来不会 对象的一般状态 JVM停止运行时终止 软引用 在内存不足时 对象缓存 内存不足时终止 弱引用 在垃圾回收时 对象缓存 垃圾回收时终止 虚引用 Unkonwn Unkonwn Unkonwn 引用队列ReferenceQueue的介绍引用队列配合Reference的子类等使用,当引用对象所指向的对象被垃圾回收后,该Reference则被追加到引用队列的末尾. ReferenceQueue源码分析(简要)(1)ReferenceQueue是一个链表,这两个指针代表着头和尾 12private Reference&lt;? extends T&gt; head = null;private Reference&lt;? extends T&gt; tail = null; (2)下面看下其共有的方法 取出元素: 1Reference&lt;? extends T&gt; ReferenceQueue#poll() 如果Reference指向的对象存在则返回null,否则返回这个Reference 12345678public Reference&lt;? extends T&gt; poll() &#123; synchronized (lock) &#123; if (head == null) return null; return reallyPollLocked(); &#125;&#125; 下面是具体将Reference取出的方法: 1234567891011121314151617private Reference&lt;? extends T&gt; reallyPollLocked() &#123; if (head != null) &#123; Reference&lt;? extends T&gt; r = head; if (head == tail) &#123; tail = null; head = null; &#125; else &#123; head = head.queueNext; &#125; //更新链表,将sQueueNextUnenqueued这个虚引用对象加入,并且已经表明该Reference已经被移除了,并且取出. r.queueNext = sQueueNextUnenqueued; return r; &#125; return null;&#125; 取出元素,如果队列属于空队列,那么久阻塞到其有元素为止 1Reference&lt;? extends T&gt; ReferenceQueue#remove() 和remove()的区别是,设置一个阻塞时间 1Reference&lt;? extends T&gt; ReferenceQueue#remove(long timeout) 具体实现 123456789101112131415161718192021222324public Reference&lt;? extends T&gt; remove(long timeout) throws IllegalArgumentException, InterruptedException&#123; if (timeout &lt; 0) &#123; throw new IllegalArgumentException(&quot;Negative timeout value&quot;); &#125; synchronized (lock) &#123; Reference&lt;? extends T&gt; r = reallyPollLocked(); if (r != null) return r; long start = (timeout == 0) ? 0 : System.nanoTime(); //阻塞的具体实现过程,以及通过时间来控制的阻塞 for (;;) &#123; lock.wait(timeout); r = reallyPollLocked(); if (r != null) return r; if (timeout != 0) &#123; long end = System.nanoTime(); timeout -= (end - start) / 1000_000; if (timeout &lt;= 0) return null; start = end; &#125; &#125; &#125;&#125; WeakHashMap的相关介绍在Java集合中有一种特殊的Map类型即WeakHashMap,在这种Map中存放了键对象的弱引用,当一个键对象被垃圾回收器回收时,那么相应的值对象的引用会从Map中删除. WeakHashMap能够节约储存空间,可用来缓存那些非必须存在的数据. 而WeakHashMap是主要通过expungeStaleEntries()这个方法来实现的,而WeakHashMap也内置了一个ReferenceQueue,来获取键对象的引用情况. 这个方法,相当于遍历ReferenceQueue然后,将已经被回收的键对象,对应的值对象滞空. 1234567891011121314151617181920212223242526272829private void expungeStaleEntries() &#123; for (Object x; (x = queue.poll()) != null; ) &#123; synchronized (queue) &#123; @SuppressWarnings(&quot;unchecked&quot;) Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;) x; int i = indexFor(e.hash, table.length); Entry&lt;K,V&gt; prev = table[i]; Entry&lt;K,V&gt; p = prev; while (p != null) &#123; Entry&lt;K,V&gt; next = p.next; if (p == e) &#123; if (prev == e) table[i] = next; else prev.next = next; // Must not null out e.next; // stale entries may be in use by a HashIterator //通过滞空,来帮助垃圾回收 e.value = null; size--; break; &#125; prev = p; p = next; &#125; &#125; &#125;&#125; 而且需要注意的是: expungeStaleEntries()并不是自动调用的,需要外部对WeakHashMap对象进行查询或者操作,才会进行自动释放的操作.如下我们看个例子: 下面例子是不断的增加1000*1000容量的WeakHashMap存入List中 123456789101112public static void main(String[] args) throws Exception &#123; List&lt;WeakHashMap&lt;byte[][], byte[][]&gt;&gt; maps = new ArrayList&lt;WeakHashMap&lt;byte[][], byte[][]&gt;&gt;(); for (int i = 0; i &lt; 1000; i++) &#123; WeakHashMap&lt;byte[][], byte[][]&gt; d = new WeakHashMap&lt;byte[][], byte[][]&gt;(); d.put(new byte[1000][1000], new byte[1000][1000]); maps.add(d); System.gc(); System.err.println(i); &#125; &#125; 由于Java默认内存是64M，所以再不改变内存参数的情况下，该测试跑不了几步循环就内存溢出了。果不其然，WeakHashMap这个时候并没有自动帮我们释放不用的内存。 12345678910111213141516public static void main(String[] args) throws Exception &#123; List&lt;WeakHashMap&lt;byte[][], byte[][]&gt;&gt; maps = new ArrayList&lt;WeakHashMap&lt;byte[][], byte[][]&gt;&gt;(); for (int i = 0; i &lt; 1000; i++) &#123; WeakHashMap&lt;byte[][], byte[][]&gt; d = new WeakHashMap&lt;byte[][], byte[][]&gt;(); d.put(new byte[1000][1000], new byte[1000][1000]); maps.add(d); System.gc(); System.err.println(i); for (int j = 0; j &lt; i; j++) &#123; System.err.println(j+ &quot; size&quot; + maps.get(j).size()); &#125; &#125; &#125; 而通过访问WeakHashMap的size()方法,这些就可以跑通了. 这样就能够说明了WeakHashMap并不是自动进行键值的垃圾回收操作的,而需要做对WeakHashMap的访问操作这时候才进行对键对象的垃圾回收清理. 来一张总结图: 由图可以看出,WeakHashMap中只要调用其操作方法,那么就会调用其expungeStaleEntries(). 参考Java四种引用—强、软、弱、虚的知识点总结","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"}],"author":"Marlowe"},{"title":"MySQL长连接、短连接、连接池","slug":"数据库/MySQL长连接、短连接、连接池","date":"2021-05-21T13:40:42.000Z","updated":"2021-05-21T14:46:24.150Z","comments":true,"path":"2021/05/21/数据库/MySQL长连接、短连接、连接池/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/21/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL%E9%95%BF%E8%BF%9E%E6%8E%A5%E3%80%81%E7%9F%AD%E8%BF%9E%E6%8E%A5%E3%80%81%E8%BF%9E%E6%8E%A5%E6%B1%A0/","excerpt":"","text":"什么是短连接？短连接是指程序和数据库通信时需要建立连接，执行操作后，连接关闭。短连接简单来说就是每一次操作数据库，都要打开和关闭数据库连接，基本步骤是：连接 →数据传输 →关闭连接。 什么是长连接？长连接是指程序之间的连接在建立之后，就一直打开，被后续程序重用。使用长连接的初衷是减少连接的开销，尽管MySQL的连接比其他数据库要快得多。 以PHP程序为例，当收到一个永久连接的请求时，PHP将检查是否已经存在一个（前面已经开启了的）相同的永久连接。如果存在，则将直接使用这个连接；如果不存在，则建立一个新的连接。所谓“相同”的连接是指用相同的用户名和密码到相同主机的连接。 从客户端的角度来说，使用长连接有一个好处，可以不用每次创建新连接，若客户端对MySQL服务器的连接请求很频繁，永久连接将更加高效。对于高并发业务，如果可能会碰到连接的冲击，推荐使用长连接或连接池。 从服务器的角度来看，情况则略有不同，它可以节省创建连接的开销，但维持连接也是需要内存的。如果滥用长连接的话，可能会使用过多的MySQL服务器连接。现代的操作系统可以拥有几千个MySQL连接，但很有可能绝大部分都是睡眠（sleep）状态的，这样的工作方式不够高效，而且连接占据内存，也会导致内存的浪费。 对于扩展性好的站点来说，其实大部分的访问并不需要连接数据库。如果用户需要频繁访问数据库，那么可能会在流量增大的时候产生性能问题，此时长短连接都是无法解决问题的，所以应该进行合理的设计和优化来避免性能问题。 如果客户端和MySQL数据库之间有连接池或Proxy代理，一般在客户端推荐使用短连接。对于长连接的使用一定要慎重，不可滥用。如果没有每秒几百、上千的新连接请求，就不一定需要长连接，也无法从长连接中得到太多好处。在Java语言中，由于有连接池，如果控制得当，则不会对数据库有较大的冲击，但PHP的长连接可能导致数据库的连接数超过限制，或者占用过多的内存。 对此，研发工程师、系统运维工程师、DBA需要保持沟通，确定合理的连接策略，千万不要不假思索就采用长连接。 注意全部使用长连接后，你可能会发现，有些时候 MySQL 占用内存涨得特别快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。怎么解决这个问题呢？ 解决方案 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 总结长连接主要用于在少数客户端与服务端的频繁通信，因为这时候如果用短连接频繁通信常会发生Socket出错，并且频繁创建Socket连接也是对资源的浪费。 但是对于服务端来说，长连接也会耗费一定的资源，需要专门的线程（unix下可以用进程管理）来负责维护连接状态。 总之，长连接和短连接的选择要视情况而定。 1、在频繁的与数据库服务通信，并且又非高并发的情况下，使用长连接更合适；2、太多持久连接，大部分是sleep状态的，或者系统是高并发的，使用短连接更合适。 连接池主要的作用1、减少与数据服务器建立TCP连接三次握手及连接关闭四次挥手的开销，从而降低客户端和mysql服务端的负载，缩短请求响应时间。 2、减少数据库的并发连接数，即解决应用服务器过多导致的数据库 too many connections 问题。 如果是为了解决问题1则在workerman中数据库连接池不是最高效的方法，反而是自找麻烦的做法。由于PHP是单进程单线程的，使用PHP实现数据库连接池，肯定需要用单独的进程去做，那么就会涉及到进程间的通讯，使得原本和mysql直接通讯的过程变成 与连接池再到mysql的通讯，增加了应用端的负载。 解决问题1最高效的方法是为每个业务进程建立一个数据库单例（例如workerman提供的DB类），实现数据库长连接，这样每个进程的所有请求都使用自己的这一个数据库长连接，整个进程的生命周期只有一次TCP握手和断开连接挥手的开销，并且应用与mysql直接通讯，没有连接池那样中间一层进程间IPC通讯，性能是最高的，没有之一。 如果是为了问题2 首先看下自己到底有多少台应用服务器，每台服务器与mysql有多收并发连接。假如你只有10台应用服务器，每个服务器50个进程，每个进程1个数据库连接，那么到mysql服务端总共只有10*50=500个并发连接（并非活跃连接），500个并发连接对于mysql来说就是小菜一碟，为了解决问题2完全没有使用连接池的必要。 假如你有1000台应用服务器，那么连接池是有必要的，但是这个连接池不能是运行在本地应用服务器上的连接池，因为1000台应用服务器就有1000个连接池，即使每个连接池只开10个连接，那么数据库的连接数也会轻松打满。所以不要指望在当前服务器上开几个task进程实现的连接池就能解决这个问题。 1000台应用服务器的集群，每台服务器上搞几个进程实现连接池同样是不靠谱的方法。真正能够解决问题2的方法是建立一个独立的数据库连接池服务器或者说集群，全局管理所有的数据库链接。 参考Mysql 的 长连接？ 短连接？一篇读懂mysql长链接、短连接、连接池","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://xmmarlowe.github.io/tags/MySQL/"},{"name":"连接池","slug":"连接池","permalink":"https://xmmarlowe.github.io/tags/%E8%BF%9E%E6%8E%A5%E6%B1%A0/"}],"author":"Marlowe"},{"title":"初识CSRF攻击","slug":"网络安全/初识CSRF攻击","date":"2021-05-20T14:44:30.000Z","updated":"2021-05-21T01:16:24.098Z","comments":true,"path":"2021/05/20/网络安全/初识CSRF攻击/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/20/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/%E5%88%9D%E8%AF%86CSRF%E6%94%BB%E5%87%BB/","excerpt":"","text":"什么是CSRF攻击?CSRF: Cross Site Regust Forgery跨站请求伪造 一个正常的请求会将合法用户的session id保存到浏览器的cookie。这时候，如果用户在浏览器中打来另一个tab页，那这个tab页也是可以获得浏览器的cookie。黑客就可以利用这个cookie信息进行攻击。 攻击过程原理及过程 用户C打开浏览器，访问受信任网站A，输入用户名和密码请求登录网站A； 在用户信息通过验证后，网站A产生Cookie信息并返回给浏览器，此时用户登录网站A成功，可以正常发送请求到网站A； 用户未退出网站A之前，在同一浏览器中，打开一个TAB页访问网站B； 网站B接收到用户请求后，返回一些攻击性代码，并发出一个请求要求访问第三方站点A； 浏览器在接收到这些攻击性代码后，根据网站B的请求，在用户不知情的情况下携带Cookie信息，向网站A发出请求。网站A并不知道该请求其实是由B发起的，所以会根据用户C的Cookie信息以C的权限处理该请求，导致来自网站B的恶意代码被执行。 例子1、某银行网站A可以以GET请求的方式发起转账操作。www.xx.com/transfor.do?accountNum= 100&amp;money= 1000,accountNum表示目标账户。这个请求肯定是需要登录才可以正常访问的。 2、攻击者在某个论坛或者网站上，上传一 个图片，链接地址是www.xxx.com/transfer.do?accountNum=888&amp;money=10000,其中这个accountNum就是攻击者自己的银行账户。 3、如果有一个用户，登录了银行网站，然后又打开浏览器的另一个tab页，点击了这个图片。这时，银行就会受理到一个带了正确cookie的请求，就会完成转账。用户的钱就被盗了。 CSRF漏洞检测检测CSRF漏洞是一项比较繁琐的工作，最简单的方法就是抓取一个正常请求的数据包，去掉Referer字段后再重新提交，如果该提交还有效，那么基本上可以确定存在CSRF漏洞。 随着对CSRF漏洞研究的不断深入，不断涌现出一些专门针对CSRF漏洞进行检测的工具，如CSRFTester，CSRF Request Builder等。 以CSRFTester工具为例，CSRF漏洞检测工具的测试原理如下：使用CSRFTester进行测试时，首先需要抓取我们在浏览器中访问过的所有链接以及所有的表单等信息，然后通过在CSRFTester中修改相应的表单等信息，重新提交，这相当于一次伪造客户端请求。如果修改后的测试请求成功被网站服务器接受，则说明存在CSRF漏洞，当然此款工具也可以被用来进行CSRF攻击。 CSRF防止方式:简述1、尽量使用POST请求，限制GET请求。POST请求可以带请求体，攻击者就不容易伪造出请求。 2、将cookie设置 为HttpOnly : respose.setHeader(“Set-Cookie”,” cookiename=cookievalue;HttpOnly”)。 3、增加token：在请求中放入一个攻击者无法伪造的信息，并且该信息不存在于cookie当中。&lt;input type= &#39;hidden’value= &#39; adfasdf&#39;/&gt;这也是Spring Security框架中采用的防范方式。 详细方式目前防御 CSRF 攻击主要有三种策略：验证 HTTP Referer 字段；在请求地址中添加 token 并验证；在 HTTP 头中自定义属性并验证。 （1）验证 HTTP Referer 字段根据 HTTP 协议，在 HTTP 头中有一个字段叫 Referer，它记录了该 HTTP 请求的来源地址。在通常情况下，访问一个安全受限页面的请求来自于同一个网站，比如需要访问 http://bank.example/withdraw?account=bob&amp;amount=1000000&amp;for=Mallory，用户必须先登陆 bank.example，然后通过点击页面上的按钮来触发转账事件。这时，该转帐请求的 Referer 值就会是转账按钮所在的页面的 URL，通常是以 bank.example 域名开头的地址。而如果黑客要对银行网站实施 CSRF 攻击，他只能在他自己的网站构造请求，当用户通过黑客的网站发送请求到银行时，该请求的 Referer 是指向黑客自己的网站。因此，要防御 CSRF 攻击，银行网站只需要对于每一个转账请求验证其 Referer 值，如果是以 bank.example 开头的域名，则说明该请求是来自银行网站自己的请求，是合法的。如果 Referer 是其他网站的话，则有可能是黑客的 CSRF 攻击，拒绝该请求。 这种方法的显而易见的好处就是简单易行，网站的普通开发人员不需要操心 CSRF 的漏洞，只需要在最后给所有安全敏感的请求统一增加一个拦截器来检查 Referer 的值就可以。特别是对于当前现有的系统，不需要改变当前系统的任何已有代码和逻辑，没有风险，非常便捷。 然而，这种方法并非万无一失。Referer 的值是由浏览器提供的，虽然 HTTP 协议上有明确的要求，但是每个浏览器对于 Referer 的具体实现可能有差别，并不能保证浏览器自身没有安全漏洞。使用验证 Referer 值的方法，就是把安全性都依赖于第三方（即浏览器）来保障，从理论上来讲，这样并不安全。事实上，对于某些浏览器，比如 IE6 或 FF2，目前已经有一些方法可以篡改 Referer 值。如果 bank.example 网站支持 IE6 浏览器，黑客完全可以把用户浏览器的 Referer 值设为以 bank.example 域名开头的地址，这样就可以通过验证，从而进行 CSRF 攻击。 即便是使用最新的浏览器，黑客无法篡改 Referer 值，这种方法仍然有问题。因为 Referer 值会记录下用户的访问来源，有些用户认为这样会侵犯到他们自己的隐私权，特别是有些组织担心 Referer 值会把组织内网中的某些信息泄露到外网中。因此，用户自己可以设置浏览器使其在发送请求时不再提供 Referer。当他们正常访问银行网站时，网站会因为请求没有 Referer 值而认为是 CSRF 攻击，拒绝合法用户的访问。 （2）在请求地址中添加 token 并验证CSRF 攻击之所以能够成功，是因为黑客可以完全伪造用户的请求，该请求中所有的用户验证信息都是存在于 cookie 中，因此黑客可以在不知道这些验证信息的情况下直接利用用户自己的 cookie 来通过安全验证。要抵御 CSRF，关键在于在请求中放入黑客所不能伪造的信息，并且该信息不存在于 cookie 之中。可以在 HTTP 请求中以参数的形式加入一个随机产生的 token，并在服务器端建立一个拦截器来验证这个 token，如果请求中没有 token 或者 token 内容不正确，则认为可能是 CSRF 攻击而拒绝该请求。 这种方法要比检查 Referer 要安全一些，token 可以在用户登陆后产生并放于 session 之中，然后在每次请求时把 token 从 session 中拿出，与请求中的 token 进行比对，但这种方法的难点在于如何把 token 以参数的形式加入请求。对于 GET 请求，token 将附在请求地址之后，这样 URL 就变成 http://url?csrftoken=tokenvalue。 而对于 POST 请求来说，要在 form 的最后加上 &lt;input type=”hidden” name=”csrftoken” value=”tokenvalue”/&gt;，这样就把 token 以参数的形式加入请求了。但是，在一个网站中，可以接受请求的地方非常多，要对于每一个请求都加上 token 是很麻烦的，并且很容易漏掉，通常使用的方法就是在每次页面加载时，使用 javascript 遍历整个 dom 树，对于 dom 中所有的 a 和 form 标签后加入 token。这样可以解决大部分的请求，但是对于在页面加载之后动态生成的 html 代码，这种方法就没有作用，还需要程序员在编码时手动添加 token。 该方法还有一个缺点是难以保证 token 本身的安全。特别是在一些论坛之类支持用户自己发表内容的网站，黑客可以在上面发布自己个人网站的地址。由于系统也会在这个地址后面加上 token，黑客可以在自己的网站上得到这个 token，并马上就可以发动 CSRF 攻击。为了避免这一点，系统可以在添加 token 的时候增加一个判断，如果这个链接是链到自己本站的，就在后面添加 token，如果是通向外网则不加。不过，即使这个 csrftoken 不以参数的形式附加在请求之中，黑客的网站也同样可以通过 Referer 来得到这个 token 值以发动 CSRF 攻击。这也是一些用户喜欢手动关闭浏览器 Referer 功能的原因。 （3）在 HTTP 头中自定义属性并验证这种方法也是使用 token 并进行验证，和上一种方法不同的是，这里并不是把 token 以参数的形式置于 HTTP 请求之中，而是把它放到 HTTP 头中自定义的属性里。通过 XMLHttpRequest 这个类，可以一次性给所有该类请求加上 csrftoken 这个 HTTP 头属性，并把 token 值放入其中。这样解决了上种方法在请求中加入 token 的不便，同时，通过 XMLHttpRequest 请求的地址不会被记录到浏览器的地址栏，也不用担心 token 会透过 Referer 泄露到其他网站中去。 然而这种方法的局限性非常大。XMLHttpRequest 请求通常用于 Ajax 方法中对于页面局部的异步刷新，并非所有的请求都适合用这个类来发起，而且通过该类请求得到的页面不能被浏览器所记录下，从而进行前进，后退，刷新，收藏等操作，给用户带来不便。另外，对于没有进行 CSRF 防护的遗留系统来说，要采用这种方法来进行防护，要把所有请求都改为 XMLHttpRequest 请求，这样几乎是要重写整个网站，这代价无疑是不能接受的。","categories":[{"name":"网络安全","slug":"网络安全","permalink":"https://xmmarlowe.github.io/categories/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"}],"tags":[{"name":"CSRF","slug":"CSRF","permalink":"https://xmmarlowe.github.io/tags/CSRF/"}],"author":"Marlowe"},{"title":"ES写数据以及文档读写原理","slug":"中间件/ES写数据以及文档读写原理","date":"2021-05-19T14:34:02.000Z","updated":"2021-08-21T02:06:50.663Z","comments":true,"path":"2021/05/19/中间件/ES写数据以及文档读写原理/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/19/%E4%B8%AD%E9%97%B4%E4%BB%B6/ES%E5%86%99%E6%95%B0%E6%8D%AE%E4%BB%A5%E5%8F%8A%E6%96%87%E6%A1%A3%E8%AF%BB%E5%86%99%E5%8E%9F%E7%90%86/","excerpt":"es写入数据的工作原理是什么啊？es查询数据的工作原理是什么？底层的lucence介绍一下呗？倒排索引了解吗？","text":"es写入数据的工作原理是什么啊？es查询数据的工作原理是什么？底层的lucence介绍一下呗？倒排索引了解吗？ ES写数据过程1、客户端选择一个node发送请求过去，这个node就是coordinating node（协调节点） 2、coordinating node 对document进行路由，将请求转发给对应的node（有primary shard） 3、实际的node上的primary shard 处理请求，然后将数据同步到replica node。 4、coordinating node如果发现 primary node和所有replica node都搞定之后，就返回响应结果给客户端。 ES读数据过程可以通过doc id 来查询，会根据doc id进行hash，判断出来当时把doc id分配到了哪个shard上面去，从那个shard去查询。 1、客户端发送请求到任意一个node，成为coordinate node 2、coordinate node 对doc id进行哈希路由，将请求转发到对应node，此时会使用round-robin随机轮询算法，在primary shard 以及其所有replica中随机选择一个，让读请求负载均衡。 3、接收请求的node返回document给coordinate node。 4、coordinate node返回document给客户端。 ES搜索数据过程ES最强大的是做全文检索。 1、客户端发送请求到一个coordinate node。 2、协调节点将搜索请求转发到所有的shard对应的primary shard 或 replica shard ，都可以。 3、query phase：每个shard将自己的搜索结果（其实就是一些doc id）返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果。 4、fetch phase：接着由协调节点根据doc id去各个节点上拉取实际的document数据，最终返回给客户端。 写请求是写入primary shard，然后同步给所有的replica shard 读请求可以从primary shard 或者 replica shard 读取，采用的是随机轮询算法。 写数据底层原理 1、先写入内存buffer，在buffer里的时候数据是搜索不到的；同时将数据写入translog日志文件。 如果buffer快满了，或者到一定时间，就会将内存buffer数据refresh 到一个新的segment file中，但是此时数据不是直接进入segment file磁盘文件，而是先进入 os cache。这个过程就是 refresh。 每隔1秒钟，es将buffer中的数据写入一个新的segment file，每秒钟会写入一个新的segment file，这个segment file中就存储最近1秒内 buffer中写入的数据。 2、但是如果buffer里面此时没有数据，那当然不会执行refresh操作，如果buffer里面有数据，默认1秒钟执行一次refresh操作，刷入一个新的segment file中。 操作系统里面，磁盘文件其实都有一个东西，叫做os cache，即操作系统缓存，就是说数据写入磁盘文件之前，会先进入os cache，先进入操作系统级别的 一个内存缓存中去。只要buffer中的数据被refresh 操作刷入os cache中，这个数据就可以被搜索到了。 3、为什么叫es是准实时的？ NRT，全称 near real-time。默认是每隔1秒refresh一次的，所以es是准实时的，因为写入的数据1s之后才能被看到。 可以通过es的restful api或者 java api，手动执行一次 refresh操作，就是手动将buffer中的数据刷入os cache中，让数据立马就可以被搜索到。只要 数据被输入os cache中，buffer 就会被清空了，因为不需要保留buffer了，数据在translog里面已经持久化到磁盘去一份了。 4、重复上面的步骤，新的数据不断进入buffer和translog，不断将buffer数据写入一个又一个新的segment file中去，每次refresh完buffer清空，translog保留。 随着这个过程的推进，translog会变得越来越大。当translog达到一定长度的时候，就会触发commit操作。 5、commit操作发生的第一步，就是将buffer中现有的数据refresh到os cache中去，清空buffer。然后将一个commit point写入磁盘文件，里面标识者这个commit point 对应的所有segment file，同时强行将os cache中目前所有的数据都fsync到磁盘文件中去。最后清空现有 translog日志文件，重启一个translog，此时commit操作完成。 6、这个commit操作叫做flush。默认30分钟自动执行一次flush，但如果translog过大，也会触发flush。flush操作就对应着commit的全过程，我们可以通过es api，手动执行flush操作，手动将os cache中数据fsync强刷到磁盘上去。 7、translog日志文件的作用是什么？ 执行commit 操作之前，数据要么是停留在buffer中，要么是停留在os cache中，无论是buffer 还是os cache都是内存，一旦这台机器死了，内存中的数据就全丢了。 所以需要将数据对应的操作写入一个专门的日志文件translog中，一旦此时机器宕机了，再次重启的时候，es会自动读取translog日志文件中的数据，恢复到内存buffer和os cache中去。 8、translog其实也是先写入os cache的，默认每隔5秒刷一次到磁盘中去，所以默认情况下，可能有5s的数据会仅仅停留在buffer或者translog文件的os cache中，如果此时机器挂了，会丢失5秒钟的数据。但是这样性能比较好，最多丢5秒的数据。也可以将translog设置成每次写操作必须是直接fsync到磁盘，但是性能会差很多。 9、es第一是准实时的，数据写入1秒后就可以搜索到：可能会丢失数据的。有5秒的数据，停留在buffer、translog os cache 、segment file os cache中，而不在磁盘上， 此时如果宕机，会导致5秒的数据丢失。 10、总结： 数据先写入内存buffer，然后每隔1s，将数据refresh到 os cache，到了 os cache数据就能被搜索到（所以我们才说es从写入到能被搜索到，中间有1s的延迟）。 每隔5s，将数据写入到translog文件（这样如果机器宕机，内存数据全没，最多会有5s的数据丢失），translog达到一定程度，或者默认每隔30min，会触发commit操作，将缓冲区的 数据都flush到segment file磁盘文件中。 数据写入 segment file之后，同时就建立好了倒排索引。 删除/更新数据底层原理如果是删除操作，commit的时候会生成一个 .del文件，里面将某个doc标识为 deleted状态，那么搜索的时候根据 .del文件就知道这个doc是否被删除了。 如果是更新操作，就是将原来的doc标识为deleted状态，然后重新写入一条数据。 buffer 每refresh一次，就会产生一个segment file，所以默认情况下是1秒钟一个segment file，这样下来segment file会越来越多，此时会定期执行merge。 每次merge的时候，会将多个segment file合并成一个，同时这里会将标识为 deleted的doc给物理删除掉，然后将新的segment file写入磁盘，这里会写一个 commit point，标识所有新的 segment file，然后打开segment file供搜索使用，同时删除旧的segment file。 文档读写模型实现原理简介ElasticSearch，每个索引被分成多个分片（默认每个索引5个主分片primary shard），每个分片又可以有多个副本。当一个文档被添加或删除时（主分片中新增或删除），其对应的复制分片之间必须保持同步。那如何保持分片副本同步呢？这就是本篇重点要阐述的，即数据复制模型。 ElasticSearch的数据复制模型是基于主从备份模型的。每一个复制组中会有一个主分片，其他分片均为复制分片。主分片服务器是所有索引操作的主要入口点（索引、更新、删除操作）。一旦一个索引操作被主服务器接受之后主分片服务器会将其数据复制到其他副本。 基本写模型ElasticSearch每个索引操作首先会进行路由选择定位到一个复制组，默认基于文档ID(routing)，其基本算法为hash(routing) % (primary count)。一旦确定了复制组，则该操作将被转发到该组的主分片（primary shard）。主分片服务器负责验证操作并将其转发到其他副本。 由于副本可以离线（一个复制组并不是要求所有复制分片都在线才能运作），可能不需要复制到所有副本。ElasticSearch会维护一个当前在线的副本服务器列表，这个列表被称为in-sync副本，由主节点维护。也就是当主分片接收一个文档后，需要将该文档复制到in-sync列表中的每一台服务器。 主分片的处理流程如下：验证请求是否符合Elasticsearch的接口规范，如果不符合，直接拒绝。在主分片上执行操作(例如索引、更新或删除一个文档)。如果执行过程中出错，直接返回错误。将操作转发到当前同步副本集的每个副本。如果有多个副本，则并行执行。（in-sync当前可用、激活的副本）。一旦所有的副本成功地执行了操作并对主服务器进行了响应，主服务器向客户端返回成功。 写请求的流程如下图所示（图片来源于《Elasticsearch权威指南》，如有侵权，马上删除）： 异常处理机制：在索引过程中，许多情况会引发异常，例如磁盘可能会被破坏、节点之间网络彼此断开，或者一些配置错误可能导致一个副本的操作失败，尽管它在主服务器上是成功的。上述原因虽然很少会发生，但我们在设计时还是必须考虑如果发生错误了该如何处理。 另外一种情况异常情况也不得不考虑。如果主服务器不可用，ES集群该如何处理呢？ 此时会触发复制组内的主服务器选举，选举后新的主节点会向master服务器发送一条消息，然后，该请求会将被转发到新的主服务器进行处理。此过程该请求在主节点选主期间会阻塞（等待），默认情况下最多等待1分钟。 注：主服务器(master)会监控各个节点的健康状况，并可能决定主动降级主节点。这通常发生在持有主节点的节点通过网络问题与集群隔离的情况下。 为了更好的理解master服务器与主分片所在服务器的关系，下面给出一个ElasticSearch的集群说明图：（图片来源于《Elasticsearch权威指南》，如有侵权，马上删除） 其中NODE1为整个集群的master服务器，而第一个复制组(P0,R0,RO,其主分片所在服务器NODE3)，第二个复制组(P1,R1,R1,其主分片所在服务器NODE1)。 一旦在主服务器上成功执行了操作，主服务器就必须确保数据最终一致，即使由于在副本上执行失败或由于网络问题导致操作无法到达副本（或阻止副本响应）造成的。 为了避免数据在复制组内数据的不一致性（例如在主分片中执行成功，但在其中一两个复制分片中执行失败），主分片如果未在指定时间内（默认一分钟）未收到复制分片的成功响应或是收到错误响应，主分片会向Master服务器发送一个请求，请求从同步副本中删除有问题的分片，最终当主分片服务器确向Master服务器的确认要删除有问题的副本时，Master会指示删除有问题的副本。同时，master还会指示另一个节点开始构建新的分片副本，以便将系统恢复到一个健康状态。 主分片将一个操作转发到副本时，首先会使用副本数来验证它仍然是活动的主节点。如果由于网络分区（或长GC）而被隔离，那么在意识到它已经被降级之前，它可能会继续处理传入的索引操作并转发到从服务器。来自陈旧的主服务器的操作将会被副本拒绝。当主接受到来自副本的响应为拒绝它的请求时，此时的主分片会向Master服务器发送请求，最终将知道它已经被替换了，后续操作将会路由到新的主分片服务器上。 如果没有副本，那会发生什么呢？ 这是一个有效的场景，可能由于配置而发生，或者是因为所有的副本都失败了。在这种情况下，主分片要在没有任何外部验证的情况下处理操作，这可能看起来有问题。另一方面，主分片服务器不能自己失败其他的分片（副本），而是请求master服务器代表它这样做。这意味着master服务器知道主分片是该复制组唯一的可用拷贝。因此，我们保证master不会将任何其他（过时的）分片副本提升为一个新的主分片，并且任何索引到主分片服务器的操作都不会丢失。当然这样意味着我们只使用单一的数据副本，物理硬件问题可能导致数据丢失。请参阅Wait For Active Shards，以获得一些缓解选项,(该参数项将在下一节中详细描述)。 注：在一个ElasticSearch集群中，存在两个维度的选主。Master节点的选主、各个复制组主分片的选主。 基本读模型在Elasticsearch中，可以通过ID进行非常轻量级的查找，也可以使用复杂的聚合来获取非平凡的CPU能力。主备份模型的优点之一是它使所有的分片副本保持相同（除了异常情况恢复中）。通常，一个副本就足以满足读取请求。 当一个节点接收到read请求时，该节点根据路由规则负责将其转发给相应的数据节点，对响应进行整理，并对客户端作出响应。我们称该节点为该请求的协调节点。基本流程如下： 将读请求路由到到相关的分片节点。注意，由于大多数搜索条件中不包含分片字段，所以它们通常需要从多个分片组中读取数据，每个分片代表一个不同的数据子集（默认5个数据子集，因为ElasticSearch默认的主分片个数为5个）。从每个分片复制组中选择一个副本。读请求可以是复制组中的主分片，也可以是其副本分片。在默认情况下，ElasticSearch分片组内的读请求负载算法为轮询。根据第二步选择的各个分片，向选中的分片发送请求。汇聚各个分片节点返回的数据，然后返回个客户端，注意，如果带有分片字段的查询，将之后转发给一个节点，该步骤可省略。异常处理：当一个碎片不能响应一个read请求时，协调节点将从同一个复制组中选择另一个副本，并向其发送查询请求。重复的失败会导致没有分片副本可用。在某些情况下，比如搜索，ElasticSearch会更倾向于快速响应（失败后不重试），返回成功的分片数据给客户端 ，并在响应包中指明哪些分片节点发生了错误。 Elasticsearch主备模型隐含含义在正常操作下，每个读取操作一次为每个相关的复制组执行一次。只有在失败条件下，同一个复制组的多个副本执行相同的搜索。由于数据首先是在主分片上进行索引后，然后才转发请求到副本，在转发之前数据已经在主分片上发生了变化，所以在并发读时，如果读请求被转发到主分片节点上，那该数据在它被确认之前（主分片再等待所有副本全部执行成功）就已经看到了变化。【有点类似于数据库的读未提交】。主备模型的容错能力为两个分片（1主分片，1副本） ElasticSearch 读写模型异常时可造成的影响在失败的情况下，以下是可能的： 1个分片节点可能减慢整个集群的索引性能因为在每次操作期间(索引)，主分片在本地成功执行索引动作后，会转发请求到期复制分片节点上，此时主分片需要等待所有同步副本节点的响应，单个慢分片可以减慢整个复制组的速度。当然，一个缓慢的分片也会减慢那些被路由到它的搜索。脏读一个孤立的主服务器可以公开不被承认的写入。这是由于一个孤立的主节点只会意识到它在向副本发送请求或向主人发送请求时被隔离。在这一点上，操作已经被索引到主节点，并且可以通过并发读取读取。Elasticsearch可以通过在每秒钟（默认情况下）对master进行ping来减少这种风险，并且如果没有已知的主节点，则拒绝索引操作。本文详细介绍了ElasticSearch文档的读写模型的设计思路，涉及到写模型及其异常处理、读模型及其异常处理、主备负载模型背后隐含的设计缺陷与ElasticSearch在异常情况带来的影响。 参考ES读写数据的工作原理以及文档读写模型实现原理","categories":[{"name":"中间件","slug":"中间件","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"ES","slug":"ES","permalink":"https://xmmarlowe.github.io/tags/ES/"}],"author":"Marlowe"},{"title":"高并发之数据库分库分表","slug":"数据库/高并发之数据库分库分表","date":"2021-05-19T12:28:54.000Z","updated":"2021-05-19T14:47:17.855Z","comments":true,"path":"2021/05/19/数据库/高并发之数据库分库分表/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/19/%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8/","excerpt":"记录一些数据库分库分表问题…","text":"记录一些数据库分库分表问题… 为什么要分库分表？（设计高并发系统的时候，数据库层面该如何设计？）说白了，分库分表是两回事儿，大家可别搞混了，可能是光分库不分表，也可能是光分表不分库，都有可能。 我先给大家抛出来一个场景。 假如我们现在是一个小创业公司（或者是一个 BAT 公司刚兴起的一个新部门），现在注册用户就 20 万，每天活跃用户就 1 万，每天单表数据量就 1000，然后高峰期每秒钟并发请求最多就 10。天，就这种系统，随便找一个有几年工作经验的，然后带几个刚培训出来的，随便干干都可以。 结果没想到我们运气居然这么好，碰上个 CEO 带着我们走上了康庄大道，业务发展迅猛，过了几个月，注册用户数达到了 2000 万！每天活跃用户数 100 万！每天单表数据量 10 万条！高峰期每秒最大请求达到 1000！同时公司还顺带着融资了两轮，进账了几个亿人民币啊！公司估值达到了惊人的几亿美金！这是小独角兽的节奏！ 好吧，没事，现在大家感觉压力已经有点大了，为啥呢？因为每天多 10 万条数据，一个月就多 300 万条数据，现在咱们单表已经几百万数据了，马上就破千万了。但是勉强还能撑着。高峰期请求现在是 1000，咱们线上部署了几台机器，负载均衡搞了一下，数据库撑 1000QPS 也还凑合。但是大家现在开始感觉有点担心了，接下来咋整呢…… 再接下来几个月，我的天，CEO 太牛逼了，公司用户数已经达到 1 亿，公司继续融资几十亿人民币啊！公司估值达到了惊人的几十亿美金，成为了国内今年最牛逼的明星创业公司！天，我们太幸运了。 但是我们同时也是不幸的，因为此时每天活跃用户数上千万，每天单表新增数据多达 50 万，目前一个表总数据量都已经达到了两三千万了！扛不住啊！数据库磁盘容量不断消耗掉！高峰期并发达到惊人的 5000~8000！别开玩笑了，哥。我跟你保证，你的系统支撑不到现在，已经挂掉了！ 好吧，所以你看到这里差不多就理解分库分表是怎么回事儿了，实际上这是跟着你的公司业务发展走的，你公司业务发展越好，用户就越多，数据量越大，请求量越大，那你单个数据库一定扛不住。 分表比如你单表都几千万数据了，你确定你能扛住么？绝对不行，单表数据量太大，会极大影响你的 sql 执行的性能，到了后面你的 sql 可能就跑的很慢了。一般来说，就以我的经验来看，单表到几百万的时候，性能就会相对差一些了，你就得分表了。 分表是啥意思？就是把一个表的数据放到多个表中，然后查询的时候你就查一个表。比如按照用户 id 来分表，将一个用户的数据就放在一个表中。然后操作的时候你对一个用户就操作那个表就好了。这样可以控制每个表的数据量在可控的范围内，比如每个表就固定在 200 万以内。 分库分库是啥意思？就是你一个库一般我们经验而言，最多支撑到并发 2000，一定要扩容了，而且一个健康的单库并发值你最好保持在每秒 1000 左右，不要太大。那么你可以将一个库的数据拆分到多个库中，访问的时候就访问一个库好了。 这就是所谓的分库分表，为啥要分库分表？你明白了吧。 分库分表前 分库分表后 并发支撑情况 MySQL 单机部署，扛不住高并发 MySQL从单机到多机，能承受的并发增加了多倍 磁盘使用情况 MySQL 单机磁盘容量几乎撑满 拆分为多个库，数据库服务器磁盘使用率大大降低 SQL执行性能 单表数据量太大，SQL 越跑越慢 单表数据量减少，SQL 执行效率明显提升 用过哪些分库分表中间件？不同的分库分表中间件都有什么优点和缺点？这个其实就是看看你了解哪些分库分表的中间件，各个中间件的优缺点是啥？然后你用过哪些分库分表的中间件。 比较常见的包括： cobar TDDL atlas sharding-jdbc mycat cobar阿里 b2b 团队开发和开源的，属于 proxy 层方案。早些年还可以用，但是最近几年都没更新了，基本没啥人用，差不多算是被抛弃的状态吧。而且不支持读写分离、存储过程、跨库 join 和分页等操作。 TDDL淘宝团队开发的，属于 client 层方案。支持基本的 crud 语法和读写分离，但不支持 join、多表查询等语法。目前使用的也不多，因为还依赖淘宝的 diamond 配置管理系统。 atlas360 开源的，属于 proxy 层方案，以前是有一些公司在用的，但是确实有一个很大的问题就是社区最新的维护都在 5 年前了。所以，现在用的公司基本也很少了。 sharding-jdbc当当开源的，属于 client 层方案。确实之前用的还比较多一些，因为 SQL 语法支持也比较多，没有太多限制，而且目前推出到了 2.0 版本，支持分库分表、读写分离、分布式 id 生成、柔性事务（最大努力送达型事务、TCC 事务）。而且确实之前使用的公司会比较多一些（这个在官网有登记使用的公司，可以看到从 2017 年一直到现在，是有不少公司在用的），目前社区也还一直在开发和维护，还算是比较活跃，个人认为算是一个现在也可以选择的方案。 mycat基于 cobar 改造的，属于 proxy 层方案，支持的功能非常完善，而且目前应该是非常火的而且不断流行的数据库中间件，社区很活跃，也有一些公司开始在用了。但是确实相比于 sharding jdbc 来说，年轻一些，经历的锤炼少一些。 总结综上，现在其实建议考量的，就是 sharding-jdbc 和 mycat，这两个都可以去考虑使用。 sharding-jdbc 这种 client 层方案的优点在于不用部署，运维成本低，不需要代理层的二次转发请求，性能很高，但是如果遇到升级啥的需要各个系统都重新升级版本再发布，各个系统都需要耦合 sharding-jdbc 的依赖； mycat 这种 proxy 层方案的缺点在于需要部署，自己运维一套中间件，运维成本高，但是好处在于对于各个项目是透明的，如果遇到升级之类的都是自己中间件那里搞就行了。 通常来说，这两个方案其实都可以选用，但是我个人建议中小型公司选用 sharding-jdbc，client 层方案轻便，而且维护成本低，不需要额外增派人手，而且中小型公司系统复杂度会低一些，项目也没那么多；但是中大型公司最好还是选用 mycat 这类 proxy 层方案，因为可能大公司系统和项目非常多，团队很大，人员充足，那么最好是专门弄个人来研究和维护 mycat，然后大量项目直接透明使用即可。 你们具体是如何对数据库如何进行垂直拆分或水平拆分的？水平拆分水平拆分的意思，就是把一个表的数据给弄到多个库的多个表里去，但是每个库的表结构都一样，只不过每个库表放的数据是不同的，所有库表的数据加起来就是全部数据。水平拆分的意义，就是将数据均匀放更多的库里，然后用多个库来抗更高的并发，还有就是用多个库的存储容量来进行扩容。 垂直拆分垂直拆分的意思，就是把一个有很多字段的表给拆分成多个表，或者是多个库上去。每个库表的结构都不一样，每个库表都包含部分字段。一般来说，会将较少的访问频率很高的字段放到一个表里去，然后将较多的访问频率很低的字段放到另外一个表里去。因为数据库是有缓存的，你访问频率高的行字段越少，就可以在缓存里缓存更多的行，性能就越好。这个一般在表层面做的较多一些。 这个其实挺常见的，不一定我说，大家很多同学可能自己都做过，把一个大表拆开，订单表、订单支付表、订单商品表。 还有表层面的拆分，就是分表，将一个表变成 N 个表，就是让每个表的数据量控制在一定范围内，保证 SQL 的性能。否则单表数据量越大，SQL 性能就越差。一般是 200 万行左右，不要太多，但是也得看具体你怎么操作，也可能是 500 万，或者是 100 万。你的SQL越复杂，就最好让单表行数越少。 好了，无论分库还是分表，上面说的那些数据库中间件都是可以支持的。就是基本上那些中间件可以做到你分库分表之后，中间件可以根据你指定的某个字段值，比如说 userid，自动路由到对应的库上去，然后再自动路由到对应的表里去。 你就得考虑一下，你的项目里该如何分库分表？一般来说，垂直拆分，你可以在表层面来做，对一些字段特别多的表做一下拆分；水平拆分，你可以说是并发承载不了，或者是数据量太大，容量承载不了，你给拆了，按什么字段来拆，你自己想好；分表，你考虑一下，你如果哪怕是拆到每个库里去，并发和容量都ok了，但是每个库的表还是太大了，那么你就分表，将这个表分开，保证每个表的数据量并不是很大。 而且这儿还有两种分库分表的方式： 一种是按照 range 来分，就是每个库一段连续的数据，这个一般是按比如时间范围来的，但是这种一般较少用，因为很容易产生热点问题，大量的流量都打在最新的数据上了。 或者是按照某个字段hash一下均匀分散，这个较为常用。 range 来分，好处在于说，扩容的时候很简单，因为你只要预备好，给每个月都准备一个库就可以了，到了一个新的月份的时候，自然而然，就会写新的库了；缺点，但是大部分的请求，都是访问最新的数据。实际生产用 range，要看场景。 hash 分发，好处在于说，可以平均分配每个库的数据量和请求压力；坏处在于说扩容起来比较麻烦，会有一个数据迁移的过程，之前的数据需要重新计算 hash 值重新分配到不同的库或表。 现在有一个未分库分表的系统，未来要分库分表，如何设计才可以让系统从未分库分表动态切换到分库分表上？停机迁移方案我先给你说一个最 low 的方案，就是很简单，大家伙儿凌晨 12 点开始运维，网站或者 app 挂个公告，说 0 点到早上 6 点进行运维，无法访问。 接着到 0 点停机，系统停掉，没有流量写入了，此时老的单库单表数据库静止了。然后你之前得写好一个导数的一次性工具，此时直接跑起来，然后将单库单表的数据哗哗哗读出来，写到分库分表里面去。 导数完了之后，就 ok 了，修改系统的数据库连接配置啥的，包括可能代码和 SQL 也许有修改，那你就用最新的代码，然后直接启动连到新的分库分表上去。 验证一下，ok了，完美，大家伸个懒腰，看看看凌晨 4 点钟的北京夜景，打个滴滴回家吧。 但是这个方案比较 low，谁都能干，我们来看看高大上一点的方案。 双写迁移方案这个是我们常用的一种迁移方案，比较靠谱一些，不用停机，不用看北京凌晨 4 点的风景。 简单来说，就是在线上系统里面，之前所有写库的地方，增删改操作，除了对老库增删改，都加上对新库的增删改，这就是所谓的双写，同时写俩库，老库和新库。 然后系统部署之后，新库数据差太远，用之前说的导数工具，跑起来读老库数据写新库，写的时候要根据 gmt_modified 这类字段判断这条数据最后修改的时间，除非是读出来的数据在新库里没有，或者是比新库的数据新才会写。简单来说，就是不允许用老数据覆盖新数据。 导完一轮之后，有可能数据还是存在不一致，那么就程序自动做一轮校验，比对新老库每个表的每条数据，接着如果有不一样的，就针对那些不一样的，从老库读数据再次写。反复循环，直到两个库每个表的数据都完全一致为止。 接着当数据完全一致了，就 ok 了，基于仅仅使用分库分表的最新代码，重新部署一次，不就仅仅基于分库分表在操作了么，还没有几个小时的停机时间，很稳。所以现在基本玩儿数据迁移之类的，都是这么干的。 如何设计可以动态扩容缩容的分库分表方案？对于分库分表来说，主要是面对以下问题： 选择一个数据库中间件，调研、学习、测试； 设计你的分库分表的一个方案，你要分成多少个库，每个库分成多少个表，比如 3 个库，每个库 4 个表； 基于选择好的数据库中间件，以及在测试环境建立好的分库分表的环境，然后测试一下能否正常进行分库分表的读写； 完成单库单表到分库分表的迁移，双写方案； 线上系统开始基于分库分表对外提供服务； 扩容了，扩容成 6 个库，每个库需要 12 个表，你怎么来增加更多库和表呢？ 这个是你必须面对的一个事儿，就是你已经弄好分库分表方案了，然后一堆库和表都建好了，基于分库分表中间件的代码开发啥的都好了，测试都 ok 了，数据能均匀分布到各个库和各个表里去，而且接着你还通过双写的方案咔嚓一下上了系统，已经直接基于分库分表方案在搞了。 那么现在问题来了，你现在这些库和表又支撑不住了，要继续扩容咋办？这个可能就是说你的每个库的容量又快满了，或者是你的表数据量又太大了，也可能是你每个库的写并发太高了，你得继续扩容。 这都是玩儿分库分表线上必须经历的事儿。 停机扩容（不推荐）这个方案就跟停机迁移一样，步骤几乎一致，唯一的一点就是那个导数的工具，是把现有库表的数据抽出来慢慢倒入到新的库和表里去。但是最好别这么玩儿，有点不太靠谱，因为既然分库分表就说明数据量实在是太大了，可能多达几亿条，甚至几十亿，你这么玩儿，可能会出问题。 从单库单表迁移到分库分表的时候，数据量并不是很大，单表最大也就两三千万。那么你写个工具，多弄几台机器并行跑，1小时数据就导完了。这没有问题。 如果 3 个库 + 12 个表，跑了一段时间了，数据量都 1~2 亿了。光是导 2 亿数据，都要导个几个小时，6 点，刚刚导完数据，还要搞后续的修改配置，重启系统，测试验证，10 点才可以搞完。所以不能这么搞。 优化后的方案一开始上来就是 32 个库，每个库 32 个表，那么总共是 1024 张表。 我可以告诉各位同学，这个分法，第一，基本上国内的互联网肯定都是够用了，第二，无论是并发支撑还是数据量支撑都没问题。 每个库正常承载的写入并发量是 1000，那么 32 个库就可以承载32 * 1000 = 32000 的写并发，如果每个库承载 1500 的写并发，32 * 1500 = 48000 的写并发，接近 5万/s 的写入并发，前面再加一个MQ，削峰，每秒写入 MQ 8 万条数据，每秒消费 5 万条数据。 有些除非是国内排名非常靠前的这些公司，他们的最核心的系统的数据库，可能会出现几百台数据库的这么一个规模，128个库，256个库，512个库。 1024 张表，假设每个表放 500 万数据，在 MySQL 里可以放 50 亿条数据。 每秒的 5 万写并发，总共 50 亿条数据，对于国内大部分的互联网公司来说，其实一般来说都够了。 谈分库分表的扩容，第一次分库分表，就一次性给他分个够，32 个库，1024 张表，可能对大部分的中小型互联网公司来说，已经可以支撑好几年了。 一个实践是利用 32 * 32 来分库分表，即分为 32 个库，每个库里一个表分为 32 张表。一共就是 1024 张表。根据某个 id 先根据 32 取模路由到库，再根据 32 取模路由到库里的表。 orderId id| % 32 (库)| id / 32 % 32 (表)||:—:|:—:|:—:|259| 3| 8|1189| 5| 5|352| 0| 11|4593| 17| 15| 刚开始的时候，这个库可能就是逻辑库，建在一个数据库上的，就是一个mysql服务器可能建了 n 个库，比如 32 个库。后面如果要拆分，就是不断在库和 mysql 服务器之间做迁移就可以了。然后系统配合改一下配置即可。 比如说最多可以扩展到32个数据库服务器，每个数据库服务器是一个库。如果还是不够？最多可以扩展到 1024 个数据库服务器，每个数据库服务器上面一个库一个表。因为最多是1024个表。 这么搞，是不用自己写代码做数据迁移的，都交给 dba 来搞好了，但是 dba 确实是需要做一些库表迁移的工作，但是总比你自己写代码，然后抽数据导数据来的效率高得多吧。 哪怕是要减少库的数量，也很简单，其实说白了就是按倍数缩容就可以了，然后修改一下路由规则。 这里对步骤做一个总结： 设定好几台数据库服务器，每台服务器上几个库，每个库多少个表，推荐是 32库 * 32表，对于大部分公司来说，可能几年都够了。 路由的规则，orderId 模 32 = 库，orderId / 32 模 32 = 表 扩容的时候，申请增加更多的数据库服务器，装好 mysql，呈倍数扩容，4 台服务器，扩到 8 台服务器，再到 16 台服务器。 由 dba 负责将原先数据库服务器的库，迁移到新的数据库服务器上去，库迁移是有一些便捷的工具的。 我们这边就是修改一下配置，调整迁移的库所在数据库服务器的地址。 重新发布系统，上线，原先的路由规则变都不用变，直接可以基于 n 倍的数据库服务器的资源，继续进行线上系统的提供服务。 分库分表之后，id 主键如何处理？其实这是分库分表之后你必然要面对的一个问题，就是 id 咋生成？因为要是分成多个表之后，每个表都是从 1 开始累加，那肯定不对啊，需要一个全局唯一的 id 来支持。所以这都是你实际生产环境中必须考虑的问题。 基于数据库的实现方案数据库自增 id这个就是说你的系统里每次得到一个 id，都是往一个库的一个表里插入一条没什么业务含义的数据，然后获取一个数据库自增的一个 id。拿到这个 id 之后再往对应的分库分表里去写入。 这个方案的好处就是方便简单，谁都会用；缺点就是单库生成自增 id，要是高并发的话，就会有瓶颈的；如果你硬是要改进一下，那么就专门开一个服务出来，这个服务每次就拿到当前 id 最大值，然后自己递增几个 id，一次性返回一批 id，然后再把当前最大 id 值修改成递增几个 id 之后的一个值；但是无论如何都是基于单个数据库。 适合的场景：你分库分表就俩原因，要不就是单库并发太高，要不就是单库数据量太大；除非是你并发不高，但是数据量太大导致的分库分表扩容，你可以用这个方案，因为可能每秒最高并发最多就几百，那么就走单独的一个库和表生成自增主键即可。 设置数据库 sequence 或者表自增字段步长可以通过设置数据库 sequence 或者表的自增字段步长来进行水平伸缩。 比如说，现在有 8 个服务节点，每个服务节点使用一个 sequence 功能来产生 ID，每个 sequence 的起始 ID 不同，并且依次递增，步长都是 8。 适合的场景：在用户防止产生的 ID 重复时，这种方案实现起来比较简单，也能达到性能目标。但是服务节点固定，步长也固定，将来如果还要增加服务节点，就不好搞了。 UUID好处就是本地生成，不要基于数据库来了；不好之处就是，UUID 太长了、占用空间大，作为主键性能太差了；更重要的是，UUID 不具有有序性，会导致 B+ 树索引在写的时候有过多的随机写操作（连续的 ID 可以产生部分顺序写），还有，由于在写的时候不能产生有顺序的 append 操作，而需要进行 insert 操作，将会读取整个 B+ 树节点到内存，在插入这条记录后会将整个节点写回磁盘，这种操作在记录占用空间比较大的情况下，性能下降明显。 适合的场景：如果你是要随机生成个什么文件名、编号之类的，你可以用 UUID，但是作为主键是不能用 UUID 的。 1UUID.randomUUID().toString().replace(“-”, “”) -&gt; sfsdf23423rr234sfdaf 获取系统当前时间这个就是获取当前时间即可，但是问题是，并发很高的时候，比如一秒并发几千，会有重复的情况，这个是肯定不合适的。基本就不用考虑了。 适合的场景：一般如果用这个方案，是将当前时间跟很多其他的业务字段拼接起来，作为一个 id，如果业务上你觉得可以接受，那么也是可以的。你可以将别的业务字段值跟当前时间拼接起来，组成一个全局唯一的编号。 snowflake 算法snowflake 算法是 twitter 开源的分布式 id 生成算法，采用 Scala 语言实现，是把一个 64 位的 long 型的 id，1 个 bit 是不用的，用其中的 41 bit 作为毫秒数，用 10 bit 作为工作机器 id，12 bit 作为序列号。 1 bit：不用，为啥呢？因为二进制里第一个 bit 为如果是 1，那么都是负数，但是我们生成的 id 都是正数，所以第一个 bit 统一都是 0。 41 bit：表示的是时间戳，单位是毫秒。41 bit 可以表示的数字多达 2^41 - 1，也就是可以标识 2^41 - 1 个毫秒值，换算成年就是表示69年的时间。 10 bit：记录工作机器 id，代表的是这个服务最多可以部署在 2^10台机器上哪，也就是1024台机器。但是 10 bit 里 5 个 bit 代表机房 id，5 个 bit 代表机器 id。意思就是最多代表 2^5个机房（32个机房），每个机房里可以代表 2^5 个机器（32台机器）。 12 bit：这个是用来记录同一个毫秒内产生的不同 id，12 bit 可以代表的最大正整数是 2^12 - 1 = 4096，也就是说可以用这个 12 bit 代表的数字来区分同一个毫秒内的 4096 个不同的 id。 10 | 0001100 10100010 10111110 10001001 01011100 00 | 10001 | 1 1001 | 0000 00000000 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110public class IdWorker &#123; private long workerId; private long datacenterId; private long sequence; public IdWorker(long workerId, long datacenterId, long sequence) &#123; // sanity check for workerId // 这儿不就检查了一下，要求就是你传递进来的机房id和机器id不能超过32，不能小于0 if (workerId &gt; maxWorkerId || workerId &lt; 0) &#123; throw new IllegalArgumentException( String.format(&quot;worker Id can&#x27;t be greater than %d or less than 0&quot;, maxWorkerId)); &#125; if (datacenterId &gt; maxDatacenterId || datacenterId &lt; 0) &#123; throw new IllegalArgumentException( String.format(&quot;datacenter Id can&#x27;t be greater than %d or less than 0&quot;, maxDatacenterId)); &#125; System.out.printf( &quot;worker starting. timestamp left shift %d, datacenter id bits %d, worker id bits %d, sequence bits %d, workerid %d&quot;, timestampLeftShift, datacenterIdBits, workerIdBits, sequenceBits, workerId); this.workerId = workerId; this.datacenterId = datacenterId; this.sequence = sequence; &#125; private long twepoch = 1288834974657L; private long workerIdBits = 5L; private long datacenterIdBits = 5L; // 这个是二进制运算，就是 5 bit最多只能有31个数字，也就是说机器id最多只能是32以内 private long maxWorkerId = -1L ^ (-1L &lt;&lt; workerIdBits); // 这个是一个意思，就是 5 bit最多只能有31个数字，机房id最多只能是32以内 private long maxDatacenterId = -1L ^ (-1L &lt;&lt; datacenterIdBits); private long sequenceBits = 12L; private long workerIdShift = sequenceBits; private long datacenterIdShift = sequenceBits + workerIdBits; private long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits; private long sequenceMask = -1L ^ (-1L &lt;&lt; sequenceBits); private long lastTimestamp = -1L; public long getWorkerId() &#123; return workerId; &#125; public long getDatacenterId() &#123; return datacenterId; &#125; public long getTimestamp() &#123; return System.currentTimeMillis(); &#125; public synchronized long nextId() &#123; // 这儿就是获取当前时间戳，单位是毫秒 long timestamp = timeGen(); if (timestamp &lt; lastTimestamp) &#123; System.err.printf(&quot;clock is moving backwards. Rejecting requests until %d.&quot;, lastTimestamp); throw new RuntimeException(String.format( &quot;Clock moved backwards. Refusing to generate id for %d milliseconds&quot;, lastTimestamp - timestamp)); &#125; if (lastTimestamp == timestamp) &#123; // 这个意思是说一个毫秒内最多只能有4096个数字 // 无论你传递多少进来，这个位运算保证始终就是在4096这个范围内，避免你自己传递个sequence超过了4096这个范围 sequence = (sequence + 1) &amp; sequenceMask; if (sequence == 0) &#123; timestamp = tilNextMillis(lastTimestamp); &#125; &#125; else &#123; sequence = 0; &#125; // 这儿记录一下最近一次生成id的时间戳，单位是毫秒 lastTimestamp = timestamp; // 这儿就是将时间戳左移，放到 41 bit那儿； // 将机房 id左移放到 5 bit那儿； // 将机器id左移放到5 bit那儿；将序号放最后12 bit； // 最后拼接起来成一个 64 bit的二进制数字，转换成 10 进制就是个 long 型 return ((timestamp - twepoch) &lt;&lt; timestampLeftShift) | (datacenterId &lt;&lt; datacenterIdShift) | (workerId &lt;&lt; workerIdShift) | sequence; &#125; private long tilNextMillis(long lastTimestamp) &#123; long timestamp = timeGen(); while (timestamp &lt;= lastTimestamp) &#123; timestamp = timeGen(); &#125; return timestamp; &#125; private long timeGen() &#123; return System.currentTimeMillis(); &#125; // ---------------测试--------------- public static void main(String[] args) &#123; IdWorker worker = new IdWorker(1, 1, 1); for (int i = 0; i &lt; 30; i++) &#123; System.out.println(worker.nextId()); &#125; &#125;&#125; 怎么说呢，大概这个意思吧，就是说 41 bit 是当前毫秒单位的一个时间戳，就这意思；然后 5 bit 是你传递进来的一个机房 id（但是最大只能是 32 以内），另外 5 bit 是你传递进来的机器 id（但是最大只能是 32 以内），剩下的那个 12 bit序列号，就是如果跟你上次生成 id 的时间还在一个毫秒内，那么会把顺序给你累加，最多在 4096 个序号以内。 所以你自己利用这个工具类，自己搞一个服务，然后对每个机房的每个机器都初始化这么一个东西，刚开始这个机房的这个机器的序号就是 0。然后每次接收到一个请求，说这个机房的这个机器要生成一个 id，你就找到对应的 Worker 生成。 利用这个 snowflake 算法，你可以开发自己公司的服务，甚至对于机房 id 和机器 id，反正给你预留了 5 bit + 5 bit，你换成别的有业务含义的东西也可以的。 这个 snowflake 算法相对来说还是比较靠谱的，所以你要真是搞分布式 id 生成，如果是高并发啥的，那么用这个应该性能比较好，一般每秒几万并发的场景，也足够你用了。 参考高并发下数据库分库分表面试题整理","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/tags/%E5%B9%B6%E5%8F%91/"},{"name":"分库分表","slug":"分库分表","permalink":"https://xmmarlowe.github.io/tags/%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8/"}],"author":"Marlowe"},{"title":"2021-5-18更新日志","slug":"个人博客/2021-5-18更新日志","date":"2021-05-18T13:51:22.000Z","updated":"2021-05-18T14:38:04.418Z","comments":true,"path":"2021/05/18/个人博客/2021-5-18更新日志/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/18/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/2021-5-18%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97/","excerpt":"记录2021-5-18博客更新日志…","text":"记录2021-5-18博客更新日志… 更新头像更新上一版头像： 现役头像： 个人主页更新上一版主页： 现役主页： 添加友链功能具体操作方式可参考官方文档：友链页面 效果图： 评论通知功能保姆级教程推荐：Valine Admin 好文推荐：最佳评论系统Valine+Valine-Admin简洁且带邮件通知 测试： 收到邮件提示 修复valine 评论系统失效 解决办法： 登录 LeanCloud 点击恢复按钮，稍等即可 删除删除侧边栏标签由于标签太多,影响侧边栏美观，若想查看标签，可直接在导航栏中寻找。 修改前 修改后 直接修改配置文件即可： 123sidebar: # 主页、分类、归档等独立页面 侧边栏标签配置：tagcloud(已移除)、dnate(二维码) for_page: [blogger, category, webinfo]","categories":[{"name":"个人博客","slug":"个人博客","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"}],"tags":[{"name":"博客","slug":"博客","permalink":"https://xmmarlowe.github.io/tags/%E5%8D%9A%E5%AE%A2/"}],"author":"Marlowe"},{"title":"Java内存区域","slug":"Java/Java内存区域","date":"2021-05-18T02:38:54.000Z","updated":"2021-05-18T06:58:53.440Z","comments":true,"path":"2021/05/18/Java/Java内存区域/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/18/Java/Java%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F/","excerpt":"对于 Java 程序员来说，在虚拟机自动内存管理机制下，不再需要像 C/C++程序开发程序员这样为每一个 new 操作去写对应的 delete/free 操作，不容易出现内存泄漏和内存溢出问题。正是因为 Java 程序员把内存控制权利交给 Java 虚拟机，一旦出现内存泄漏和溢出方面的问题，如果不了解虚拟机是怎样使用内存的，那么排查错误将会是一个非常艰巨的任务。","text":"对于 Java 程序员来说，在虚拟机自动内存管理机制下，不再需要像 C/C++程序开发程序员这样为每一个 new 操作去写对应的 delete/free 操作，不容易出现内存泄漏和内存溢出问题。正是因为 Java 程序员把内存控制权利交给 Java 虚拟机，一旦出现内存泄漏和溢出方面的问题，如果不了解虚拟机是怎样使用内存的，那么排查错误将会是一个非常艰巨的任务。 运行时数据区域Java 虚拟机在执行 Java 程序的过程中会把它管理的内存划分成若干个不同的数据区域。JDK. 1.8 和之前的版本略有不同，下面会介绍到。 JDK 1.8 之前： JDK 1.8 ： 线程私有的： 程序计数器 虚拟机栈 本地方法栈 线程共享的： 堆 方法区 直接内存 (非运行时数据区的一部分) 程序计数器程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完成。 另外，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 从上面的介绍中我们知道程序计数器主要有两个作用： 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 注意：程序计数器是唯一一个不会出现 OutOfMemoryError 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。 Java 虚拟机栈与程序计数器一样，Java 虚拟机栈也是线程私有的，它的生命周期和线程相同，描述的是 Java 方法执行的内存模型，每次方法调用的数据都是通过栈传递的。 Java 内存可以粗糙的区分为堆内存（Heap）和栈内存 (Stack),其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。 （实际上，Java 虚拟机栈是由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法出口信息。） 局部变量表主要存放了编译期可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference 类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。 Java 虚拟机栈会出现两种错误：StackOverFlowError 和 OutOfMemoryError。 StackOverFlowError： 若 Java 虚拟机栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度的时候，就抛出 StackOverFlowError 错误。 OutOfMemoryError： Java 虚拟机栈的内存大小可以动态扩展， 如果虚拟机在动态扩展栈时无法申请到足够的内存空间，则抛出OutOfMemoryError异常异常。 Java 虚拟机栈也是线程私有的，每个线程都有各自的 Java 虚拟机栈，而且随着线程的创建而创建，随着线程的死亡而死亡。 扩展：那么方法/函数如何调用？ Java 栈可用类比数据结构中栈，Java 栈中保存的主要内容是栈帧，每一次函数调用都会有一个对应的栈帧被压入 Java 栈，每一个函数调用结束后，都会有一个栈帧被弹出。 Java 方法有两种返回方式： return 语句。 抛出异常。 不管哪种返回方式都会导致栈帧被弹出。 本地方法栈和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。 方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现 StackOverFlowError 和 OutOfMemoryError 两种错误。 堆Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。 Java 世界中“几乎”所有的对象都在堆中分配，但是，随着 JIT 编译期的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换优化技术将会导致一些微妙的变化，所有的对象都分配到堆上也渐渐变得不那么“绝对”了。从 JDK 1.7 开始已经默认开启逃逸分析，如果某些方法中的对象引用没有被返回或者未被外面使用（也就是未逃逸出去），那么对象可以直接在栈上分配内存。 Java 堆是垃圾收集器管理的主要区域，因此也被称作GC 堆（Garbage Collected Heap）.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代：再细致一点有：Eden 空间、From Survivor、To Survivor 空间等。进一步划分的目的是更好地回收内存，或者更快地分配内存。 在 JDK 7 版本及 JDK 7 版本之前，堆内存被通常被分为下面三部分： 新生代内存(Young Generation) 老生代(Old Generation) 永生代(Permanent Generation) JDK 8 版本之后方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。 上图所示的 Eden 区、两个 Survivor 区都属于新生代（为了区分，这两个 Survivor 区域按照顺序被命名为 from 和 to），中间一层属于老年代。 Hotspot 遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了 survivor 区的一半时，取这个年龄和 MaxTenuringThreshold 中更小的一个值，作为新的晋升年龄阈值。 动态年龄计算的代码如下： 12345678910111213uint ageTable::compute_tenuring_threshold(size_t survivor_capacity) &#123; //survivor_capacity是survivor空间的大小size_t desired_survivor_size = (size_t)((((double) survivor_capacity)*TargetSurvivorRatio)/100);size_t total = 0;uint age = 1;while (age &lt; table_size) &#123;total += sizes[age];//sizes数组是每个年龄段对象大小if (total &gt; desired_survivor_size) break;age++;&#125;uint result = age &lt; MaxTenuringThreshold ? age : MaxTenuringThreshold; ...&#125; 堆这里最容易出现的就是 OutOfMemoryError 错误，并且出现这种错误之后的表现形式还会有几种，比如： OutOfMemoryError: GC Overhead Limit Exceeded： 当 JVM 花太多时间执行垃圾回收并且只能回收很少的堆空间时，就会发生此错误。 java.lang.OutOfMemoryError: Java heap space: 假如在创建新的对象时, 堆内存中的空间不足以存放新创建的对象, 就会引发java.lang.OutOfMemoryError: Java heap space 错误。(和本机物理内存无关，和你配置的内存大小有关！) …… 方法区方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然 Java 虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。 方法区也被称为永久代。很多人都会分不清方法区和永久代的关系，为此我也查阅了文献。 方法区和永久代的关系《Java 虚拟机规范》只是规定了有方法区这么个概念和它的作用，并没有规定如何去实现它。那么，在不同的 JVM 上方法区的实现肯定是不同的了。 方法区和永久代的关系很像 Java 中接口和类的关系，类实现了接口，而永久代就是 HotSpot 虚拟机对虚拟机规范中方法区的一种实现方式。 也就是说，永久代是 HotSpot 的概念，方法区是 Java 虚拟机规范中的定义，是一种规范，而永久代是一种实现，一个是标准一个是实现，其他的虚拟机实现并没有永久代这一说法。 常用参数JDK 1.8 之前永久代还没被彻底移除的时候通常通过下面这些参数来调节方法区大小 12-XX:PermSize=N //方法区 (永久代) 初始大小-XX:MaxPermSize=N //方法区 (永久代) 最大大小,超过这个值将会抛出 OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGen 相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。 JDK 1.8 的时候，方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。 下面是一些常用参数： 12-XX:MetaspaceSize=N //设置 Metaspace 的初始（和最小大小）-XX:MaxMetaspaceSize=N //设置 Metaspace 的最大大小 与永久代很大的不同就是，如果不指定大小的话，随着更多类的创建，虚拟机会耗尽所有可用的系统内存。 为什么要将永久代 (PermGen) 替换为元空间 (MetaSpace) 呢?下图来自《深入理解 Java 虚拟机》第 3 版 2.2.5 整个永久代有一个 JVM 本身设置的固定大小上限，无法进行调整，而元空间使用的是直接内存，受本机可用内存的限制，虽然元空间仍旧可能溢出，但是比原来出现的几率会更小。 当元空间溢出时会得到如下错误： java.lang.OutOfMemoryError: MetaSpace 你可以使用 -XX：MaxMetaspaceSize 标志设置最大元空间大小，默认值为 unlimited，这意味着它只受系统内存的限制。-XX：MetaspaceSize 调整标志定义元空间的初始大小如果未指定此标志，则 Metaspace 将根据运行时的应用程序需求动态地重新调整大小。 元空间里面存放的是类的元数据，这样加载多少类的元数据就不由 MaxPermSize 控制了, 而由系统的实际可用空间来控制，这样能加载的类就更多了。 在 JDK8，合并 HotSpot 和 JRockit 的代码时, JRockit 从来没有一个叫永久代的东西, 合并之后就没有必要额外的设置这么一个永久代的地方了。 运行时常量池运行时常量池是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池表（用于存放编译期生成的各种字面量和符号引用） 既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 错误。 JDK1.7 之前运行时常量池逻辑包含字符串常量池存放在方法区, 此时 hotspot 虚拟机对方法区的实现为永久代 JDK1.7 字符串常量池被从方法区拿到了堆中, 这里没有提到运行时常量池,也就是说字符串常量池被单独拿到堆,运行时常量池剩下的东西还在方法区, 也就是 hotspot 中的永久代 。 JDK1.8 hotspot 移除了永久代用元空间(Metaspace)取而代之, 这时候字符串常量池还在堆, 运行时常量池还在方法区, 只不过方法区的实现从永久代变成了元空间(Metaspace)。 直接内存直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致 OutOfMemoryError 错误出现。 JDK1.4 中新加入的 NIO(New Input/Output) 类，引入了一种基于通道（Channel） 与缓存区（Buffer） 的 I/O 方式，它可以直接使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆之间来回复制数据。 本机直接内存的分配不会受到 Java 堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制。 参考Java 内存区域详解","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"内存","slug":"内存","permalink":"https://xmmarlowe.github.io/tags/%E5%86%85%E5%AD%98/"}],"author":"Marlowe"},{"title":"7种阻塞队列","slug":"并发/7种阻塞队列","date":"2021-05-17T13:19:15.000Z","updated":"2021-05-21T13:29:38.338Z","comments":true,"path":"2021/05/17/并发/7种阻塞队列/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/17/%E5%B9%B6%E5%8F%91/7%E7%A7%8D%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97/","excerpt":"队列：FIFO(先进先出)的数据结构即为队列阻塞队列：操作会被阻塞的队列即为阻塞队列, 在java中 BlockingQueue 接口在 Queue 接口的基础上增加了两组阻塞方法, offer(e,time) put , poll(time) take()","text":"队列：FIFO(先进先出)的数据结构即为队列阻塞队列：操作会被阻塞的队列即为阻塞队列, 在java中 BlockingQueue 接口在 Queue 接口的基础上增加了两组阻塞方法, offer(e,time) put , poll(time) take() 7种阻塞队列 有界: 在创建队列时必须或允许指定队列大小, 允许调用抛出异常的 add 方法 无界: 在创建队列时无需或不可以指定队列大小, 无限制的插入 add = offer 操作 阻塞队列的几个操作方法 抛出异常 特殊值 阻塞 超时 插入 add(e) offer(e) put(e) offer(e,time,unit) 移除 remove() poll() take() poll(time,unit) 检查 element() peek() 不可用 不可用 1. ArrayBlockingQueue[有界]一个使用数组实现的有界阻塞队列. 创建队列时必须给定队列大小, 同时可以通过创建队列的时候设置公平访问(通过重入锁的公平访问实现) 12345678public ArrayBlockingQueue(int capacity, boolean fair) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.items = new Object[capacity]; lock = new ReentrantLock(fair); notEmpty = lock.newCondition(); notFull = lock.newCondition(); &#125; 元素达到队列容量上限时再入队根据调用不同的方法, 返回不同的状态 add 方法将抛出异常 123456public boolean add(E e) &#123; if (offer(e)) return true; else throw new IllegalStateException(&quot;Queue full&quot;); &#125; offer 方法将返回 false 123456789101112131415public boolean offer(E e) &#123; checkNotNull(e); final ReentrantLock lock = this.lock; lock.lock(); try &#123; if (count == items.length) return false; else &#123; enqueue(e); return true; &#125; &#125; finally &#123; lock.unlock(); &#125; &#125; put 方法将无限时阻塞 123456789101112public void put(E e) throws InterruptedException &#123; checkNotNull(e); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; while (count == items.length) notFull.await(); enqueue(e); &#125; finally &#123; lock.unlock(); &#125; &#125; offer(E e, long timeout, TimeUnit unit) 将超时阻塞 12345678910111213141516171819public boolean offer(E e, long timeout, TimeUnit unit) throws InterruptedException &#123; checkNotNull(e); long nanos = unit.toNanos(timeout); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; while (count == items.length) &#123; if (nanos &lt;= 0) return false; nanos = notFull.awaitNanos(nanos); &#125; enqueue(e); return true; &#125; finally &#123; lock.unlock(); &#125; &#125; 队列为空时获取元素时poll 方法返回空 123456789public E poll() &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; return (count == 0) ? null : dequeue(); &#125; finally &#123; lock.unlock(); &#125;&#125; take 将无限时阻塞 1234567891011public E take() throws InterruptedException &#123; final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; while (count == 0) notEmpty.await(); return dequeue(); &#125; finally &#123; lock.unlock(); &#125;&#125; poll(long timeout, TimeUnit unit) 超时阻塞 123456789101112131415public E poll(long timeout, TimeUnit unit) throws InterruptedException &#123; long nanos = unit.toNanos(timeout); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; while (count == 0) &#123; if (nanos &lt;= 0) return null; nanos = notEmpty.awaitNanos(nanos); &#125; return dequeue(); &#125; finally &#123; lock.unlock(); &#125;&#125; 总结 数组有界队列 可加入公平策略 插入时提供了可抛出异常操作 插入元素不能为空 该队列模式适合在需要公平访问的场景下使用, 若无公平性要求该队列个人拙见不建议使用, 因操作数组和公平性原因,其吞吐量较低 2. LinkedBlockingQueue[有界]一个使用链表实现的有界队列 12345public LinkedBlockingQueue(int capacity) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.capacity = capacity; last = head = new Node&lt;E&gt;(null);&#125; 如果不指定大小, 默认值为 int 的最大值 123public LinkedBlockingQueue() &#123; this(Integer.MAX_VALUE);&#125; 元素达到队列容量上限时再入队队列为空时获取元素时与 [ArrayBlockingQueue](#1. ArrayBlockingQueue[有界]) 相同 总结: 结论不指定队列大小, 默认值为 int 最大值 吞吐量要比ArrayBlockingQueue高 链表有界队列 不可加入公平策略 插入时提供了可抛出异常操作 插入元素不能为空 3. LinkedBlockingDeque[有界]通过链表实现的一个双端阻塞队列(LikedBlockingQueue增加了队尾的操作) 该队列增加了一组队首队尾的操作方法 12345678910111213add -&gt; addFirst(push)/addLastoffer -&gt; offerFirst/offerLastoffer(time) -&gt;offerFirst(time)/offerLast(time)peek -&gt; peekFirst/peekLastpoll -&gt; pollFirst/pollLastpoll(time) -&gt; pollFirst(time)/pollLast(time)put -&gt; putFrist/putLast 增加类(add/put/offer) 原方法调用与其调用相同前缀last方法操作相同 例: add = addLast ; put = putLast 获取类(peek/poll/element) 原方法调用与其调用相同前缀first方法操作相同 例: peek = peekFirst; poll = peekFirst 元素达到队列容量上限时再入队add 方法在队列容量达到最大值时抛出异常 throw new IllegalStateException(“Deque full”); 队列为空时获取元素时element/getFirst/getLast 方法在队列为空时抛出异常 if (x == null) throw new NoSuchElementException(); 总结: 如果创建队列时不指定队列大小, 默认值为 int 最大值 吞吐量要比LinkedBlockingQueue高 链表有界双端队列 不可加入公平策略 插入时提供了可抛出异常操作 插入元素不能为空 可以通过队首队尾插入或取出元素 4. LinkedTransferQueue[无界]一个由链表实现的无界转换队列, 相对 [LinkedBlockingQueue](#2. LinkedBlockingQueue[有界]) 增加了几个方法 1.transfer(E e) 等待消费者调用返回 向队列的调用阻塞者直接提供元素, 如果没有人来获取, 则将这个元素放入队尾, 当这个元素出队的时候返回, 否则一直阻塞 2.tryTransfer(E e) 调用一次即返回 尝试向队列的调用阻塞者直接提供元素, 立即返回false or true, 提供的元素不入队. 3.tryTransfer(E e, long timeout, TimeUnit unit) 等待消费者调用返回, 一定时间内等不到亦返回 在 tryTransfer 的基础上加入了时间, 在给定时间内尝试 如果有阻塞调用者直接调用该队列的take 或者 poll(time) 方法, 阻塞状态下返回该值 如果未有阻塞调用者调用, 将元素放入队尾, 当在给定时间内被调用 返回 true, 如果在给定时间内未被调用, 返回false 且元素从队列中移除. 总结: 创建时无需指定队列大小, 且无最大值即无阻塞插入知道内存溢出 吞吐量要比LinkedBlockingQueue高 链表无界队列 在调用队列元素被阻塞时, 提供了可以将入队元素直接返回的 transfer方法 插入元素不能为空 5. PriorityBlockingQueue[无界]一个使用数组 + 比较器实现的优先级队列 这个队列使用了二叉堆排序的方式来实现优先级 关于这个队列的重点内容也是在二叉堆排序上, 这里延伸的内容还是比较多的, 堆结构, 二叉堆, 堆排序, 选择排序… 总结: 如果创建队列时不指定队列大小, 默认值为 11, 超出时不会阻塞而是扩容(当扩容超过 int 最大值 - 8 时将抛出堆内存溢出异常) 每次扩容为当前队列大小的 50% 数组无界队列(最大长度 int最大值 - 8) 如果指定了比较器, 则必须指定大小 插入元素不能为空 6. DelayQueue[无界]使用 PriorityQueue 实现的一个无界延迟队列, 使用这个队列需要自己实现一些内容, 包括延迟配置、比较器的实现。该队列可以用于定时任务调度，周期任务调度 当你需要指定元素的优先级，执行的时机，那这个队列即是不二之选。 总结: 元素存储使用的 priorityqueue 可以指定元素的访问延迟时间及优先级 插入元素不能为空 7. SynchronousQueue[无容量]dual queue + dual stack 双队列 + 双栈实现 这个队列也是一个比较特殊的队列, 在 JDK 1.6的时候改写了底层实现, 就是用了上面提到的方法. 这个队列是一个没有容量的队列，所以在调用方法上有一些不同。 add 方法将会抛出一个 java.lang.IllegalStateException: Queue full异常 offer 方法会返回false put 方法将会被阻塞 调用出队方法也会有一些问题 poll 方法返回null take 方法将被阻塞 同步执行入队和出队即可, 这也是为什么该队列是吞吐量最高的队列原因 总结: 没有容量 吞吐量要比ArrayBlockingQueue与LinkedBlockingQueue高 可加入公平策略 插入时提供了可抛出异常操作 插入元素不能为空 8、一些问题线程池中为什么要使用阻塞队列？在线程池中活跃线程数达到corePoolSize时，线程池将会将后续的task提交到BlockingQueue中，为什么这样设计呢？ 在一个task提交到线程池时，假设可以被线程池中的一个线程执行，则进行以下过程： exeute —&gt; addWorker（Runnable command， boolean core）—&gt; workers.add（w），启动线程执行任务（获取全局锁ReentrantLock mainLock） 具体源码如下： 1234567891011121314151617181920212223242526272829public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); int c = ctl.get(); //如果当前正在运行的线程数小于corePoolSize，则创建新的线程 //执行当前任务 if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; //如果当前运行的线程数大于等于corePoolSize或者线程创建失败 //则把当前任务放入工作队列 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); //判断之前是否已经添加过线程执行该任务（因为可能之前） //创建的线程已经死亡了）或者线程池是否已经关闭。如果 //两个答案都是肯定的，那么选择拒绝执行任务 if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; //如果线程池任务无法加入到工作队列（说明工作队列满了） //创建一个线程执行任务。如果新创建后当前运行的线程数大于 //maximumPoolSize则拒绝执行任务 else if (!addWorker(command, false)) reject(command); &#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243private boolean addWorker(Runnable firstTask, boolean core)&#123; //省略部分代码 boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; //这里就将提交的任务封装成为Worker了 w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; //使用加锁的方式原子添加工作线程 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; //在获得锁期间再次检查线程池的运行状态：如果 //线程池已经关闭或者任务为空则抛出异常 int rs = runStateOf(ctl.get()); if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) throw new IllegalThreadStateException(); //加入Worker数组 workers.add(w); int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; //如果添加成功则启动线程执行任务 t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted; &#125; 上述代码中： 12w = new Worker(firstTask);final Thread t = w.thread; Worker实现了Runnable接口，里面定义了一个final变量Thread threadWorker的构造函数为： 12345Worker(Runnable firstTask) &#123; setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this);&#125; run函数为： 123public void run() &#123; runWorker(this);&#125; 12345678910111213141516171819202122232425final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // allow interrupts boolean completedAbruptly = true; try &#123; while (task != null || (task = getTask()) != null) &#123; w.lock(); try &#123; beforeExecute(wt, task); Throwable thrown = null； task.run(); afterExecute(task, thrown); &#125; finally &#123; task = null; w.completedTasks++; w.unlock(); &#125; &#125; completedAbruptly = false; &#125; finally &#123; processWorkerExit(w, completedAbruptly); &#125; &#125; 看完上述代码后，我们可以得出： 线程池创建线程需要获取mainlock这个全局锁，影响并发效率，阻塞队列可以很好的缓冲。 另外一方面，如果新任务的到达速率超过了线程池的处理速率，那么新到来的请求将累加起来，这样的话将耗尽资源。 阻塞队列的特点阻塞队列区别于其他类型的队列的最主要的特点就是“阻塞”这两个字，阻塞功能使得生产者和消费者两端的能力得以平衡，当有任何一端速度过快时，阻塞队列便会把过快的速度给降下来。实现阻塞最重要的两个方法是 take 方法和 put 方法 take 方法take 方法的功能是获取并移除队列的头结点，通常在队列里有数据的时候是可以正常移除的。可是一旦执行 take 方法的时候，队列里无数据，则阻塞，直到队列里有数据。 一旦队列里有数据了，就会立刻解除阻塞状态，并且取到数据。过程如图所示: put 方法put 方法插入元素时，如果队列没有满，那就和普通的插入一样是正常的插入，但是如果队列已满，那么就无法继续插入，则阻塞，直到队列里有了空闲空间。如果后续队列有了空闲空间，比如消费者消费了一个元素，那么此时队列就会解除阻塞状态，并把需要添加的数据添加到队列中。过程如图所示: 是否有界（容量有多大）阻塞队列还有一个非常重要的属性，那就是容量的大小，分为有界和无界两种 无界队列意味着里面可以容纳非常多的元素，例如 LinkedBlockingQueue 的上限是 Integer.MAX_VALUE，约为 2 的 31 次方，是非常大的一个数，可以近似认为是无限容量，因为我们几乎无法把这个容量装满 有的阻塞队列是有界的，例如 ArrayBlockingQueue 如果容量满了，也不会扩容，所以一旦满了就无法再往里放数据了。 参考7种阻塞队列 线程池中为什么要使用阻塞队列？ 什么是阻塞队列？","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"队列","slug":"队列","permalink":"https://xmmarlowe.github.io/tags/%E9%98%9F%E5%88%97/"},{"name":"阻塞","slug":"阻塞","permalink":"https://xmmarlowe.github.io/tags/%E9%98%BB%E5%A1%9E/"}],"author":"Marlowe"},{"title":"简述ConcurrentLinkedQueue原理","slug":"Java/简述ConcurrentLinkedQueue原理","date":"2021-05-17T09:07:42.000Z","updated":"2021-05-17T13:41:50.634Z","comments":true,"path":"2021/05/17/Java/简述ConcurrentLinkedQueue原理/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/17/Java/%E7%AE%80%E8%BF%B0ConcurrentLinkedQueue%E5%8E%9F%E7%90%86/","excerpt":"","text":"简介在并发编程中我们有时候需要使用线程安全的队列。如果我们要实现一个线程安全的队列有两种实现方式一种是使用阻塞算法，另一种是使用非阻塞算法。使用阻塞算法的队列可以用一个锁（入队和出队用同一把锁）或两个锁（入队和出队用不同的锁）等方式来实现，而非阻塞的实现方式则可以使用循环CAS的方式来实现，下面我们一起来研究下Doug Lea是如何使用非阻塞的方式来实现线程安全队列ConcurrentLinkedQueue的。 ConcurrentLinkedQueue是一个基于链接节点的无界线程安全队列，它采用先进先出的规则对节点进行排序，当我们添加一个元素的时候，它会添加到队列的尾部，当我们获取一个元素时，它会返回队列头部的元素。它采用了“wait－free”算法来实现，该算法在Michael &amp; Scott算法上进行了一些修改。 ConcurrentLinkedQueue的类图如下： ConcurrentLinkedQueue由head节点和tail节点组成，每个节点（Node）由节点元素（item）和指向下一个节点的引用(next)组成，节点与节点之间就是通过这个next关联起来，从而组成一张链表结构的队列。 ConcurrentLinkedQueue源码详解我们前面介绍了，ConcurrentLinkedQueue的节点都是Node类型的： 12345678910111213141516171819202122232425262728293031323334353637private static class Node&lt;E&gt; &#123; volatile E item; volatile Node&lt;E&gt; next; Node(E item) &#123; UNSAFE.putObject(this, itemOffset, item); &#125; boolean casItem(E cmp, E val) &#123; return UNSAFE.compareAndSwapObject(this, itemOffset, cmp, val); &#125; void lazySetNext(Node&lt;E&gt; val) &#123; UNSAFE.putOrderedObject(this, nextOffset, val); &#125; boolean casNext(Node&lt;E&gt; cmp, Node&lt;E&gt; val) &#123; return UNSAFE.compareAndSwapObject(this, nextOffset, cmp, val); &#125; private static final sun.misc.Unsafe UNSAFE; private static final long itemOffset; private static final long nextOffset; static &#123; try &#123; UNSAFE = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = Node.class; itemOffset = UNSAFE.objectFieldOffset (k.getDeclaredField(&quot;item&quot;)); nextOffset = UNSAFE.objectFieldOffset (k.getDeclaredField(&quot;next&quot;)); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125;&#125; Node类也比较简单，不再解释，ConcurrentLinkedQueue类有下面两个构造方法： 12345678910111213141516171819202122232425// 默认构造方法，head节点存储的元素为空，tail节点等于head节点public ConcurrentLinkedQueue() &#123; head = tail = new Node&lt;E&gt;(null);&#125; // 根据其他集合来创建队列public ConcurrentLinkedQueue(Collection&lt;? extends E&gt; c) &#123; Node&lt;E&gt; h = null, t = null; // 遍历节点 for (E e : c) &#123; // 若节点为null，则直接抛出NullPointerException异常 checkNotNull(e); Node&lt;E&gt; newNode = new Node&lt;E&gt;(e); if (h == null) h = t = newNode; else &#123; t.lazySetNext(newNode); t = newNode; &#125; &#125; if (h == null) h = t = new Node&lt;E&gt;(null); head = h; tail = t;&#125; 默认情况下head节点存储的元素为空，tail节点等于head节点。 1head = tail = new Node&lt;E&gt;(null); 下面我们主要来看一下ConcurrentLinkedQueue的入队与出队操作。 入队操作入队列就是将入队节点添加到队列的尾部。为了方便理解入队时队列的变化，以及head节点和tail节点的变化，每添加一个节点我就做了一个队列的快照图： 上图所示的元素添加过程如下： 添加元素1： 队列更新head节点的next节点为元素1节点。又因为tail节点默认情况下等于head节点，所以它们的next节点都指向元素1节点。 添加元素2： 队列首先设置元素1节点的next节点为元素2节点，然后更新tail节点指向元素2节点。 添加元素3： 设置tail节点的next节点为元素3节点。 添加元素4： 设置元素3的next节点为元素4节点，然后将tail节点指向元素4节点。入队操作主要做两件事情，第一是将入队节点设置成当前队列尾节点的下一个节点。第二是更新tail节点，如果tail节点的next节点不为空，则将入队节点设置成tail节点，如果tail节点的next节点为空，则将入队节点设置成tail的next节点，所以tail节点不总是尾节点，理解这一点很重要。 上面的分析让我们从单线程入队的角度来理解入队过程，但是多个线程同时进行入队情况就变得更加复杂，因为可能会出现其他线程插队的情况。如果有一个线程正在入队，那么它必须先获取尾节点，然后设置尾节点的下一个节点为入队节点，但这时可能有另外一个线程插队了，那么队列的尾节点就会发生变化，这时当前线程要暂停入队操作，然后重新获取尾节点。 下面我们来看ConcurrentLinkedQueue的add(E e)入队方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public boolean add(E e) &#123; return offer(e);&#125; public boolean offer(E e) &#123; // 如果e为null，则直接抛出NullPointerException异常 checkNotNull(e); // 创建入队节点 final Node&lt;E&gt; newNode = new Node&lt;E&gt;(e); // 循环CAS直到入队成功 // 1、根据tail节点定位出尾节点（last node）；2、将新节点置为尾节点的下一个节点；3、casTail更新尾节点 for (Node&lt;E&gt; t = tail, p = t;;) &#123; // p用来表示队列的尾节点，初始情况下等于tail节点 // q是p的next节点 Node&lt;E&gt; q = p.next; // 判断p是不是尾节点，tail节点不一定是尾节点，判断是不是尾节点的依据是该节点的next是不是null // 如果p是尾节点 if (q == null) &#123; // p is last node // 设置p节点的下一个节点为新节点，设置成功则casNext返回true；否则返回false，说明有其他线程更新过尾节点 if (p.casNext(null, newNode)) &#123; // Successful CAS is the linearization point // for e to become an element of this queue, // and for newNode to become &quot;live&quot;. // 如果p != t，则将入队节点设置成tail节点，更新失败了也没关系，因为失败了表示有其他线程成功更新了tail节点 if (p != t) // hop two nodes at a time casTail(t, newNode); // Failure is OK. return true; &#125; // Lost CAS race to another thread; re-read next &#125; // 多线程操作时候，由于poll时候会把旧的head变为自引用，然后将head的next设置为新的head // 所以这里需要重新找新的head，因为新的head后面的节点才是激活的节点 else if (p == q) // We have fallen off list. If tail is unchanged, it // will also be off-list, in which case we need to // jump to head, from which all live nodes are always // reachable. Else the new tail is a better bet. p = (t != (t = tail)) ? t : head; // 寻找尾节点 else // Check for tail updates after two hops. p = (p != t &amp;&amp; t != (t = tail)) ? t : q; &#125;&#125; 从源代码角度来看整个入队过程主要做两件事情： 第一是定位出尾节点 第二是使用CAS算法能将入队节点设置成尾节点的next节点，如不成功则重试。 第一步定位尾节点。tail节点并不总是尾节点，所以每次入队都必须先通过tail节点来找到尾节点，尾节点可能就是tail节点，也可能是tail节点的next节点。代码中循环体中的第一个if就是判断tail是否有next节点，有则表示next节点可能是尾节点。获取tail节点的next节点需要注意的是p节点等于q节点的情况，出现这种情况的原因我们后续再来介绍。 第二步设置入队节点为尾节点。p.casNext(null, newNode)方法用于将入队节点设置为当前队列尾节点的next节点，q如果是null表示p是当前队列的尾节点，如果不为null表示有其他线程更新了尾节点，则需要重新获取当前队列的尾节点。 tail节点不一定为尾节点的设计意图对于先进先出的队列入队所要做的事情就是将入队节点设置成尾节点，doug lea写的代码和逻辑还是稍微有点复杂。那么我用以下方式来实现行不行？ 123456789101112public boolean offer(E e) &#123; checkNotNull(e); final Node&lt;E&gt; newNode = new Node&lt;E&gt;(e); for (;;) &#123; Node&lt;E&gt; t = tail; if (t.casNext(null ,newNode) &amp;&amp; casTail(t, newNode)) &#123; return true; &#125; &#125;&#125; 让tail节点永远作为队列的尾节点，这样实现代码量非常少，而且逻辑非常清楚和易懂。但是这么做有个缺点就是每次都需要使用循环CAS更新tail节点。如果能减少CAS更新tail节点的次数，就能提高入队的效率。 在JDK 1.7的实现中，doug lea使用hops变量来控制并减少tail节点的更新频率，并不是每次节点入队后都将 tail节点更新成尾节点，而是当tail节点和尾节点的距离大于等于常量HOPS的值（默认等于1）时才更新tail节点，tail和尾节点的距离越长使用CAS更新tail节点的次数就会越少，但是距离越长带来的负面效果就是每次入队时定位尾节点的时间就越长，因为循环体需要多循环一次来定位出尾节点，但是这样仍然能提高入队的效率，因为从本质上来看它通过增加对volatile变量的读操作来减少了对volatile变量的写操作，而对volatile变量的写操作开销要远远大于读操作，所以入队效率会有所提升。 在JDK 1.8的实现中，tail的更新时机是通过p和t是否相等来判断的，其实现结果和JDK 1.7相同，即当tail节点和尾节点的距离大于等于1时，更新tail。 ConcurrentLinkedQueue的入队操作整体逻辑如下图所示： 出队操作出队列的就是从队列里返回一个节点元素，并清空该节点对元素的引用。让我们通过每个节点出队的快照来观察下head节点的变化： 从上图可知，并不是每次出队时都更新head节点，当head节点里有元素时，直接弹出head节点里的元素，而不会更新head节点。只有当head节点里没有元素时，出队操作才会更新head节点。采用这种方式也是为了减少使用CAS更新head节点的消耗，从而提高出队效率。让我们再通过源码来深入分析下出队过程。 1234567891011121314151617181920212223242526272829303132public E poll() &#123; restartFromHead: for (;;) &#123; // p节点表示首节点，即需要出队的节点 for (Node&lt;E&gt; h = head, p = h, q;;) &#123; E item = p.item; // 如果p节点的元素不为null，则通过CAS来设置p节点引用的元素为null，如果成功则返回p节点的元素 if (item != null &amp;&amp; p.casItem(item, null)) &#123; // Successful CAS is the linearization point // for item to be removed from this queue. // 如果p != h，则更新head if (p != h) // hop two nodes at a time updateHead(h, ((q = p.next) != null) ? q : p); return item; &#125; // 如果头节点的元素为空或头节点发生了变化，这说明头节点已经被另外一个线程修改了。 // 那么获取p节点的下一个节点，如果p节点的下一节点为null，则表明队列已经空了 else if ((q = p.next) == null) &#123; // 更新头结点 updateHead(h, p); return null; &#125; // p == q，则使用新的head重新开始 else if (p == q) continue restartFromHead; // 如果下一个元素不为空，则将头节点的下一个节点设置成头节点 else p = q; &#125; &#125;&#125; 该方法的主要逻辑就是首先获取头节点的元素，然后判断头节点元素是否为空，如果为空，表示另外一个线程已经进行了一次出队操作将该节点的元素取走，如果不为空，则使用CAS的方式将头节点的引用设置成null，如果CAS成功，则直接返回头节点的元素，如果不成功，表示另外一个线程已经进行了一次出队操作更新了head节点，导致元素发生了变化，需要重新获取头节点。 在入队和出队操作中，都有p == q的情况，那这种情况是怎么出现的呢？我们来看这样一种操作： 在弹出一个节点之后，tail节点有一条指向自己的虚线，这是什么意思呢？我们来看poll()方法，在该方法中，移除元素之后，会调用updateHead方法： 12345final void updateHead(Node&lt;E&gt; h, Node&lt;E&gt; p) &#123; if (h != p &amp;&amp; casHead(h, p)) // 将旧的头结点h的next域指向为h h.lazySetNext(h);&#125; 我们可以看到，在更新完head之后，会将旧的头结点h的next域指向为h，上图中所示的虚线也就表示这个节点的自引用。 如果这时，再有一个线程来添加元素，通过tail获取的next节点则仍然是它本身，这就出现了p == q的情况，出现该种情况之后，则会触发执行head的更新，将p节点重新指向为head，所有“活着”的节点（指未删除节点），都能从head通过遍历可达，这样就能通过head成功获取到尾节点，然后添加元素了。 其他相关方法peek()方法1234567891011121314151617// 获取链表的首部元素（只读取而不移除）public E peek() &#123; restartFromHead: for (;;) &#123; for (Node&lt;E&gt; h = head, p = h, q;;) &#123; E item = p.item; if (item != null || (q = p.next) == null) &#123; updateHead(h, p); return item; &#125; else if (p == q) continue restartFromHead; else p = q; &#125; &#125;&#125; 从源码中可以看到，peek操作会改变head指向，执行peek()方法后head会指向第一个具有非空元素的节点。 size()方法123456789101112public int size() &#123; int count = 0; // first()获取第一个具有非空元素的节点，若不存在，返回null // succ(p)方法获取p的后继节点，若p == p的后继节点，则返回head for (Node&lt;E&gt; p = first(); p != null; p = succ(p)) if (p.item != null) // Collection.size() spec says to max out // 最大返回Integer.MAX_VALUE if (++count == Integer.MAX_VALUE) break; return count;&#125; size()方法用来获取当前队列的元素个数，但在并发环境中，其结果可能不精确，因为整个过程都没有加锁，所以从调用size方法到返回结果期间有可能增删元素，导致统计的元素个数不精确。 remove(Object o)方法1234567891011121314151617181920212223242526272829303132public boolean remove(Object o) &#123; // 删除的元素不能为null if (o != null) &#123; Node&lt;E&gt; next, pred = null; for (Node&lt;E&gt; p = first(); p != null; pred = p, p = next) &#123; boolean removed = false; E item = p.item; // 节点元素不为null if (item != null) &#123; // 若不匹配，则获取next节点继续匹配 if (!o.equals(item)) &#123; next = succ(p); continue; &#125; // 若匹配，则通过CAS操作将对应节点元素置为null removed = p.casItem(item, null); &#125; // 获取删除节点的后继节点 next = succ(p); // 将被删除的节点移除队列 if (pred != null &amp;&amp; next != null) // unlink pred.casNext(p, next); if (removed) return true; &#125; &#125; return false;&#125; contains(Object o)方法123456789101112public boolean contains(Object o) &#123; if (o == null) return false; // 遍历队列 for (Node&lt;E&gt; p = first(); p != null; p = succ(p)) &#123; E item = p.item; // 若找到匹配节点，则返回true if (item != null &amp;&amp; o.equals(item)) return true; &#125; return false;&#125; 该方法和size方法类似，有可能返回错误结果，比如调用该方法时，元素还在队列里面，但是遍历过程中，该元素被删除了，那么就会返回false。 总结ConcurrentLinkedQueue 的非阻塞算法实现可概括为下面 5 点： 使用 CAS 原子指令来处理对数据的并发访问，这是非阻塞算法得以实现的基础。 head/tail 并非总是指向队列的头 / 尾节点，也就是说允许队列处于不一致状态。 这个特性把入队 / 出队时，原本需要一起原子化执行的两个步骤分离开来，从而缩小了入队 / 出队时需要原子化更新值的范围到唯一变量。这是非阻塞算法得以实现的关键。 由于队列有时会处于不一致状态。为此，ConcurrentLinkedQueue 使用三个不变式来维护非阻塞算法的正确性。 以批处理方式来更新 head/tail，从整体上减少入队 / 出队操作的开销。 为了有利于垃圾收集，队列使用特有的 head 更新机制；为了确保从已删除节点向后遍历，可到达所有的非删除节点，队列使用了特有的向后推进策略。 参考Java并发编程之ConcurrentLinkedQueue详解","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"ConcurrentLinkedQueue","slug":"ConcurrentLinkedQueue","permalink":"https://xmmarlowe.github.io/tags/ConcurrentLinkedQueue/"}],"author":"Marlowe"},{"title":"ConcurrentHashMap扩容原理","slug":"Java/ConcurrentHashMap扩容原理","date":"2021-05-17T08:56:14.000Z","updated":"2021-05-17T13:41:50.630Z","comments":true,"path":"2021/05/17/Java/ConcurrentHashMap扩容原理/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/17/Java/ConcurrentHashMap%E6%89%A9%E5%AE%B9%E5%8E%9F%E7%90%86/","excerpt":"ConcurrentHashMap从名称是可以看出，它是一个HashMap而且是线程安全的。在多线程编程中使用非常广泛。ConcurrentHashMap的实现方式，在jdk6,7,8中都不一样。本文只针对jdk8中的实现作一些说明。","text":"ConcurrentHashMap从名称是可以看出，它是一个HashMap而且是线程安全的。在多线程编程中使用非常广泛。ConcurrentHashMap的实现方式，在jdk6,7,8中都不一样。本文只针对jdk8中的实现作一些说明。 ConcurrentHashMap实现原理 先来看看ConcurrentHashMap底层是发何实现的。总的来说，它是采用Node&lt;K,V&gt;类型(继承了Map.Entry)的数组table+单向链表+红黑树的结构。table数组的大小默认为16，数组中的每一项称为桶(bucket),桶中存放的是链表或者是红黑树结构，取决于链表的长度是否达到了阀值8（大于等于8）(默认)，如果是，接着再判断数组的长度是否小于64，如果小于则优先扩容table容量来解决单个桶中元素增多的问题，如果不是则转换成红黑树结构存放。 再次，我们看到ConcurrentHashMap类中，Unsafe类。说明线程安全的实现是基于CAS算法的无锁化修改值的操作，它可以大大降低锁带来的性能消耗。其基本思想是不停的去比较当前内存中的变量值与给定的值是否相同(值相等且引用也相等)，如果相同则修改成指定的值，否则什么也不做。这与乐观锁的思想类似。缺点就是消耗CPU性能。 123456789101112131415private static final sun.misc.Unsafe U;U = sun.misc.Unsafe.getUnsafe(); static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125; static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v);&#125; static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v);&#125; 源码分析先来看看ConcurrentHashMap扩容是如何发生的，主要是在put一个KV时，如果达到某些阀值则会重新new一个nextTable其长度是原table的2倍。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public V put(K key, V value) &#123; return putVal(key, value, false);&#125; /** Implementation for put and putIfAbsent *///onlyIfAbsent的意思是在put一个KV时，如果K已经存在什么也不做则返回null//如果不存在则put操作后返回V值final V putVal(K key, V value, boolean onlyIfAbsent) &#123; //ConcurrentHashMap中是不能有空K或空V的 if (key == null || value == null) throw new NullPointerException(); //hash算法得到hash值 int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; //如果table是空的，就去初始化，下一个循环就不是空的了 if (tab == null || (n = tab.length) == 0) tab = initTable(); //如果没有取到值，即取i位的元素是空的，为什么i取值是(n-1)&amp;hash?? //这是hash的精华所在，在这里可以先思考一下 //此时直接到KV包装成Node节点放在i位置即可 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; //MOVED，定义为-1。标记原table正在执行扩容任务，可以去帮忙(支持多线程扩容) else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else &#123; //这种情况是，在i的位置找到了一个元素，说明此元素的K与之间的某个K的hash结果是一样的 // V oldVal = null; synchronized (f) &#123;//同步锁住第一个元素 if (tabAt(tab, i) == f) &#123;//为了安全起见，再一次判断 if (fh &gt;= 0) &#123;//节点的hash值大于0，说明是一个链表结构 binCount = 1;//记录链表的元素个数 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; //判断给定的key是否与取出的key相同，如果是则替换元素 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break;//直接跳出，这是一种思想。在编程时可以减少一些if else判断 &#125; //否则就是不相等，那就把此元素放在链表的最后一个元素 Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; //如果不是链表，而是红黑树 else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; //把元素放入树中的对应位置 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; //链表的元素大于等于8时，就把链表转换为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; //新添加一个元素，size加1，可能会触发扩容 addCount(1L, binCount); return null;&#125; 上面是对put操作的整个流程的分析，可以看出需要关注的几个点 hash算法及table下标i的计算方法 首次放元素时，initTable方法做了哪些事情 当前为正在扩容时help做了哪些操作？ table中的元素有可能是链表结构，也有可能是红黑树结构 什么条件下会去执行链表转换成红黑树？ 下面，我们先来看看链表转成红黑树的方法操作 12345678910111213141516171819202122232425262728293031323334/** * Replaces all linked nodes in bin at given index unless table is * too small, in which case resizes instead. */private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) &#123; Node&lt;K,V&gt; b; int n, sc; if (tab != null) &#123; //先判断table的长度是否小于64，如果小于，则优先使用扩容来解决问题 if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) //扩容为原来的一位，调整某一个桶中元素过多的问题(超出了8个)) //会触发某些桶中的元素重新分配，避免在一个桶中有太多的元素影响访问效率 tryPresize(n &lt;&lt; 1); //桶中存在结点，并且此结点的hash值大于0，调整红黑树的结构 else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) &#123; synchronized (b) &#123;//锁住节点，把元素添加到树中 if (tabAt(tab, index) == b) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; for (Node&lt;K,V&gt; e = b; e != null; e = e.next) &#123; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val, null, null); if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; &#125; setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd)); &#125; &#125; &#125; &#125;&#125; 还有一个就是addCount方法，这个方法在执行时，有可能会触发扩容操作 12345678910111213141516171819202122private final void addCount(long x, int check) &#123; ............省略无关代码..... if (check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; int n, sc; while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; int rs = resizeStamp(n); if (sc &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt);//可见是通过原子修改sizectl的值来判断是否需要扩容操作 &#125; else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); s = sumCount(); &#125; &#125;&#125; 在多线的环境下，用volatile的方式读取sizectrl属性的值，来判断map所处的状态，通过cas修改操作来告诉其它线程Map的状态类型。不同的数值类型，代表着不同的状态： 未初始化 等于0，表示未指定初始化容量，则使用默认容量 大于0，为指定的初始化容量 初始化中 等于-1，表示正在初始化，并且通过cas告诉其它线程 正常状态 等于原table长度n*0.75，扩容阀值 扩容中 小于0，表示有其他线程正在执行扩容操作 等于(resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) + 2表示此时只有一个线程在执行扩容 接下来我们来看看扩容方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155/** * Moves and/or copies the nodes in each bin to new table. See * above for explanation. */private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; //取CPU的数量，确定每次迁移的Node的数量，确保不会少于MIN_TRANSFER_STRIDE=16个 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range if (nextTab == null) &#123; // initiating try &#123; @SuppressWarnings(&quot;unchecked&quot;) //扩容一倍 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; //扩容索引，表示已经分配给扩容线程的table数组索引位置。 //主要用来协调多个线程，安全地获取迁移&quot;桶&quot;。 transferIndex = n; &#125; int nextn = nextTab.length; //标记当前节点已经迁移完成，它的hash值是MOVED=-1 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); boolean advance = true; boolean finishing = false; // to ensure sweep before committing nextTab //1 逆序迁移已经获取到的hash桶集合，如果迁移完毕，则更新transferIndex，获取下一批待迁移的hash桶 //2 如果transferIndex=0，表示所以hash桶均被分配，将i置为-1，准备退出transfer方法 for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; while (advance) &#123; int nextIndex, nextBound; if (--i &gt;= bound || finishing) advance = false; else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; if (finishing) &#123; nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; &#125; /** 第一个扩容的线程，执行transfer方法之前，会设置 sizeCtl = (resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) + 2) 后续帮其扩容的线程，执行transfer方法之前，会设置 sizeCtl = sizeCtl+1 每一个退出transfer的方法的线程，退出之前，会设置 sizeCtl = sizeCtl-1 那么最后一个线程退出时： 必然有sc == (resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) + 2)，即 (sc - 2) == resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT */ //不相等，说明不到最后一个线程，直接退出transfer方法 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; finishing = advance = true; i = n; // recheck before commit &#125; &#125; else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); else if ((fh = f.hash) == MOVED) advance = true; // already processed else &#123;//开始迁移 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; //迁移链表，将node链表分成两个新的链表 if (fh &gt;= 0) &#123; int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n;//取桶中每个节点的hash值 if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; //将node链表放在新的table对应的位置 setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; //迁移红黑树 else if (f instanceof TreeBin) &#123; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; 关于上面迁移链表的操作，比较有意思，我们来分析一下。还记得，在putVal方法有有一段代码 1else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) 用于计算tab中元素的下标的，n就是tab的长度，只会是2的x次幂，先来熟悉一下&amp;运算，它是指对应的二进制位上，如果都是1则结果为1，否则为0。假设现在，tab的长度为16，换成二进制就是10000，减1就是01111，取hash的值，这个值有点特别，就是从右起第x位(log以2为底的16)=4(从0开始数)。如果是10000&amp;此数，则结果一定是0，例如： 120000000000010000 00000000000011110101001000001001 结果为0 0101001000001001 结果是9，即i下标是9 如果此时tab扩容到32，也就是100000，再来看看(n-1)&amp;hash的结果 1200000000000111110101001000001001 结果也是9，即i下标是9 说明，如果右起第x位为0的话，runbit==0成立，此时扩容到原来的2倍的话在新数组中的下标是不变的，所在可以看到把ln链表直接放到nextTable的i位了。 再来看看，右起第x位为1的情况 120000000000010000 00000000000011110101001000011001 结果为16不等于0 0101001000011001 结果是9，即i下标是9 如果此时扩容到了32，也就是100000时，再来看看(n-1)&amp;hash的结果 1200000000000111110101001000011001 结果是16+8+1=25 即扩容后新下标变成了25，也就是原来的下标9再加扩容的量16，就是i+n的结果，所以对于hn来说在新table中的位置就变成了i+n了。 总结通过代码我们可以看出，这里面的思想还是值得学习借鉴的。下标取(n-1)&amp;hash并不是随便设计出来的，而是经过精心设计的。扩容后，桶的数量发生了变化，但无论是当前时刻使用的是新table还是扩容后的table访问的位置相对table长度来说都没有发生变化，为访问get提供便利。扩容时也不用重新计算hash值，同时结合多线程操作扩容提升操作效率。 参考ConcurrentHashMap扩容原理","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"扩容","slug":"扩容","permalink":"https://xmmarlowe.github.io/tags/%E6%89%A9%E5%AE%B9/"},{"name":"ConcurrentHashMap","slug":"ConcurrentHashMap","permalink":"https://xmmarlowe.github.io/tags/ConcurrentHashMap/"}],"author":"Marlowe"},{"title":"Condition接口","slug":"并发/Condition接口","date":"2021-05-16T13:33:40.000Z","updated":"2021-05-16T14:24:53.951Z","comments":true,"path":"2021/05/16/并发/Condition接口/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/16/%E5%B9%B6%E5%8F%91/Condition%E6%8E%A5%E5%8F%A3/","excerpt":"","text":"简介wait()、notify()与synchronized配合可以实现等待通知，condition和Lock配合同样也可以实现等待通知，但是两者之前还是有区别的。 Condition定义了等待/通知两种类型的方法，当前线程调用这些方法时，需要提前获取到Condition对象关联的锁，**Condition对象是由Lock对象创建出来的(Lock.newCondition)**，换句话说，Condition是依赖Lock对象的。 Condition常用APIvoid await():当前线程从运行状态进入等待状态或者中断，直到被通知唤醒。boolean await(long time, TimeUnit unit)；当前线程进入等待状态，直到被通知、中断或者超时boolean awaitUntil(Date deadline)当前线程进入等待状态，直到被通知、中断或者到达指定的时间。到达指定的时间返回false，否则返回true（还没有导致指定时间就被唤醒）void signal():唤醒一个等待在Condition上的线程，但是必须获得与该Condition相关的锁void signalAll():唤醒所有等待在Condition上的线程，但是必须获得与该Condition相关的锁 案例：实现有界队列有界队列是一种特殊的队列，当队列为空时，队列的获取操作将会阻塞获取线程，直到队列中有新增元素，当队列已满时，队列的插入操作将会阻塞插入线程，直到队列出现“空位” 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * 使用Condition实现有界队列 */public class BoundedQueue&lt;T&gt; &#123; //数组队列 private Object[] items; //添加下标 private int addIndex; //删除下标 private int removeIndex; //当前队列数据数量 private int count; //互斥锁 private Lock lock = new ReentrantLock(); //队列不为空的条件 private Condition notEmpty = lock.newCondition(); //队列没有满的条件 private Condition notFull = lock.newCondition(); public BoundedQueue(int size) &#123; items = new Object[size]; &#125; //添加一个元素，如果数组满了，添加线程进入等待状态，直到有“空位” public void add(T t)&#123; lock.lock(); try &#123; while(count == items.length) notFull.wait(); items[addIndex] = t; if(++addIndex == items.length) addIndex = 0; ++count; //唤醒一个等待删除的线程 notEmpty.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally&#123; lock.unlock(); &#125; &#125; //由头部删除一个元素，如果数组空，则删除线程进入等待状态，知道有新元素加入 public T remove() throws InterruptedException &#123; lock.lock(); try &#123; while(count == 0)&#123; notEmpty.await(); &#125; Object res = items[removeIndex]; if(++removeIndex==items.length) removeIndex=0; --count; //唤醒一个等待插入的线程 notFull.signal(); return (T)res; &#125; finally&#123; lock.unlock(); &#125; &#125;&#125; BoundedQueue通过add(T t)方法添加一个元素，通过remove()方法移出一个元素。以添加方法为例。 首先需要获得锁，目的是确保数组修改的可见性和排他性。 当数组数量等于数组长度时，表示数组已满，则调用notFull.await()，当前线程随之释放锁并进入等待状态。如果数组数量不等于数组长度，表示数组未满，则添加元素到数组中，同时通知等待在notEmpty上的线程，数组中已经有新元素可以获取。 在添加和删除方法中使用while循环而非if判断，目的是防止过早或意外的通知，只有条件符合才能够退出循环。 Condition实现分析ConditionObject是同步器AbstractQueuedSynchronizer的内部类，每个Condition对象都包含着一个等待队列，该队列是Condition对象实现等待/通知功能的关键。 1、等待队列等待队列是一个FIFO的队列，在队列中的每个节点都包含了一个线程引用，该线程就是在Condition对象上等待的线程。 如果一个线程调用了Condition.await()方法，那么该线程将会释放锁、构造成节点加入等待队列并进入等待状态。事实上，节点的定义复用了同步器中节点的定义，也就是说，同步队列和等待队列中节点类型都是同步器的静态内部类AbstractQueuedSynchronizer.Node。 一个Condition包含一个等待队列，Condition拥有首节点（firstWaiter）和尾节（lastWaiter）。当前线程调用Condition.await()方法，将会以当前线程构造节点，并将节点从尾部加入等待队列。 注意：上述节点引用更新的过程并没有使用CAS保证，原因在于调用await()方法的线程必定是获取了锁的线程，也就是说该过程是由锁来保证线程安全的。 2、等待调用Condition的await()方法，会使当前线程进入等待队列并释放锁，同时线程状态变为等待状态。调用该方法之前，当前线程一定获取了Condition相关联的锁。 如果从队列（同步队列和等待队列）的角度看await()方法，当调用await()方法时，相当于同步队列的首节点（获取了锁的节点）移动到Condition的等待队列中。 在AQS中提供了ConditionObject内部类，如果调用该内部列中的await方法，首先调用该方法的线程会成功获取了锁的线程，也就是同步队列中的首节点，其次该方法会将当前线程构造成节点并加入等待队列中，然后释放同步状态，唤醒同步队列中的后继节点，然后当前线程会进入等待状态。 如果从队列的角度去看，同步队列中当前线程加入Condition的等待队列。 3、通知调用Condition的 signal() 方法，将会唤醒在等待队列中等待时间最长的节点（首节点），在唤醒节点之前，会将节点移到同步队列中。 通过调用AQS的 enq(Node node) 方法，等待队列中的头节点线程安全地移动到同步队列。当节点移动到同步队列后，当前线程再使用LockSupport唤醒该节点的线程。 Condition的 signalAll() 方法，相当于对等待队列中的每个节点均执行一次 signal() 方法，效果就是将等待队列中所有节点全部移动到同步队列中，并唤醒每个节点的线程。 参考Condition接口","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"Condition","slug":"Condition","permalink":"https://xmmarlowe.github.io/tags/Condition/"}],"author":"Marlowe"},{"title":"三个线程循环打印ABC代码实现","slug":"并发/三个线程循环打印ABC代码实现","date":"2021-05-16T13:06:06.000Z","updated":"2021-05-16T14:24:53.966Z","comments":true,"path":"2021/05/16/并发/三个线程循环打印ABC代码实现/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/16/%E5%B9%B6%E5%8F%91/%E4%B8%89%E4%B8%AA%E7%BA%BF%E7%A8%8B%E5%BE%AA%E7%8E%AF%E6%89%93%E5%8D%B0ABC%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/","excerpt":"三个线程分别打印A，B，C，要求这三个线程一起运行，打印n次，输出形如“ABCABCABC….”的字符串。","text":"三个线程分别打印A，B，C，要求这三个线程一起运行，打印n次，输出形如“ABCABCABC….”的字符串。 Semaphore1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package concurrent;import java.util.concurrent.Semaphore;public class PrintABCBySemaphore &#123; private int times; private Semaphore semaphoreA =new Semaphore(1); private Semaphore semaphoreB =new Semaphore(0); private Semaphore semaphoreC =new Semaphore(0); public PrintABCBySemaphore(int times) &#123; this.times=times; &#125; public static void main(String[] args) &#123; PrintABCBySemaphore printABCBySemaphore =new PrintABCBySemaphore(10); new Thread(printABCBySemaphore::printA).start(); new Thread(printABCBySemaphore::printB).start(); new Thread(printABCBySemaphore::printC).start(); &#125; public void printA() &#123; print(&quot;A&quot;,semaphoreA,semaphoreB); &#125; public void printB() &#123; print(&quot;B&quot;,semaphoreB,semaphoreC); &#125; public void printC() &#123; print(&quot;C&quot;,semaphoreC,semaphoreA); &#125; public void print(String name,Semaphore current,Semaphore next) &#123; for(int i=0;i&lt;times;i++) &#123; try &#123; current.acquire(); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() +&quot;:&quot; + i+ &quot; :&quot; + name); next.release(); &#125; &#125;&#125; Lock12345678910111213141516171819202122232425262728293031323334353637383940414243444546package concurrent;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class PrintABCByLock &#123; private int times; private int state; private Lock lock =new ReentrantLock(); public PrintABCByLock(int times) &#123; this.times=times; &#125; public static void main(String[] args) &#123; PrintABCByLock printABC =new PrintABCByLock(10); new Thread(printABC::printA).start(); new Thread(printABC::printB).start(); new Thread(printABC::printC).start(); &#125; public void printA() &#123; print(&quot;A&quot;,0); &#125; public void printB() &#123; print(&quot;B&quot;,1); &#125; public void printC() &#123; print(&quot;C&quot;,2); &#125; public void print(String name,int stateNow) &#123; for (int i = 0; i &lt; times;) &#123; lock.lock(); if(stateNow == state % 3) &#123; state++; i++; System.out.println(Thread.currentThread().getName() + &quot;:i=&quot; + i + &quot;:stateNow=&quot;+stateNow + &quot;:&quot; +name); &#125; lock.unlock(); &#125; &#125;&#125; Condition123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package concurrent;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class PrintABCByLockCondition &#123; public static void main(String[] args) &#123; final Business business = new Business(); new Thread(new Runnable() &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; business.sub2(&quot;B&quot;); &#125; &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; business.sub3(&quot;C&quot;); &#125; &#125; &#125;).start(); for (int i = 0; i &lt; 10; i++) &#123; business.sub1(&quot;A&quot;); &#125; &#125; static class Business &#123; private int flag = 1; Lock lock = new ReentrantLock(); Condition condition1 = lock.newCondition(); Condition condition2 = lock.newCondition(); Condition condition3 = lock.newCondition(); public void sub1(String s) &#123; lock.lock(); try&#123; while(flag != 1) &#123; condition1.await(); &#125; System.out.println(&quot;A线程输出&quot; + s); flag = 2; condition2.signal(); &#125;catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void sub2(String s) &#123; lock.lock(); try&#123; while(flag != 2) &#123; condition2.await(); &#125; System.out.println(&quot;B线程输出&quot; + s); flag = 3; condition3.signal(); &#125;catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void sub3(String s) &#123; lock.lock(); try&#123; while(flag != 3) &#123; condition3.await(); &#125; System.out.println(&quot;C线程输出&quot; + s); flag = 1; condition1.signal(); &#125;catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125;&#125; 参考并发编程】如果让你用三个线程循环打印ABC，你有几种写法？","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"线程","slug":"线程","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B/"},{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/tags/%E5%B9%B6%E5%8F%91/"}],"author":"Marlowe"},{"title":"Spring IOC 容器源码分析","slug":"Spring/Spring+IOC+容器源码分析","date":"2021-05-16T13:02:41.000Z","updated":"2021-08-26T10:09:02.571Z","comments":true,"path":"2021/05/16/Spring/Spring+IOC+容器源码分析/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/16/Spring/Spring+IOC+%E5%AE%B9%E5%99%A8%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"本文转载于Spring IOC 容器源码分析","text":"本文转载于Spring IOC 容器源码分析 Spring 最重要的概念是 IOC 和 AOP，本篇文章其实就是要带领大家来分析下 Spring 的 IOC 容器。既然大家平时都要用到 Spring，怎么可以不好好了解 Spring 呢？阅读本文并不能让你成为 Spring 专家，不过一定有助于大家理解 Spring 的很多概念，帮助大家排查应用中和 Spring 相关的一些问题。 本文采用的源码版本是 4.3.11.RELEASE，算是 5.0.x 前比较新的版本了。为了降低难度，本文所说的所有的内容都是基于 xml 的配置的方式，实际使用已经很少人这么做了，至少不是纯 xml 配置，不过从理解源码的角度来看用这种方式来说无疑是最合适的。 阅读建议：读者至少需要知道怎么配置 Spring，了解 Spring 中的各种概念，少部分内容我还假设读者使用过 SpringMVC。本文要说的 IOC 总体来说有两处地方最重要，一个是创建 Bean 容器，一个是初始化 Bean，如果读者觉得一次性看完本文压力有点大，那么可以按这个思路分两次消化。读者不一定对 Spring 容器的源码感兴趣，也许附录部分介绍的知识对读者有些许作用。 希望通过本文可以让读者不惧怕阅读 Spring 源码，也希望大家能反馈表述错误或不合理的地方。 引言先看下最基本的启动 Spring 容器的例子： 123public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;classpath:applicationfile.xml&quot;);&#125; 以上代码就可以利用配置文件来启动一个 Spring 容器了，请使用 maven 的小伙伴直接在 dependencies 中加上以下依赖即可，个人比较反对那些不知道要添加什么依赖，然后把 Spring 的所有相关的东西都加进来的方式。 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;4.3.11.RELEASE&lt;/version&gt;&lt;/dependency&gt; spring-context 会自动将 spring-core、spring-beans、spring-aop、spring-expression 这几个基础 jar 包带进来。 多说一句，很多开发者入门就直接接触的 SpringMVC，对 Spring 其实不是很了解，Spring 是渐进式的工具，并不具有很强的侵入性，它的模块也划分得很合理，即使你的应用不是 web 应用，或者之前完全没有使用到 Spring，而你就想用 Spring 的依赖注入这个功能，其实完全是可以的，它的引入不会对其他的组件产生冲突。 废话说完，我们继续。ApplicationContext context = new ClassPathXmlApplicationContext(...) 其实很好理解，从名字上就可以猜出一二，就是在 ClassPath 中寻找 xml 配置文件，根据 xml 文件内容来构建 ApplicationContext。当然，除了 ClassPathXmlApplicationContext 以外，我们也还有其他构建 ApplicationContext 的方案可供选择，我们先来看看大体的继承结构是怎么样的： 读者可以大致看一下类名，源码分析的时候不至于找不着看哪个类，因为 Spring 为了适应各种使用场景，提供的各个接口都可能有很多的实现类。对于我们来说，就是揪着一个完整的分支看完。 当然，读本文的时候读者也不必太担心，每个代码块分析的时候，我都会告诉读者我们在说哪个类第几行。 我们可以看到，ClassPathXmlApplicationContext 兜兜转转了好久才到 ApplicationContext 接口，同样的，我们也可以使用绿颜色的 FileSystemXmlApplicationContext 和 AnnotationConfigApplicationContext 这两个类。 1、FileSystemXmlApplicationContext 的构造函数需要一个 xml 配置文件在系统中的路径，其他和 ClassPathXmlApplicationContext 基本上一样。 2、AnnotationConfigApplicationContext 是基于注解来使用的，它不需要配置文件，采用 java 配置类和各种注解来配置，是比较简单的方式，也是大势所趋吧。 不过本文旨在帮助大家理解整个构建流程，所以决定使用 ClassPathXmlApplicationContext 进行分析。 我们先来一个简单的例子来看看怎么实例化 ApplicationContext。 首先，定义一个接口： 123public interface MessageService &#123; String getMessage();&#125; 定义接口实现类： 123456public class MessageServiceImpl implements MessageService &#123; public String getMessage() &#123; return &quot;hello world&quot;; &#125;&#125; 接下来，我们在 resources 目录新建一个配置文件，文件名随意，通常叫 application.xml 或 application-xxx.xml 就可以了： 1234567&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;beans xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot; default-autowire=&quot;byName&quot;&gt; &lt;bean id=&quot;messageService&quot; class=&quot;com.javadoop.example.MessageServiceImpl&quot;/&gt;&lt;/beans&gt; 这样，我们就可以跑起来了： 12345678910111213public class App &#123; public static void main(String[] args) &#123; // 用我们的配置文件来启动一个 ApplicationContext ApplicationContext context = new ClassPathXmlApplicationContext(&quot;classpath:application.xml&quot;); System.out.println(&quot;context 启动成功&quot;); // 从 context 中取出我们的 Bean，而不是用 new MessageServiceImpl() 这种方式 MessageService messageService = context.getBean(MessageService.class); // 这句将输出: hello world System.out.println(messageService.getMessage()); &#125;&#125; 以上例子很简单，不过也够引出本文的主题了，就是怎么样通过配置文件来启动 Spring 的 ApplicationContext ？也就是我们今天要分析的 IOC 的核心了。ApplicationContext 启动过程中，会负责创建实例 Bean，往各个 Bean 中注入依赖等。 BeanFactory 简介BeanFactory，从名字上也很好理解，生产 bean 的工厂，它负责生产和管理各个 bean 实例。 初学者可别以为我之前说那么多和 BeanFactory 无关，前面说的 ApplicationContext 其实就是一个 BeanFactory。我们来看下和 BeanFactory 接口相关的主要的继承结构： 我想，大家看完这个图以后，可能就不是很开心了。ApplicationContext 往下的继承结构前面一张图说过了，这里就不重复了。这张图呢，背下来肯定是不需要的，有几个重点和大家说明下就好。 ApplicationContext 继承了 ListableBeanFactory，这个 Listable 的意思就是，通过这个接口，我们可以获取多个 Bean，大家看源码会发现，最顶层 BeanFactory 接口的方法都是获取单个 Bean 的。 ApplicationContext 继承了 HierarchicalBeanFactory，Hierarchical 单词本身已经能说明问题了，也就是说我们可以在应用中起多个 BeanFactory，然后可以将各个 BeanFactory 设置为父子关系。 AutowireCapableBeanFactory 这个名字中的 Autowire 大家都非常熟悉，它就是用来自动装配 Bean 用的，但是仔细看上图，ApplicationContext 并没有继承它，不过不用担心，不使用继承，不代表不可以使用组合，如果你看到 ApplicationContext 接口定义中的最后一个方法 getAutowireCapableBeanFactory() 就知道了。 ConfigurableListableBeanFactory 也是一个特殊的接口，看图，特殊之处在于它继承了第二层所有的三个接口，而 ApplicationContext 没有。这点之后会用到。 请先不用花时间在其他的接口和类上，先理解我说的这几点就可以了。 然后，请读者打开编辑器，翻一下 BeanFactory、ListableBeanFactory、HierarchicalBeanFactory、AutowireCapableBeanFactory、ApplicationContext 这几个接口的代码，大概看一下各个接口中的方法，大家心里要有底，限于篇幅，我就不贴代码介绍了。 启动过程分析下面将会是冗长的代码分析，记住，一定要自己打开源码来看，不然纯看是很累的。 第一步，我们肯定要从 ClassPathXmlApplicationContext 的构造方法说起。 1234567891011121314151617181920public class ClassPathXmlApplicationContext extends AbstractXmlApplicationContext &#123; private Resource[] configResources; // 如果已经有 ApplicationContext 并需要配置成父子关系，那么调用这个构造方法 public ClassPathXmlApplicationContext(ApplicationContext parent) &#123; super(parent); &#125; ... public ClassPathXmlApplicationContext(String[] configLocations, boolean refresh, ApplicationContext parent) throws BeansException &#123; super(parent); // 根据提供的路径，处理成配置文件数组(以分号、逗号、空格、tab、换行符分割) setConfigLocations(configLocations); if (refresh) &#123; refresh(); // 核心方法 &#125; &#125; ...&#125; 接下来，就是 refresh()，这里简单说下为什么是 refresh()，而不是 init() 这种名字的方法。因为 ApplicationContext 建立起来以后，其实我们是可以通过调用 refresh() 这个方法重建的，refresh() 会将原来的 ApplicationContext 销毁，然后再重新执行一次初始化操作。 往下看，refresh() 方法里面调用了那么多方法，就知道肯定不简单了，请读者先看个大概，细节之后会详细说。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778@Overridepublic void refresh() throws BeansException, IllegalStateException &#123; // 来个锁，不然 refresh() 还没结束，你又来个启动或销毁容器的操作，那不就乱套了嘛 synchronized (this.startupShutdownMonitor) &#123; // 准备工作，记录下容器的启动时间、标记“已启动”状态、处理配置文件中的占位符 prepareRefresh(); // 这步比较关键，这步完成后，配置文件就会解析成一个个 Bean 定义，注册到 BeanFactory 中， // 当然，这里说的 Bean 还没有初始化，只是配置信息都提取出来了， // 注册也只是将这些信息都保存到了注册中心(说到底核心是一个 beanName-&gt; beanDefinition 的 map) ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 设置 BeanFactory 的类加载器，添加几个 BeanPostProcessor，手动注册几个特殊的 bean // 这块待会会展开说 prepareBeanFactory(beanFactory); try &#123; // 【这里需要知道 BeanFactoryPostProcessor 这个知识点，Bean 如果实现了此接口， // 那么在容器初始化以后，Spring 会负责调用里面的 postProcessBeanFactory 方法。】 // 这里是提供给子类的扩展点，到这里的时候，所有的 Bean 都加载、注册完成了，但是都还没有初始化 // 具体的子类可以在这步的时候添加一些特殊的 BeanFactoryPostProcessor 的实现类或做点什么事 postProcessBeanFactory(beanFactory); // 调用 BeanFactoryPostProcessor 各个实现类的 postProcessBeanFactory(factory) 方法 invokeBeanFactoryPostProcessors(beanFactory); // 注册 BeanPostProcessor 的实现类，注意看和 BeanFactoryPostProcessor 的区别 // 此接口两个方法: postProcessBeforeInitialization 和 postProcessAfterInitialization // 两个方法分别在 Bean 初始化之前和初始化之后得到执行。注意，到这里 Bean 还没初始化 registerBeanPostProcessors(beanFactory); // 初始化当前 ApplicationContext 的 MessageSource，国际化这里就不展开说了，不然没完没了了 initMessageSource(); // 初始化当前 ApplicationContext 的事件广播器，这里也不展开了 initApplicationEventMulticaster(); // 从方法名就可以知道，典型的模板方法(钩子方法)， // 具体的子类可以在这里初始化一些特殊的 Bean（在初始化 singleton beans 之前） onRefresh(); // 注册事件监听器，监听器需要实现 ApplicationListener 接口。这也不是我们的重点，过 registerListeners(); // 重点，重点，重点 // 初始化所有的 singleton beans //（lazy-init 的除外） finishBeanFactoryInitialization(beanFactory); // 最后，广播事件，ApplicationContext 初始化完成 finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(&quot;Exception encountered during context initialization - &quot; + &quot;cancelling refresh attempt: &quot; + ex); &#125; // Destroy already created singletons to avoid dangling resources. // 销毁已经初始化的 singleton 的 Beans，以免有些 bean 会一直占用资源 destroyBeans(); // Reset &#x27;active&#x27; flag. cancelRefresh(ex); // 把异常往外抛 throw ex; &#125; finally &#123; // Reset common introspection caches in Spring&#x27;s core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125;&#125; 下面，我们开始一步步来肢解这个 refresh() 方法。 创建 Bean 容器前的准备工作这个比较简单，直接看代码中的几个注释即可。 12345678910111213141516171819protected void prepareRefresh() &#123; // 记录启动时间， // 将 active 属性设置为 true，closed 属性设置为 false，它们都是 AtomicBoolean 类型 this.startupDate = System.currentTimeMillis(); this.closed.set(false); this.active.set(true); if (logger.isInfoEnabled()) &#123; logger.info(&quot;Refreshing &quot; + this); &#125; // Initialize any placeholder property sources in the context environment initPropertySources(); // 校验 xml 配置文件 getEnvironment().validateRequiredProperties(); this.earlyApplicationEvents = new LinkedHashSet&lt;ApplicationEvent&gt;();&#125; 创建 Bean 容器，加载并注册 Bean我们回到 refresh() 方法中的下一行 obtainFreshBeanFactory()。 注意，这个方法是全文最重要的部分之一，这里将会初始化 BeanFactory、加载 Bean、注册 Bean 等等。 当然，这步结束后，Bean 并没有完成初始化。这里指的是 Bean 实例并未在这一步生成。 // AbstractApplicationContext.java 1234567891011protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; // 关闭旧的 BeanFactory (如果有)，创建新的 BeanFactory，加载 Bean 定义、注册 Bean 等等 refreshBeanFactory(); // 返回刚刚创建的 BeanFactory ConfigurableListableBeanFactory beanFactory = getBeanFactory(); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Bean factory for &quot; + getDisplayName() + &quot;: &quot; + beanFactory); &#125; return beanFactory;&#125; // AbstractRefreshableApplicationContext.java 120 1234567891011121314151617181920212223242526272829@Overrideprotected final void refreshBeanFactory() throws BeansException &#123; // 如果 ApplicationContext 中已经加载过 BeanFactory 了，销毁所有 Bean，关闭 BeanFactory // 注意，应用中 BeanFactory 本来就是可以多个的，这里可不是说应用全局是否有 BeanFactory，而是当前 // ApplicationContext 是否有 BeanFactory if (hasBeanFactory()) &#123; destroyBeans(); closeBeanFactory(); &#125; try &#123; // 初始化一个 DefaultListableBeanFactory，为什么用这个，我们马上说。 DefaultListableBeanFactory beanFactory = createBeanFactory(); // 用于 BeanFactory 的序列化，我想不部分人应该都用不到 beanFactory.setSerializationId(getId()); // 下面这两个方法很重要，别跟丢了，具体细节之后说 // 设置 BeanFactory 的两个配置属性：是否允许 Bean 覆盖、是否允许循环引用 customizeBeanFactory(beanFactory); // 加载 Bean 到 BeanFactory 中 loadBeanDefinitions(beanFactory); synchronized (this.beanFactoryMonitor) &#123; this.beanFactory = beanFactory; &#125; &#125; catch (IOException ex) &#123; throw new ApplicationContextException(&quot;I/O error parsing bean definition source for &quot; + getDisplayName(), ex); &#125;&#125; 看到这里的时候，我觉得读者就应该站在高处看 ApplicationContext 了，ApplicationContext 继承自 BeanFactory，但是它不应该被理解为 BeanFactory 的实现类，而是说其内部持有一个实例化的 BeanFactory（DefaultListableBeanFactory）。以后所有的 BeanFactory 相关的操作其实是委托给这个实例来处理的。 我们说说为什么选择实例化 DefaultListableBeanFactory ？前面我们说了有个很重要的接口 ConfigurableListableBeanFactory，它实现了 BeanFactory 下面一层的所有三个接口，我把之前的继承图再拿过来大家再仔细看一下： 我们可以看到 ConfigurableListableBeanFactory 只有一个实现类 DefaultListableBeanFactory，而且实现类 DefaultListableBeanFactory 还通过实现右边的 AbstractAutowireCapableBeanFactory 通吃了右路。所以结论就是，最底下这个家伙 DefaultListableBeanFactory 基本上是最牛的 BeanFactory 了，这也是为什么这边会使用这个类来实例化的原因。 如果你想要在程序运行的时候动态往 Spring IOC 容器注册新的 bean，就会使用到这个类。那我们怎么在运行时获得这个实例呢？ 之前我们说过 ApplicationContext 接口能获取到 AutowireCapableBeanFactory，就是最右上角那个，然后它向下转型就能得到 DefaultListableBeanFactory 了。 那怎么拿到 ApplicationContext 实例呢？如果你不会，说明你没用过 Spring。 在继续往下之前，我们需要先了解 BeanDefinition。我们说 BeanFactory 是 Bean 容器，那么 Bean 又是什么呢？ 这里的 BeanDefinition 就是我们所说的 Spring 的 Bean，我们自己定义的各个 Bean 其实会转换成一个个 BeanDefinition 存在于 Spring 的 BeanFactory 中。 所以，如果有人问你 Bean 是什么的时候，你要知道 Bean 在代码层面上可以简单认为是 BeanDefinition 的实例。 BeanDefinition 中保存了我们的 Bean 信息，比如这个 Bean 指向的是哪个类、是否是单例的、是否懒加载、这个 Bean 依赖了哪些 Bean 等等。 BeanDefinition 接口定义我们来看下 BeanDefinition 的接口定义： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788public interface BeanDefinition extends AttributeAccessor, BeanMetadataElement &#123; // 我们可以看到，默认只提供 sington 和 prototype 两种， // 很多读者可能知道还有 request, session, globalSession, application, websocket 这几种， // 不过，它们属于基于 web 的扩展。 String SCOPE_SINGLETON = ConfigurableBeanFactory.SCOPE_SINGLETON; String SCOPE_PROTOTYPE = ConfigurableBeanFactory.SCOPE_PROTOTYPE; // 比较不重要，直接跳过吧 int ROLE_APPLICATION = 0; int ROLE_SUPPORT = 1; int ROLE_INFRASTRUCTURE = 2; // 设置父 Bean，这里涉及到 bean 继承，不是 java 继承。请参见附录的详细介绍 // 一句话就是：继承父 Bean 的配置信息而已 void setParentName(String parentName); // 获取父 Bean String getParentName(); // 设置 Bean 的类名称，将来是要通过反射来生成实例的 void setBeanClassName(String beanClassName); // 获取 Bean 的类名称 String getBeanClassName(); // 设置 bean 的 scope void setScope(String scope); String getScope(); // 设置是否懒加载 void setLazyInit(boolean lazyInit); boolean isLazyInit(); // 设置该 Bean 依赖的所有的 Bean，注意，这里的依赖不是指属性依赖(如 @Autowire 标记的)， // 是 depends-on=&quot;&quot; 属性设置的值。 void setDependsOn(String... dependsOn); // 返回该 Bean 的所有依赖 String[] getDependsOn(); // 设置该 Bean 是否可以注入到其他 Bean 中，只对根据类型注入有效， // 如果根据名称注入，即使这边设置了 false，也是可以的 void setAutowireCandidate(boolean autowireCandidate); // 该 Bean 是否可以注入到其他 Bean 中 boolean isAutowireCandidate(); // 主要的。同一接口的多个实现，如果不指定名字的话，Spring 会优先选择设置 primary 为 true 的 bean void setPrimary(boolean primary); // 是否是 primary 的 boolean isPrimary(); // 如果该 Bean 采用工厂方法生成，指定工厂名称。对工厂不熟悉的读者，请参加附录 // 一句话就是：有些实例不是用反射生成的，而是用工厂模式生成的 void setFactoryBeanName(String factoryBeanName); // 获取工厂名称 String getFactoryBeanName(); // 指定工厂类中的 工厂方法名称 void setFactoryMethodName(String factoryMethodName); // 获取工厂类中的 工厂方法名称 String getFactoryMethodName(); // 构造器参数 ConstructorArgumentValues getConstructorArgumentValues(); // Bean 中的属性值，后面给 bean 注入属性值的时候会说到 MutablePropertyValues getPropertyValues(); // 是否 singleton boolean isSingleton(); // 是否 prototype boolean isPrototype(); // 如果这个 Bean 是被设置为 abstract，那么不能实例化， // 常用于作为 父bean 用于继承，其实也很少用...... boolean isAbstract(); int getRole(); String getDescription(); String getResourceDescription(); BeanDefinition getOriginatingBeanDefinition();&#125; 这个 BeanDefinition 其实已经包含很多的信息了，暂时不清楚所有的方法对应什么东西没关系，希望看完本文后读者可以彻底搞清楚里面的所有东西。 这里接口虽然那么多，但是没有类似 getInstance() 这种方法来获取我们定义的类的实例，真正的我们定义的类生成的实例到哪里去了呢？别着急，这个要很后面才能讲到。 有了 BeanDefinition 的概念以后，我们再往下看 refreshBeanFactory() 方法中的剩余部分： 12customizeBeanFactory(beanFactory);loadBeanDefinitions(beanFactory); 虽然只有两个方法，但路还很长啊。。。 customizeBeanFactorycustomizeBeanFactory(beanFactory) 比较简单，就是配置是否允许 BeanDefinition 覆盖、是否允许循环引用。 12345678910protected void customizeBeanFactory(DefaultListableBeanFactory beanFactory) &#123; if (this.allowBeanDefinitionOverriding != null) &#123; // 是否允许 Bean 定义覆盖 beanFactory.setAllowBeanDefinitionOverriding(this.allowBeanDefinitionOverriding); &#125; if (this.allowCircularReferences != null) &#123; // 是否允许 Bean 间的循环依赖 beanFactory.setAllowCircularReferences(this.allowCircularReferences); &#125;&#125; BeanDefinition 的覆盖问题可能会有开发者碰到这个坑，就是在配置文件中定义 bean 时使用了相同的 id 或 name，默认情况下，allowBeanDefinitionOverriding 属性为 null，如果在同一配置文件中重复了，会抛错，但是如果不是同一配置文件中，会发生覆盖。 循环引用也很好理解：A 依赖 B，而 B 依赖 A。或 A 依赖 B，B 依赖 C，而 C 依赖 A。 默认情况下，Spring 允许循环依赖，当然如果你在 A 的构造方法中依赖 B，在 B 的构造方法中依赖 A 是不行的。 至于这两个属性怎么配置？我在附录中进行了介绍，尤其对于覆盖问题，很多人都希望禁止出现 Bean 覆盖，可是 Spring 默认是不同文件的时候可以覆盖的。 之后的源码中还会出现这两个属性，读者有个印象就可以了，它们不是非常重要。 加载 Bean: loadBeanDefinitions接下来是最重要的 loadBeanDefinitions(beanFactory) 方法了，这个方法将根据配置，加载各个 Bean，然后放到 BeanFactory 中。 读取配置的操作在 XmlBeanDefinitionReader 中，其负责加载配置、解析。 // AbstractXmlApplicationContext.java 80 123456789101112131415161718/** 我们可以看到，此方法将通过一个 XmlBeanDefinitionReader 实例来加载各个 Bean。*/@Overrideprotected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123; // 给这个 BeanFactory 实例化一个 XmlBeanDefinitionReader XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); // Configure the bean definition reader with this context&#x27;s // resource loading environment. beanDefinitionReader.setEnvironment(this.getEnvironment()); beanDefinitionReader.setResourceLoader(this); beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); // 初始化 BeanDefinitionReader，其实这个是提供给子类覆写的， // 我看了一下，没有类覆写这个方法，我们姑且当做不重要吧 initBeanDefinitionReader(beanDefinitionReader); // 重点来了，继续往下 loadBeanDefinitions(beanDefinitionReader);&#125; 现在还在这个类中，接下来用刚刚初始化的 Reader 开始来加载 xml 配置，这块代码读者可以选择性跳过，不是很重要。也就是说，下面这个代码块，读者可以很轻松地略过。 // AbstractXmlApplicationContext.java 120 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException &#123; Resource[] configResources = getConfigResources(); if (configResources != null) &#123; // 往下看 reader.loadBeanDefinitions(configResources); &#125; String[] configLocations = getConfigLocations(); if (configLocations != null) &#123; // 2 reader.loadBeanDefinitions(configLocations); &#125;&#125;// 上面虽然有两个分支，不过第二个分支很快通过解析路径转换为 Resource 以后也会进到这里@Overridepublic int loadBeanDefinitions(Resource... resources) throws BeanDefinitionStoreException &#123; Assert.notNull(resources, &quot;Resource array must not be null&quot;); int counter = 0; // 注意这里是个 for 循环，也就是每个文件是一个 resource for (Resource resource : resources) &#123; // 继续往下看 counter += loadBeanDefinitions(resource); &#125; // 最后返回 counter，表示总共加载了多少的 BeanDefinition return counter;&#125;// XmlBeanDefinitionReader 303@Overridepublic int loadBeanDefinitions(Resource resource) throws BeanDefinitionStoreException &#123; return loadBeanDefinitions(new EncodedResource(resource));&#125;// XmlBeanDefinitionReader 314public int loadBeanDefinitions(EncodedResource encodedResource) throws BeanDefinitionStoreException &#123; Assert.notNull(encodedResource, &quot;EncodedResource must not be null&quot;); if (logger.isInfoEnabled()) &#123; logger.info(&quot;Loading XML bean definitions from &quot; + encodedResource.getResource()); &#125; // 用一个 ThreadLocal 来存放配置文件资源 Set&lt;EncodedResource&gt; currentResources = this.resourcesCurrentlyBeingLoaded.get(); if (currentResources == null) &#123; currentResources = new HashSet&lt;EncodedResource&gt;(4); this.resourcesCurrentlyBeingLoaded.set(currentResources); &#125; if (!currentResources.add(encodedResource)) &#123; throw new BeanDefinitionStoreException( &quot;Detected cyclic loading of &quot; + encodedResource + &quot; - check your import definitions!&quot;); &#125; try &#123; InputStream inputStream = encodedResource.getResource().getInputStream(); try &#123; InputSource inputSource = new InputSource(inputStream); if (encodedResource.getEncoding() != null) &#123; inputSource.setEncoding(encodedResource.getEncoding()); &#125; // 核心部分是这里，往下面看 return doLoadBeanDefinitions(inputSource, encodedResource.getResource()); &#125; finally &#123; inputStream.close(); &#125; &#125; catch (IOException ex) &#123; throw new BeanDefinitionStoreException( &quot;IOException parsing XML document from &quot; + encodedResource.getResource(), ex); &#125; finally &#123; currentResources.remove(encodedResource); if (currentResources.isEmpty()) &#123; this.resourcesCurrentlyBeingLoaded.remove(); &#125; &#125;&#125;// 还在这个文件中，第 388 行protected int doLoadBeanDefinitions(InputSource inputSource, Resource resource) throws BeanDefinitionStoreException &#123; try &#123; // 这里就不看了，将 xml 文件转换为 Document 对象 Document doc = doLoadDocument(inputSource, resource); // 继续 return registerBeanDefinitions(doc, resource); &#125; catch (...&#125;// 还在这个文件中，第 505 行// 返回值：返回从当前配置文件加载了多少数量的 Beanpublic int registerBeanDefinitions(Document doc, Resource resource) throws BeanDefinitionStoreException &#123; BeanDefinitionDocumentReader documentReader = createBeanDefinitionDocumentReader(); int countBefore = getRegistry().getBeanDefinitionCount(); // 这里 documentReader.registerBeanDefinitions(doc, createReaderContext(resource)); return getRegistry().getBeanDefinitionCount() - countBefore;&#125;// DefaultBeanDefinitionDocumentReader 90@Overridepublic void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) &#123; this.readerContext = readerContext; logger.debug(&quot;Loading bean definitions&quot;); Element root = doc.getDocumentElement(); // 从 xml 根节点开始解析文件 doRegisterBeanDefinitions(root);&#125; 经过漫长的链路，一个配置文件终于转换为一颗 DOM 树了，注意，这里指的是其中一个配置文件，不是所有的，读者可以看到上面有个 for 循环的。下面开始从根节点开始解析： doRegisterBeanDefinitions：123456789101112131415161718192021222324252627282930313233// DefaultBeanDefinitionDocumentReader 116protected void doRegisterBeanDefinitions(Element root) &#123; // 我们看名字就知道，BeanDefinitionParserDelegate 必定是一个重要的类，它负责解析 Bean 定义， // 这里为什么要定义一个 parent? 看到后面就知道了，是递归问题， // 因为 &lt;beans /&gt; 内部是可以定义 &lt;beans /&gt; 的，所以这个方法的 root 其实不一定就是 xml 的根节点，也可以是嵌套在里面的 &lt;beans /&gt; 节点，从源码分析的角度，我们当做根节点就好了 BeanDefinitionParserDelegate parent = this.delegate; this.delegate = createDelegate(getReaderContext(), root, parent); if (this.delegate.isDefaultNamespace(root)) &#123; // 这块说的是根节点 &lt;beans ... profile=&quot;dev&quot; /&gt; 中的 profile 是否是当前环境需要的， // 如果当前环境配置的 profile 不包含此 profile，那就直接 return 了，不对此 &lt;beans /&gt; 解析 // 不熟悉 profile 为何物，不熟悉怎么配置 profile 读者的请移步附录区 String profileSpec = root.getAttribute(PROFILE_ATTRIBUTE); if (StringUtils.hasText(profileSpec)) &#123; String[] specifiedProfiles = StringUtils.tokenizeToStringArray( profileSpec, BeanDefinitionParserDelegate.MULTI_VALUE_ATTRIBUTE_DELIMITERS); if (!getReaderContext().getEnvironment().acceptsProfiles(specifiedProfiles)) &#123; if (logger.isInfoEnabled()) &#123; logger.info(&quot;Skipped XML bean definition file due to specified profiles [&quot; + profileSpec + &quot;] not matching: &quot; + getReaderContext().getResource()); &#125; return; &#125; &#125; &#125; preProcessXml(root); // 钩子 // 往下看 parseBeanDefinitions(root, this.delegate); postProcessXml(root); // 钩子 this.delegate = parent;&#125; preProcessXml(root) 和 postProcessXml(root) 是给子类用的钩子方法，鉴于没有被使用到，也不是我们的重点，我们直接跳过。 这里涉及到了 profile 的问题，对于不了解的读者，我在附录中对 profile 做了简单的解释，读者可以参考一下。 接下来，看核心解析方法 parseBeanDefinitions(root, this.delegate) : 123456789101112131415161718192021222324// default namespace 涉及到的就四个标签 &lt;import /&gt;、&lt;alias /&gt;、&lt;bean /&gt; 和 &lt;beans /&gt;，// 其他的属于 custom 的protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) &#123; if (delegate.isDefaultNamespace(root)) &#123; NodeList nl = root.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (node instanceof Element) &#123; Element ele = (Element) node; if (delegate.isDefaultNamespace(ele)) &#123; // 解析 default namespace 下面的几个元素 parseDefaultElement(ele, delegate); &#125; else &#123; // 解析其他 namespace 的元素 delegate.parseCustomElement(ele); &#125; &#125; &#125; &#125; else &#123; delegate.parseCustomElement(root); &#125;&#125; 从上面的代码，我们可以看到，对于每个配置来说，分别进入到 parseDefaultElement(ele, delegate); 和 delegate.parseCustomElement(ele); 这两个分支了。 parseDefaultElement(ele, delegate) 代表解析的节点是 &lt;import /&gt;、&lt;alias /&gt;、&lt;bean /&gt;、&lt;beans /&gt; 这几个。 这里的四个标签之所以是 default 的，是因为它们是处于这个 namespace 下定义的： 1http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans 又到初学者科普时间，不熟悉 namespace 的读者请看下面贴出来的 xml，这里的第二行 xmlns 就是咯。 123456&lt;beans xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot; default-autowire=&quot;byName&quot;&gt; 而对于其他的标签，将进入到 delegate.parseCustomElement(element) 这个分支。如我们经常会使用到的 &lt;mvc /&gt;、&lt;task /&gt;、&lt;context /&gt;、&lt;aop /&gt;等。 这些属于扩展，如果需要使用上面这些 ”非 default“ 标签，那么上面的 xml 头部的地方也要引入相应的 namespace 和 .xsd 文件的路径，如下所示。同时代码中需要提供相应的 parser 来解析，如 MvcNamespaceHandler、TaskNamespaceHandler、ContextNamespaceHandler、AopNamespaceHandler 等。 假如读者想分析 &lt;context:property-placeholder location=&quot;classpath:xx.properties&quot; /&gt; 的实现原理，就应该到 ContextNamespaceHandler 中找答案。 12345678910111213&lt;beans xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd &quot; default-autowire=&quot;byName&quot;&gt; 同理，以后你要是碰到 &lt;dubbo /&gt; 这种标签，那么就应该搜一搜是不是有 DubboNamespaceHandler 这个处理类。 回过神来，看看处理 default 标签的方法： 12345678910111213141516171819private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) &#123; if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) &#123; // 处理 &lt;import /&gt; 标签 importBeanDefinitionResource(ele); &#125; else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) &#123; // 处理 &lt;alias /&gt; 标签定义 // &lt;alias name=&quot;fromName&quot; alias=&quot;toName&quot;/&gt; processAliasRegistration(ele); &#125; else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) &#123; // 处理 &lt;bean /&gt; 标签定义，这也算是我们的重点吧 processBeanDefinition(ele, delegate); &#125; else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) &#123; // 如果碰到的是嵌套的 &lt;beans /&gt; 标签，需要递归 doRegisterBeanDefinitions(ele); &#125;&#125; 如果每个标签都说，那我不吐血，你们都要吐血了。我们挑我们的重点 &lt;bean /&gt; 标签出来说。 processBeanDefinition 解析 bean 标签下面是 processBeanDefinition 解析 &lt;bean /&gt; 标签： // DefaultBeanDefinitionDocumentReader 298 1234567891011121314151617181920protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123; // 将 &lt;bean /&gt; 节点中的信息提取出来，然后封装到一个 BeanDefinitionHolder 中，细节往下看 BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); // 下面的几行先不要看，跳过先，跳过先，跳过先，后面会继续说的 if (bdHolder != null) &#123; bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try &#123; // Register the final decorated instance. BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); &#125; catch (BeanDefinitionStoreException ex) &#123; getReaderContext().error(&quot;Failed to register bean definition with name &#x27;&quot; + bdHolder.getBeanName() + &quot;&#x27;&quot;, ele, ex); &#125; // Send registration event. getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); &#125;&#125; 继续往下看怎么解析之前，我们先看下 &lt;bean /&gt; 标签中可以定义哪些属性： Property class 类的全限定名 name 可指定 id、name(用逗号、分号、空格分隔) scope 作用域 constructor arguments 指定构造参数 properties 设置属性的值 autowiring mode no(默认值)、byName、byType、 constructor lazy-initialization mode 是否懒加载(如果被非懒加载的bean依赖了那么其实也就不能懒加载了) initialization method bean 属性设置完成后，会调用这个方法 destruction method bean 销毁后的回调方法 上面表格中的内容我想大家都非常熟悉吧，如果不熟悉，那就是你不够了解 Spring 的配置了。 简单地说就是像下面这样子： 123456789101112131415&lt;bean id=&quot;exampleBean&quot; name=&quot;name1, name2, name3&quot; class=&quot;com.javadoop.ExampleBean&quot; scope=&quot;singleton&quot; lazy-init=&quot;true&quot; init-method=&quot;init&quot; destroy-method=&quot;cleanup&quot;&gt; &lt;!-- 可以用下面三种形式指定构造参数 --&gt; &lt;constructor-arg type=&quot;int&quot; value=&quot;7500000&quot;/&gt; &lt;constructor-arg name=&quot;years&quot; value=&quot;7500000&quot;/&gt; &lt;constructor-arg index=&quot;0&quot; value=&quot;7500000&quot;/&gt; &lt;!-- property 的几种情况 --&gt; &lt;property name=&quot;beanOne&quot;&gt; &lt;ref bean=&quot;anotherExampleBean&quot;/&gt; &lt;/property&gt; &lt;property name=&quot;beanTwo&quot; ref=&quot;yetAnotherBean&quot;/&gt; &lt;property name=&quot;integerProperty&quot; value=&quot;1&quot;/&gt;&lt;/bean&gt; 当然，除了上面举例出来的这些，还有 factory-bean、factory-method、&lt;lockup-method /&gt;、&lt;replaced-method /&gt;、&lt;meta /&gt;、&lt;qualifier /&gt; 这几个，大家是不是熟悉呢？自己检验一下自己对 Spring 中 bean 的了解程度。 有了以上这些知识以后，我们再继续往里看怎么解析 bean 元素，是怎么转换到 BeanDefinitionHolder 的。 // BeanDefinitionParserDelegate 428 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public BeanDefinitionHolder parseBeanDefinitionElement(Element ele) &#123; return parseBeanDefinitionElement(ele, null);&#125;public BeanDefinitionHolder parseBeanDefinitionElement(Element ele, BeanDefinition containingBean) &#123; String id = ele.getAttribute(ID_ATTRIBUTE); String nameAttr = ele.getAttribute(NAME_ATTRIBUTE); List&lt;String&gt; aliases = new ArrayList&lt;String&gt;(); // 将 name 属性的定义按照 “逗号、分号、空格” 切分，形成一个 别名列表数组， // 当然，如果你不定义 name 属性的话，就是空的了 // 我在附录中简单介绍了一下 id 和 name 的配置，大家可以看一眼，有个20秒就可以了 if (StringUtils.hasLength(nameAttr)) &#123; String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, MULTI_VALUE_ATTRIBUTE_DELIMITERS); aliases.addAll(Arrays.asList(nameArr)); &#125; String beanName = id; // 如果没有指定id, 那么用别名列表的第一个名字作为beanName if (!StringUtils.hasText(beanName) &amp;&amp; !aliases.isEmpty()) &#123; beanName = aliases.remove(0); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;No XML &#x27;id&#x27; specified - using &#x27;&quot; + beanName + &quot;&#x27; as bean name and &quot; + aliases + &quot; as aliases&quot;); &#125; &#125; if (containingBean == null) &#123; checkNameUniqueness(beanName, aliases, ele); &#125; // 根据 &lt;bean ...&gt;...&lt;/bean&gt; 中的配置创建 BeanDefinition，然后把配置中的信息都设置到实例中, // 细节后面细说，先知道下面这行结束后，一个 BeanDefinition 实例就出来了。 AbstractBeanDefinition beanDefinition = parseBeanDefinitionElement(ele, beanName, containingBean); // 到这里，整个 &lt;bean /&gt; 标签就算解析结束了，一个 BeanDefinition 就形成了。 if (beanDefinition != null) &#123; // 如果都没有设置 id 和 name，那么此时的 beanName 就会为 null，进入下面这块代码产生 // 如果读者不感兴趣的话，我觉得不需要关心这块代码，对本文源码分析来说，这些东西不重要 if (!StringUtils.hasText(beanName)) &#123; try &#123; if (containingBean != null) &#123;// 按照我们的思路，这里 containingBean 是 null 的 beanName = BeanDefinitionReaderUtils.generateBeanName( beanDefinition, this.readerContext.getRegistry(), true); &#125; else &#123; // 如果我们不定义 id 和 name，那么我们引言里的那个例子： // 1. beanName 为：com.javadoop.example.MessageServiceImpl#0 // 2. beanClassName 为：com.javadoop.example.MessageServiceImpl beanName = this.readerContext.generateBeanName(beanDefinition); String beanClassName = beanDefinition.getBeanClassName(); if (beanClassName != null &amp;&amp; beanName.startsWith(beanClassName) &amp;&amp; beanName.length() &gt; beanClassName.length() &amp;&amp; !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) &#123; // 把 beanClassName 设置为 Bean 的别名 aliases.add(beanClassName); &#125; &#125; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Neither XML &#x27;id&#x27; nor &#x27;name&#x27; specified - &quot; + &quot;using generated bean name [&quot; + beanName + &quot;]&quot;); &#125; &#125; catch (Exception ex) &#123; error(ex.getMessage(), ele); return null; &#125; &#125; String[] aliasesArray = StringUtils.toStringArray(aliases); // 返回 BeanDefinitionHolder return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray); &#125; return null;&#125; 然后，我们再看看怎么根据配置创建 BeanDefinition 实例的： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public AbstractBeanDefinition parseBeanDefinitionElement( Element ele, String beanName, BeanDefinition containingBean) &#123; this.parseState.push(new BeanEntry(beanName)); String className = null; if (ele.hasAttribute(CLASS_ATTRIBUTE)) &#123; className = ele.getAttribute(CLASS_ATTRIBUTE).trim(); &#125; try &#123; String parent = null; if (ele.hasAttribute(PARENT_ATTRIBUTE)) &#123; parent = ele.getAttribute(PARENT_ATTRIBUTE); &#125; // 创建 BeanDefinition，然后设置类信息而已，很简单，就不贴代码了 AbstractBeanDefinition bd = createBeanDefinition(className, parent); // 设置 BeanDefinition 的一堆属性，这些属性定义在 AbstractBeanDefinition 中 parseBeanDefinitionAttributes(ele, beanName, containingBean, bd); bd.setDescription(DomUtils.getChildElementValueByTagName(ele, DESCRIPTION_ELEMENT)); /** * 下面的一堆是解析 &lt;bean&gt;......&lt;/bean&gt; 内部的子元素， * 解析出来以后的信息都放到 bd 的属性中 */ // 解析 &lt;meta /&gt; parseMetaElements(ele, bd); // 解析 &lt;lookup-method /&gt; parseLookupOverrideSubElements(ele, bd.getMethodOverrides()); // 解析 &lt;replaced-method /&gt; parseReplacedMethodSubElements(ele, bd.getMethodOverrides()); // 解析 &lt;constructor-arg /&gt; parseConstructorArgElements(ele, bd); // 解析 &lt;property /&gt; parsePropertyElements(ele, bd); // 解析 &lt;qualifier /&gt; parseQualifierElements(ele, bd); bd.setResource(this.readerContext.getResource()); bd.setSource(extractSource(ele)); return bd; &#125; catch (ClassNotFoundException ex) &#123; error(&quot;Bean class [&quot; + className + &quot;] not found&quot;, ele, ex); &#125; catch (NoClassDefFoundError err) &#123; error(&quot;Class that bean class [&quot; + className + &quot;] depends on not found&quot;, ele, err); &#125; catch (Throwable ex) &#123; error(&quot;Unexpected failure during bean definition parsing&quot;, ele, ex); &#125; finally &#123; this.parseState.pop(); &#125; return null;&#125; 到这里，我们已经完成了根据 &lt;bean /&gt; 配置创建了一个 BeanDefinitionHolder 实例。注意，是一个。 我们回到解析 &lt;bean /&gt; 的入口方法: 123456789101112131415161718protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123; // 将 &lt;bean /&gt; 节点转换为 BeanDefinitionHolder，就是上面说的一堆 BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); if (bdHolder != null) &#123; // 如果有自定义属性的话，进行相应的解析，先忽略 bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try &#123; // 我们把这步叫做 注册Bean 吧 BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); &#125; catch (BeanDefinitionStoreException ex) &#123; getReaderContext().error(&quot;Failed to register bean definition with name &#x27;&quot; + bdHolder.getBeanName() + &quot;&#x27;&quot;, ele, ex); &#125; // 注册完成后，发送事件，本文不展开说这个 getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); &#125;&#125; 大家再仔细看一下这块吧，我们后面就不回来说这个了。这里已经根据一个 &lt;bean /&gt; 标签产生了一个 BeanDefinitionHolder 的实例，这个实例里面也就是一个 BeanDefinition 的实例和它的 beanName、aliases 这三个信息，注意，我们的关注点始终在 BeanDefinition 上： 12345678public class BeanDefinitionHolder implements BeanMetadataElement &#123; private final BeanDefinition beanDefinition; private final String beanName; private final String[] aliases;... 然后我们准备注册这个 BeanDefinition，最后，把这个注册事件发送出去。 下面，我们开始说说注册 Bean 吧。 注册 Bean// BeanDefinitionReaderUtils 143 123456789101112131415161718public static void registerBeanDefinition( BeanDefinitionHolder definitionHolder, BeanDefinitionRegistry registry) throws BeanDefinitionStoreException &#123; String beanName = definitionHolder.getBeanName(); // 注册这个 Bean registry.registerBeanDefinition(beanName, definitionHolder.getBeanDefinition()); // 如果还有别名的话，也要根据别名全部注册一遍，不然根据别名就会找不到 Bean 了 String[] aliases = definitionHolder.getAliases(); if (aliases != null) &#123; for (String alias : aliases) &#123; // alias -&gt; beanName 保存它们的别名信息，这个很简单，用一个 map 保存一下就可以了， // 获取的时候，会先将 alias 转换为 beanName，然后再查找 registry.registerAlias(beanName, alias); &#125; &#125;&#125; 别名注册的放一边，毕竟它很简单，我们看看怎么注册 Bean。 // DefaultListableBeanFactory 793 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182@Overridepublic void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException &#123; Assert.hasText(beanName, &quot;Bean name must not be empty&quot;); Assert.notNull(beanDefinition, &quot;BeanDefinition must not be null&quot;); if (beanDefinition instanceof AbstractBeanDefinition) &#123; try &#123; ((AbstractBeanDefinition) beanDefinition).validate(); &#125; catch (BeanDefinitionValidationException ex) &#123; throw new BeanDefinitionStoreException(...); &#125; &#125; // old? 还记得 “允许 bean 覆盖” 这个配置吗？allowBeanDefinitionOverriding BeanDefinition oldBeanDefinition; // 之后会看到，所有的 Bean 注册后会放入这个 beanDefinitionMap 中 oldBeanDefinition = this.beanDefinitionMap.get(beanName); // 处理重复名称的 Bean 定义的情况 if (oldBeanDefinition != null) &#123; if (!isAllowBeanDefinitionOverriding()) &#123; // 如果不允许覆盖的话，抛异常 throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription()... &#125; else if (oldBeanDefinition.getRole() &lt; beanDefinition.getRole()) &#123; // log...用框架定义的 Bean 覆盖用户自定义的 Bean &#125; else if (!beanDefinition.equals(oldBeanDefinition)) &#123; // log...用新的 Bean 覆盖旧的 Bean &#125; else &#123; // log...用同等的 Bean 覆盖旧的 Bean，这里指的是 equals 方法返回 true 的 Bean &#125; // 覆盖 this.beanDefinitionMap.put(beanName, beanDefinition); &#125; else &#123; // 判断是否已经有其他的 Bean 开始初始化了. // 注意，&quot;注册Bean&quot; 这个动作结束，Bean 依然还没有初始化，我们后面会有大篇幅说初始化过程， // 在 Spring 容器启动的最后，会 预初始化 所有的 singleton beans if (hasBeanCreationStarted()) &#123; // Cannot modify startup-time collection elements anymore (for stable iteration) synchronized (this.beanDefinitionMap) &#123; this.beanDefinitionMap.put(beanName, beanDefinition); List&lt;String&gt; updatedDefinitions = new ArrayList&lt;String&gt;(this.beanDefinitionNames.size() + 1); updatedDefinitions.addAll(this.beanDefinitionNames); updatedDefinitions.add(beanName); this.beanDefinitionNames = updatedDefinitions; if (this.manualSingletonNames.contains(beanName)) &#123; Set&lt;String&gt; updatedSingletons = new LinkedHashSet&lt;String&gt;(this.manualSingletonNames); updatedSingletons.remove(beanName); this.manualSingletonNames = updatedSingletons; &#125; &#125; &#125; else &#123; // 最正常的应该是进到这个分支。 // 将 BeanDefinition 放到这个 map 中，这个 map 保存了所有的 BeanDefinition this.beanDefinitionMap.put(beanName, beanDefinition); // 这是个 ArrayList，所以会按照 bean 配置的顺序保存每一个注册的 Bean 的名字 this.beanDefinitionNames.add(beanName); // 这是个 LinkedHashSet，代表的是手动注册的 singleton bean， // 注意这里是 remove 方法，到这里的 Bean 当然不是手动注册的 // 手动指的是通过调用以下方法注册的 bean ： // registerSingleton(String beanName, Object singletonObject) // 这不是重点，解释只是为了不让大家疑惑。Spring 会在后面&quot;手动&quot;注册一些 Bean， // 如 &quot;environment&quot;、&quot;systemProperties&quot; 等 bean，我们自己也可以在运行时注册 Bean 到容器中的 this.manualSingletonNames.remove(beanName); &#125; // 这个不重要，在预初始化的时候会用到，不必管它。 this.frozenBeanDefinitionNames = null; &#125; if (oldBeanDefinition != null || containsSingleton(beanName)) &#123; resetBeanDefinition(beanName); &#125;&#125; 总结一下，到这里已经初始化了 Bean 容器，&lt;bean /&gt; 配置也相应的转换为了一个个 BeanDefinition，然后注册了各个 BeanDefinition 到注册中心，并且发送了注册事件。 ——— 分割线 ——— 到这里是一个分水岭，前面的内容都还算比较简单，不过应该也比较繁琐，大家要清楚地知道前面都做了哪些事情。 Bean 容器实例化完成后说到这里，我们回到 refresh() 方法，我重新贴了一遍代码，看看我们说到哪了。是的，我们才说完 obtainFreshBeanFactory() 方法。 考虑到篇幅，这里开始大幅缩减掉没必要详细介绍的部分，大家直接看下面的代码中的注释就好了。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980@Overridepublic void refresh() throws BeansException, IllegalStateException &#123; // 来个锁，不然 refresh() 还没结束，你又来个启动或销毁容器的操作，那不就乱套了嘛 synchronized (this.startupShutdownMonitor) &#123; // 准备工作，记录下容器的启动时间、标记“已启动”状态、处理配置文件中的占位符 prepareRefresh(); // 这步比较关键，这步完成后，配置文件就会解析成一个个 Bean 定义，注册到 BeanFactory 中， // 当然，这里说的 Bean 还没有初始化，只是配置信息都提取出来了， // 注册也只是将这些信息都保存到了注册中心(说到底核心是一个 beanName-&gt; beanDefinition 的 map) ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 设置 BeanFactory 的类加载器，添加几个 BeanPostProcessor，手动注册几个特殊的 bean // 这块待会会展开说 prepareBeanFactory(beanFactory); try &#123; // 【这里需要知道 BeanFactoryPostProcessor 这个知识点，Bean 如果实现了此接口， // 那么在容器初始化以后，Spring 会负责调用里面的 postProcessBeanFactory 方法。】 // 这里是提供给子类的扩展点，到这里的时候，所有的 Bean 都加载、注册完成了，但是都还没有初始化 // 具体的子类可以在这步的时候添加一些特殊的 BeanFactoryPostProcessor 的实现类或做点什么事 postProcessBeanFactory(beanFactory); // 调用 BeanFactoryPostProcessor 各个实现类的 postProcessBeanFactory(factory) 回调方法 invokeBeanFactoryPostProcessors(beanFactory); // 注册 BeanPostProcessor 的实现类，注意看和 BeanFactoryPostProcessor 的区别 // 此接口两个方法: postProcessBeforeInitialization 和 postProcessAfterInitialization // 两个方法分别在 Bean 初始化之前和初始化之后得到执行。这里仅仅是注册，之后会看到回调这两方法的时机 registerBeanPostProcessors(beanFactory); // 初始化当前 ApplicationContext 的 MessageSource，国际化这里就不展开说了，不然没完没了了 initMessageSource(); // 初始化当前 ApplicationContext 的事件广播器，这里也不展开了 initApplicationEventMulticaster(); // 从方法名就可以知道，典型的模板方法(钩子方法)，不展开说 // 具体的子类可以在这里初始化一些特殊的 Bean（在初始化 singleton beans 之前） onRefresh(); // 注册事件监听器，监听器需要实现 ApplicationListener 接口。这也不是我们的重点，过 registerListeners(); // 重点，重点，重点 // 初始化所有的 singleton beans //（lazy-init 的除外） finishBeanFactoryInitialization(beanFactory); // 最后，广播事件，ApplicationContext 初始化完成，不展开 finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(&quot;Exception encountered during context initialization - &quot; + &quot;cancelling refresh attempt: &quot; + ex); &#125; // Destroy already created singletons to avoid dangling resources. // 销毁已经初始化的 singleton 的 Beans，以免有些 bean 会一直占用资源 destroyBeans(); // Reset &#x27;active&#x27; flag. cancelRefresh(ex); // 把异常往外抛 throw ex; &#125; finally &#123; // Reset common introspection caches in Spring&#x27;s core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125;&#125; 准备 Bean 容器: prepareBeanFactory之前我们说过，Spring 把我们在 xml 配置的 bean 都注册以后，会”手动”注册一些特殊的 bean。 这里简单介绍下 prepareBeanFactory(factory) 方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374/** * Configure the factory&#x27;s standard context characteristics, * such as the context&#x27;s ClassLoader and post-processors. * @param beanFactory the BeanFactory to configure */protected void prepareBeanFactory(ConfigurableListableBeanFactory beanFactory) &#123; // 设置 BeanFactory 的类加载器，我们知道 BeanFactory 需要加载类，也就需要类加载器， // 这里设置为加载当前 ApplicationContext 类的类加载器 beanFactory.setBeanClassLoader(getClassLoader()); // 设置 BeanExpressionResolver beanFactory.setBeanExpressionResolver(new StandardBeanExpressionResolver(beanFactory.getBeanClassLoader())); // beanFactory.addPropertyEditorRegistrar(new ResourceEditorRegistrar(this, getEnvironment())); // 添加一个 BeanPostProcessor，这个 processor 比较简单： // 实现了 Aware 接口的 beans 在初始化的时候，这个 processor 负责回调， // 这个我们很常用，如我们会为了获取 ApplicationContext 而 implement ApplicationContextAware // 注意：它不仅仅回调 ApplicationContextAware， // 还会负责回调 EnvironmentAware、ResourceLoaderAware 等，看下源码就清楚了 beanFactory.addBeanPostProcessor(new ApplicationContextAwareProcessor(this)); // 下面几行的意思就是，如果某个 bean 依赖于以下几个接口的实现类，在自动装配的时候忽略它们， // Spring 会通过其他方式来处理这些依赖。 beanFactory.ignoreDependencyInterface(EnvironmentAware.class); beanFactory.ignoreDependencyInterface(EmbeddedValueResolverAware.class); beanFactory.ignoreDependencyInterface(ResourceLoaderAware.class); beanFactory.ignoreDependencyInterface(ApplicationEventPublisherAware.class); beanFactory.ignoreDependencyInterface(MessageSourceAware.class); beanFactory.ignoreDependencyInterface(ApplicationContextAware.class); /** * 下面几行就是为特殊的几个 bean 赋值，如果有 bean 依赖了以下几个，会注入这边相应的值， * 之前我们说过，&quot;当前 ApplicationContext 持有一个 BeanFactory&quot;，这里解释了第一行。 * ApplicationContext 还继承了 ResourceLoader、ApplicationEventPublisher、MessageSource * 所以对于这几个依赖，可以赋值为 this，注意 this 是一个 ApplicationContext * 那这里怎么没看到为 MessageSource 赋值呢？那是因为 MessageSource 被注册成为了一个普通的 bean */ beanFactory.registerResolvableDependency(BeanFactory.class, beanFactory); beanFactory.registerResolvableDependency(ResourceLoader.class, this); beanFactory.registerResolvableDependency(ApplicationEventPublisher.class, this); beanFactory.registerResolvableDependency(ApplicationContext.class, this); // 这个 BeanPostProcessor 也很简单，在 bean 实例化后，如果是 ApplicationListener 的子类， // 那么将其添加到 listener 列表中，可以理解成：注册 事件监听器 beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(this)); // 这里涉及到特殊的 bean，名为：loadTimeWeaver，这不是我们的重点，忽略它 // tips: ltw 是 AspectJ 的概念，指的是在运行期进行织入，这个和 Spring AOP 不一样， // 感兴趣的读者请参考我写的关于 AspectJ 的另一篇文章 https://www.javadoop.com/post/aspectj if (beanFactory.containsBean(LOAD_TIME_WEAVER_BEAN_NAME)) &#123; beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory)); // Set a temporary ClassLoader for type matching. beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader())); &#125; /** * 从下面几行代码我们可以知道，Spring 往往很 &quot;智能&quot; 就是因为它会帮我们默认注册一些有用的 bean， * 我们也可以选择覆盖 */ // 如果没有定义 &quot;environment&quot; 这个 bean，那么 Spring 会 &quot;手动&quot; 注册一个 if (!beanFactory.containsLocalBean(ENVIRONMENT_BEAN_NAME)) &#123; beanFactory.registerSingleton(ENVIRONMENT_BEAN_NAME, getEnvironment()); &#125; // 如果没有定义 &quot;systemProperties&quot; 这个 bean，那么 Spring 会 &quot;手动&quot; 注册一个 if (!beanFactory.containsLocalBean(SYSTEM_PROPERTIES_BEAN_NAME)) &#123; beanFactory.registerSingleton(SYSTEM_PROPERTIES_BEAN_NAME, getEnvironment().getSystemProperties()); &#125; // 如果没有定义 &quot;systemEnvironment&quot; 这个 bean，那么 Spring 会 &quot;手动&quot; 注册一个 if (!beanFactory.containsLocalBean(SYSTEM_ENVIRONMENT_BEAN_NAME)) &#123; beanFactory.registerSingleton(SYSTEM_ENVIRONMENT_BEAN_NAME, getEnvironment().getSystemEnvironment()); &#125;&#125; 在上面这块代码中，Spring 对一些特殊的 bean 进行了处理，读者如果暂时还不能消化它们也没有关系，慢慢往下看。 初始化所有的 singleton beans我们的重点当然是 finishBeanFactoryInitialization(beanFactory); 这个巨头了，这里会负责初始化所有的 singleton beans。 注意，后面的描述中，我都会使用初始化或预初始化来代表这个阶段，Spring 会在这个阶段完成所有的 singleton beans 的实例化。 我们来总结一下，到目前为止，应该说 BeanFactory 已经创建完成，并且所有的实现了 BeanFactoryPostProcessor 接口的 Bean 都已经初始化并且其中的 postProcessBeanFactory(factory) 方法已经得到回调执行了。而且 Spring 已经“手动”注册了一些特殊的 Bean，如 environment、systemProperties 等。 剩下的就是初始化 singleton beans 了，我们知道它们是单例的，如果没有设置懒加载，那么 Spring 会在接下来初始化所有的 singleton beans。 // AbstractApplicationContext.java 834 1234567891011121314151617181920212223242526272829303132333435363738394041// 初始化剩余的 singleton beansprotected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) &#123; // 首先，初始化名字为 conversionService 的 Bean。本着送佛送到西的精神，我在附录中简单介绍了一下 ConversionService，因为这实在太实用了 // 什么，看代码这里没有初始化 Bean 啊！ // 注意了，初始化的动作包装在 beanFactory.getBean(...) 中，这里先不说细节，先往下看吧 if (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) &amp;&amp; beanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) &#123; beanFactory.setConversionService( beanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)); &#125; // Register a default embedded value resolver if no bean post-processor // (such as a PropertyPlaceholderConfigurer bean) registered any before: // at this point, primarily for resolution in annotation attribute values. if (!beanFactory.hasEmbeddedValueResolver()) &#123; beanFactory.addEmbeddedValueResolver(new StringValueResolver() &#123; @Override public String resolveStringValue(String strVal) &#123; return getEnvironment().resolvePlaceholders(strVal); &#125; &#125;); &#125; // 先初始化 LoadTimeWeaverAware 类型的 Bean // 之前也说过，这是 AspectJ 相关的内容，放心跳过吧 String[] weaverAwareNames = beanFactory.getBeanNamesForType(LoadTimeWeaverAware.class, false, false); for (String weaverAwareName : weaverAwareNames) &#123; getBean(weaverAwareName); &#125; // Stop using the temporary ClassLoader for type matching. beanFactory.setTempClassLoader(null); // 没什么别的目的，因为到这一步的时候，Spring 已经开始预初始化 singleton beans 了， // 肯定不希望这个时候还出现 bean 定义解析、加载、注册。 beanFactory.freezeConfiguration(); // 开始初始化 beanFactory.preInstantiateSingletons();&#125; 从上面最后一行往里看，我们就又回到 DefaultListableBeanFactory 这个类了，这个类大家应该都不陌生了吧。 preInstantiateSingletons// DefaultListableBeanFactory 728 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768@Overridepublic void preInstantiateSingletons() throws BeansException &#123; if (this.logger.isDebugEnabled()) &#123; this.logger.debug(&quot;Pre-instantiating singletons in &quot; + this); &#125; // this.beanDefinitionNames 保存了所有的 beanNames List&lt;String&gt; beanNames = new ArrayList&lt;String&gt;(this.beanDefinitionNames); // 下面这个循环，触发所有的非懒加载的 singleton beans 的初始化操作 for (String beanName : beanNames) &#123; // 合并父 Bean 中的配置，注意 &lt;bean id=&quot;&quot; class=&quot;&quot; parent=&quot;&quot; /&gt; 中的 parent，用的不多吧， // 考虑到这可能会影响大家的理解，我在附录中解释了一下 &quot;Bean 继承&quot;，不了解的请到附录中看一下 RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); // 非抽象、非懒加载的 singletons。如果配置了 &#x27;abstract = true&#x27;，那是不需要初始化的 if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) &#123; // 处理 FactoryBean(读者如果不熟悉 FactoryBean，请移步附录区了解) if (isFactoryBean(beanName)) &#123; // FactoryBean 的话，在 beanName 前面加上 ‘&amp;’ 符号。再调用 getBean，getBean 方法别急 final FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) getBean(FACTORY_BEAN_PREFIX + beanName); // 判断当前 FactoryBean 是否是 SmartFactoryBean 的实现，此处忽略，直接跳过 boolean isEagerInit; if (System.getSecurityManager() != null &amp;&amp; factory instanceof SmartFactoryBean) &#123; isEagerInit = AccessController.doPrivileged(new PrivilegedAction&lt;Boolean&gt;() &#123; @Override public Boolean run() &#123; return ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit(); &#125; &#125;, getAccessControlContext()); &#125; else &#123; isEagerInit = (factory instanceof SmartFactoryBean &amp;&amp; ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit()); &#125; if (isEagerInit) &#123; getBean(beanName); &#125; &#125; else &#123; // 对于普通的 Bean，只要调用 getBean(beanName) 这个方法就可以进行初始化了 getBean(beanName); &#125; &#125; &#125; // 到这里说明所有的非懒加载的 singleton beans 已经完成了初始化 // 如果我们定义的 bean 是实现了 SmartInitializingSingleton 接口的，那么在这里得到回调，忽略 for (String beanName : beanNames) &#123; Object singletonInstance = getSingleton(beanName); if (singletonInstance instanceof SmartInitializingSingleton) &#123; final SmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance; if (System.getSecurityManager() != null) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; @Override public Object run() &#123; smartSingleton.afterSingletonsInstantiated(); return null; &#125; &#125;, getAccessControlContext()); &#125; else &#123; smartSingleton.afterSingletonsInstantiated(); &#125; &#125; &#125;&#125; 接下来，我们就进入到 getBean(beanName) 方法了，这个方法我们经常用来从 BeanFactory 中获取一个 Bean，而初始化的过程也封装到了这个方法里。 getBean在继续前进之前，读者应该具备 FactoryBean 的知识，如果读者还不熟悉，请移步附录部分了解 FactoryBean。 // AbstractBeanFactory 196 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176@Overridepublic Object getBean(String name) throws BeansException &#123; return doGetBean(name, null, null, false);&#125;// 我们在剖析初始化 Bean 的过程，但是 getBean 方法我们经常是用来从容器中获取 Bean 用的，注意切换思路，// 已经初始化过了就从容器中直接返回，否则就先初始化再返回@SuppressWarnings(&quot;unchecked&quot;)protected &lt;T&gt; T doGetBean( final String name, final Class&lt;T&gt; requiredType, final Object[] args, boolean typeCheckOnly) throws BeansException &#123; // 获取一个 “正统的” beanName，处理两种情况，一个是前面说的 FactoryBean(前面带 ‘&amp;’)， // 一个是别名问题，因为这个方法是 getBean，获取 Bean 用的，你要是传一个别名进来，是完全可以的 final String beanName = transformedBeanName(name); // 注意跟着这个，这个是返回值 Object bean; // 检查下是不是已经创建过了 Object sharedInstance = getSingleton(beanName); // 这里说下 args 呗，虽然看上去一点不重要。前面我们一路进来的时候都是 getBean(beanName)， // 所以 args 传参其实是 null 的，但是如果 args 不为空的时候，那么意味着调用方不是希望获取 Bean，而是创建 Bean if (sharedInstance != null &amp;&amp; args == null) &#123; if (logger.isDebugEnabled()) &#123; if (isSingletonCurrentlyInCreation(beanName)) &#123; logger.debug(&quot;...&quot;); &#125; else &#123; logger.debug(&quot;Returning cached instance of singleton bean &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; &#125; // 下面这个方法：如果是普通 Bean 的话，直接返回 sharedInstance， // 如果是 FactoryBean 的话，返回它创建的那个实例对象 // (FactoryBean 知识，读者若不清楚请移步附录) bean = getObjectForBeanInstance(sharedInstance, name, beanName, null); &#125; else &#123; if (isPrototypeCurrentlyInCreation(beanName)) &#123; // 创建过了此 beanName 的 prototype 类型的 bean，那么抛异常， // 往往是因为陷入了循环引用 throw new BeanCurrentlyInCreationException(beanName); &#125; // 检查一下这个 BeanDefinition 在容器中是否存在 BeanFactory parentBeanFactory = getParentBeanFactory(); if (parentBeanFactory != null &amp;&amp; !containsBeanDefinition(beanName)) &#123; // 如果当前容器不存在这个 BeanDefinition，试试父容器中有没有 String nameToLookup = originalBeanName(name); if (args != null) &#123; // 返回父容器的查询结果 return (T) parentBeanFactory.getBean(nameToLookup, args); &#125; else &#123; // No args -&gt; delegate to standard getBean method. return parentBeanFactory.getBean(nameToLookup, requiredType); &#125; &#125; if (!typeCheckOnly) &#123; // typeCheckOnly 为 false，将当前 beanName 放入一个 alreadyCreated 的 Set 集合中。 markBeanAsCreated(beanName); &#125; /* * 稍稍总结一下： * 到这里的话，要准备创建 Bean 了，对于 singleton 的 Bean 来说，容器中还没创建过此 Bean； * 对于 prototype 的 Bean 来说，本来就是要创建一个新的 Bean。 */ try &#123; final RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName); checkMergedBeanDefinition(mbd, beanName, args); // 先初始化依赖的所有 Bean，这个很好理解。 // 注意，这里的依赖指的是 depends-on 中定义的依赖 String[] dependsOn = mbd.getDependsOn(); if (dependsOn != null) &#123; for (String dep : dependsOn) &#123; // 检查是不是有循环依赖，这里的循环依赖和我们前面说的循环依赖又不一样，这里肯定是不允许出现的，不然要乱套了，读者想一下就知道了 if (isDependent(beanName, dep)) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Circular depends-on relationship between &#x27;&quot; + beanName + &quot;&#x27; and &#x27;&quot; + dep + &quot;&#x27;&quot;); &#125; // 注册一下依赖关系 registerDependentBean(dep, beanName); // 先初始化被依赖项 getBean(dep); &#125; &#125; // 如果是 singleton scope 的，创建 singleton 的实例 if (mbd.isSingleton()) &#123; sharedInstance = getSingleton(beanName, new ObjectFactory&lt;Object&gt;() &#123; @Override public Object getObject() throws BeansException &#123; try &#123; // 执行创建 Bean，详情后面再说 return createBean(beanName, mbd, args); &#125; catch (BeansException ex) &#123; destroySingleton(beanName); throw ex; &#125; &#125; &#125;); bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); &#125; // 如果是 prototype scope 的，创建 prototype 的实例 else if (mbd.isPrototype()) &#123; // It&#x27;s a prototype -&gt; create a new instance. Object prototypeInstance = null; try &#123; beforePrototypeCreation(beanName); // 执行创建 Bean prototypeInstance = createBean(beanName, mbd, args); &#125; finally &#123; afterPrototypeCreation(beanName); &#125; bean = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd); &#125; // 如果不是 singleton 和 prototype 的话，需要委托给相应的实现类来处理 else &#123; String scopeName = mbd.getScope(); final Scope scope = this.scopes.get(scopeName); if (scope == null) &#123; throw new IllegalStateException(&quot;No Scope registered for scope name &#x27;&quot; + scopeName + &quot;&#x27;&quot;); &#125; try &#123; Object scopedInstance = scope.get(beanName, new ObjectFactory&lt;Object&gt;() &#123; @Override public Object getObject() throws BeansException &#123; beforePrototypeCreation(beanName); try &#123; // 执行创建 Bean return createBean(beanName, mbd, args); &#125; finally &#123; afterPrototypeCreation(beanName); &#125; &#125; &#125;); bean = getObjectForBeanInstance(scopedInstance, name, beanName, mbd); &#125; catch (IllegalStateException ex) &#123; throw new BeanCreationException(beanName, &quot;Scope &#x27;&quot; + scopeName + &quot;&#x27; is not active for the current thread; consider &quot; + &quot;defining a scoped proxy for this bean if you intend to refer to it from a singleton&quot;, ex); &#125; &#125; &#125; catch (BeansException ex) &#123; cleanupAfterBeanCreationFailure(beanName); throw ex; &#125; &#125; // 最后，检查一下类型对不对，不对的话就抛异常，对的话就返回了 if (requiredType != null &amp;&amp; bean != null &amp;&amp; !requiredType.isInstance(bean)) &#123; try &#123; return getTypeConverter().convertIfNecessary(bean, requiredType); &#125; catch (TypeMismatchException ex) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Failed to convert bean &#x27;&quot; + name + &quot;&#x27; to required type &#x27;&quot; + ClassUtils.getQualifiedName(requiredType) + &quot;&#x27;&quot;, ex); &#125; throw new BeanNotOfRequiredTypeException(name, requiredType, bean.getClass()); &#125; &#125; return (T) bean;&#125; 大家应该也猜到了，接下来当然是分析 createBean 方法： 1protected abstract Object createBean(String beanName, RootBeanDefinition mbd, Object[] args) throws BeanCreationException; 第三个参数 args 数组代表创建实例需要的参数，不就是给构造方法用的参数，或者是工厂 Bean 的参数嘛，不过要注意，在我们的初始化阶段，args 是 null。 这回我们要到一个新的类了 AbstractAutowireCapableBeanFactory，看类名，AutowireCapable？类名是不是也说明了点问题了。 主要是为了以下场景，采用 @Autowired 注解注入属性值： 12345678public class MessageServiceImpl implements MessageService &#123; @Autowired private UserService userService; public String getMessage() &#123; return userService.getMessage(); &#125;&#125; 1&lt;bean id=&quot;messageService&quot; class=&quot;com.javadoop.example.MessageServiceImpl&quot; /&gt; 以上这种属于混用了 xml 和 注解 两种方式的配置方式，Spring 会处理这种情况。 好了，读者要知道这么回事就可以了，继续向前。 // AbstractAutowireCapableBeanFactory 447 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * Central method of this class: creates a bean instance, * populates the bean instance, applies post-processors, etc. * @see #doCreateBean */@Overrideprotected Object createBean(String beanName, RootBeanDefinition mbd, Object[] args) throws BeanCreationException &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Creating instance of bean &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; RootBeanDefinition mbdToUse = mbd; // 确保 BeanDefinition 中的 Class 被加载 Class&lt;?&gt; resolvedClass = resolveBeanClass(mbd, beanName); if (resolvedClass != null &amp;&amp; !mbd.hasBeanClass() &amp;&amp; mbd.getBeanClassName() != null) &#123; mbdToUse = new RootBeanDefinition(mbd); mbdToUse.setBeanClass(resolvedClass); &#125; // 准备方法覆写，这里又涉及到一个概念：MethodOverrides，它来自于 bean 定义中的 &lt;lookup-method /&gt; // 和 &lt;replaced-method /&gt;，如果读者感兴趣，回到 bean 解析的地方看看对这两个标签的解析。 // 我在附录中也对这两个标签的相关知识点进行了介绍，读者可以移步去看看 try &#123; mbdToUse.prepareMethodOverrides(); &#125; catch (BeanDefinitionValidationException ex) &#123; throw new BeanDefinitionStoreException(mbdToUse.getResourceDescription(), beanName, &quot;Validation of method overrides failed&quot;, ex); &#125; try &#123; // 让 InstantiationAwareBeanPostProcessor 在这一步有机会返回代理， // 在 《Spring AOP 源码分析》那篇文章中有解释，这里先跳过 Object bean = resolveBeforeInstantiation(beanName, mbdToUse); if (bean != null) &#123; return bean; &#125; &#125; catch (Throwable ex) &#123; throw new BeanCreationException(mbdToUse.getResourceDescription(), beanName, &quot;BeanPostProcessor before instantiation of bean failed&quot;, ex); &#125; // 重头戏，创建 bean Object beanInstance = doCreateBean(beanName, mbdToUse, args); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Finished creating instance of bean &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; return beanInstance;&#125; 创建 Bean我们继续往里看 doCreateBean 这个方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125/** * Actually create the specified bean. Pre-creation processing has already happened * at this point, e.g. checking &#123;@code postProcessBeforeInstantiation&#125; callbacks. * &lt;p&gt;Differentiates between default bean instantiation, use of a * factory method, and autowiring a constructor. * @param beanName the name of the bean * @param mbd the merged bean definition for the bean * @param args explicit arguments to use for constructor or factory method invocation * @return a new instance of the bean * @throws BeanCreationException if the bean could not be created * @see #instantiateBean * @see #instantiateUsingFactoryMethod * @see #autowireConstructor */protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final Object[] args) throws BeanCreationException &#123; // Instantiate the bean. BeanWrapper instanceWrapper = null; if (mbd.isSingleton()) &#123; instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); &#125; if (instanceWrapper == null) &#123; // 说明不是 FactoryBean，这里实例化 Bean，这里非常关键，细节之后再说 instanceWrapper = createBeanInstance(beanName, mbd, args); &#125; // 这个就是 Bean 里面的 我们定义的类 的实例，很多地方我直接描述成 &quot;bean 实例&quot; final Object bean = (instanceWrapper != null ? instanceWrapper.getWrappedInstance() : null); // 类型 Class&lt;?&gt; beanType = (instanceWrapper != null ? instanceWrapper.getWrappedClass() : null); mbd.resolvedTargetType = beanType; // 建议跳过吧，涉及接口：MergedBeanDefinitionPostProcessor synchronized (mbd.postProcessingLock) &#123; if (!mbd.postProcessed) &#123; try &#123; // MergedBeanDefinitionPostProcessor，这个我真不展开说了，直接跳过吧，很少用的 applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Post-processing of merged bean definition failed&quot;, ex); &#125; mbd.postProcessed = true; &#125; &#125; // Eagerly cache singletons to be able to resolve circular references // even when triggered by lifecycle interfaces like BeanFactoryAware. // 下面这块代码是为了解决循环依赖的问题，以后有时间，我再对循环依赖这个问题进行解析吧 boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Eagerly caching bean &#x27;&quot; + beanName + &quot;&#x27; to allow for resolving potential circular references&quot;); &#125; addSingletonFactory(beanName, new ObjectFactory&lt;Object&gt;() &#123; @Override public Object getObject() throws BeansException &#123; return getEarlyBeanReference(beanName, mbd, bean); &#125; &#125;); &#125; // Initialize the bean instance. Object exposedObject = bean; try &#123; // 这一步也是非常关键的，这一步负责属性装配，因为前面的实例只是实例化了，并没有设值，这里就是设值 populateBean(beanName, mbd, instanceWrapper); if (exposedObject != null) &#123; // 还记得 init-method 吗？还有 InitializingBean 接口？还有 BeanPostProcessor 接口？ // 这里就是处理 bean 初始化完成后的各种回调 exposedObject = initializeBean(beanName, exposedObject, mbd); &#125; &#125; catch (Throwable ex) &#123; if (ex instanceof BeanCreationException &amp;&amp; beanName.equals(((BeanCreationException) ex).getBeanName())) &#123; throw (BeanCreationException) ex; &#125; else &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Initialization of bean failed&quot;, ex); &#125; &#125; if (earlySingletonExposure) &#123; // Object earlySingletonReference = getSingleton(beanName, false); if (earlySingletonReference != null) &#123; if (exposedObject == bean) &#123; exposedObject = earlySingletonReference; &#125; else if (!this.allowRawInjectionDespiteWrapping &amp;&amp; hasDependentBean(beanName)) &#123; String[] dependentBeans = getDependentBeans(beanName); Set&lt;String&gt; actualDependentBeans = new LinkedHashSet&lt;String&gt;(dependentBeans.length); for (String dependentBean : dependentBeans) &#123; if (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) &#123; actualDependentBeans.add(dependentBean); &#125; &#125; if (!actualDependentBeans.isEmpty()) &#123; throw new BeanCurrentlyInCreationException(beanName, &quot;Bean with name &#x27;&quot; + beanName + &quot;&#x27; has been injected into other beans [&quot; + StringUtils.collectionToCommaDelimitedString(actualDependentBeans) + &quot;] in its raw version as part of a circular reference, but has eventually been &quot; + &quot;wrapped. This means that said other beans do not use the final version of the &quot; + &quot;bean. This is often the result of over-eager type matching - consider using &quot; + &quot;&#x27;getBeanNamesOfType&#x27; with the &#x27;allowEagerInit&#x27; flag turned off, for example.&quot;); &#125; &#125; &#125; &#125; // Register bean as disposable. try &#123; registerDisposableBeanIfNecessary(beanName, bean, mbd); &#125; catch (BeanDefinitionValidationException ex) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Invalid destruction signature&quot;, ex); &#125; return exposedObject;&#125; 到这里，我们已经分析完了 doCreateBean 方法，总的来说，我们已经说完了整个初始化流程。 接下来我们挑 doCreateBean 中的三个细节出来说说。一个是创建 Bean 实例的 createBeanInstance 方法，一个是依赖注入的 populateBean 方法，还有就是回调方法 initializeBean。 注意了，接下来的这三个方法要认真说那也是极其复杂的，很多地方我就点到为止了，感兴趣的读者可以自己往里看，最好就是碰到不懂的，自己写代码去调试它。 创建 Bean 实例我们先看看 createBeanInstance 方法。需要说明的是，这个方法如果每个分支都分析下去，必然也是极其复杂冗长的，我们挑重点说。此方法的目的就是实例化我们指定的类。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950protected BeanWrapper createBeanInstance(String beanName, RootBeanDefinition mbd, Object[] args) &#123; // 确保已经加载了此 class Class&lt;?&gt; beanClass = resolveBeanClass(mbd, beanName); // 校验一下这个类的访问权限 if (beanClass != null &amp;&amp; !Modifier.isPublic(beanClass.getModifiers()) &amp;&amp; !mbd.isNonPublicAccessAllowed()) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Bean class isn&#x27;t public, and non-public access not allowed: &quot; + beanClass.getName()); &#125; if (mbd.getFactoryMethodName() != null) &#123; // 采用工厂方法实例化，不熟悉这个概念的读者请看附录，注意，不是 FactoryBean return instantiateUsingFactoryMethod(beanName, mbd, args); &#125; // 如果不是第一次创建，比如第二次创建 prototype bean。 // 这种情况下，我们可以从第一次创建知道，采用无参构造函数，还是构造函数依赖注入 来完成实例化 boolean resolved = false; boolean autowireNecessary = false; if (args == null) &#123; synchronized (mbd.constructorArgumentLock) &#123; if (mbd.resolvedConstructorOrFactoryMethod != null) &#123; resolved = true; autowireNecessary = mbd.constructorArgumentsResolved; &#125; &#125; &#125; if (resolved) &#123; if (autowireNecessary) &#123; // 构造函数依赖注入 return autowireConstructor(beanName, mbd, null, null); &#125; else &#123; // 无参构造函数 return instantiateBean(beanName, mbd); &#125; &#125; // 判断是否采用有参构造函数 Constructor&lt;?&gt;[] ctors = determineConstructorsFromBeanPostProcessors(beanClass, beanName); if (ctors != null || mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_CONSTRUCTOR || mbd.hasConstructorArgumentValues() || !ObjectUtils.isEmpty(args)) &#123; // 构造函数依赖注入 return autowireConstructor(beanName, mbd, ctors, args); &#125; // 调用无参构造函数 return instantiateBean(beanName, mbd);&#125; 挑个简单的无参构造函数构造实例来看看： 123456789101112131415161718192021222324252627protected BeanWrapper instantiateBean(final String beanName, final RootBeanDefinition mbd) &#123; try &#123; Object beanInstance; final BeanFactory parent = this; if (System.getSecurityManager() != null) &#123; beanInstance = AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; @Override public Object run() &#123; return getInstantiationStrategy().instantiate(mbd, beanName, parent); &#125; &#125;, getAccessControlContext()); &#125; else &#123; // 实例化 beanInstance = getInstantiationStrategy().instantiate(mbd, beanName, parent); &#125; // 包装一下，返回 BeanWrapper bw = new BeanWrapperImpl(beanInstance); initBeanWrapper(bw); return bw; &#125; catch (Throwable ex) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Instantiation of bean failed&quot;, ex); &#125;&#125; 我们可以看到，关键的地方在于： 1beanInstance = getInstantiationStrategy().instantiate(mbd, beanName, parent); 这里会进行实际的实例化过程，我们进去看看: // SimpleInstantiationStrategy 59 123456789101112131415161718192021222324252627282930313233343536373839404142@Overridepublic Object instantiate(RootBeanDefinition bd, String beanName, BeanFactory owner) &#123; // 如果不存在方法覆写，那就使用 java 反射进行实例化，否则使用 CGLIB, // 方法覆写 请参见附录&quot;方法注入&quot;中对 lookup-method 和 replaced-method 的介绍 if (bd.getMethodOverrides().isEmpty()) &#123; Constructor&lt;?&gt; constructorToUse; synchronized (bd.constructorArgumentLock) &#123; constructorToUse = (Constructor&lt;?&gt;) bd.resolvedConstructorOrFactoryMethod; if (constructorToUse == null) &#123; final Class&lt;?&gt; clazz = bd.getBeanClass(); if (clazz.isInterface()) &#123; throw new BeanInstantiationException(clazz, &quot;Specified class is an interface&quot;); &#125; try &#123; if (System.getSecurityManager() != null) &#123; constructorToUse = AccessController.doPrivileged(new PrivilegedExceptionAction&lt;Constructor&lt;?&gt;&gt;() &#123; @Override public Constructor&lt;?&gt; run() throws Exception &#123; return clazz.getDeclaredConstructor((Class[]) null); &#125; &#125;); &#125; else &#123; constructorToUse = clazz.getDeclaredConstructor((Class[]) null); &#125; bd.resolvedConstructorOrFactoryMethod = constructorToUse; &#125; catch (Throwable ex) &#123; throw new BeanInstantiationException(clazz, &quot;No default constructor found&quot;, ex); &#125; &#125; &#125; // 利用构造方法进行实例化 return BeanUtils.instantiateClass(constructorToUse); &#125; else &#123; // 存在方法覆写，利用 CGLIB 来完成实例化，需要依赖于 CGLIB 生成子类，这里就不展开了。 // tips: 因为如果不使用 CGLIB 的话，存在 override 的情况 JDK 并没有提供相应的实例化支持 return instantiateWithMethodInjection(bd, beanName, owner); &#125;&#125; 到这里，我们就算实例化完成了。我们开始说怎么进行属性注入。 bean 属性注入看完了 createBeanInstance(…) 方法，我们来看看 populateBean(…) 方法，该方法负责进行属性设值，处理依赖。 // AbstractAutowireCapableBeanFactory 1203 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778protected void populateBean(String beanName, RootBeanDefinition mbd, BeanWrapper bw) &#123; // bean 实例的所有属性都在这里了 PropertyValues pvs = mbd.getPropertyValues(); if (bw == null) &#123; if (!pvs.isEmpty()) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, &quot;Cannot apply property values to null instance&quot;); &#125; else &#123; // Skip property population phase for null instance. return; &#125; &#125; // 到这步的时候，bean 实例化完成（通过工厂方法或构造方法），但是还没开始属性设值， // InstantiationAwareBeanPostProcessor 的实现类可以在这里对 bean 进行状态修改， // 我也没找到有实际的使用，所以我们暂且忽略这块吧 boolean continueWithPropertyPopulation = true; if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof InstantiationAwareBeanPostProcessor) &#123; InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; // 如果返回 false，代表不需要进行后续的属性设值，也不需要再经过其他的 BeanPostProcessor 的处理 if (!ibp.postProcessAfterInstantiation(bw.getWrappedInstance(), beanName)) &#123; continueWithPropertyPopulation = false; break; &#125; &#125; &#125; &#125; if (!continueWithPropertyPopulation) &#123; return; &#125; if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME || mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) &#123; MutablePropertyValues newPvs = new MutablePropertyValues(pvs); // 通过名字找到所有属性值，如果是 bean 依赖，先初始化依赖的 bean。记录依赖关系 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME) &#123; autowireByName(beanName, mbd, bw, newPvs); &#125; // 通过类型装配。复杂一些 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) &#123; autowireByType(beanName, mbd, bw, newPvs); &#125; pvs = newPvs; &#125; boolean hasInstAwareBpps = hasInstantiationAwareBeanPostProcessors(); boolean needsDepCheck = (mbd.getDependencyCheck() != RootBeanDefinition.DEPENDENCY_CHECK_NONE); if (hasInstAwareBpps || needsDepCheck) &#123; PropertyDescriptor[] filteredPds = filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching); if (hasInstAwareBpps) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof InstantiationAwareBeanPostProcessor) &#123; InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; // 这里有个非常有用的 BeanPostProcessor 进到这里: AutowiredAnnotationBeanPostProcessor // 对采用 @Autowired、@Value 注解的依赖进行设值，这里的内容也是非常丰富的，不过本文不会展开说了，感兴趣的读者请自行研究 pvs = ibp.postProcessPropertyValues(pvs, filteredPds, bw.getWrappedInstance(), beanName); if (pvs == null) &#123; return; &#125; &#125; &#125; &#125; if (needsDepCheck) &#123; checkDependencies(beanName, mbd, filteredPds, pvs); &#125; &#125; // 设置 bean 实例的属性值 applyPropertyValues(beanName, mbd, bw, pvs);&#125; initializeBean属性注入完成后，这一步其实就是处理各种回调了，这块代码比较简单。 1234567891011121314151617181920212223242526272829303132333435363738protected Object initializeBean(final String beanName, final Object bean, RootBeanDefinition mbd) &#123; if (System.getSecurityManager() != null) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; @Override public Object run() &#123; invokeAwareMethods(beanName, bean); return null; &#125; &#125;, getAccessControlContext()); &#125; else &#123; // 如果 bean 实现了 BeanNameAware、BeanClassLoaderAware 或 BeanFactoryAware 接口，回调 invokeAwareMethods(beanName, bean); &#125; Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) &#123; // BeanPostProcessor 的 postProcessBeforeInitialization 回调 wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); &#125; try &#123; // 处理 bean 中定义的 init-method， // 或者如果 bean 实现了 InitializingBean 接口，调用 afterPropertiesSet() 方法 invokeInitMethods(beanName, wrappedBean, mbd); &#125; catch (Throwable ex) &#123; throw new BeanCreationException( (mbd != null ? mbd.getResourceDescription() : null), beanName, &quot;Invocation of init method failed&quot;, ex); &#125; if (mbd == null || !mbd.isSynthetic()) &#123; // BeanPostProcessor 的 postProcessAfterInitialization 回调 wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); &#125; return wrappedBean;&#125; 大家发现没有，BeanPostProcessor 的两个回调都发生在这边，只不过中间处理了 init-method，是不是和读者原来的认知有点不一样了？ 附录id 和 name每个 Bean 在 Spring 容器中都有一个唯一的名字（beanName）和 0 个或多个别名（aliases）。 我们从 Spring 容器中获取 Bean 的时候，可以根据 beanName，也可以通过别名。 1beanFactory.getBean(&quot;beanName or alias&quot;); 在配置 &lt;bean /&gt; 的过程中，我们可以配置 id 和 name，看几个例子就知道是怎么回事了。 1&lt;bean id=&quot;messageService&quot; name=&quot;m1, m2, m3&quot; class=&quot;com.javadoop.example.MessageServiceImpl&quot;&gt; 以上配置的结果就是：beanName 为 messageService，别名有 3 个，分别为 m1、m2、m3。 1&lt;bean name=&quot;m1, m2, m3&quot; class=&quot;com.javadoop.example.MessageServiceImpl&quot; /&gt; 以上配置的结果就是：beanName 为 m1，别名有 2 个，分别为 m2、m3。 1&lt;bean class=&quot;com.javadoop.example.MessageServiceImpl&quot;&gt; beanName 为：com.javadoop.example.MessageServiceImpl#0， 别名 1 个，为： com.javadoop.example.MessageServiceImpl 1&lt;bean id=&quot;messageService&quot; class=&quot;com.javadoop.example.MessageServiceImpl&quot;&gt; 以上配置的结果就是：beanName 为 messageService，没有别名。 配置是否允许 Bean 覆盖、是否允许循环依赖我们说过，默认情况下，allowBeanDefinitionOverriding 属性为 null。如果在同一配置文件中 Bean id 或 name 重复了，会抛错，但是如果不是同一配置文件中，会发生覆盖。 可是有些时候我们希望在系统启动的过程中就严格杜绝发生 Bean 覆盖，因为万一出现这种情况，会增加我们排查问题的成本。 循环依赖说的是 A 依赖 B，而 B 又依赖 A。或者是 A 依赖 B，B 依赖 C，而 C 却依赖 A。默认 allowCircularReferences 也是 null。 它们两个属性是一起出现的，必然可以在同一个地方一起进行配置。 添加这两个属性的作者 Juergen Hoeller 在这个 jira 的讨论中说明了怎么配置这两个属性。 123456789public class NoBeanOverridingContextLoader extends ContextLoader &#123; @Override protected void customizeContext(ServletContext servletContext, ConfigurableWebApplicationContext applicationContext) &#123; super.customizeContext(servletContext, applicationContext); AbstractRefreshableApplicationContext arac = (AbstractRefreshableApplicationContext) applicationContext; arac.setAllowBeanDefinitionOverriding(false); &#125;&#125; 12345678public class MyContextLoaderListener extends org.springframework.web.context.ContextLoaderListener &#123; @Override protected ContextLoader createContextLoader() &#123; return new NoBeanOverridingContextLoader(); &#125; &#125; 123&lt;listener&gt; &lt;listener-class&gt;com.javadoop.MyContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; 如果以上方式不能满足你的需求，请参考这个链接：解决spring中不同配置文件中存在name或者id相同的bean可能引起的问题 profile我们可以把不同环境的配置分别配置到单独的文件中，举个例子： 1234567891011&lt;beans profile=&quot;development&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:jdbc=&quot;http://www.springframework.org/schema/jdbc&quot; xsi:schemaLocation=&quot;...&quot;&gt; &lt;jdbc:embedded-database id=&quot;dataSource&quot;&gt; &lt;jdbc:script location=&quot;classpath:com/bank/config/sql/schema.sql&quot;/&gt; &lt;jdbc:script location=&quot;classpath:com/bank/config/sql/test-data.sql&quot;/&gt; &lt;/jdbc:embedded-database&gt;&lt;/beans&gt; 12345678&lt;beans profile=&quot;production&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:jee=&quot;http://www.springframework.org/schema/jee&quot; xsi:schemaLocation=&quot;...&quot;&gt; &lt;jee:jndi-lookup id=&quot;dataSource&quot; jndi-name=&quot;java:comp/env/jdbc/datasource&quot;/&gt;&lt;/beans&gt; 应该不必做过多解释了吧，看每个文件第一行的 profile=””。 当然，我们也可以在一个配置文件中使用： 1234567891011121314151617&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:jdbc=&quot;http://www.springframework.org/schema/jdbc&quot; xmlns:jee=&quot;http://www.springframework.org/schema/jee&quot; xsi:schemaLocation=&quot;...&quot;&gt; &lt;beans profile=&quot;development&quot;&gt; &lt;jdbc:embedded-database id=&quot;dataSource&quot;&gt; &lt;jdbc:script location=&quot;classpath:com/bank/config/sql/schema.sql&quot;/&gt; &lt;jdbc:script location=&quot;classpath:com/bank/config/sql/test-data.sql&quot;/&gt; &lt;/jdbc:embedded-database&gt; &lt;/beans&gt; &lt;beans profile=&quot;production&quot;&gt; &lt;jee:jndi-lookup id=&quot;dataSource&quot; jndi-name=&quot;java:comp/env/jdbc/datasource&quot;/&gt; &lt;/beans&gt;&lt;/beans&gt; 理解起来也很简单吧。 接下来的问题是，怎么使用特定的 profile 呢？Spring 在启动的过程中，会去寻找 “spring.profiles.active” 的属性值，根据这个属性值来的。那怎么配置这个值呢？ Spring 会在这几个地方寻找 spring.profiles.active 的属性值：操作系统环境变量、JVM 系统变量、web.xml 中定义的参数、JNDI。 最简单的方式莫过于在程序启动的时候指定： 1-Dspring.profiles.active=&quot;profile1,profile2&quot; profile 可以激活多个 当然，我们也可以通过代码的形式从 Environment 中设置 profile： 1234AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext();ctx.getEnvironment().setActiveProfiles(&quot;development&quot;);ctx.register(SomeConfig.class, StandaloneDataConfig.class, JndiDataConfig.class);ctx.refresh(); // 重启 如果是 Spring Boot 的话更简单，我们一般会创建 application.properties、application-dev.properties、application-prod.properties 等文件，其中 application.properties 配置各个环境通用的配置，application-{profile}.properties 中配置特定环境的配置，然后在启动的时候指定 profile： 1java -Dspring.profiles.active=prod -jar JavaDoop.jar 如果是单元测试中使用的话，在测试类中使用 @ActiveProfiles 指定，这里就不展开了。 工厂模式生成 Bean请读者注意 factory-bean 和 FactoryBean 的区别。这节说的是前者，是说静态工厂或实例工厂，而后者是 Spring 中的特殊接口，代表一类特殊的 Bean，附录的下面一节会介绍 FactoryBean。 设计模式里，工厂方法模式分静态工厂和实例工厂，我们分别看看 Spring 中怎么配置这两个，来个代码示例就什么都清楚了。 静态工厂： 123&lt;bean id=&quot;clientService&quot; class=&quot;examples.ClientService&quot; factory-method=&quot;createInstance&quot;/&gt; 123456789public class ClientService &#123; private static ClientService clientService = new ClientService(); private ClientService() &#123;&#125; // 静态方法 public static ClientService createInstance() &#123; return clientService; &#125;&#125; 实例工厂： 1234567891011&lt;bean id=&quot;serviceLocator&quot; class=&quot;examples.DefaultServiceLocator&quot;&gt; &lt;!-- inject any dependencies required by this locator bean --&gt;&lt;/bean&gt;&lt;bean id=&quot;clientService&quot; factory-bean=&quot;serviceLocator&quot; factory-method=&quot;createClientServiceInstance&quot;/&gt;&lt;bean id=&quot;accountService&quot; factory-bean=&quot;serviceLocator&quot; factory-method=&quot;createAccountServiceInstance&quot;/&gt; 1234567891011121314public class DefaultServiceLocator &#123; private static ClientService clientService = new ClientServiceImpl(); private static AccountService accountService = new AccountServiceImpl(); public ClientService createClientServiceInstance() &#123; return clientService; &#125; public AccountService createAccountServiceInstance() &#123; return accountService; &#125;&#125; FactoryBeanFactoryBean 适用于 Bean 的创建过程比较复杂的场景，比如数据库连接池的创建。 12345public interface FactoryBean&lt;T&gt; &#123; T getObject() throws Exception; Class&lt;T&gt; getObjectType(); boolean isSingleton();&#125; 1234public class Person &#123; private Car car ; private void setCar(Car car)&#123; this.car = car; &#125; &#125; 我们假设现在需要创建一个 Person 的 Bean，首先我们需要一个 Car 的实例，我们这里假设 Car 的实例创建很麻烦，那么我们可以把创建 Car 的复杂过程包装起来： 123456789101112131415161718192021public class MyCarFactoryBean implements FactoryBean&lt;Car&gt;&#123; private String make; private int year ; public void setMake(String m)&#123; this.make =m ; &#125; public void setYear(int y)&#123; this.year = y; &#125; public Car getObject()&#123; // 这里我们假设 Car 的实例化过程非常复杂，反正就不是几行代码可以写完的那种 CarBuilder cb = CarBuilder.car(); if(year!=0) cb.setYear(this.year); if(StringUtils.hasText(this.make)) cb.setMake( this.make ); return cb.factory(); &#125; public Class&lt;Car&gt; getObjectType() &#123; return Car.class ; &#125; public boolean isSingleton() &#123; return false; &#125;&#125; 我们看看装配的时候是怎么配置的： 1234567&lt;bean class = &quot;com.javadoop.MyCarFactoryBean&quot; id = &quot;car&quot;&gt; &lt;property name = &quot;make&quot; value =&quot;Honda&quot;/&gt; &lt;property name = &quot;year&quot; value =&quot;1984&quot;/&gt;&lt;/bean&gt;&lt;bean class = &quot;com.javadoop.Person&quot; id = &quot;josh&quot;&gt; &lt;property name = &quot;car&quot; ref = &quot;car&quot;/&gt;&lt;/bean&gt; 看到不一样了吗？id 为 “car” 的 bean 其实指定的是一个 FactoryBean，不过配置的时候，我们直接让配置 Person 的 Bean 直接依赖于这个 FactoryBean 就可以了。中间的过程 Spring 已经封装好了。 说到这里，我们再来点干货。我们知道，现在还用 xml 配置 Bean 依赖的越来越少了，更多时候，我们可能会采用 java config 的方式来配置，这里有什么不一样呢？ 12345678910111213141516171819@Configuration public class CarConfiguration &#123; @Bean public MyCarFactoryBean carFactoryBean()&#123; MyCarFactoryBean cfb = new MyCarFactoryBean(); cfb.setMake(&quot;Honda&quot;); cfb.setYear(1984); return cfb; &#125; @Bean public Person aPerson()&#123; Person person = new Person(); // 注意这里的不同 person.setCar(carFactoryBean().getObject()); return person; &#125; &#125; 这个时候，其实我们的思路也很简单，把 MyCarFactoryBean 看成是一个简单的 Bean 就可以了，不必理会什么 FactoryBean，它是不是 FactoryBean 和我们没关系。 初始化 Bean 的回调有以下四种方案： 1&lt;bean id=&quot;exampleInitBean&quot; class=&quot;examples.ExampleBean&quot; init-method=&quot;init&quot;/&gt; 123456public class AnotherExampleBean implements InitializingBean &#123; public void afterPropertiesSet() &#123; // do some initialization work &#125;&#125; 1234@Bean(initMethod = &quot;init&quot;)public Foo foo() &#123; return new Foo();&#125; 1234@PostConstructpublic void init() &#123; &#125; 销毁 Bean 的回调1&lt;bean id=&quot;exampleInitBean&quot; class=&quot;examples.ExampleBean&quot; destroy-method=&quot;cleanup&quot;/&gt; 123456public class AnotherExampleBean implements DisposableBean &#123; public void destroy() &#123; // do some destruction work (like releasing pooled connections) &#125;&#125; 1234@Bean(destroyMethod = &quot;cleanup&quot;)public Bar bar() &#123; return new Bar();&#125; 1234@PreDestroypublic void cleanup() &#123; &#125; ConversionService既然文中说到了这个，顺便提一下好了。 最有用的场景就是，它用来将前端传过来的参数和后端的 controller 方法上的参数进行绑定的时候用。 像前端传过来的字符串、整数要转换为后端的 String、Integer 很容易，但是如果 controller 方法需要的是一个枚举值，或者是 Date 这些非基础类型（含基础类型包装类）值的时候，我们就可以考虑采用 ConversionService 来进行转换。 12345678&lt;bean id=&quot;conversionService&quot; class=&quot;org.springframework.context.support.ConversionServiceFactoryBean&quot;&gt; &lt;property name=&quot;converters&quot;&gt; &lt;list&gt; &lt;bean class=&quot;com.javadoop.learning.utils.StringToEnumConverterFactory&quot;/&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; ConversionService 接口很简单，所以要自定义一个 convert 的话也很简单。 下面再说一个实现这种转换很简单的方式，那就是实现 Converter 接口。 来看一个很简单的例子，这样比什么都管用。 1234567891011public class StringToDateConverter implements Converter&lt;String, Date&gt; &#123; @Override public Date convert(String source) &#123; try &#123; return DateUtils.parseDate(source, &quot;yyyy-MM-dd&quot;, &quot;yyyy-MM-dd HH:mm:ss&quot;, &quot;yyyy-MM-dd HH:mm&quot;, &quot;HH:mm:ss&quot;, &quot;HH:mm&quot;); &#125; catch (ParseException e) &#123; return null; &#125; &#125;&#125; 只要注册这个 Bean 就可以了。这样，前端往后端传的时间描述字符串就很容易绑定成 Date 类型了，不需要其他任何操作。 Bean 继承在初始化 Bean 的地方，我们说过了这个： 1RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); 这里涉及到的就是 &lt;bean parent=&quot;&quot; /&gt; 中的 parent 属性，我们来看看 Spring 中是用这个来干什么的。 首先，我们要明白，这里的继承和 java 语法中的继承没有任何关系，不过思路是相通的。child bean 会继承 parent bean 的所有配置，也可以覆盖一些配置，当然也可以新增额外的配置。 Spring 中提供了继承自 AbstractBeanDefinition 的 ChildBeanDefinition 来表示 child bean。 看如下一个例子: 12345678910&lt;bean id=&quot;inheritedTestBean&quot; abstract=&quot;true&quot; class=&quot;org.springframework.beans.TestBean&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;parent&quot;/&gt; &lt;property name=&quot;age&quot; value=&quot;1&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;inheritsWithDifferentClass&quot; class=&quot;org.springframework.beans.DerivedTestBean&quot; parent=&quot;inheritedTestBean&quot; init-method=&quot;initialize&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;override&quot;/&gt;&lt;/bean&gt; parent bean 设置了 abstract=&quot;true&quot; 所以它不会被实例化，child bean 继承了 parent bean 的两个属性，但是对 name 属性进行了覆写。 child bean 会继承 scope、构造器参数值、属性值、init-method、destroy-method 等等。 当然，我不是说 parent bean 中的 abstract = true 在这里是必须的，只是说如果加上了以后 Spring 在实例化 singleton beans 的时候会忽略这个 bean。 比如下面这个极端 parent bean，它没有指定 class，所以毫无疑问，这个 bean 的作用就是用来充当模板用的 parent bean，此处就必须加上 abstract = true。 1234&lt;bean id=&quot;inheritedTestBeanWithoutClass&quot; abstract=&quot;true&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;parent&quot;/&gt; &lt;property name=&quot;age&quot; value=&quot;1&quot;/&gt;&lt;/bean&gt; 方法注入一般来说，我们的应用中大多数的 Bean 都是 singleton 的。singleton 依赖 singleton，或者 prototype 依赖 prototype 都很好解决，直接设置属性依赖就可以了。 但是，如果是 singleton 依赖 prototype 呢？这个时候不能用属性依赖，因为如果用属性依赖的话，我们每次其实拿到的还是第一次初始化时候的 bean。 一种解决方案就是不要用属性依赖，每次获取依赖的 bean 的时候从 BeanFactory 中取。这个也是大家最常用的方式了吧。怎么取，我就不介绍了，大部分 Spring 项目大家都会定义那么个工具类的。 另一种解决方案就是这里要介绍的通过使用 Lookup method。 lookup-method我们来看一下 Spring Reference 中提供的一个例子： 1234567891011121314151617package fiona.apple;// no more Spring imports!public abstract class CommandManager &#123; public Object process(Object commandState) &#123; // grab a new instance of the appropriate Command interface Command command = createCommand(); // set the state on the (hopefully brand new) Command instance command.setState(commandState); return command.execute(); &#125; // okay... but where is the implementation of this method? protected abstract Command createCommand();&#125; xml 配置 &lt;lookup-method /&gt;： 123456789&lt;!-- a stateful bean deployed as a prototype (non-singleton) --&gt;&lt;bean id=&quot;myCommand&quot; class=&quot;fiona.apple.AsyncCommand&quot; scope=&quot;prototype&quot;&gt; &lt;!-- inject dependencies here as required --&gt;&lt;/bean&gt;&lt;!-- commandProcessor uses statefulCommandHelper --&gt;&lt;bean id=&quot;commandManager&quot; class=&quot;fiona.apple.CommandManager&quot;&gt; &lt;lookup-method name=&quot;createCommand&quot; bean=&quot;myCommand&quot;/&gt;&lt;/bean&gt; Spring 采用 CGLIB 生成字节码的方式来生成一个子类。我们定义的类不能定义为 final class，抽象方法上也不能加 final。 lookup-method 上的配置也可以采用注解来完成，这样就可以不用配置 &lt;lookup-method /&gt; 了，其他不变： 1234567891011public abstract class CommandManager &#123; public Object process(Object commandState) &#123; MyCommand command = createCommand(); command.setState(commandState); return command.execute(); &#125; @Lookup(&quot;myCommand&quot;) protected abstract Command createCommand();&#125; 注意，既然用了注解，要配置注解扫描：&lt;context:component-scan base-package=&quot;com.javadoop&quot; /&gt; 甚至，我们可以像下面这样： 1234567891011public abstract class CommandManager &#123; public Object process(Object commandState) &#123; MyCommand command = createCommand(); command.setState(commandState); return command.execute(); &#125; @Lookup protected abstract MyCommand createCommand();&#125; 上面的返回值用了 MyCommand，当然，如果 Command 只有一个实现类，那返回值也可以写 Command。 replaced-method记住它的功能，就是替换掉 bean 中的一些方法。 12345678public class MyValueCalculator &#123; public String computeValue(String input) &#123; // some real code... &#125; // some other methods...&#125; 方法覆写，注意要实现 MethodReplacer 接口： 123456789public class ReplacementComputeValue implements org.springframework.beans.factory.support.MethodReplacer &#123; public Object reimplement(Object o, Method m, Object[] args) throws Throwable &#123; // get the input value, work with it, and return a computed result String input = (String) args[0]; ... return ...; &#125;&#125; 配置也很简单： 12345678&lt;bean id=&quot;myValueCalculator&quot; class=&quot;x.y.z.MyValueCalculator&quot;&gt; &lt;!-- 定义 computeValue 这个方法要被替换掉 --&gt; &lt;replaced-method name=&quot;computeValue&quot; replacer=&quot;replacementComputeValue&quot;&gt; &lt;arg-type&gt;String&lt;/arg-type&gt; &lt;/replaced-method&gt;&lt;/bean&gt;&lt;bean id=&quot;replacementComputeValue&quot; class=&quot;a.b.c.ReplacementComputeValue&quot;/&gt; arg-type 明显不是必须的，除非存在方法重载，这样必须通过参数类型列表来判断这里要覆盖哪个方法。 BeanPostProcessor应该说 BeanPostProcessor 概念在 Spring 中也是比较重要的。我们看下接口定义： 1234567public interface BeanPostProcessor &#123; Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException; Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException;&#125; 看这个接口中的两个方法名字我们大体上可以猜测 bean 在初始化之前会执行 postProcessBeforeInitialization 这个方法，初始化完成之后会执行 postProcessAfterInitialization 这个方法。但是，这么理解是非常片面的。 首先，我们要明白，除了我们自己定义的 BeanPostProcessor 实现外，Spring 容器在启动时自动给我们也加了几个。如在获取 BeanFactory 的 obtainFactory() 方法结束后的 prepareBeanFactory(factory)，大家仔细看会发现，Spring 往容器中添加了这两个 BeanPostProcessor：ApplicationContextAwareProcessor、ApplicationListenerDetector。 我们回到这个接口本身，读者请看第一个方法，这个方法接受的第一个参数是 bean 实例，第二个参数是 bean 的名字，重点在返回值将会作为新的 bean 实例，所以，没事的话这里不能随便返回个 null。 那意味着什么呢？我们很容易想到的就是，我们这里可以对一些我们想要修饰的 bean 实例做一些事情。但是对于 Spring 框架来说，它会决定是不是要在这个方法中返回 bean 实例的代理，这样就有更大的想象空间了。 最后，我们说说如果我们自己定义一个 bean 实现 BeanPostProcessor 的话，它的执行时机是什么时候？ 如果仔细看了代码分析的话，其实很容易知道了，在 bean 实例化完成、属性注入完成之后，会执行回调方法，具体请参见类 AbstractAutowireCapableBeanFactory#initBean 方法。 首先会回调几个实现了 Aware 接口的 bean，然后就开始回调 BeanPostProcessor 的 postProcessBeforeInitialization 方法，之后是回调 init-method，然后再回调 BeanPostProcessor 的 postProcessAfterInitialization 方法。 总结按理说，总结应该写在附录前面，我就不讲究了。 在花了那么多时间后，这篇文章终于算是基本写完了，大家在惊叹 Spring 给我们做了那么多的事的时候，应该透过现象看本质，去理解 Spring 写得好的地方，去理解它的设计思想。 本文的缺陷在于对 Spring 预初始化 singleton beans 的过程分析不够，主要是代码量真的比较大，分支旁路众多。同时，虽然附录条目不少，但是庞大的 Spring 真的引出了很多的概念，希望日后有精力可以慢慢补充一些。 （全文完）","categories":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/tags/Spring/"},{"name":"IOC","slug":"IOC","permalink":"https://xmmarlowe.github.io/tags/IOC/"}],"author":"Marlowe"},{"title":"Spring AOP 运行原理简析","slug":"Spring/Spring-AOP-运行原理简析","date":"2021-05-16T09:06:14.000Z","updated":"2021-05-16T14:24:53.973Z","comments":true,"path":"2021/05/16/Spring/Spring-AOP-运行原理简析/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/16/Spring/Spring-AOP-%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86%E7%AE%80%E6%9E%90/","excerpt":"","text":"AOP联盟标准 AOP联盟将AOP体系分为三层，从三层结构可以看出，AOP实现方式有很多种，包括反射、元数据处理、程序处理、拦截器处理等，通过本节学习，你就会看到Spring AOP的实现使用的是Java语言本身的特性，即Java Proxy代理类、拦截器技术实现。 AOP简介概念切面（Aspect）： 官方的抽象定义为“一个关注点的模块化，这个关注点可能会横切多个对象”。连接点（Joinpoint）： 程序执行过程中的某一行为。通知（Advice）： “切面”对于某个“连接点”所产生的动作。切入点（Pointcut）： 匹配连接点的断言，在AOP中通知和一个切入点表达式关联。目标对象（Target Object）： 被一个或者多个切面所通知的对象。 AOP代理（AOP Proxy） 在Spring AOP中有两种代理方式，JDK动态代理和CGLIB代理。 通知（Advice）类型 前置通知（Before advice）： 在某连接点（JoinPoint）之前执行的通知，但这个通知不能阻止连接点前的执行。ApplicationContext中在&lt;aop:aspect&gt;里面使用aop:before元素进行声明。后通知（After advice）： 当某连接点退出的时候执行的通知（不论是正常返回还是异常退出）。ApplicationContext中在&lt;aop:aspect&gt;里面使用&lt;aop:after&gt;元素进行声明。返回后通知（After return advice）： 在某连接点正常完成后执行的通知，不包括抛出异常的情况。ApplicationContext中在&lt;aop:aspect&gt;里面使用&lt;after-returning&gt;元素进行声明。环绕通知（Around advice）： 包围一个连接点的通知，类似Web中Servlet规范中的Filter的doFilter方法。可以在方法的调用前后完成自定义的行为，也可以选择不执行。ApplicationContext中在&lt;aop:aspect&gt;里面使用&lt;aop:around&gt;元素进行声明。抛出异常后通知（After throwing advice）： 在方法抛出异常退出时执行的通知。 ApplicationContext中在aop:aspect里面使用aop:after-throwing元素进行声明。 1切入点表达式 ：如execution(* com.spring.service.*.*(..)) 特点1、降低模块之间的耦合度 2、使系统容易扩展 3、更好的代码复用。 时序图 流程说明1）AOP标签的定义解析刘彻骨肯定是从NamespaceHandlerSupport的实现类开始解析的，这个实现类就是AopNamespaceHandler。至于为什么会是从NamespaceHandlerSupport的实现类开始解析的，这个的话我想读者可以去在回去看看Spring自定义标签的解析流程，里面说的比较详细。 2）要启用AOP，我们一般会在Spring里面配置aop:aspectj-autoproxy/ ，所以在配置文件中在遇到aspectj-autoproxy标签的时候我们会采用AspectJAutoProxyBeanDefinitionParser解析器 3）进入AspectJAutoProxyBeanDefinitionParser解析器后，调用AspectJAutoProxyBeanDefinitionParser已覆盖BeanDefinitionParser的parser方法，然后parser方法把请求转交给了AopNamespaceUtils的registerAspectJAnnotationAutoProxyCreatorIfNecessary去处理 4）进入AopNamespaceUtils的registerAspectJAnnotationAutoProxyCreatorIfNecessary方法后，先调用AopConfigUtils的registerAspectJAnnotationAutoProxyCreatorIfNecessary方法，里面在转发调用给registerOrEscalateApcAsRequired，注册或者升级AnnotationAwareAspectJAutoProxyCreator类。对于AOP的实现，基本是靠AnnotationAwareAspectJAutoProxyCreator去完成的，它可以根据@point注解定义的切点来代理相匹配的bean。 5）AopConfigUtils的registerAspectJAnnotationAutoProxyCreatorIfNecessary方法处理完成之后，接下来会调用useClassProxyingIfNecessary() 处理proxy-target-class以及expose-proxy属性。如果将proxy-target-class设置为true的话，那么会强制使用CGLIB代理，否则使用jdk动态代理，expose-proxy属性是为了解决有时候目标对象内部的自我调用无法实现切面增强。 6）最后的调用registerComponentIfNecessary 方法，注册组建并且通知便于监听器做进一步处理。 创建AOP代理上面说到AOP的核心逻辑是在AnnotationAwareAspectJAutoProxyCreator类里面实现，那么我们先来看看这个类的层次关系 这个类实现了BeanPostProcessor接口，那就意味着这个类在spring加载实例化前会调用postProcessAfterInitialization方法，对于AOP的逻辑也是由此开始的。 时序图 流程说明1）spring 容器启动，每个bean的实例化之前都会先经过AbstractAutoProxyCreator类的postProcessAfterInitialization（）这个方法，然后接下来是调用wrapIfNecessary方法。 1234567891011121314/** * Create a proxy with the configured interceptors if the bean is * identified as one to proxy by the subclass. * @see #getAdvicesAndAdvisorsForBean */ public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; if (bean != null) &#123; Object cacheKey = getCacheKey(bean.getClass(), beanName); if (!this.earlyProxyReferences.containsKey(cacheKey)) &#123; return wrapIfNecessary(bean, beanName, cacheKey); &#125; &#125; return bean; &#125; 2）进入wrapIfNecessary方法后，我们直接看重点实现逻辑的方法getAdvicesAndAdvisorsForBean，这个方法会提取当前bean 的所有增强方法，然后获取到适合的当前bean 的增强方法，然后对增强方法进行排序，最后返回 12345678910111213141516171819202122232425262728293031/** * Wrap the given bean if necessary, i.e. if it is eligible for being proxied. * @param bean the raw bean instance * @param beanName the name of the bean * @param cacheKey the cache key for metadata access * @return a proxy wrapping the bean, or the raw bean instance as-is */ protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) &#123; if (beanName != null &amp;&amp; this.targetSourcedBeans.containsKey(beanName)) &#123; return bean; &#125; if (Boolean.FALSE.equals(this.advisedBeans.get(cacheKey))) &#123; return bean; &#125; if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) &#123; this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; &#125; // Create proxy if we have advice. Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); if (specificInterceptors != DO_NOT_PROXY) &#123; this.advisedBeans.put(cacheKey, Boolean.TRUE); Object proxy = createProxy(bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean)); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; &#125; this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; &#125; 3）获取到当前bean的增强方法后，便调用createProxy方法，创建代理。先创建代理工厂proxyFactory，然后获取当前bean 的增强器advisors，把当前获取到的增强器添加到代理工厂proxyFactory，然后设置当前的代理工的代理目标对象为当前bean，最后根据配置创建JDK的动态代理工厂，或者CGLIB的动态代理工厂，然后返回proxyFactory 123456789101112131415161718192021222324252627282930313233343536373839404142/** * Create an AOP proxy for the given bean. * @param beanClass the class of the bean * @param beanName the name of the bean * @param specificInterceptors the set of interceptors that is * specific to this bean (may be empty, but not null) * @param targetSource the TargetSource for the proxy, * already pre-configured to access the bean * @return the AOP proxy for the bean * @see #buildAdvisors */ protected Object createProxy( Class&lt;?&gt; beanClass, String beanName, Object[] specificInterceptors, TargetSource targetSource) &#123; ProxyFactory proxyFactory = new ProxyFactory(); // Copy our properties (proxyTargetClass etc) inherited from ProxyConfig. proxyFactory.copyFrom(this); if (!shouldProxyTargetClass(beanClass, beanName)) &#123; // Must allow for introductions; can&#x27;t just set interfaces to // the target&#x27;s interfaces only. Class&lt;?&gt;[] targetInterfaces = ClassUtils.getAllInterfacesForClass(beanClass, this.proxyClassLoader); for (Class&lt;?&gt; targetInterface : targetInterfaces) &#123; proxyFactory.addInterface(targetInterface); &#125; &#125; Advisor[] advisors = buildAdvisors(beanName, specificInterceptors); for (Advisor advisor : advisors) &#123; proxyFactory.addAdvisor(advisor); &#125; proxyFactory.&lt;strong&gt;setTargetSource&lt;/strong&gt;(targetSource); customizeProxyFactory(proxyFactory); proxyFactory.setFrozen(this.freezeProxy); if (advisorsPreFiltered()) &#123; proxyFactory.setPreFiltered(true); &#125; return proxyFactory.getProxy(this.proxyClassLoader); &#125; AOP动态代理执行关于AOP的动态代理执行，有两种主要的方式JDK的动态代理和CGLIB的动态代理，接下来，我们先来看看AOP动态代理的实现选择方式，先上核心实现代码： 12345678910111213141516public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException &#123; if (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) &#123; Class targetClass = config.getTargetClass(); if (targetClass == null) &#123; throw new AopConfigException(&quot;TargetSource cannot determine target class: &quot; + &quot;Either an interface or a target is required for proxy creation.&quot;); &#125; if (targetClass.isInterface()) &#123; return new JdkDynamicAopProxy(config); &#125; return CglibProxyFactory.createCglibProxy(config); &#125; else &#123; return new JdkDynamicAopProxy(config); &#125; &#125; Spring JDK动态代理实现在上面的第三步骤说道或根据用户的配置（例如是否配置了 proxyTargetClass属性为true），选择创建的代理类型，这个的代理类型分两种实现，都是比较高效的，下面根据JDK的动态代理来说明AOP的执行，也是先上JdkDynamicAopProxy的核心代码invoke方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public Object invoke(Object proxy, Method method, Object[] args) throwsThrowable &#123; MethodInvocation invocation = null; Object oldProxy = null; boolean setProxyContext = false; TargetSource targetSource = this.advised.targetSource; Class targetClass = null; Object target = null; try &#123; //eqauls()方法，具目标对象未实现此方法 if (!this.equalsDefined &amp;&amp; AopUtils.isEqualsMethod(method))&#123; return (equals(args[0])? Boolean.TRUE : Boolean.FALSE); &#125; //hashCode()方法，具目标对象未实现此方法 if (!this.hashCodeDefined &amp;&amp; AopUtils.isHashCodeMethod(method))&#123; return newInteger(hashCode()); &#125; //Advised接口或者其父接口中定义的方法,直接反射调用,不应用通知 if (!this.advised.opaque &amp;&amp;method.getDeclaringClass().isInterface() &amp;&amp;method.getDeclaringClass().isAssignableFrom(Advised.class)) &#123; // Service invocations onProxyConfig with the proxy config... return AopUtils.invokeJoinpointUsingReflection(this.advised,method, args); &#125; Object retVal = null; if (this.advised.exposeProxy) &#123; // Make invocation available ifnecessary. oldProxy = AopContext.setCurrentProxy(proxy); setProxyContext = true; &#125; //获得目标对象的类 target = targetSource.getTarget(); if (target != null) &#123; targetClass = target.getClass(); &#125; //获取可以应用到此方法上的Interceptor列表 List chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method,targetClass); //如果没有可以应用到此方法的通知(Interceptor)，此直接反射调用 method.invoke(target, args) if (chain.isEmpty()) &#123; retVal = AopUtils.invokeJoinpointUsingReflection(target,method, args); &#125; else &#123; //创建MethodInvocation invocation = newReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain); retVal = invocation.proceed(); &#125; // Massage return value if necessary. if (retVal != null &amp;&amp; retVal == target &amp;&amp;method.getReturnType().isInstance(proxy) &amp;&amp;!RawTargetAccess.class.isAssignableFrom(method.getDeclaringClass())) &#123; // Special case: it returned&quot;this&quot; and the return type of the method // is type-compatible. Notethat we can&#x27;t help if the target sets // a reference to itself inanother returned object. retVal = proxy; &#125; return retVal; &#125; finally &#123; if (target != null &amp;&amp; !targetSource.isStatic()) &#123; // Must have come fromTargetSource. targetSource.releaseTarget(target); &#125; if (setProxyContext) &#123; // Restore old proxy. AopContext.setCurrentProxy(oldProxy); &#125; &#125; &#125; 其实上面的注释也说的比较清楚，各个步骤执行的说明： 1）获取拦截器2）判断拦截器链是否为空，如果是空的话直接调用切点方法3）如果拦截器不为空的话那么便创建ReflectiveMethodInvocation类，把拦截器方法都封装在里面，也就是执行 getInterceptorsAndDynamicInterceptionAdvice方法 12345678910public List&lt;Object&gt; getInterceptorsAndDynamicInterceptionAdvice(Method method, Class targetClass) &#123; MethodCacheKeycacheKey = new MethodCacheKey(method); List&lt;Object&gt;cached = this.methodCache.get(cacheKey); if(cached == null) &#123; cached= this.advisorChainFactory.getInterceptorsAndDynamicInterceptionAdvice( this,method, targetClass); this.methodCache.put(cacheKey,cached); &#125; returncached; &#125; 4)其实实际的获取工作其实是由AdvisorChainFactory. getInterceptorsAndDynamicInterceptionAdvice()这个方法来完成的，获取到的结果会被缓存，下面来分析下这个方法的实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * 从提供的配置实例config中获取advisor列表,遍历处理这些advisor.如果是IntroductionAdvisor, * 则判断此Advisor能否应用到目标类targetClass上.如果是PointcutAdvisor,则判断 * 此Advisor能否应用到目标方法method上.将满足条件的Advisor通过AdvisorAdaptor转化成Interceptor列表返回. */ publicList getInterceptorsAndDynamicInterceptionAdvice(Advised config, Methodmethod, Class targetClass) &#123; // This is somewhat tricky... we have to process introductions first, // but we need to preserve order in the ultimate list. List interceptorList = new ArrayList(config.getAdvisors().length); //查看是否包含IntroductionAdvisor boolean hasIntroductions = hasMatchingIntroductions(config,targetClass); //这里实际上注册一系列AdvisorAdapter,用于将Advisor转化成MethodInterceptor AdvisorAdapterRegistry registry = GlobalAdvisorAdapterRegistry.getInstance(); Advisor[] advisors = config.getAdvisors(); for (int i = 0; i &lt;advisors.length; i++) &#123; Advisor advisor = advisors[i]; if (advisor instanceof PointcutAdvisor) &#123; // Add it conditionally. PointcutAdvisor pointcutAdvisor= (PointcutAdvisor) advisor; if(config.isPreFiltered() ||pointcutAdvisor.getPointcut().getClassFilter().matches(targetClass)) &#123; //TODO: 这个地方这两个方法的位置可以互换下 //将Advisor转化成Interceptor MethodInterceptor[]interceptors = registry.getInterceptors(advisor); //检查当前advisor的pointcut是否可以匹配当前方法 MethodMatcher mm =pointcutAdvisor.getPointcut().getMethodMatcher(); if (MethodMatchers.matches(mm,method, targetClass, hasIntroductions)) &#123; if(mm.isRuntime()) &#123; // Creating a newobject instance in the getInterceptors() method // isn&#x27;t a problemas we normally cache created chains. for (intj = 0; j &lt; interceptors.length; j++) &#123; interceptorList.add(new InterceptorAndDynamicMethodMatcher(interceptors[j],mm)); &#125; &#125; else &#123; interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; &#125; &#125; else if (advisor instanceof IntroductionAdvisor)&#123; IntroductionAdvisor ia =(IntroductionAdvisor) advisor; if(config.isPreFiltered() || ia.getClassFilter().matches(targetClass)) &#123; Interceptor[] interceptors= registry.getInterceptors(advisor); interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; else &#123; Interceptor[] interceptors =registry.getInterceptors(advisor); interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; return interceptorList; &#125; 5）这个方法执行完成后，Advised中配置能够应用到连接点或者目标类的Advisor全部被转化成了MethodInterceptor.6）接下来货到invoke方法中的proceed方法 ，我们再看下得到的拦截器链是怎么起作用的，也就是proceed方法的执行过程 123456789101112131415161718192021222324252627282930313233public Object proceed() throws Throwable &#123; // We start with an index of -1and increment early. if (this.currentInterceptorIndex == this.interceptorsAndDynamicMethodMatchers.size()- 1) &#123; //如果Interceptor执行完了，则执行joinPoint return invokeJoinpoint(); &#125; Object interceptorOrInterceptionAdvice = this.interceptorsAndDynamicMethodMatchers.get(++this.currentInterceptorIndex); //如果要动态匹配joinPoint if (interceptorOrInterceptionAdvice instanceof InterceptorAndDynamicMethodMatcher)&#123; // Evaluate dynamic method matcher here: static part will already have // been evaluated and found to match. InterceptorAndDynamicMethodMatcher dm = (InterceptorAndDynamicMethodMatcher)interceptorOrInterceptionAdvice; //动态匹配：运行时参数是否满足匹配条件 if (dm.methodMatcher.matches(this.method, this.targetClass,this.arguments)) &#123; //执行当前Intercetpor returndm.interceptor.invoke(this); &#125; else &#123; //动态匹配失败时,略过当前Intercetpor,调用下一个Interceptor return proceed(); &#125; &#125; else &#123; // It&#x27;s an interceptor, so we just invoke it: The pointcutwill have // been evaluated statically before this object was constructed. //执行当前Intercetpor return ((MethodInterceptor) interceptorOrInterceptionAdvice).invoke(this); &#125; &#125; 7）好了拦截器到这边就可以执行了，复杂的代理终于可以起到他的作用了 Spring CGLIB动态代理实现由于CGLIB的动态代理代码量比较长，在这就不贴出来代码了，其实这两个代理的实现方式都差不多，都是创建方法调用链，不同的是jdk的动态代理创建的是ReflectiveMethodInvocation调用链，而cglib创建的是Cglib MethodInvocation。 参考Spring AOP实现原理简介","categories":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/tags/Spring/"},{"name":"AOP","slug":"AOP","permalink":"https://xmmarlowe.github.io/tags/AOP/"}],"author":"Marlowe"},{"title":"Spring中BeanFactory和FactoryBean的区别","slug":"Spring/Spring中BeanFactory和FactoryBean的区别","date":"2021-05-16T08:36:13.000Z","updated":"2021-05-16T14:24:53.922Z","comments":true,"path":"2021/05/16/Spring/Spring中BeanFactory和FactoryBean的区别/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/16/Spring/Spring%E4%B8%ADBeanFactory%E5%92%8CFactoryBean%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"BeanFactory是个Factory，也就是IOC容器或对象工厂，FactoryBean是个Bean。在Spring中，所有的Bean都是由BeanFactory(也就是IOC容器)来进行管理的。但对FactoryBean而言，这个Bean不是简单的Bean，而是一个能生产或者修饰对象生成的工厂Bean,它的实现与设计模式中的工厂模式和修饰器模式类似。","text":"BeanFactory是个Factory，也就是IOC容器或对象工厂，FactoryBean是个Bean。在Spring中，所有的Bean都是由BeanFactory(也就是IOC容器)来进行管理的。但对FactoryBean而言，这个Bean不是简单的Bean，而是一个能生产或者修饰对象生成的工厂Bean,它的实现与设计模式中的工厂模式和修饰器模式类似。 BeanFactoryBeanFactory定义了IOC容器的最基本形式，并提供了IOC容器应遵守的的最基本的接口，也就是Spring IOC所遵守的最底层和最基本的编程规范。在Spring代码中，BeanFactory只是个接口，并不是IOC容器的具体实现，但是Spring容器给出了很多种实现，如 DefaultListableBeanFactory、XmlBeanFactory、ApplicationContext等，都是附加了某种功能的实现。 123456789101112131415package org.springframework.beans.factory; import org.springframework.beans.BeansException; public interface BeanFactory &#123; String FACTORY_BEAN_PREFIX = &quot;&amp;&quot;; Object getBean(String name) throws BeansException; &lt;T&gt; T getBean(String name, Class&lt;T&gt; requiredType) throws BeansException; &lt;T&gt; T getBean(Class&lt;T&gt; requiredType) throws BeansException; Object getBean(String name, Object... args) throws BeansException; boolean containsBean(String name); boolean isSingleton(String name) throws NoSuchBeanDefinitionException; boolean isPrototype(String name) throws NoSuchBeanDefinitionException; boolean isTypeMatch(String name, Class&lt;?&gt; targetType) throws NoSuchBeanDefinitionException; Class&lt;?&gt; getType(String name) throws NoSuchBeanDefinitionException; String[] getAliases(String name); &#125; FactoryBean一般情况下，Spring通过反射机制利用&lt;bean&gt;的class属性指定实现类实例化Bean，在某些情况下，实例化Bean过程比较复杂，如果按照传统的方式，则需要在&lt;bean&gt;中提供大量的配置信息。配置方式的灵活性是受限的，这时采用编码的方式可能会得到一个简单的方案。Spring为此提供了一个org.springframework.bean.factory.FactoryBean的工厂类接口，用户可以通过实现该接口定制实例化Bean的逻辑。FactoryBean接口对于Spring框架来说占用重要的地位，Spring自身就提供了70多个FactoryBean的实现。它们隐藏了实例化一些复杂Bean的细节，给上层应用带来了便利。从Spring3.0开始，FactoryBean开始支持泛型，即接口声明改为FactoryBean&lt;T&gt;的形式 123456package org.springframework.beans.factory; public interface FactoryBean&lt;T&gt; &#123; T getObject() throws Exception; Class&lt;?&gt; getObjectType(); boolean isSingleton(); &#125; 在该接口中还定义了以下3个方法： T getObject()： 返回由FactoryBean创建的Bean实例，如果isSingleton()返回true，则该实例会放到Spring容器中单实例缓存池中； boolean isSingleton()： 返回由FactoryBean创建的Bean实例的作用域是singleton还是prototype； Class getObjectType()： 返回FactoryBean创建的Bean类型。当配置文件中的class属性配置的实现类是FactoryBean时，通过getBean()方法返回的不是FactoryBean本身，而是FactoryBean#getObject()方法所返回的对象，相当于FactoryBean#getObject()代理了getBean()方法。 例：如果使用传统方式配置下面Car的&lt;bean&gt;时，Car的每个属性分别对应一个&lt;property&gt;元素标签。 123456789101112131415161718192021222324package com.baobaotao.factorybean; public class Car &#123; private int maxSpeed ; private String brand ; private double price ; public int getMaxSpeed () &#123; return this . maxSpeed ; &#125; public void setMaxSpeed ( int maxSpeed ) &#123; this . maxSpeed = maxSpeed; &#125; public String getBrand () &#123; return this . brand ; &#125; public void setBrand ( String brand ) &#123; this . brand = brand; &#125; public double getPrice () &#123; return this . price ; &#125; public void setPrice ( double price ) &#123; this . price = price; &#125; &#125; 如果用FactoryBean的方式实现就灵活点，下例通过逗号分割符的方式一次性的为Car的所有属性指定配置值： 123456789101112131415161718192021222324252627package com.baobaotao.factorybean; import org.springframework.beans.factory.FactoryBean; public class CarFactoryBean implements FactoryBean&lt;Car&gt; &#123; private String carInfo ; public Car getObject () throws Exception &#123; Car car = new Car () ; String [] infos = carInfo .split ( &quot;,&quot; ) ; car.setBrand ( infos [ 0 ]) ; car.setMaxSpeed ( Integer. valueOf ( infos [ 1 ])) ; car.setPrice ( Double. valueOf ( infos [ 2 ])) ; return car; &#125; public Class&lt;Car&gt; getObjectType () &#123; return Car. class ; &#125; public boolean isSingleton () &#123; return false ; &#125; public String getCarInfo () &#123; return this . carInfo ; &#125; // 接受逗号分割符设置属性信息 public void setCarInfo ( String carInfo ) &#123; this . carInfo = carInfo; &#125; &#125; 有了这个CarFactoryBean后，就可以在配置文件中使用下面这种自定义的配置方式配置CarBean了：&lt;beanid=&quot;car&quot;class=&quot;com.baobaotao.factorybean.CarFactoryBean&quot; P:carInfo=&quot;法拉利,400,2000000&quot;/&gt;当调用getBean(“car”)时，Spring通过反射机制发现CarFactoryBean实现了FactoryBean的接口，这时Spring容器就调用接口方法CarFactoryBean#getObject()方法返回。如果希望获取CarFactoryBean的实例，则需要在使用getBean(beanName)方法时在beanName前显示的加上”&amp;”前缀：如getBean(“&amp;car”); 区别BeanFactory是个Factory，也就是IOC容器或对象工厂，FactoryBean是个Bean。在Spring中，所有的Bean都是由BeanFactory(也就是IOC容器)来进行管理的。但对FactoryBean而言，这个Bean不是简单的Bean，而是一个能生产或者修饰对象生成的工厂Bean,它的实现与设计模式中的工厂模式和修饰器模式类似。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/categories/Spring/"}],"tags":[{"name":"Bean","slug":"Bean","permalink":"https://xmmarlowe.github.io/tags/Bean/"},{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/tags/Spring/"},{"name":"Factory","slug":"Factory","permalink":"https://xmmarlowe.github.io/tags/Factory/"}],"author":"Marlowe"},{"title":"Spring的启动过程","slug":"Spring/Spring的启动过程","date":"2021-05-16T07:51:21.000Z","updated":"2021-05-17T14:03:25.728Z","comments":true,"path":"2021/05/16/Spring/Spring的启动过程/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/16/Spring/Spring%E7%9A%84%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B/","excerpt":"","text":"spring的启动是建筑在servlet容器之上的，所有web工程的初始位置就是web.xml,它配置了servlet的上下文（context）和监听器（Listener），下面就来看看web.xml里面的配置： 123456789101112131415161718192021222324252627282930313233343536&lt;!--上下文监听器，用于监听servlet的启动过程--&gt;&lt;listener&gt; &lt;description&gt;ServletContextListener&lt;/description&gt; &lt;!--这里是自定义监听器，个性化定制项目启动提示--&gt; &lt;listener-class&gt;com.trace.app.framework.listeners.ApplicationListener&lt;/listener-class&gt; &lt;/listener&gt;&lt;!--dispatcherServlet的配置，这个servlet主要用于前端控制，这是springMVC的基础--&gt; &lt;servlet&gt; &lt;servlet-name&gt;service_dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;/WEB-INF/spring/services/service_dispatcher-servlet.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt;&lt;!--spring资源上下文定义，在指定地址找到spring的xml配置文件--&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;/WEB-INF/spring/application_context.xml&lt;/param-value&gt; &lt;/context-param&gt;&lt;!--spring的上下文监听器--&gt; &lt;listener&gt; &lt;listener-class&gt; org.springframework.web.context.ContextLoaderListener &lt;/listener-class&gt; &lt;/listener&gt;&lt;!--Session监听器，Session作为公共资源存在上下文资源当中，这里也是自定义监听器--&gt; &lt;listener&gt; &lt;listener-class&gt; com.trace.app.framework.listeners.MySessionListener &lt;/listener-class&gt; &lt;/listener&gt; 接下来就一点的来解析这样一个启动过程。 spring的上下文监听器代码如下： 12345678910&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;/WEB-INF/spring/application_context.xml&lt;/param-value&gt;&lt;/context-param&gt;&lt;listener&gt; &lt;listener-class&gt; org.springframework.web.context.ContextLoaderListener &lt;/listener-class&gt; &lt;/listener&gt; spring的启动其实就是IOC容器的启动过程，通过上述的第一段配置&lt;context-param&gt;是初始化上下文，然后通过后一段的的来加载配置文件，其中调用的spring包中的ContextLoaderListener这个上下文监听器，ContextLoaderListener是一个实现了ServletContextListener接口的监听器，他的父类是 ContextLoader，在启动项目时会触发contextInitialized上下文初始化方法。下面我们来看看这个方法： 123public void contextInitialized(ServletContextEvent event) &#123; initWebApplicationContext(event.getServletContext());&#125; 可以看到，这里是调用了父类ContextLoader的initWebApplicationContext(event.getServletContext());方法，很显然，这是对ApplicationContext的初始化方法，也就是到这里正是进入了springIOC的初始化。 接下来再来看看initWebApplicationContext又做了什么工作，先看看代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364if (servletContext.getAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE) != null) &#123; throw new IllegalStateException( &quot;Cannot initialize context because there is already a root application context present - &quot; + &quot;check whether you have multiple ContextLoader* definitions in your web.xml!&quot;); &#125; Log logger = LogFactory.getLog(ContextLoader.class); servletContext.log(&quot;Initializing Spring root WebApplicationContext&quot;); if (logger.isInfoEnabled()) &#123; logger.info(&quot;Root WebApplicationContext: initialization started&quot;); &#125; long startTime = System.currentTimeMillis(); try &#123; // Store context in local instance variable, to guarantee that // it is available on ServletContext shutdown. if (this.context == null) &#123; this.context = createWebApplicationContext(servletContext); &#125; if (this.context instanceof ConfigurableWebApplicationContext) &#123; ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) this.context; if (!cwac.isActive()) &#123; // The context has not yet been refreshed -&gt; provide services such as // setting the parent context, setting the application context id, etc if (cwac.getParent() == null) &#123; // The context instance was injected without an explicit parent -&gt; // determine parent for root web application context, if any. ApplicationContext parent = loadParentContext(servletContext); cwac.setParent(parent); &#125; configureAndRefreshWebApplicationContext(cwac, servletContext); &#125; &#125; servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, this.context); ClassLoader ccl = Thread.currentThread().getContextClassLoader(); if (ccl == ContextLoader.class.getClassLoader()) &#123; currentContext = this.context; &#125; else if (ccl != null) &#123; currentContextPerThread.put(ccl, this.context); &#125; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Published root WebApplicationContext as ServletContext attribute with name [&quot; + WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE + &quot;]&quot;); &#125; if (logger.isInfoEnabled()) &#123; long elapsedTime = System.currentTimeMillis() - startTime; logger.info(&quot;Root WebApplicationContext: initialization completed in &quot; + elapsedTime + &quot; ms&quot;); &#125; return this.context; &#125; catch (RuntimeException ex) &#123; logger.error(&quot;Context initialization failed&quot;, ex); servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, ex); throw ex; &#125; catch (Error err) &#123; logger.error(&quot;Context initialization failed&quot;, err); servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, err); throw err; &#125; 这个方法还是有点长的，其实仔细看看，出去异常错误处理，这个方法主要做了三件事： 创建WebApplicationContext。 加载对应的spring配置文件中的Bean。 将WebApplicationContext放入ServletContext（Java Web的全局变量）中。 上述代码中createWebApplicationContext(servletContext)方法即是完成创建WebApplicationContext工作，也就是说这个方法创建爱你了上下文对象，支持用户自定义上下文对象，但必须继承ConfigurableWebApplicationContext，而Spring MVC默认使用ConfigurableWebApplicationContext作为ApplicationContext（它仅仅是一个接口）的实现。 再往下走，有一个方法configureAndRefreshWebApplicationContext就是用来加载spring配置文件中的Bean实例的。这个方法于封装ApplicationContext数据并且初始化所有相关Bean对象。它会从web.xml中读取名为 contextConfigLocation的配置，这就是spring xml数据源设置，然后放到ApplicationContext中，最后调用传说中的refresh方法执行所有Java对象的创建。 最后完成ApplicationContext创建之后就是将其放入ServletContext中，注意它存储的key值常量。 1servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, this.context); 启动流程图 Spring 启动过程使用AnnotationConfigApplicationContext来跟踪一下启动流程: this()： 初始化reader和scanner scan(basePackages)： 使用scanner组件扫描basePackage下的所有对象，将配置类的BeanDefinition注册到容器中。 refresh()： 刷新容器。 prepareRefresh： 刷新前的预处理 obtainFreshBeanFactory: 获取在容器初始化时创建的BeanFactory prepareBeanFactory: BeanFactory的预处理工作，会向容器中添加一些组件。 postProcessBeanFactory: 子类重写该方法， 可以实现在BeanFactory创建并预处理完成后做进一步的设置。 invokeBeanFactoryPostProcessors: 在BeanFactory初始化之 后执行BeanFactory的后处理器。 registerBeanPostProcessors: 向容器中注册Bean的后处理器 ，他的主要作用就是干预Spring初始化Bean的流程， 完成代理、自动注入、循环依赖等这些功能。 initMessageSource: 初始化messagesource组件， 主要用于国际化。 initApplicationEventMulticaster: 初始化事件分发器 onRefresh: 留给子容器，子类重写的方法，在容器刷新的时候可以自定义一些逻辑。 registerListeners: 注册监听器。 finishBeanFactoryInitialization: 完成BeanFactory的初始化， 主要作用是初始化所有剩下的单例Bean。 finishRefresh: 完成整个容器的初始化，发布BeanFactory容器刷新完成的事件。| 总结 首先，对于一个web应用，其部署在web容器中，web容器提供其一个全局的上下文环境，这个上下文就是ServletContext，其为后面的spring IoC容器提供宿主环境； 其次，在web.xml中会提供有contextLoaderListener。在web容器启动时，会触发容器初始化事件，此时 contextLoaderListener会监听到这个事件，其contextInitialized方法会被调用，在这个方法中，spring会初始 化一个启动上下文，这个上下文被称为根上下文，即WebApplicationContext，这是一个接口类，确切的说，其实际的实现类是 XmlWebApplicationContext。这个就是spring的IoC容器，其对应的Bean定义的配置由web.xml中的 context-param标签指定。在这个IoC容器初始化完毕后，spring以WebApplicationContext.ROOTWEBAPPLICATIONCONTEXTATTRIBUTE为属性Key，将其存储到ServletContext中，便于获取； 再次，contextLoaderListener监听器初始化完毕后，开始初始化web.xml中配置的Servlet，这里是DispatcherServlet，这个servlet实际上是一个标准的前端控制器，用以转发、匹配、处理每个servlet请 求。DispatcherServlet上下文在初始化的时候会建立自己的IoC上下文，用以持有spring mvc相关的bean。在建立DispatcherServlet自己的IoC上下文时，会利用WebApplicationContext.ROOTWEBAPPLICATIONCONTEXTATTRIBUTE先从ServletContext中获取之前的根上下文(即WebApplicationContext)作为自己上下文的parent上下文。有了这个 parent上下文之后，再初始化自己持有的上下文。这个DispatcherServlet初始化自己上下文的工作在其initStrategies方 法中可以看到，大概的工作就是初始化处理器映射、视图解析等。这个servlet自己持有的上下文默认实现类也是 XmlWebApplicationContext。初始化完毕后，spring以与servlet的名字相关(此处不是简单的以servlet名为 Key，而是通过一些转换，具体可自行查看源码)的属性为属性Key，也将其存到ServletContext中，以便后续使用。这样每个servlet 就持有自己的上下文，即拥有自己独立的bean空间，同时各个servlet共享相同的bean，即根上下文(第2步中初始化的上下文)定义的那些 bean。 参考Spring的启动流程 好文推荐：【Spring启动过程分析】启动流程简介 好文推荐：SPRING容器启动过程","categories":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/tags/Spring/"}],"author":"Marlowe"},{"title":"Spring Bean 相关问题","slug":"Spring/Spring-Bean-相关问题","date":"2021-05-16T03:09:23.000Z","updated":"2021-05-18T01:24:45.099Z","comments":true,"path":"2021/05/16/Spring/Spring-Bean-相关问题/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/16/Spring/Spring-Bean-%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/","excerpt":"","text":"Spring 中 bean 的创建过程首先: 简单来说，Spring框架中的Bean经过四个阶段:实例化 -&gt; 属性赋值 -&gt; 初始化 -&gt; 销毁 然后: 具体来说，Spring中Bean 经过了以下几个步骤: 实例化: new xxx(); 两个时机: 1、当客户端向容器申请一 个Bean时，2、 当容器在初始化一 个Bean时发现还需要依赖另一个Bean。BeanDefinition 对象保存。 – 到底new一个对象还是创建一个动态代理？ 设置对象属性(依赖注入): Spring通过BeanDefinition找到对象依赖的其他对象，并将这些对象赋予当前对象。 处理Aware接口: Spring会检测对象是否实现了xxxAware接口，如果实现了，就会调用对应的方法。BeanNameAware、BeanClassLoaderAware、 BeanFactoryAware、 ApplicationContextAware BeanPostProcessor前置处理: 调用BeanPostProcessor的postProcessBeforelnitialization方法 InitializingBean: Spring检测对象如果实现了这个接口， 就会执行他的afterPropertiesSet()方法， 定制初始化逻辑。 init-method: &lt;bean init-method=xXx&gt; 如果Spring发现Bean配置了这个属性，就会调用他的配置方法，执行初始化逻辑。@PostConstruct BeanPostProcessor后置处理: 调用BeanPostProcessor的postProcessAfterlnitialization方法 到这里，这个Bean的创建过程就完成了，Bean就可以正常使用了。 DisposableBean: 当Bean实现了这个接口， 在对象销毁前就会调用destory(方法。 destroy-method: &lt;bean destroy-method=xxx&gt; @PreDestroy Spring 中的 bean 的作用域有哪些? singleton: 唯一 bean 实例，Spring 中的 bean 默认都是单例的。 prototype: 每次请求都会创建一个新的 bean 实例。 request: 每一次HTTP请求都会产生一个新的bean，该bean仅在当前HTTP request内有效。 session: 每一次HTTP请求都会产生一个新的 bean，该bean仅在当前 HTTP session 内有效。 global-session： 全局session作用域，仅仅在基于portlet的web应用中才有意义，Spring5已经没有了。Portlet是能够生成语义代码(例如：HTML)片段的小型Java Web插件。它们基于portlet容器，可以像servlet一样处理HTTP请求。但是，与 servlet 不同，每个 portlet 都有不同的会话 Spring 中的单例 bean 的线程安全问题了解吗？的确是存在安全问题的。因为，当多个线程操作同一个对象的时候，对这个对象的成员变量的写操作会存在线程安全问题。 但是，一般情况下，我们常用的 Controller、Service、Dao 这些 Bean 是无状态的。无状态的 Bean 不能保存数据，因此是线程安全的。 常见的有 2 种解决办法： 在类中定义一个 ThreadLocal 成员变量，将需要的可变成员变量保存在 ThreadLocal 中（推荐的一种方式）。 改变 Bean 的作用域为 “prototype”：每次请求都会创建一个新的 bean 实例，自然不会存在线程安全问题。 Spring 框架中的Bean是线程安全的吗？如果线程不安全，要如何处理？Spring容器本身没有提供Bean的线程安全策略，因此，也可以说Spring容器中的Bean不是线程安全的。 要如何处理线程安全问题，就要分情况来分析。 Spring中的作用域:1、sington2、prototype: 为每个Bean请求创建给实例。3、request: 为每个request请求创建一个实例，请求完成后失效。4、session: 与request是类似的。5、global-session: 全局作用域。 对于线程安全问题: 1&gt; 对于prototype作用域，每次都是生成一个新的对象，所以不存在线程安全问题。2&gt; sington作用域:默认就是线程不完全的。 但是对于开发中大部分的Bean,其实是无状态的，不需要保证线程安全。所以在平常的MVC开发中，是不会有线程安全问题的。 无状态表示这个实例没有属性对象，不能保存数据， 是不变的类。比如: controller. service、 dao有状态表示示例是有属性对象，可以保存数据，是线程不安全的，比如pojo. 但是如果要保证线程安全，可以将Bean的作用域改为prototype比如像Model View。 另外还可以采用ThreadLocal来解决线程安全问题。ThreadLocal为每 个线程保存一个副本变量， 每个线程只操作自己的副本变量。 @Component 和 @Bean 的区别是什么？ 作用对象不同: @Component 注解作用于类，而@Bean注解作用于方法。 @Component通常是通过类路径扫描来自动侦测以及自动装配到Spring容器中（我们可以使用 @ComponentScan 注解定义要扫描的路径从中找出标识了需要装配的类自动装配到 Spring 的 bean 容器中）。@Bean 注解通常是我们在标有该注解的方法中定义产生这个 bean,@Bean告诉了Spring这是某个类的示例，当我需要用它的时候还给我。 @Bean 注解比 Component 注解的自定义性更强，而且很多地方我们只能通过 @Bean 注解来注册bean。比如当我们引用第三方库中的类需要装配到Spring容器时，则只能通过 @Bean来实现。 @Bean注解使用示例： 12345678@Configurationpublic class AppConfig &#123; @Bean public TransferService transferService() &#123; return new TransferServiceImpl(); &#125;&#125; 上面的代码相当于下面的 xml 配置 123&lt;beans&gt; &lt;bean id=&quot;transferService&quot; class=&quot;com.acme.TransferServiceImpl&quot;/&gt;&lt;/beans&gt; 下面这个例子是通过 @Component 无法实现的。 1234567891011@Beanpublic OneService getService(status) &#123; case (status) &#123; when 1: return new serviceImpl1(); when 2: return new serviceImpl2(); when 3: return new serviceImpl3(); &#125;&#125; 将一个类声明为Spring的 bean 的注解有哪些?我们一般使用 @Autowired 注解自动装配 bean，要想把类标识成可用于 @Autowired 注解自动装配的 bean 的类,采用以下注解可实现： @Component：通用的注解，可标注任意类为 Spring 组件。如果一个Bean不知道属于哪个层，可以使用@Component 注解标注。 @Repository: 对应持久层即 Dao 层，主要用于数据库相关操作。 @Service: 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao层。 @Controller: 对应 Spring MVC 控制层，主要用于接受用户请求并调用 Service 层返回数据给前端页面。 Spring 中的 bean 生命周期? Bean 容器找到配置文件中 Spring Bean 的定义。 Bean 容器利用 Java Reflection API 创建一个Bean的实例。 如果涉及到一些属性值 利用 set()方法设置一些属性值。 如果 Bean 实现了 BeanNameAware 接口，调用 setBeanName()方法，传入Bean的名字。 如果 Bean 实现了 BeanClassLoaderAware 接口，调用 setBeanClassLoader()方法，传入 ClassLoader对象的实例。 与上面的类似，如果实现了其他 *.Aware接口，就调用相应的方法。 如果有和加载这个 Bean 的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessBeforeInitialization() 方法 如果Bean实现了InitializingBean接口，执行afterPropertiesSet()方法。 如果 Bean 在配置文件中的定义包含 init-method 属性，执行指定的方法。 如果有和加载这个 Bean的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessAfterInitialization() 方法 当要销毁 Bean 的时候，如果 Bean 实现了 DisposableBean 接口，执行 destroy() 方法。 当要销毁 Bean 的时候，如果 Bean 在配置文件中的定义包含 destroy-method 属性，执行指定的方法。 图示： 与之比较类似的中文版本: 参考Spring bean","categories":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/categories/Spring/"}],"tags":[{"name":"Bean","slug":"Bean","permalink":"https://xmmarlowe.github.io/tags/Bean/"},{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/tags/Spring/"}],"author":"Marlowe"},{"title":"Spring中用到的设计模式","slug":"Spring/Spring中用到的设计模式","date":"2021-05-16T02:22:58.000Z","updated":"2021-05-16T14:24:53.984Z","comments":true,"path":"2021/05/16/Spring/Spring中用到的设计模式/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/16/Spring/Spring%E4%B8%AD%E7%94%A8%E5%88%B0%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","excerpt":"简单聊聊Spring中所用到的设计模式…","text":"简单聊聊Spring中所用到的设计模式… 控制反转(IoC)和依赖注入(DI)IoC(Inversion of Control,控制翻转) 是Spring 中一个非常非常重要的概念，它不是什么技术，而是一种解耦的设计思想。它的主要目的是借助于“第三方”(Spring 中的 IOC 容器) 实现具有依赖关系的对象之间的解耦(IOC容易管理对象，你只管使用即可)，从而降低代码之间的耦合度。IOC 是一个原则，而不是一个模式，以下模式（但不限于）实现了IoC原则。 Spring IOC 容器就像是一个工厂一样，当我们需要创建一个对象的时候，只需要配置好配置文件/注解即可，完全不用考虑对象是如何被创建出来的。 IOC 容器负责创建对象，将对象连接在一起，配置这些对象，并从创建中处理这些对象的整个生命周期，直到它们被完全销毁。 在实际项目中一个 Service 类如果有几百甚至上千个类作为它的底层，我们需要实例化这个 Service，你可能要每次都要搞清这个 Service 所有底层类的构造函数，这可能会把人逼疯。如果利用 IOC 的话，你只需要配置好，然后在需要的地方引用就行了，这大大增加了项目的可维护性且降低了开发难度。关于Spring IOC 的理解，推荐看这一下知乎的一个回答：https://www.zhihu.com/question/23277575/answer/169698662 ，非常不错。 **控制反转怎么理解呢? ** 举个例子：”对象a 依赖了对象 b，当对象 a 需要使用 对象 b的时候必须自己去创建。但是当系统引入了 IOC 容器后， 对象a 和对象 b 之前就失去了直接的联系。这个时候，当对象 a 需要使用 对象 b的时候， 我们可以指定 IOC 容器去创建一个对象b注入到对象 a 中”。 对象 a 获得依赖对象 b 的过程,由主动行为变为了被动行为，控制权翻转，这就是控制反转名字的由来。 DI(Dependecy Inject,依赖注入)是实现控制反转的一种设计模式，依赖注入就是将实例变量传入到一个对象中去。 工厂设计模式Spring使用工厂模式可以通过 BeanFactory 或 ApplicationContext 创建 bean 对象。 两者对比： BeanFactory： 延迟注入(使用到某个 bean 的时候才会注入),相比于BeanFactory 来说会占用更少的内存，程序启动速度更快。 ApplicationContext： 容器启动的时候，不管你用没用到，一次性创建所有 bean 。BeanFactory 仅提供了最基本的依赖注入支持，ApplicationContext 扩展了 BeanFactory ,除了有BeanFactory的功能还有额外更多功能，所以一般开发人员使用ApplicationContext会更多。 ApplicationContext的三个实现类： ClassPathXmlApplication： 把上下文文件当成类路径资源。 FileSystemXmlApplication： 从文件系统中的 XML 文件载入上下文定义信息。 XmlWebApplicationContext： 从Web系统中的XML文件载入上下文定义信息。 Example: 123456789101112import org.springframework.context.ApplicationContext;import org.springframework.context.support.FileSystemXmlApplicationContext;public class App &#123; public static void main(String[] args) &#123; ApplicationContext context = new FileSystemXmlApplicationContext( &quot;C:/work/IOC Containers/springframework.applicationcontext/src/main/resources/bean-factory-config.xml&quot;); HelloApplicationContext obj = (HelloApplicationContext) context.getBean(&quot;helloApplicationContext&quot;); obj.getMsg(); &#125;&#125; 单例设计模式在我们的系统中，有一些对象其实我们只需要一个，比如说：线程池、缓存、对话框、注册表、日志对象、充当打印机、显卡等设备驱动程序的对象。事实上，这一类对象只能有一个实例，如果制造出多个实例就可能会导致一些问题的产生，比如：程序的行为异常、资源使用过量、或者不一致性的结果。 使用单例模式的好处: 对于频繁使用的对象，可以省略创建对象所花费的时间，这对于那些重量级对象而言，是非常可观的一笔系统开销； 由于 new 操作的次数减少，因而对系统内存的使用频率也会降低，这将减轻 GC 压力，缩短 GC 停顿时间。 Spring 中 bean 的默认作用域就是 singleton(单例)的。 除了 singleton 作用域，Spring 中 bean 还有下面几种作用域： prototype : 每次请求都会创建一个新的 bean 实例。 request : 每一次HTTP请求都会产生一个新的bean，该bean仅在当前HTTP request内有效。 session : 每一次HTTP请求都会产生一个新的 bean，该bean仅在当前 HTTP session 内有效。 global-session： 全局session作用域，仅仅在基于portlet的web应用中才有意义，Spring5已经没有了。Portlet是能够生成语义代码(例如：HTML)片段的小型Java Web插件。它们基于portlet容器，可以像servlet一样处理HTTP请求。但是，与 servlet 不同，每个 portlet 都有不同的会话 Spring 实现单例的方式： xml: &lt;bean id=&quot;userService&quot; class=&quot;top.snailclimb.UserService&quot; scope=&quot;singleton&quot;/&gt; 注解： @Scope(value = “singleton”) Spring 通过 ConcurrentHashMap 实现单例注册表的特殊方式实现单例模式。Spring 实现单例的核心代码如下： 12345678910111213141516171819202122232425262728// 通过 ConcurrentHashMap（线程安全） 实现单例注册表private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;String, Object&gt;(64);public Object getSingleton(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; Assert.notNull(beanName, &quot;&#x27;beanName&#x27; must not be null&quot;); synchronized (this.singletonObjects) &#123; // 检查缓存中是否存在实例 Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) &#123; //...省略了很多代码 try &#123; singletonObject = singletonFactory.getObject(); &#125; //...省略了很多代码 // 如果实例对象在不存在，我们注册到单例注册表中。 addSingleton(beanName, singletonObject); &#125; return (singletonObject != NULL_OBJECT ? singletonObject : null); &#125; &#125; //将对象添加到单例注册表 protected void addSingleton(String beanName, Object singletonObject) &#123; synchronized (this.singletonObjects) &#123; this.singletonObjects.put(beanName, (singletonObject != null ? singletonObject : NULL_OBJECT)); &#125; &#125;&#125; 代理设计模式代理模式在 AOP 中的应用AOP(Aspect-Oriented Programming:面向切面编程)能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。 Spring AOP 就是基于动态代理的，如果要代理的对象，实现了某个接口，那么Spring AOP会使用JDK Proxy，去创建代理对象，而对于没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了，这时候Spring AOP会使用Cglib，这时候Spring AOP会使用 Cglib 生成一个被代理对象的子类来作为代理，如下图所示： 当然你也可以使用 AspectJ ,Spring AOP 已经集成了AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。 使用 AOP 之后我们可以把一些通用功能抽象出来，在需要用到的地方直接使用即可，这样大大简化了代码量。我们需要增加新功能时也方便，这样也提高了系统扩展性。日志功能、事务管理等等场景都用到了 AOP。 Spring AOP 和 AspectJ AOP 有什么区别?Spring AOP 属于运行时增强，而 AspectJ 是编译时增强。 Spring AOP 基于代理(Proxying)，而 AspectJ 基于字节码操作(Bytecode Manipulation)。 Spring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。AspectJ 相比于 Spring AOP 功能更加强大，但是 Spring AOP 相对来说更简单， 如果我们的切面比较少，那么两者性能差异不大。但是，当切面太多的话，最好选择 AspectJ ，它比Spring AOP 快很多。 模板方法模板方法模式是一种行为设计模式，它定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。 模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤的实现方式。 1234567891011121314151617181920212223242526272829public abstract class Template &#123; //这是我们的模板方法 public final void TemplateMethod()&#123; PrimitiveOperation1(); PrimitiveOperation2(); PrimitiveOperation3(); &#125; protected void PrimitiveOperation1()&#123; //当前类实现 &#125; //被子类实现的方法 protected abstract void PrimitiveOperation2(); protected abstract void PrimitiveOperation3();&#125;public class TemplateImpl extends Template &#123; @Override public void PrimitiveOperation2() &#123; //当前类实现 &#125; @Override public void PrimitiveOperation3() &#123; //当前类实现 &#125;&#125; Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。一般情况下，我们都是使用继承的方式来实现模板模式，但是 Spring 并没有使用这种方式，而是使用Callback 模式与模板方法模式配合，既达到了代码复用的效果，同时增加了灵活性。 观察者模式观察者模式是一种对象行为型模式。它表示的是一种对象与对象之间具有依赖关系，当一个对象发生改变的时候，这个对象所依赖的对象也会做出反应。Spring 事件驱动模型就是观察者模式很经典的一个应用。Spring 事件驱动模型非常有用，在很多场景都可以解耦我们的代码。比如我们每次添加商品的时候都需要重新更新商品索引，这个时候就可以利用观察者模式来解决这个问题。 Spring 事件驱动模型中的三种角色事件角色ApplicationEvent (org.springframework.context包下)充当事件的角色,这是一个抽象类，它继承了java.util.EventObject并实现了 java.io.Serializable接口。 Spring 中默认存在以下事件，他们都是对 ApplicationContextEvent 的实现(继承自ApplicationContextEvent)： ContextStartedEvent：ApplicationContext 启动后触发的事件; ContextStoppedEvent：ApplicationContext 停止后触发的事件; ContextRefreshedEvent：ApplicationContext 初始化或刷新完成后触发的事件; ContextClosedEvent：ApplicationContext 关闭后触发的事件。 事件监听者角色ApplicationListener 充当了事件监听者角色，它是一个接口，里面只定义了一个 onApplicationEvent（）方法来处理ApplicationEvent。ApplicationListener接口类源码如下，可以看出接口定义看出接口中的事件只要实现了 ApplicationEvent就可以了。所以，在 Spring中我们只要实现 ApplicationListener 接口实现 onApplicationEvent() 方法即可完成监听事件 123456package org.springframework.context;import java.util.EventListener;@FunctionalInterfacepublic interface ApplicationListener&lt;E extends ApplicationEvent&gt; extends EventListener &#123; void onApplicationEvent(E var1);&#125; 事件发布者角色ApplicationEventPublisher 充当了事件的发布者，它也是一个接口。 12345678@FunctionalInterfacepublic interface ApplicationEventPublisher &#123; default void publishEvent(ApplicationEvent event) &#123; this.publishEvent((Object)event); &#125; void publishEvent(Object var1);&#125; ApplicationEventPublisher 接口的publishEvent（）这个方法在AbstractApplicationContext类中被实现，阅读这个方法的实现，你会发现实际上事件真正是通过ApplicationEventMulticaster来广播出去的。具体内容过多，就不在这里分析了，后面可能会单独写一篇文章提到。 Spring 的事件流程总结 定义一个事件: 实现一个继承自 ApplicationEvent，并且写相应的构造函数； 定义一个事件监听者：实现 ApplicationListener 接口，重写 onApplicationEvent() 方法； 使用事件发布者发布消息: 可以通过 ApplicationEventPublisher 的 publishEvent() 方法发布消息。 Example: 12345678910111213141516171819202122232425262728293031323334353637383940// 定义一个事件,继承自ApplicationEvent并且写相应的构造函数public class DemoEvent extends ApplicationEvent&#123; private static final long serialVersionUID = 1L; private String message; public DemoEvent(Object source,String message)&#123; super(source); this.message = message; &#125; public String getMessage() &#123; return message; &#125;// 定义一个事件监听者,实现ApplicationListener接口，重写 onApplicationEvent() 方法；@Componentpublic class DemoListener implements ApplicationListener&lt;DemoEvent&gt;&#123; //使用onApplicationEvent接收消息 @Override public void onApplicationEvent(DemoEvent event) &#123; String msg = event.getMessage(); System.out.println(&quot;接收到的信息是：&quot;+msg); &#125;&#125;// 发布事件，可以通过ApplicationEventPublisher 的 publishEvent() 方法发布消息。@Componentpublic class DemoPublisher &#123; @Autowired ApplicationContext applicationContext; public void publish(String message)&#123; //发布事件 applicationContext.publishEvent(new DemoEvent(this, message)); &#125;&#125; 当调用 DemoPublisher 的 publish() 方法的时候，比如 demoPublisher.publish(&quot;你好&quot;) ，控制台就会打印出:接收到的信息是：你好。 适配器模式适配器模式(Adapter Pattern) 将一个接口转换成客户希望的另一个接口，适配器模式使接口不兼容的那些类可以一起工作，其别名为包装器(Wrapper)。 spring AOP中的适配器模式我们知道 Spring AOP 的实现是基于代理模式，但是 Spring AOP 的增强或通知(Advice)使用到了适配器模式，与之相关的接口是AdvisorAdapter 。Advice 常用的类型有：BeforeAdvice（目标方法调用前,前置通知）、AfterAdvice（目标方法调用后,后置通知）、AfterReturningAdvice(目标方法执行结束后，return之前)等等。每个类型Advice（通知）都有对应的拦截器:MethodBeforeAdviceInterceptor、AfterReturningAdviceAdapter、AfterReturningAdviceInterceptor。Spring预定义的通知要通过对应的适配器，适配成 MethodInterceptor接口(方法拦截器)类型的对象（如：MethodBeforeAdviceInterceptor 负责适配 MethodBeforeAdvice）。 spring MVC中的适配器模式在Spring MVC中，DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler。解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由HandlerAdapter适配器处理。HandlerAdapter 作为期望接口，具体的适配器实现类用于对目标类进行适配，Controller 作为需要适配的类。 为什么要在 Spring MVC 中使用适配器模式？ Spring MVC 中的 Controller 种类众多，不同类型的 Controller 通过不同的方法来对请求进行处理。如果不利用适配器模式的话，DispatcherServlet 直接获取对应类型的 Controller，需要的自行来判断，像下面这段代码一样： 1234567if(mappedHandler.getHandler() instanceof MultiActionController)&#123; ((MultiActionController)mappedHandler.getHandler()).xxx &#125;else if(mappedHandler.getHandler() instanceof XXX)&#123; ... &#125;else if(...)&#123; ... &#125; 假如我们再增加一个 Controller类型就要在上面代码中再加入一行 判断语句，这种形式就使得程序难以维护，也违反了设计模式中的开闭原则 – 对扩展开放，对修改关闭。 装饰者模式装饰者模式可以动态地给对象添加一些额外的属性或行为。相比于使用继承，装饰者模式更加灵活。简单点儿说就是当我们需要修改原有的功能，但我们又不愿直接去修改原有的代码时，设计一个Decorator套在原有代码外面。其实在 JDK 中就有很多地方用到了装饰者模式，比如 InputStream家族，InputStream 类下有 FileInputStream (读取文件)、BufferedInputStream (增加缓存,使读取文件速度大大提升)等子类都在不修改InputStream 代码的情况下扩展了它的功能。 Spring 中配置 DataSource 的时候，DataSource 可能是不同的数据库和数据源。我们能否根据客户的需求在少修改原有类的代码下动态切换不同的数据源？这个时候就要用到装饰者模式(这一点我自己还没太理解具体原理)。Spring 中用到的包装器模式在类名上含有 Wrapper或者 Decorator。这些类基本上都是动态地给一个对象添加一些额外的职责 总结Spring 框架中用到了哪些设计模式： 工厂设计模式: Spring使用工厂模式通过 BeanFactory、ApplicationContext 创建 bean 对象。 代理设计模式: Spring AOP 功能的实现。 单例设计模式: Spring 中的 Bean 默认都是单例的。 模板方法模式: Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。 包装器设计模式: 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。 观察者模式: Spring 事件驱动模型就是观察者模式很经典的一个应用。 适配器模式: Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配Controller。 …… 参考面试官:“谈谈Spring中都用到了那些设计模式?”。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/tags/Spring/"},{"name":"设计模式","slug":"设计模式","permalink":"https://xmmarlowe.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"author":"Marlowe"},{"title":"Spring事务实现方式","slug":"Spring/Spring事务实现方式","date":"2021-05-15T12:47:10.000Z","updated":"2021-05-16T14:24:53.931Z","comments":true,"path":"2021/05/15/Spring/Spring事务实现方式/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/15/Spring/Spring%E4%BA%8B%E5%8A%A1%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F/","excerpt":"","text":"Spring 管理事务的方式有几种？ 编程式事务，在代码中硬编码。(不推荐使用) 声明式事务，在配置文件中配置（推荐使用） 声明式事务又分为两种： 基于XML的声明式事务 基于注解的声明式事务 Spring 事务中的隔离级别有哪几种?TransactionDefinition 接口中定义了五个表示隔离级别的常量： TransactionDefinition.ISOLATION_DEFAULT: 使用后端数据库默认的隔离级别，Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别. TransactionDefinition.ISOLATION_READ_UNCOMMITTED: 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 TransactionDefinition.ISOLATION_READ_COMMITTED: 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 TransactionDefinition.ISOLATION_REPEATABLE_READ: 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 TransactionDefinition.ISOLATION_SERIALIZABLE: 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 但是这将严重影响程序的性能。通常情况下也不会用到该级别。 Spring 事务中哪几种事务传播行为?支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRED： 如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。 TransactionDefinition.PROPAGATION_SUPPORTS： 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_MANDATORY： 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性） 不支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRES_NEW： 创建一个新的事务，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED： 以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER： 以非事务方式运行，如果当前存在事务，则抛出异常。 其他情况： TransactionDefinition.PROPAGATION_NESTED： 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。 @Transactional(rollbackFor = Exception.class)注解了解吗？我们知道：Exception分为运行时异常RuntimeException和非运行时异常。事务管理对于企业应用来说是至关重要的，即使出现异常情况，它也可以保证数据的一致性。 当@Transactional注解作用于类上时，该类的所有 public 方法将都具有该类型的事务属性，同时，我们也可以在方法级别使用该标注来覆盖类级别的定义。如果类或者方法加了这个注解，那么这个类里面的方法抛出异常，就会回滚，数据库里面的数据也会回滚。 在@Transactional注解中如果不配置rollbackFor属性,那么事务只会在遇到RuntimeException的时候才会回滚,加上rollbackFor=Exception.class,可以让事务在遇到非运行时异常时也回滚。 参考Spring 事务","categories":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/tags/Spring/"},{"name":"事务","slug":"事务","permalink":"https://xmmarlowe.github.io/tags/%E4%BA%8B%E5%8A%A1/"}],"author":"Marlowe"},{"title":"Java限流-实现每秒n个请求","slug":"并发/Java限流-实现每秒n个请求","date":"2021-05-15T06:33:10.000Z","updated":"2021-05-16T14:24:53.956Z","comments":true,"path":"2021/05/15/并发/Java限流-实现每秒n个请求/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/15/%E5%B9%B6%E5%8F%91/Java%E9%99%90%E6%B5%81-%E5%AE%9E%E7%8E%B0%E6%AF%8F%E7%A7%92n%E4%B8%AA%E8%AF%B7%E6%B1%82/","excerpt":"","text":"概要在大数据量高并发访问时，经常会出现服务或接口面对暴涨的请求而不可用的情况，甚至引发连锁反映导致整个系统崩溃。此时你需要使用的技术手段之一就是限流，当请求达到一定的并发数或速率，就进行等待、排队、降级、拒绝服务等。在限流时，常见的两种算法是漏桶和令牌桶算法算法。 限流算法令牌桶(Token Bucket)、漏桶(leaky bucket)和计数器算法是最常用的三种限流的算法。 1. 令牌桶算法 令牌桶算法的原理是系统会以一个恒定的速度往桶里放入令牌，而如果请求需要被处理，则需要先从桶里获取一个令牌，当桶里没有令牌可取时，则拒绝服务。 当桶满时，新添加的令牌被丢弃或拒绝。 代码示例： 1234567891011121314public class RateLimiterDemo &#123; private static RateLimiter limiter = RateLimiter.create(5); public static void exec() &#123; limiter.acquire(1); try &#123; // 处理核心逻辑 TimeUnit.SECONDS.sleep(1); System.out.println(&quot;--&quot; + System.currentTimeMillis() / 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; Guava RateLimiter 提供了令牌桶算法可用于平滑突发限流策略。该示例为每秒中产生5个令牌，每200毫秒会产生一个令牌。limiter.acquire() 表示消费一个令牌。当桶中有足够的令牌时，则直接返回0，否则阻塞，直到有可用的令牌数才返回，返回的值为阻塞的时间。 2. 漏桶算法 它的主要目的是控制数据注入到网络的速率，平滑网络上的突发流量，数据可以以任意速度流入到漏桶中。漏桶算法提供了一种机制，通过它，突发流量可以被整形以便为网络提供一个稳定的流量。 漏桶可以看作是一个带有常量服务时间的单服务器队列，如果漏桶为空，则不需要流出水滴，如果漏桶（包缓存）溢出，那么水滴会被溢出丢弃。 3. 计数器限流算法计数器限流算法也是比较常用的，主要用来限制总并发数，比如数据库连接池大小、线程池大小、程序访问并发数等都是使用计数器算法。 代码示例1： 123456789101112131415161718192021public class CountRateLimiterDemo1 &#123; private static AtomicInteger count = new AtomicInteger(0); public static void exec() &#123; if (count.get() &gt;= 5) &#123; System.out.println(&quot;请求用户过多，请稍后在试！&quot;+System.currentTimeMillis()/1000); &#125; else &#123; count.incrementAndGet(); try &#123; //处理核心逻辑 TimeUnit.SECONDS.sleep(1); System.out.println(&quot;--&quot;+System.currentTimeMillis()/1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; count.decrementAndGet(); &#125; &#125; &#125;&#125; 使用AomicInteger来进行统计当前正在并发执行的次数，如果超过域值就简单粗暴的直接响应给用户，说明系统繁忙，请稍后再试或其它跟业务相关的信息。 弊端：使用 AomicInteger 简单粗暴超过域值就拒绝请求，可能只是瞬时的请求量高，也会拒绝请求。 代码示例2： 1234567891011121314151617181920public class CountRateLimiterDemo2 &#123; private static Semaphore semphore = new Semaphore(5); public static void exec() &#123; if(semphore.getQueueLength()&gt;100)&#123; System.out.println(&quot;当前等待排队的任务数大于100，请稍候再试...&quot;); &#125; try &#123; semphore.acquire(); // 处理核心逻辑 TimeUnit.SECONDS.sleep(1); System.out.println(&quot;--&quot; + System.currentTimeMillis() / 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; semphore.release(); &#125; &#125;&#125; 使用Semaphore信号量来控制并发执行的次数，如果超过域值信号量，则进入阻塞队列中排队等待获取信号量进行执行。如果阻塞队列中排队的请求过多超出系统处理能力，则可以在拒绝请求。 相对Atomic优点：如果是瞬时的高并发，可以使请求在阻塞队列中排队，而不是马上拒绝请求，从而达到一个流量削峰的目的。 参考Java限流策略","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"限流","slug":"限流","permalink":"https://xmmarlowe.github.io/tags/%E9%99%90%E6%B5%81/"}],"author":"Marlowe"},{"title":"REST与RPC区别？","slug":"分布式/REST与RPC区别？","date":"2021-05-14T09:22:42.000Z","updated":"2021-08-24T13:43:22.591Z","comments":true,"path":"2021/05/14/分布式/REST与RPC区别？/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/14/%E5%88%86%E5%B8%83%E5%BC%8F/REST%E4%B8%8ERPC%E5%8C%BA%E5%88%AB%EF%BC%9F/","excerpt":"","text":"REST与RPC概念什么是RESTREST是一种架构风格，指的是一组架构约束条件和原则。满足这些约束条件和原则的应用程序或设计就是 RESTful。REST规范把所有内容都视为资源，网络上一切皆资源。 REST并没有创造新的技术，组件或服务，只是使用Web的现有特征和能力。 可以完全通过HTTP协议实现，使用 HTTP 协议处理数据通信。REST架构对资源的操作包括获取、创建、修改和删除资源的操作正好对应HTTP协议提供的GET、POST、PUT和DELETE方法。 HTTP动词与REST风格CRUD对应关系： Method CRUD POST create、update、delete GET read PUT update、create DELETE delete 什么是RPC远程方法调用，就是像调用本地方法一样调用远程方法。常见RPC框架结构图： RPC框架要做到的最基本的三件事： 1、服务端如何确定客户端要调用的函数； 在远程调用中，客户端和服务端分别维护一个【ID-&gt;函数】的对应表， ID在所有进程中都是唯一确定的。客户端在做远程过程调用时，附上这个ID，服务端通过查表，来确定客户端需要调用的函数，然后执行相应函数的代码。 2、如何进行序列化和反序列化； 客户端和服务端交互时将参数或结果转化为字节流在网络中传输，那么数据转化为字节流的或者将字节流转换成能读取的固定格式时就需要进行序列化和反序列化，序列化和反序列化的速度也会影响远程调用的效率。 3、如何进行网络传输（选择何种网络协议）； 多数RPC框架选择TCP作为传输协议，也有部分选择HTTP。如gRPC使用HTTP2。不同的协议各有利弊。TCP更加高效，而HTTP在实际应用中更加的灵活。 REST与RPC比较都是网络交互的协议规范。通常用于多个微服务之间的通信协议。 比较项 REST RPC 通信协议 HTTP 一般使用TCP 性能 低 高 灵活度 高 低 高与低是对实现两种规范框架的相对比较，但也不是绝对的，需要根据实际情况而定。 REST与RPC应用场景REST和RPC都常用于微服务架构中。 1、HTTP相对更规范，更标准，更通用，无论哪种语言都支持http协议。如果你是对外开放API，例如开放平台，外部的编程语言多种多样，你无法拒绝对每种语言的支持，现在开源中间件，基本最先支持的几个协议都包含RESTful。 2、 RPC 框架作为架构微服务化的基础组件，它能大大降低架构微服务化的成本，提高调用方与服务提供方的研发效率，屏蔽跨进程调用函数（服务）的各类复杂细节。让调用方感觉就像调用本地函数一样调用远端函数、让服务提供方感觉就像实现一个本地函数一样来实现服务。 使用建议REST调用及测试都很方便，RPC就显得有点繁琐，但是RPC的效率是毋庸置疑的，所以建议在多系统之间的内部调用采用RPC。对外提供的服务，Rest更加合适。 参考面试问题：REST与RPC区别？","categories":[{"name":"分布式","slug":"分布式","permalink":"https://xmmarlowe.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"RESTful","slug":"RESTful","permalink":"https://xmmarlowe.github.io/tags/RESTful/"},{"name":"RPC","slug":"RPC","permalink":"https://xmmarlowe.github.io/tags/RPC/"}],"author":"Marlowe"},{"title":"spring循环依赖","slug":"Spring/Spring循环依赖","date":"2021-05-14T08:09:52.000Z","updated":"2021-08-26T11:24:08.357Z","comments":true,"path":"2021/05/14/Spring/Spring循环依赖/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/14/Spring/Spring%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96/","excerpt":"","text":"什么是循环依赖?循环依赖其实就是循环引用，也就是两个或则两个以上的bean互相持有对方，最终形成闭环。比如A依赖于B，B依赖于C，C又依赖于A。如下图： 注意：这里不是函数的循环调用，是对象的相互依赖关系。循环调用其实就是一个死循环，除非有终结条件。 Spring中循环依赖场景有：（1）构造器的循环依赖（2）field属性的循环依赖。 怎么检测是否存在循环依赖?检测循环依赖相对比较容易，Bean在创建的时候可以给该Bean打标，如果递归调用回来发现正在创建中的话，即说明了循环依赖了。 Spring怎么解决循环依赖?@Lazy注解： 解决构造方法造成的循环依赖问题 对于对象之间的普通引用： 二级缓存 会保存new出来的不完整对象，这样当单例池中找到不依赖的属性时，就可以先从二级缓存中获取到不完整对象，完成对象创建，在后续的依赖注入过程中，将单例池中对象的引用关系调整完成。 三级缓存: 如果引用的对象配置了AOP，那在单例池中最终就会需要注入动态代理对象，而不是原对象。而生成动态代理是要在对象初始化完成之后才开始的。于是Spring增加三E级缓存，保存所有对象的动态代理配置信息。在发现有循环依赖时，将这个对象的动态代理信息获取出来，提前进行AOP，生成动态代理。 核心代码就在DefaultSingletonBeanRegistry的getSingleton方法当中。 Spring的循环依赖的理论依据其实是基于Java的引用传递，当我们获取到对象的引用时，对象的field或则属性是可以延后设置的(但是构造器必须是在获取引用之前)。 Spring的单例对象的初始化主要分为三步： （1）createBeanInstance：实例化，其实也就是调用对象的构造方法实例化对象 （2）populateBean：填充属性，这一步主要是多bean的依赖属性进行填充 （3）initializeBean：调用spring xml中的init 方法。 从上面讲述的单例bean初始化步骤我们可以知道，循环依赖主要发生在第一、第二部。也就是构造器循环依赖和field循环依赖。 那么我们要解决循环引用也应该从初始化过程着手，对于单例来说，在Spring容器整个生命周期内，有且只有一个对象，所以很容易想到这个对象应该存在Cache中，Spring为了解决单例的循环依赖问题，使用了三级缓存。 首先我们看源码，三级缓存主要指： 12345678/** Cache of singleton objects: bean name --&gt; bean instance */private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;String, Object&gt;(256);/** Cache of singleton factories: bean name --&gt; ObjectFactory */private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap&lt;String, ObjectFactory&lt;?&gt;&gt;(16);/** Cache of early singleton objects: bean name --&gt; bean instance */private final Map&lt;String, Object&gt; earlySingletonObjects = new HashMap&lt;String, Object&gt;(16); 这三级缓存分别指： singletonFactories： 单例对象工厂的cache earlySingletonObjects： 提前暴光的单例对象的Cache singletonObjects： 单例对象的cache 我们在创建bean的时候，首先想到的是从cache中获取这个单例的bean，这个缓存就是singletonObjects。主要调用方法就就是： 1234567891011121314151617protected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123; synchronized (this.singletonObjects) &#123; singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null &amp;&amp; allowEarlyReference) &#123; ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) &#123; singletonObject = singletonFactory.getObject(); this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); &#125; &#125; &#125; &#125; return (singletonObject != NULL_OBJECT ? singletonObject : null);&#125; 上面的代码需要解释两个参数： isSingletonCurrentlyInCreation()判断当前单例bean是否正在创建中，也就是没有初始化完成(比如A的构造器依赖了B对象所以得先去创建B对象， 或则在A的populateBean过程中依赖了B对象，得先去创建B对象，这时的A就是处于创建中的状态。) allowEarlyReference 是否允许从singletonFactories中通过getObject拿到对象 分析getSingleton()的整个过程，Spring首先从一级缓存singletonObjects中获取。如果获取不到，并且对象正在创建中，就再从二级缓存earlySingletonObjects中获取。如果还是获取不到且允许singletonFactories通过getObject()获取，就从三级缓存singletonFactory.getObject()(三级缓存)获取，如果获取到了则： 12this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); 从singletonFactories中移除，并放入earlySingletonObjects中。其实也就是从三级缓存移动到了二级缓存。 从上面三级缓存的分析，我们可以知道，Spring解决循环依赖的诀窍就在于singletonFactories这个三级cache。这个cache的类型是ObjectFactory，定义如下： 123public interface ObjectFactory&lt;T&gt; &#123; T getObject() throws BeansException;&#125; 这个接口在下面被引用 12345678910protected void addSingletonFactory(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; Assert.notNull(singletonFactory, &quot;Singleton factory must not be null&quot;); synchronized (this.singletonObjects) &#123; if (!this.singletonObjects.containsKey(beanName)) &#123; this.singletonFactories.put(beanName, singletonFactory); this.earlySingletonObjects.remove(beanName); this.registeredSingletons.add(beanName); &#125; &#125;&#125; 这里就是解决循环依赖的关键，这段代码发生在createBeanInstance之后，也就是说单例对象此时已经被创建出来(调用了构造器)。这个对象已经被生产出来了，虽然还不完美（还没有进行初始化的第二步和第三步），但是已经能被人认出来了（根据对象引用能定位到堆中的对象），所以Spring此时将这个对象提前曝光出来让大家认识，让大家使用。 这样做有什么好处呢？ 让我们来分析一下“A的某个field或者setter依赖了B的实例对象，同时B的某个field或者setter依赖了A的实例对象”这种循环依赖的情况。A首先完成了初始化的第一步，并且将自己提前曝光到singletonFactories中，此时进行初始化的第二步，发现自己依赖对象B，此时就尝试去get(B)，发现B还没有被create，所以走create流程，B在初始化第一步的时候发现自己依赖了对象A，于是尝试get(A)，尝试一级缓存singletonObjects(肯定没有，因为A还没初始化完全)，尝试二级缓存earlySingletonObjects（也没有），尝试三级缓存singletonFactories，由于A通过ObjectFactory将自己提前曝光了，所以B能够通过ObjectFactory.getObject拿到A对象(虽然A还没有初始化完全，但是总比没有好呀)，B拿到A对象后顺利完成了初始化阶段1、2、3，完全初始化之后将自己放入到一级缓存singletonObjects中。此时返回A中，A此时能拿到B的对象顺利完成自己的初始化阶段2、3，最终A也完成了初始化，进去了一级缓存singletonObjects中，而且更加幸运的是，由于B拿到了A的对象引用，所以B现在hold住的A对象完成了初始化。 知道了这个原理时候，肯定就知道为啥Spring不能解决“A的构造方法中依赖了B的实例对象，同时B的构造方法中依赖了A的实例对象”这类问题了！因为加入singletonFactories三级缓存的前提是执行了构造器，所以构造器的循环依赖没法解决。 参考Spring-bean的循环依赖以及解决方式 好文推荐：Spring 是如何解决循环依赖的？","categories":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/tags/Spring/"}],"author":"Marlowe"},{"title":"读写锁之ReadWriteLock源码分析","slug":"并发/读写锁之ReadWriteLock源码分析","date":"2021-05-10T11:41:51.000Z","updated":"2021-05-10T14:43:23.002Z","comments":true,"path":"2021/05/10/并发/读写锁之ReadWriteLock源码分析/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/10/%E5%B9%B6%E5%8F%91/%E8%AF%BB%E5%86%99%E9%94%81%E4%B9%8BReadWriteLock%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"ReadWriteLock管理一组锁，一个是只读的锁，一个是写锁。读锁可以在没有写锁的时候被多个线程同时持有，写锁是独占的。所有读写锁的实现必须确保写操作对读操作的内存影响。换句话说，一个获得了读锁的线程必须能看到前一个释放的写锁所更新的内容。读写锁比互斥锁允许对于共享数据更大程度的并发。每次只能有一个写线程，但是同时可以有多个线程并发地读数据。ReadWriteLock适用于读多写少的并发情况。Java并发包中ReadWriteLock是一个接口，主要有两个方法，如下： 1234567891011public interface ReadWriteLock &#123; /** * 返回读锁 */ Lock readLock(); /** * 返回写锁 */ Lock writeLock();&#125; Java并发库中ReetrantReadWriteLock实现了ReadWriteLock接口并添加了可重入的特性。 特性ReentrantReadWriteLock有如下特性： 获取顺序 非公平模式（默认）当以非公平初始化时，读锁和写锁的获取的顺序是不确定的。非公平锁主张竞争获取，可能会延缓一个或多个读或写线程，但是会比公平锁有更高的吞吐量。 公平模式当以公平模式初始化时，线程将会以队列的顺序获取锁。当当前线程释放锁后，等待时间最长的写锁线程就会被分配写锁；或者有一组读线程组等待时间比写线程长，那么这组读线程组将会被分配读锁。当有写线程持有写锁或者有等待的写线程时，一个尝试获取公平的读锁（非重入）的线程就会阻塞。这个线程直到等待时间最长的写锁获得锁后并释放掉锁后才能获取到读锁。 可重入允许读锁可写锁可重入。写锁可以获得读锁，读锁不能获得写锁。 锁降级允许写锁降低为读锁 中断锁的获取在读锁和写锁的获取过程中支持中断 支持Condition写锁提供Condition实现 监控提供确定锁是否被持有等辅助方法 使用下面一段代码展示了锁降低的操作： 1234567891011121314151617181920212223242526272829303132class CachedData &#123; Object data; volatile boolean cacheValid; final ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); void processCachedData() &#123; rwl.readLock().lock(); if (!cacheValid) &#123; // Must release read lock before acquiring write lock rwl.readLock().unlock(); rwl.writeLock().lock(); try &#123; // Recheck state because another thread might have // acquired write lock and changed state before we did. if (!cacheValid) &#123; data = ... cacheValid = true; &#125; // Downgrade by acquiring read lock before releasing write lock rwl.readLock().lock(); &#125; finally &#123; rwl.writeLock().unlock(); // Unlock write, still hold read &#125; &#125; try &#123; use(data); &#125; finally &#123; rwl.readLock().unlock(); &#125; &#125; &#125; ReentrantReadWriteLock可以用来提高某些集合的并发性能。当集合比较大，并且读比写频繁时，可以使用该类。下面是TreeMap使用ReentrantReadWriteLock进行封装成并发性能提高的一个例子： 123456789101112131415161718192021222324252627class RWDictionary &#123; private final Map&lt;String, Data&gt; m = new TreeMap&lt;String, Data&gt;(); private final ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); private final Lock r = rwl.readLock(); private final Lock w = rwl.writeLock(); public Data get(String key) &#123; r.lock(); try &#123; return m.get(key); &#125; finally &#123; r.unlock(); &#125; &#125; public String[] allKeys() &#123; r.lock(); try &#123; return m.keySet().toArray(); &#125; finally &#123; r.unlock(); &#125; &#125; public Data put(String key, Data value) &#123; w.lock(); try &#123; return m.put(key, value); &#125; finally &#123; w.unlock(); &#125; &#125; public void clear() &#123; w.lock(); try &#123; m.clear(); &#125; finally &#123; w.unlock(); &#125; &#125; &#125; 源码分析构造方法ReentrantReadWriteLock有两个构造方法，如下： 12345678910public ReentrantReadWriteLock() &#123; this(false);&#125;public ReentrantReadWriteLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync(); readerLock = new ReadLock(this); writerLock = new WriteLock(this);&#125; 可以看到，默认的构造方法使用的是非公平模式，创建的Sync是NonfairSync对象，然后初始化读锁和写锁。一旦初始化后，ReadWriteLock接口中的两个方法就有返回值了，如下： 12public ReentrantReadWriteLock.WriteLock writeLock() &#123; return writerLock; &#125; public ReentrantReadWriteLock.ReadLock readLock() &#123; return readerLock; &#125; 从上面可以看到，构造方法决定了Sync是FairSync还是NonfairSync。Sync继承了AbstractQueuedSynchronizer，而Sync是一个抽象类，NonfairSync和FairSync继承了Sync，并重写了其中的抽象方法。 Sync分析Sync中提供了很多方法，但是有两个方法是抽象的，子类必须实现。下面以FairSync为例，分析一下这两个抽象方法： 123456789static final class FairSync extends Sync &#123; private static final long serialVersionUID = -2274990926593161451L; final boolean writerShouldBlock() &#123; return hasQueuedPredecessors(); &#125; final boolean readerShouldBlock() &#123; return hasQueuedPredecessors(); &#125; &#125; writerShouldBlock和readerShouldBlock方法都表示当有别的线程也在尝试获取锁时，是否应该阻塞。对于公平模式，hasQueuedPredecessors()方法表示前面是否有等待线程。一旦前面有等待线程，那么为了遵循公平，当前线程也就应该被挂起。下面再来看NonfairSync的实现： 12345678910111213141516static final class NonfairSync extends Sync &#123; private static final long serialVersionUID = -8159625535654395037L; final boolean writerShouldBlock() &#123; return false; // writers can always barge &#125; final boolean readerShouldBlock() &#123; /* As a heuristic to avoid indefinite writer starvation, * block if the thread that momentarily appears to be head * of queue, if one exists, is a waiting writer. This is * only a probabilistic effect since a new reader will not * block if there is a waiting writer behind other enabled * readers that have not yet drained from the queue. */ return apparentlyFirstQueuedIsExclusive(); &#125; &#125; 从上面可以看到，非公平模式下，writerShouldBlock直接返回false，说明不需要阻塞；而readShouldBlock调用了apparentFirstQueuedIsExcluisve()方法。该方法在当前线程是写锁占用的线程时，返回true；否则返回false。也就说明，如果当前有一个写线程正在写，那么该读线程应该阻塞。继承AQS的类都需要使用state变量代表某种资源，ReentrantReadWriteLock中的state代表了读锁的数量和写锁的持有与否，整个结构如下： 可以看到state的高16位代表读锁的个数；低16位代表写锁的状态。 获取锁读锁的获取当需要使用读锁时，首先调用lock方法，如下： 1234public void lock() &#123; sync.acquireShared(1); &#125; 从代码可以看到，读锁使用的是AQS的共享模式，AQS的acquireShared方法如下： 12if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg); 当tryAcquireShared()方法小于0时，那么会执行doAcquireShared方法将该线程加入到等待队列中。Sync实现了tryAcquireShared方法，如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152protected final int tryAcquireShared(int unused) &#123; /* * Walkthrough: * 1. If write lock held by another thread, fail. * 2. Otherwise, this thread is eligible for * lock wrt state, so ask if it should block * because of queue policy. If not, try * to grant by CASing state and updating count. * Note that step does not check for reentrant * acquires, which is postponed to full version * to avoid having to check hold count in * the more typical non-reentrant case. * 3. If step 2 fails either because thread * apparently not eligible or CAS fails or count * saturated, chain to version with full retry loop. */ Thread current = Thread.currentThread(); int c = getState(); //如果当前有写线程并且本线程不是写线程，不符合重入，失败 if (exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current) return -1; //得到读锁的个数 int r = sharedCount(c); //如果读不应该阻塞并且读锁的个数小于最大值65535，并且可以成功更新状态值,成功 if (!readerShouldBlock() &amp;&amp; r &lt; MAX_COUNT &amp;&amp; compareAndSetState(c, c + SHARED_UNIT)) &#123; //如果当前读锁为0 if (r == 0) &#123; //第一个读线程就是当前线程 firstReader = current; firstReaderHoldCount = 1; &#125; //如果当前线程重入了，记录firstReaderHoldCount else if (firstReader == current) &#123; firstReaderHoldCount++; &#125; //当前读线程和第一个读线程不同,记录每一个线程读的次数 else &#123; HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) cachedHoldCounter = rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); rh.count++; &#125; return 1; &#125; //否则，循环尝试 return fullTryAcquireShared(current); &#125; 从上面的代码以及注释可以看到，分为三步： 如果当前有写线程并且本线程不是写线程，那么失败，返回-1 否则，说明当前没有写线程或者本线程就是写线程（可重入）,接下来判断是否应该读线程阻塞并且读锁的个数是否小于最小值，并且CAS成功使读锁+1，成功，返回1。其余的操作主要是用于计数的 如果2中失败了，失败的原因有三，第一是应该读线程应该阻塞；第二是因为读锁达到了上线；第三是因为CAS失败，有其他线程在并发更新state，那么会调动fullTryAcquireShared方法。 fullTryAcquiredShared方法如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354final int fullTryAcquireShared(Thread current) &#123; HoldCounter rh = null; for (;;) &#123; int c = getState(); //一旦有别的线程获得了写锁，返回-1，失败 if (exclusiveCount(c) != 0) &#123; if (getExclusiveOwnerThread() != current) return -1; &#125; //如果读线程需要阻塞 else if (readerShouldBlock()) &#123; // Make sure we&#x27;re not acquiring read lock reentrantly if (firstReader == current) &#123; // assert firstReaderHoldCount &gt; 0; &#125; //说明有别的读线程占有了锁 else &#123; if (rh == null) &#123; rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) &#123; rh = readHolds.get(); if (rh.count == 0) readHolds.remove(); &#125; &#125; if (rh.count == 0) return -1; &#125; &#125; //如果读锁达到了最大值，抛出异常 if (sharedCount(c) == MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); //如果成功更改状态，成功返回 if (compareAndSetState(c, c + SHARED_UNIT)) &#123; if (sharedCount(c) == 0) &#123; firstReader = current; firstReaderHoldCount = 1; &#125; else if (firstReader == current) &#123; firstReaderHoldCount++; &#125; else &#123; if (rh == null) rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); rh.count++; cachedHoldCounter = rh; // cache for release &#125; return 1; &#125; &#125; &#125; 从上面可以看到fullTryAcquireShared与tryAcquireShared有很多类似的地方。在上面可以看到多次调用了readerShouldBlock方法，对于公平锁，只要队列中有线程在等待，那么将会返回true，也就意味着读线程需要阻塞；对于非公平锁，如果当前有线程获取了写锁，则返回true。一旦不阻塞，那么读线程将会有机会获得读锁。 写锁的获取写锁的lock方法如下： 123public void lock() &#123; sync.acquire(1); &#125; AQS的acquire方法如下： 12345public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); &#125; 从上面可以看到，写锁使用的是AQS的独占模式。首先尝试获取锁，如果获取失败，那么将会把该线程加入到等待队列中。Sync实现了tryAcquire方法用于尝试获取一把锁，如下： 12345678910111213141516171819202122232425262728293031323334353637protected final boolean tryAcquire(int acquires) &#123; /* * Walkthrough: * 1. If read count nonzero or write count nonzero * and owner is a different thread, fail. * 2. If count would saturate, fail. (This can only * happen if count is already nonzero.) * 3. Otherwise, this thread is eligible for lock if * it is either a reentrant acquire or * queue policy allows it. If so, update state * and set owner. */ //得到调用lock方法的当前线程 Thread current = Thread.currentThread(); int c = getState(); //得到写锁的个数 int w = exclusiveCount(c); //如果当前有写锁或者读锁 if (c != 0) &#123; // 如果写锁为0或者当前线程不是独占线程（不符合重入），返回false if (w == 0 || current != getExclusiveOwnerThread()) return false; //如果写锁的个数超过了最大值，抛出异常 if (w + exclusiveCount(acquires) &gt; MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); // 写锁重入，返回true setState(c + acquires); return true; &#125; //如果当前没有写锁或者读锁，如果写线程应该阻塞或者CAS失败，返回false if (writerShouldBlock() || !compareAndSetState(c, c + acquires)) return false; //否则将当前线程置为获得写锁的线程,返回true setExclusiveOwnerThread(current); return true; &#125; 从代码和注释可以看到，获取写锁时有三步： 如果当前有写锁或者读锁。如果只有读锁，返回false，因为这时如果可以写，那么读线程得到的数据就有可能错误；如果有写锁，但是线程不同，即不符合写锁重入规则，返回false 如果写锁的数量将会超过最大值65535，抛出异常；否则，写锁重入 如果没有读锁或写锁的话，如果需要阻塞或者CAS失败，返回false；否则将当前线程置为获得写锁的线程 从上面可以看到调用了writerShouldBlock方法，FairSync的实现是如果等待队列中有等待线程，则返回false，说明公平模式下，只要队列中有线程在等待，那么后来的这个线程也是需要记入队列等待的；NonfairSync中的直接返回的直接是false，说明不需要阻塞。从上面的代码可以得出，当没有锁时，如果使用的非公平模式下的写锁的话，那么返回false，直接通过CAS就可以获得写锁。 总结从上面分析可以得出结论： 如果当前没有写锁或读锁时，第一个获取锁的线程都会成功，无论该锁是写锁还是读锁。 如果当前已经有了读锁，那么这时获取写锁将失败，获取读锁有可能成功也有可能失败 如果当前已经有了写锁，那么这时获取读锁或写锁，如果线程相同（可重入），那么成功；否则失败 释放锁获取锁要做的是更改AQS的状态值以及将需要等待的线程放入到队列中；释放锁要做的就是更改AQS的状态值以及唤醒队列中的等待线程来继续获取锁。 读锁的释放ReadLock的unlock方法如下： 123public void unlock() &#123; sync.releaseShared(1); &#125; 调用了Sync的releaseShared方法，该方法在AQS中提供，如下： 1234567public final boolean releaseShared(int arg) &#123; if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false; &#125; 调用tryReleaseShared方法尝试释放锁，如果释放成功，调用doReleaseShared尝试唤醒下一个节点。AQS的子类需要实现tryReleaseShared方法，Sync中的实现如下： 12345678910111213141516171819202122232425262728293031323334353637protected final boolean tryReleaseShared(int unused) &#123; //得到调用unlock的线程 Thread current = Thread.currentThread(); //如果是第一个获得读锁的线程 if (firstReader == current) &#123; // assert firstReaderHoldCount &gt; 0; if (firstReaderHoldCount == 1) firstReader = null; else firstReaderHoldCount--; &#125; //否则，是HoldCounter中计数-1 else &#123; HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); int count = rh.count; if (count &lt;= 1) &#123; readHolds.remove(); if (count &lt;= 0) throw unmatchedUnlockException(); &#125; --rh.count; &#125; //死循环 for (;;) &#123; int c = getState(); //释放一把读锁 int nextc = c - SHARED_UNIT; //如果CAS更新状态成功，返回读锁是否等于0；失败的话，则重试 if (compareAndSetState(c, nextc)) // Releasing the read lock has no effect on readers, // but it may allow waiting writers to proceed if // both read and write locks are now free. return nextc == 0; &#125; &#125; 从上面可以看到，释放锁的第一步是更新firstReader或HoldCounter的计数，接下来进入死循环，尝试更新AQS的状态，一旦更新成功，则返回；否则，则重试。释放读锁对读线程没有影响，但是可能会使等待的写线程解除挂起开始运行。所以，一旦没有锁了，就返回true，否则false；返回true后，那么则需要释放等待队列中的线程，这时读线程和写线程都有可能再获得锁。 写锁的释放WriteLock的unlock方法如下： 123public void unlock() &#123; sync.release(1); &#125; Sync的release方法使用的AQS中的，如下： 123456789public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false; &#125; 调用tryRelease尝试释放锁，一旦释放成功了，那么如果等待队列中有线程再等待，那么调用unparkSuccessor将下一个线程解除挂起。Sync需要实现tryRelease方法，如下： 12345678910111213protected final boolean tryRelease(int releases) &#123; //如果没有线程持有写锁，但是仍要释放，抛出异常 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); int nextc = getState() - releases; boolean free = exclusiveCount(nextc) == 0; //如果没有写锁了，那么将AQS的线程置为null if (free) setExclusiveOwnerThread(null); //更新状态 setState(nextc); return free; &#125; 从上面可以看到，写锁的释放主要有三步： 如果当前没有线程持有写锁，但是还要释放写锁，抛出异常 得到解除一把写锁后的状态，如果没有写锁了，那么将AQS的线程置为null 不管第二步中是否需要将AQS的线程置为null，AQS的状态总是要更新的 从上面可以看到，返回true当且只当没有写锁的情况下，还有写锁则返回false。 总结从上面的分析可以得出： 如果当前是写锁被占有了，只有当写锁的数据降为0时才认为释放成功；否则失败。因为只要有写锁，那么除了占有写锁的那个线程，其他线程即不可以获得读锁，也不能获得写锁 如果当前是读锁被占有了，那么只有在写锁的个数为0时才认为释放成功。因为一旦有写锁，别的任何线程都不应该再获得读锁了，除了获得写锁的那个线程。 其他方法看完了ReentrantReadWriteLock中的读锁的获取和释放，写锁的获取和释放，再来看一下其余的一些辅助方法来加深我们对读写锁的理解。 getOwner()getOwner方法用于返回当前获得写锁的线程，如果没有线程占有写锁，那么返回null。实现如下： 123protected Thread getOwner() &#123; return sync.getOwner(); &#125; 可以看到直接调用了Sync的getOwner方法，下面是Sync的getOwner方法： 123456final Thread getOwner() &#123; // Must read state before owner to ensure memory consistency return ((exclusiveCount(getState()) == 0) ? null : getExclusiveOwnerThread()); &#125; 如果独占锁的个数为0，说明没有线程占有写锁，那么返回null；否则返回占有写锁的线程。 getReadLockCount()getReadLockCount()方法用于返回读锁的个数，实现如下： 123public int getReadLockCount() &#123; return sync.getReadLockCount(); &#125; Sync的实现如下： 12345final int getReadLockCount() &#123; return sharedCount(getState()); &#125;static int sharedCount(int c) &#123; return c &gt;&gt;&gt; SHARED_SHIFT; &#125; 从上面代码可以看出，要想得到读锁的个数，就是看AQS的state的高16位。这和前面讲过的一样，高16位表示读锁的个数，低16位表示写锁的个数。 getReadHoldCount()getReadHoldCount()方法用于返回当前线程所持有的读锁的个数，如果当前线程没有持有读锁，则返回0。直接看Sync的实现即可： 12345678910111213141516171819202122final int getReadHoldCount() &#123; //如果没有读锁，自然每个线程都是返回0 if (getReadLockCount() == 0) return 0; //得到当前线程 Thread current = Thread.currentThread(); //如果当前线程是第一个读线程，返回firstReaderHoldCount参数 if (firstReader == current) return firstReaderHoldCount; //如果当前线程不是第一个读线程，得到HoldCounter，返回其中的count HoldCounter rh = cachedHoldCounter; //如果缓存的HoldCounter不为null并且是当前线程的HoldCounter，直接返回count if (rh != null &amp;&amp; rh.tid == getThreadId(current)) return rh.count; //如果缓存的HoldCounter不是当前线程的HoldCounter，那么从ThreadLocal中得到本线程的HoldCounter，返回计数 int count = readHolds.get().count; //如果本线程持有的读锁为0，从ThreadLocal中移除 if (count == 0) readHolds.remove(); return count; &#125; 从上面的代码中，可以看到两个熟悉的变量，firstReader和HoldCounter类型。这两个变量在读锁的获取中接触过，前面没有细说，这里细说一下。HoldCounter类的实现如下： 12345static final class HoldCounter &#123; int count = 0; // Use id, not reference, to avoid garbage retention final long tid = getThreadId(Thread.currentThread()); &#125; readHolds是ThreadLocalHoldCounter类，定义如下： 123456static final class ThreadLocalHoldCounter extends ThreadLocal&lt;HoldCounter&gt; &#123; public HoldCounter initialValue() &#123; return new HoldCounter(); &#125; &#125; 可以看到，readHolds存储了每一个线程的HoldCounter，而HoldCounter中的count变量就是用来记录线程获得的写锁的个数。所以可以得出结论：Sync维持总的读锁的个数，在state的高16位；由于读线程可以同时存在，所以每个线程还保存了获得的读锁的个数，这个是通过HoldCounter来保存的。除此之外，对于第一个读线程有特殊的处理，Sync中有如下两个变量： 12private transient Thread firstReader = null; private transient int firstReaderHoldCount; 看完了HoldCounter和firstReader，再来看一下getReadLockCount的实现，主要有三步： 当前没有读锁，那么自然每一个线程获得的读锁都是0； 如果当前线程是第一个获取到读锁的线程，那么返回firstReadHoldCount； 如果当前线程不是第一个获取到读锁的线程，得到该线程的HoldCounter，然后返回其count字段。如果count字段为0，说明该线程没有占有读锁，那么从readHolds中移除。获取HoldCounter分为两步，第一步是与cachedHoldCounter比较，如果不是，则从readHolds中获取。 getWriteLockCount()getWriteLockCount()方法返回写锁的个数，Sync的实现如下： 123final int getWriteHoldCount() &#123; return isHeldExclusively() ? exclusiveCount(getState()) : 0; &#125; 可以看到如果没有线程持有写锁，那么返回0；否则返回AQS的state的低16位。 总结当分析ReentranctReadWriteLock时，或者说分析内部使用AQS实现的工具类时，需要明白的就是AQS的state代表的是什么。ReentrantLockReadWriteLock中的state同时表示写锁和读锁的个数。为了实现这种功能，state的高16位表示读锁的个数，低16位表示写锁的个数。AQS有两种模式：共享模式和独占模式，读写锁的实现中，读锁使用共享模式；写锁使用独占模式；另外一点需要记住的即使，当有读锁时，写锁就不能获得；而当有写锁时，除了获得写锁的这个线程可以获得读锁外，其他线程不能获得读锁。 参考深入理解读写锁—ReadWriteLock源码分析","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"ReadWriteLock","slug":"ReadWriteLock","permalink":"https://xmmarlowe.github.io/tags/ReadWriteLock/"},{"name":"源码","slug":"源码","permalink":"https://xmmarlowe.github.io/tags/%E6%BA%90%E7%A0%81/"}],"author":"Marlowe"},{"title":"Java四种引用-强软弱虚","slug":"Java/Java四种引用-强软弱虚","date":"2021-05-10T11:31:58.000Z","updated":"2021-05-16T13:58:31.959Z","comments":true,"path":"2021/05/10/Java/Java四种引用-强软弱虚/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/10/Java/Java%E5%9B%9B%E7%A7%8D%E5%BC%95%E7%94%A8-%E5%BC%BA%E8%BD%AF%E5%BC%B1%E8%99%9A/","excerpt":"","text":"","categories":[],"tags":[],"author":"Marlowe"},{"title":"Fork/Join框架基本使用","slug":"并发/Fork-Join框架基本使用","date":"2021-05-10T08:56:53.000Z","updated":"2021-05-10T14:43:22.998Z","comments":true,"path":"2021/05/10/并发/Fork-Join框架基本使用/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/10/%E5%B9%B6%E5%8F%91/Fork-Join%E6%A1%86%E6%9E%B6%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","excerpt":"","text":"1. 概述Java.util.concurrent.ForkJoinPool由Java大师Doug Lea主持编写，它可以将一个大的任务拆分成多个子任务进行并行处理，最后将子任务结果合并成最后的计算结果，并进行输出。本文中对Fork/Join框架的讲解，基于JDK1.8+中的Fork/Join框架实现，参考的Fork/Join框架主要源代码也基于JDK1.8+。 这几篇文章将试图解释Fork/Join框架的知识点，以便对自己、对各位读者在并发程序的设计思路上进行一些启发。文章将首先讲解Fork/Join框架的基本使用，以及其中需要注意的使用要点；接着使用Fork/Join框架解决一些实际问题；最后再讲解Fork/Join框架的工作原理。 2. Fork/Join框架基本使用这里是一个简单的Fork/Join框架使用示例，在这个示例中我们计算了1-1001累加后的值： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * 这是一个简单的Join/Fork计算过程，将1—1001数字相加 */public class TestForkJoinPool &#123; private static final Integer MAX = 200; static class MyForkJoinTask extends RecursiveTask&lt;Integer&gt; &#123; // 子任务开始计算的值 private Integer startValue; // 子任务结束计算的值 private Integer endValue; public MyForkJoinTask(Integer startValue , Integer endValue) &#123; this.startValue = startValue; this.endValue = endValue; &#125; @Override protected Integer compute() &#123; // 如果条件成立，说明这个任务所需要计算的数值分为足够小了 // 可以正式进行累加计算了 if(endValue - startValue &lt; MAX) &#123; System.out.println(&quot;开始计算的部分：startValue = &quot; + startValue + &quot;;endValue = &quot; + endValue); Integer totalValue = 0; for(int index = this.startValue ; index &lt;= this.endValue ; index++) &#123; totalValue += index; &#125; return totalValue; &#125; // 否则再进行任务拆分，拆分成两个任务 else &#123; MyForkJoinTask subTask1 = new MyForkJoinTask(startValue, (startValue + endValue) / 2); subTask1.fork(); MyForkJoinTask subTask2 = new MyForkJoinTask((startValue + endValue) / 2 + 1 , endValue); subTask2.fork(); return subTask1.join() + subTask2.join(); &#125; &#125; &#125; public static void main(String[] args) &#123; // 这是Fork/Join框架的线程池 ForkJoinPool pool = new ForkJoinPool(); ForkJoinTask&lt;Integer&gt; taskFuture = pool.submit(new MyForkJoinTask(1,1001)); try &#123; Integer result = taskFuture.get(); System.out.println(&quot;result = &quot; + result); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(System.out); &#125; &#125;&#125; 以上代码很简单，在关键的位置有相关的注释说明。这里本文再对以上示例中的要点进行说明。首先看看以上示例代码的可能执行结果： 123456789开始计算的部分：startValue = 1;endValue = 126开始计算的部分：startValue = 127;endValue = 251开始计算的部分：startValue = 252;endValue = 376开始计算的部分：startValue = 377;endValue = 501开始计算的部分：startValue = 502;endValue = 626开始计算的部分：startValue = 627;endValue = 751开始计算的部分：startValue = 752;endValue = 876开始计算的部分：startValue = 877;endValue = 1001result = 501501 2-1. 工作顺序图下图展示了以上代码的工作过程概要，但实际上Fork/Join框架的内部工作过程要比这张图复杂得多，例如如何决定某一个recursive task是使用哪条线程进行运行；再例如如何决定当一个任务/子任务提交到Fork/Join框架内部后，是创建一个新的线程去运行还是让它进行队列等待。 所以如果不深入理解Fork/Join框架的运行原理，只是根据之上最简单的使用例子观察运行效果，那么我们只能知道子任务在Fork/Join框架中被拆分得足够小后，并且其内部使用多线程并行完成这些小任务的计算后再进行结果向上的合并动作，最终形成顶层结果。不急，一步一步来，我们先从这张概要的过程图开始讨论。 图中最顶层的任务使用submit方式被提交到Fork/Join框架中，后者将前者放入到某个线程中运行，工作任务中的compute方法的代码开始对这个任务T1进行分析。如果当前任务需要累加的数字范围过大（代码中设定的是大于200），则将这个计算任务拆分成两个子任务（T1.1和T1.2），每个子任务各自负责计算一半的数据累加，请参见代码中的fork方法。如果当前子任务中需要累加的数字范围足够小（小于等于200），就进行累加然后返回到上层任务中。 2-2. ForkJoinPool构造函数ForkJoinPool有四个构造函数，其中参数最全的那个构造函数如下所示： 1234public ForkJoinPool(int parallelism, ForkJoinWorkerThreadFactory factory, UncaughtExceptionHandler handler, boolean asyncMode) parallelism： 可并行级别，Fork/Join框架将依据这个并行级别的设定，决定框架内并行执行的线程数量。并行的每一个任务都会有一个线程进行处理，但是千万不要将这个属性理解成Fork/Join框架中最多存在的线程数量，也不要将这个属性和ThreadPoolExecutor线程池中的corePoolSize、maximumPoolSize属性进行比较，因为ForkJoinPool的组织结构和工作方式与后者完全不一样。而后续的讨论中，读者还可以发现Fork/Join框架中可存在的线程数量和这个参数值的关系并不是绝对的关联（有依据但并不全由它决定）。 factory： 当Fork/Join框架创建一个新的线程时，同样会用到线程创建工厂。只不过这个线程工厂不再需要实现ThreadFactory接口，而是需要实现ForkJoinWorkerThreadFactory接口。后者是一个函数式接口，只需要实现一个名叫newThread的方法。在Fork/Join框架中有一个默认的ForkJoinWorkerThreadFactory接口实现：DefaultForkJoinWorkerThreadFactory。 handler： 异常捕获处理器。当执行的任务中出现异常，并从任务中被抛出时，就会被handler捕获。 asyncMode： 这个参数也非常重要，从字面意思来看是指的异步模式，它并不是说Fork/Join框架是采用同步模式还是采用异步模式工作。Fork/Join框架中为每一个独立工作的线程准备了对应的待执行任务队列，这个任务队列是使用数组进行组合的双向队列。即是说存在于队列中的待执行任务，即可以使用先进先出的工作模式，也可以使用后进先出的工作模式。 当asyncMode设置为ture的时候，队列采用先进先出方式工作；反之则是采用后进先出的方式工作，该值默认为false 123......asyncMode ? FIFO_QUEUE : LIFO_QUEUE,...... ForkJoinPool还有另外两个构造函数，一个构造函数只带有parallelism参数，既是可以设定Fork/Join框架的最大并行任务数量；另一个构造函数则不带有任何参数，对于最大并行任务数量也只是一个默认值——当前操作系统可以使用的CPU内核数量（Runtime.getRuntime().availableProcessors()）。实际上ForkJoinPool还有一个私有的、原生构造函数，之上提到的三个构造函数都是对这个私有的、原生构造函数的调用。 1234567891011121314......private ForkJoinPool(int parallelism, ForkJoinWorkerThreadFactory factory, UncaughtExceptionHandler handler, int mode, String workerNamePrefix) &#123; this.workerNamePrefix = workerNamePrefix; this.factory = factory; this.ueh = handler; this.config = (parallelism &amp; SMASK) | mode; long np = (long)(-parallelism); // offset ctl counts this.ctl = ((np &lt;&lt; AC_SHIFT) &amp; AC_MASK) | ((np &lt;&lt; TC_SHIFT) &amp; TC_MASK); &#125;...... 如果你对Fork/Join框架没有特定的执行要求，可以直接使用不带有任何参数的构造函数。也就是说推荐基于当前操作系统可以使用的CPU内核数作为Fork/Join框架内最大并行任务数量，这样可以保证CPU在处理并行任务时，尽量少发生任务线程间的运行状态切换（实际上单个CPU内核上的线程间状态切换基本上无法避免，因为操作系统同时运行多个线程和多个进程）。 2-3. fork方法和join方法Fork/Join框架中提供的fork方法和join方法，可以说是该框架中提供的最重要的两个方法，它们和parallelism“可并行任务数量”配合工作，可以导致拆分的子任务T1.1、T1.2甚至TX在Fork/Join框架中不同的运行效果。例如TX子任务或等待其它已存在的线程运行关联的子任务，或在运行TX的线程中“递归”执行其它任务，又或者启动一个新的线程运行子任务…… fork方法用于将新创建的子任务放入当前线程的work queue队列中，Fork/Join框架将根据当前正在并发执行ForkJoinTask任务的ForkJoinWorkerThread线程状态，决定是让这个任务在队列中等待，还是创建一个新的ForkJoinWorkerThread线程运行它，又或者是唤起其它正在等待任务的ForkJoinWorkerThread线程运行它。 这里面有几个元素概念需要注意，ForkJoinTask任务是一种能在Fork/Join框架中运行的特定任务，也只有这种类型的任务可以在Fork/Join框架中被拆分运行和合并运行。ForkJoinWorkerThread线程是一种在Fork/Join框架中运行的特性线程，它除了具有普通线程的特性外，最主要的特点是每一个ForkJoinWorkerThread线程都具有一个独立的任务等待队列（work queue），这个任务队列用于存储在本线程中被拆分的若干子任务。 join方法用于让当前线程阻塞，直到对应的子任务完成运行并返回执行结果。或者，如果这个子任务存在于当前线程的任务等待队列（work queue）中，则取出这个子任务进行“递归”执行。其目的是尽快得到当前子任务的运行结果，然后继续执行。 3. 使用Fork/Join解决实际问题之前所举的的例子是使用Fork/Join框架完成1-1000的整数累加。这个示例如果只是演示Fork/Join框架的使用，那还行，但这种例子和实际工作中所面对的问题还有一定差距。本篇文章我们使用Fork/Join框架解决一个实际问题，就是高效排序的问题。 3-1. 使用归并算法解决排序问题排序问题是我们工作中的常见问题。目前也有很多现成算法是为了解决这个问题而被发明的，例如多种插值排序算法、多种交换排序算法。而并归排序算法是目前所有排序算法中，平均时间复杂度较好（O(nlgn)），算法稳定性较好的一种排序算法。它的核心算法思路将大的问题分解成多个小问题，并将结果进行合并。 整个算法的拆分阶段，是将未排序的数字集合，从一个较大集合递归拆分成若干较小的集合，这些较小的集合要么包含最多两个元素，要么就认为不够小需要继续进行拆分。 那么对于一个集合中元素的排序问题就变成了两个问题：1、较小集合中最多两个元素的大小排序；2、如何将两个有序集合合并成一个新的有序集合。第一个问题很好解决，那么第二个问题是否会很复杂呢？实际上第二个问题也很简单，只需要将两个集合同时进行一次遍历即可完成——比较当前集合中最小的元素，将最小元素放入新的集合，它的时间复杂度为O(n)： 以下是归并排序算法的简单实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788package test.thread.pool.merge; import java.util.Arrays;import java.util.Random; /** * 归并排序 * @author yinwenjie */public class Merge1 &#123; private static int MAX = 10000; private static int inits[] = new int[MAX]; // 这是为了生成一个数量为MAX的随机整数集合，准备计算数据 // 和算法本身并没有什么关系 static &#123; Random r = new Random(); for(int index = 1 ; index &lt;= MAX ; index++) &#123; inits[index - 1] = r.nextInt(10000000); &#125; &#125; public static void main(String[] args) &#123; long beginTime = System.currentTimeMillis(); int results[] = forkits(inits); long endTime = System.currentTimeMillis(); // 如果参与排序的数据非常庞大，记得把这种打印方式去掉 System.out.println(&quot;耗时=&quot; + (endTime - beginTime) + &quot; | &quot; + Arrays.toString(results)); &#125; // 拆分成较小的元素或者进行足够小的元素集合的排序 private static int[] forkits(int source[]) &#123; int sourceLen = source.length; if(sourceLen &gt; 2) &#123; int midIndex = sourceLen / 2; int result1[] = forkits(Arrays.copyOf(source, midIndex)); int result2[] = forkits(Arrays.copyOfRange(source, midIndex , sourceLen)); // 将两个有序的数组，合并成一个有序的数组 int mer[] = joinInts(result1 , result2); return mer; &#125; // 否则说明集合中只有一个或者两个元素，可以进行这两个元素的比较排序了 else &#123; // 如果条件成立，说明数组中只有一个元素，或者是数组中的元素都已经排列好位置了 if(sourceLen == 1 || source[0] &lt;= source[1]) &#123; return source; &#125; else &#123; int targetp[] = new int[sourceLen]; targetp[0] = source[1]; targetp[1] = source[0]; return targetp; &#125; &#125; &#125; /** * 这个方法用于合并两个有序集合 * @param array1 * @param array2 */ private static int[] joinInts(int array1[] , int array2[]) &#123; int destInts[] = new int[array1.length + array2.length]; int array1Len = array1.length; int array2Len = array2.length; int destLen = destInts.length; // 只需要以新的集合destInts的长度为标准，遍历一次即可 for(int index = 0 , array1Index = 0 , array2Index = 0 ; index &lt; destLen ; index++) &#123; int value1 = array1Index &gt;= array1Len?Integer.MAX_VALUE:array1[array1Index]; int value2 = array2Index &gt;= array2Len?Integer.MAX_VALUE:array2[array2Index]; // 如果条件成立，说明应该取数组array1中的值 if(value1 &lt; value2) &#123; array1Index++; destInts[index] = value1; &#125; // 否则取数组array2中的值 else &#123; array2Index++; destInts[index] = value2; &#125; &#125; return destInts; &#125;&#125; 以上归并算法对1万条随机数进行排序只需要2-3毫秒，对10万条随机数进行排序只需要20毫秒左右的时间，对100万条随机数进行排序的平均时间大约为160毫秒（这还要看随机生成的待排序数组是否本身的凌乱程度）。可见归并算法本身是具有良好的性能的。使用JMX工具和操作系统自带的CPU监控器监视应用程序的执行情况，可以发现整个算法是单线程运行的，且同一时间CPU只有单个内核在作为主要的处理内核工作： JMX中观察到的线程情况： CPU的运作情况： 3-2. 使用Fork/Join运行归并算法但是随着待排序集合中数据规模继续增大，以上归并算法的代码实现就有一些力不从心了，例如以上算法对1亿条随机数集合进行排序时，耗时为27秒左右。 接着我们可以使用Fork/Join框架来优化归并算法的执行性能，将拆分后的子任务实例化成多个ForkJoinTask任务放入待执行队列，并由Fork/Join框架在多个ForkJoinWorkerThread线程间调度这些任务。如下图所示： 以下为使用Fork/Join框架后的归并算法代码，请注意joinInts方法中对两个有序集合合并成一个新的有序集合的代码，是没有变化的可以参见本文上一小节中的内容。所以在代码中就不再赘述了： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283....../** * 使用Fork/Join框架的归并排序算法 * @author yinwenjie */public class Merge2 &#123; private static int MAX = 100000000; private static int inits[] = new int[MAX]; // 同样进行随机队列初始化，这里就不再赘述了 static &#123; ...... &#125; public static void main(String[] args) throws Exception &#123; // 正式开始 long beginTime = System.currentTimeMillis(); ForkJoinPool pool = new ForkJoinPool(); MyTask task = new MyTask(inits); ForkJoinTask&lt;int[]&gt; taskResult = pool.submit(task); try &#123; taskResult.get(); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(System.out); &#125; long endTime = System.currentTimeMillis(); System.out.println(&quot;耗时=&quot; + (endTime - beginTime)); &#125; /** * 单个排序的子任务 * @author yinwenjie */ static class MyTask extends RecursiveTask&lt;int[]&gt; &#123; private int source[]; public MyTask(int source[]) &#123; this.source = source; &#125; /* (non-Javadoc) * @see java.util.concurrent.RecursiveTask#compute() */ @Override protected int[] compute() &#123; int sourceLen = source.length; // 如果条件成立，说明任务中要进行排序的集合还不够小 if(sourceLen &gt; 2) &#123; int midIndex = sourceLen / 2; // 拆分成两个子任务 MyTask task1 = new MyTask(Arrays.copyOf(source, midIndex)); task1.fork(); MyTask task2 = new MyTask(Arrays.copyOfRange(source, midIndex , sourceLen)); task2.fork(); // 将两个有序的数组，合并成一个有序的数组 int result1[] = task1.join(); int result2[] = task2.join(); int mer[] = joinInts(result1 , result2); return mer; &#125; // 否则说明集合中只有一个或者两个元素，可以进行这两个元素的比较排序了 else &#123; // 如果条件成立，说明数组中只有一个元素，或者是数组中的元素都已经排列好位置了 if(sourceLen == 1 || source[0] &lt;= source[1]) &#123; return source; &#125; else &#123; int targetp[] = new int[sourceLen]; targetp[0] = source[1]; targetp[1] = source[0]; return targetp; &#125; &#125; &#125; private int[] joinInts(int array1[] , int array2[]) &#123; // 和上文中出现的代码一致 &#125; &#125;&#125; 使用Fork/Join框架优化后，同样执行1亿条随机数的排序处理时间大约在14秒左右，当然这还和待排序集合本身的凌乱程度、CPU性能等有关系。但总体上这样的方式比不使用Fork/Join框架的归并排序算法在性能上有30%左右的性能提升。以下为执行时观察到的CPU状态和线程状态： JMX中的内存、线程状态： CPU使用情况： 除了归并算法代码实现内部可优化的细节处，使用Fork/Join框架后，我们基本上在保证操作系统线程规模的情况下，将每一个CPU内核的运算资源同时发挥了出来。 参考Fork/Join框架基本使用","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"ForkJoin","slug":"ForkJoin","permalink":"https://xmmarlowe.github.io/tags/ForkJoin/"}],"author":"Marlowe"},{"title":"MySQL一些规范以及优化问题","slug":"数据库/MySQL一些规范以及优化问题","date":"2021-05-10T08:14:20.000Z","updated":"2021-05-10T14:43:22.995Z","comments":true,"path":"2021/05/10/数据库/MySQL一些规范以及优化问题/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/10/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL%E4%B8%80%E4%BA%9B%E8%A7%84%E8%8C%83%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98/","excerpt":"","text":"如何选择索引列的顺序建立索引的目的是：希望通过索引进行数据查找，减少随机 IO，增加查询性能 ，索引能过滤出越少的数据，则从磁盘中读入的数据也就越少。 区分度最高的放在联合索引的最左侧（区分度=列中不同值的数量/列的总行数） 尽量把字段长度小的列放在联合索引的最左侧（因为字段长度越小，一页能存储的数据量越大，IO 性能也就越好） 使用最频繁的列放到联合索引的左侧（这样可以比较少的建立一些索引） 避免建立冗余索引和重复索引（增加了查询优化器生成执行计划的时间） 重复索引示例：primary key(id)、index(id)、unique index(id) 冗余索引示例：index(a,b,c)、index(a,b)、index(a) 对于频繁的查询优先考虑使用覆盖索引覆盖索引：就是包含了所有查询字段 (where,select,ordery by,group by 包含的字段) 的索引 覆盖索引的好处： 避免 Innodb 表进行索引的二次查询: Innodb 是以聚集索引的顺序来存储的，对于 Innodb 来说，二级索引在叶子节点中所保存的是行的主键信息，如果是用二级索引查询数据的话，在查找到相应的键值后，还要通过主键进行二次查询才能获取我们真实所需要的数据。而在覆盖索引中，二级索引的键值中可以获取所有的数据，避免了对主键的二次查询 ，减少了 IO 操作，提升了查询效率。 可以把随机 IO 变成顺序 IO 加快查询效率: 由于覆盖索引是按键值的顺序存储的，对于 IO 密集型的范围查找来说，对比随机从磁盘读取每一行的数据 IO 要少的多，因此利用覆盖索引在访问时也可以把磁盘的随机读取的 IO 转变成索引查找的顺序 IO。 避免使用子查询，可以把子查询优化为 join 操作通常子查询在 in 子句中，且子查询中为简单 SQL(不包含 union、group by、order by、limit 从句) 时,才可以把子查询转化为关联查询进行优化。 子查询性能差的原因： 子查询的结果集无法使用索引，通常子查询的结果集会被存储到临时表中，不论是内存临时表还是磁盘临时表都不会存在索引，所以查询性能会受到一定的影响。特别是对于返回结果集比较大的子查询，其对查询性能的影响也就越大。 由于子查询会产生大量的临时表也没有索引，所以会消耗过多的 CPU 和 IO 资源，产生大量的慢查询。 在明显不会有重复值时使用 UNION ALL 而不是 UNION UNION 会把两个结果集的所有数据放到临时表中后再进行去重操作 UNION ALL 不会再对结果集进行去重操作 拆分复杂的大 SQL 为多个小 SQL 大 SQL 逻辑上比较复杂，需要占用大量 CPU 进行计算的 SQL MySQL 中，一个 SQL 只能使用一个 CPU 进行计算 SQL 拆分后可以通过并行执行来提高处理效率 超100 万行的批量写 (UPDATE,DELETE,INSERT) 操作,要分批多次进行操作大批量操作可能会造成严重的主从延迟 主从环境中,大批量操作可能会造成严重的主从延迟，大批量的写操作一般都需要执行一定长的时间， 而只有当主库上执行完成后，才会在其他从库上执行，所以会造成主库与从库长时间的延迟情况 binlog 日志为 row 格式时会产生大量的日志 大批量写操作会产生大量日志，特别是对于 row 格式二进制数据而言，由于在 row 格式中会记录每一行数据的修改，我们一次修改的数据越多，产生的日志量也就会越多，日志的传输和恢复所需要的时间也就越长，这也是造成主从延迟的一个原因 避免产生大事务操作 大批量修改数据，一定是在一个事务中进行的，这就会造成表中大批量数据进行锁定，从而导致大量的阻塞，阻塞会对 MySQL 的性能产生非常大的影响。 特别是长时间的阻塞会占满所有数据库的可用连接，这会使生产环境中的其他应用无法连接到数据库，因此一定要注意大批量写操作要进行分批","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://xmmarlowe.github.io/tags/MySQL/"}],"author":"Marlowe"},{"title":"Java浮点数精度问题","slug":"Java/Java浮点数精度问题","date":"2021-05-10T08:07:00.000Z","updated":"2021-05-16T13:58:32.265Z","comments":true,"path":"2021/05/10/Java/Java浮点数精度问题/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/10/Java/Java%E6%B5%AE%E7%82%B9%E6%95%B0%E7%B2%BE%E5%BA%A6%E9%97%AE%E9%A2%98/","excerpt":"","text":"一、精度丢失的原因1234567891011121314151617181920212223首先我们要搞清楚下面两个问题：(1) 十进制整数如何转化为二进制数 算法很简单。举个例子，11表示成二进制数： 11/2=5 余 1 5/2=2 余 1 2/2=1 余 0 1/2=0 余 1 0结束 11二进制表示为(从下往上):1011 这里提一点：只要遇到除以后的结果为0了就结束了，大家想一想，所有的整数除以2是不是一定能够最终得到0。换句话说，所有的整数转变为二进制数的算法会不会无限循环下去呢？绝对不会，整数永远可以用二进制精确表示 ，但小数就不一定了。 (2) 十进制小数如何转化为二进制数算法是乘以2直到没有了小数为止。举个例子，0.9表示成二进制数 0.9*2=1.8 取整数部分 1 0.8(1.8的小数部分)*2=1.6 取整数部分 1 0.6*2=1.2 取整数部分 1 0.2*2=0.4 取整数部分 0 0.4*2=0.8 取整数部分 0 0.8*2=1.6 取整数部分 1 0.6*2=1.2 取整数部分 0 ......... 0.9二进制表示为(从上往下): 1100100100100......注意：上面的计算过程循环了，也就是说*2永远不可能消灭小数部分，这样算法将无限下去。很显然，小数的二进制表示有时是不可能精确的 。其实道理很简单，十进制系统中能不能准确表示出1/3呢？同样二进制系统也无法准确表示1/10。这也就解释了为什么浮点型减法出现了&quot;减不尽&quot;的精度丢失问题。 二、float存储原理1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253众所周知、 Java 的float型在内存中占4个字节。float的32个二进制位结构如下 float内存存储结构 4bytes 31 30 29----23 22----0 表示 实数符号位 指数符号位 指数位 有效数位 其中符号位1表示正，0表示负。有效位数位24位，其中一位是实数符号位。 将一个float型转化为内存存储格式的步骤为： （1）先将这个实数的绝对值化为二进制格式，注意实数的整数部分和小数部分的二进制方法在上面已经探讨过了。 （2）将这个二进制格式实数的小数点左移或右移n位，直到小数点移动到第一个有效数字的右边。 （3）从小数点右边第一位开始数出二十三位数字放入第22到第0位。 （4）如果实数是正的，则在第31位放入“0”，否则放入“1”。 （5）如果n 是左移得到的，说明指数是正的，第30位放入“1”。如果n是右移得到的或n=0，则第30位放入“0”。 （6）如果n是左移得到的，则将n减去1后化为二进制，并在左边加“0”补足七位，放入第29到第23位。如果n是右移得到的或n=0，则将n化为二进制后在左边加“0”补足七位，再各位求反，再放入第29到第23位。 举例说明： 11.9的内存存储格式 (1) 将11.9化为二进制后大约是&quot; 1011. 1110011001100110011001100...&quot;。 (2) 将小数点左移三位到第一个有效位右侧： &quot;1. 011 11100110011001100110 &quot;。 保证有效位数24位，右侧多余的截取（误差在这里产生了 ）。 (3) 这已经有了二十四位有效数字，将最左边一位“1”去掉，得到“ 011 11100110011001100110 ”共23bit。将它放入float存储结构的第22到第0位。 (4) 因为11.9是正数，因此在第31位实数符号位放入“0”。 (5) 由于我们把小数点左移，因此在第30位指数符号位放入“1”。 (6) 因为我们是把小数点左移3位，因此将3减去1得2，化为二进制，并补足7位得到0000010，放入第29到第23位。 最后表示11.9为： 0 1 0000010 011 11100110011001100110 再举一个例子：0.2356的内存存储格式 （1）将0.2356化为二进制后大约是0.00111100010100000100100000。 （2）将小数点右移三位得到1.11100010100000100100000。 （3）从小数点右边数出二十三位有效数字，即11100010100000100100000放入第22到第0位。 （4）由于0.2356是正的，所以在第31位放入“0”。 （5）由于我们把小数点右移了，所以在第30位放入“0”。 （6）因为小数点被右移了3位，所以将3化为二进制，在左边补“0”补足七位，得到0000011，各位取反，得到1111100，放入第29到第23位。 最后表示0.2356为：0 0 1111100 11100010100000100100000 将一个内存存储的float二进制格式转化为十进制的步骤： （1）将第22位到第0位的二进制数写出来，在最左边补一位“1”，得到二十四位有效数字。将小数点点在最左边那个“1”的右边。 （2）取出第29到第23位所表示的值n。当30位是“0”时将n各位求反。当30位是“1”时将n增1。 （3）将小数点左移n位（当30位是“0”时）或右移n位（当30位是“1”时），得到一个二进制表示的实数。 （4）将这个二进制实数化为十进制，并根据第31位是“0”还是“1”加上正号或负号即可。 三、浮点类型减法运算1234567891011121314浮点加减运算过程比定点运算过程复杂。完成浮点加减运算的操作过程大体分为四步： (1) 0操作数的检查； 如果判断两个需要加减的浮点数有一个为0，即可得知运算结果而没有必要再进行有序的一些列操作。 (2) 比较阶码（指数位）大小并完成对阶； 两浮点数进行加减，首先要看两数的 指数位 是否相同，即小数点位置是否对齐。若两数 指数位 相同，表示小数点是对齐的，就可以进行尾数的加减运算。反之，若两数阶码不同，表示小数点位置没有对齐，此时必须使两数的阶码相同，这个过程叫做对阶 。 如何对 阶(假设两浮点数的指数位为 Ex 和 Ey )： 通过尾数的移位以改变 Ex 或 Ey ，使之相等。 由 于浮点表示的数多是规格化的，尾数左移会引起最高有位的丢失，造成很大误差；而尾数右移虽引起最低有效位的丢失，但造成的误差较小，因此，对阶操作规定使 尾数右移，尾数右移后使阶码作相应增加，&lt;br&gt; 其数值保持不变。很显然，一个增加后的阶码与另一个相等，所增加的阶码一定是小阶。因此在对阶时，总是使小阶向大阶看齐 ，即小阶的尾数向右移位 ( 相当于小数点左移 ) ，每右移一位，其阶码加 1 ，直到两数的阶码相等为止，右移的位数等于阶差 △ E 。 (3) 尾数（有效数位）进行加或减运算； (4) 结果规格化并进行舍入处理。 四、浮点类型标识的有效数字及数值范围1、Float：比特数为32，有效数字为6-7，数值范围为 -3.4E+38 和 3.4E+38 2、Double：比特数为64，有效数字为15-16，数值范围为-1.7E-308～1.7E+308 1234对于单精度浮点数（float）来说，有一位符号位，指数位共8位，尾数共23位。指数能够表示的指数范围为-128~127。尾数为23位。当尾数全1时再加上小数点前面的1，指数取到最大正数127（8位，正数最大127，负数最小-128）,浮点数取得正数的最大值。+1.111111111111111111111*2^127（1.后面23个1，由于尾数的范围1～2，其最高位总为1，故只需存取小数部分，所以小数为是23位1），约等于2*2^127=3.4*10^38。为3.4*10^38负数亦然。 Double的计算与此类似，double的符号位为63位，指数为62～52位，共11位。表示的范围为-1024～1023。尾数为51～0。表示的范围为+1.111111111111111111111*2^1023（1.后面52个1）为1.7*10^308。负数亦然。 五、BigDecimal替代123456789大多数情况下，使用double和float计算的结果是准确的，但是在一些精度要求很高的系统中或者已知的小数计算得到的结果会不准确，这种问题是非常严重的。 《Effective Java》中提到一个原则，那就是float和double只能用来作科学计算或者是工程计算，但在商业计算中我们要用java.math.BigDecimal，通过使用BigDecimal类可以解决上述问题，java的设计者给编程人员提供了一个很有用的类BigDecimal，他可以完善float和double类无法进行精确计算的缺憾。 使用BigDecimal，但一定要用BigDecimal(String)构造器，而千万不要用BigDecimal(double)来构造（也不能将float或double型转换成String再来使用BigDecimal(String)来构造，因为在将float或double转换成String时精度已丢失）。例如new BigDecimal(0.1)，它将返回一个BigDecimal，也即0.1000000000000000055511151231257827021181583404541015625，正确使用BigDecimal，程序就可以打印出我们所期望的结果0.9：Java代码 System.out.println(new BigDecimal(&quot;2.0&quot;).subtract(new BigDecimal(&quot;1.10&quot;)));// 0.9 另外，如果要比较两个浮点数的大小，要使用BigDecimal的compareTo方法。 六、BigDecimal如何解决精度问题123456BigDecimal的底层数据结构使什么？它是怎么保证精度的？这曾经是一道阿里巴巴的面试题，由于在工作中使用最多的是BigDecimal的加、减、乘、除的的方法，还真没想过它的实现原理（完全是拿来主义惹的祸），乍这么一问还真有点懵。BigDecimal保证精度的解决思路其实极其的简单朴素，还是用一句话来解释：十进制整数在转化成二进制数时不会有精度问题，那么把十进制小数扩大N倍让它在整数的维度上进行计算，并保留相应的精度信息。————————————————源码中 scale字段记录精度信息；intCompact字段记录放大的整数信息 参考浮点类型计算精度不准确原因及如何规避","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"精度","slug":"精度","permalink":"https://xmmarlowe.github.io/tags/%E7%B2%BE%E5%BA%A6/"}],"author":"Marlowe"},{"title":"获取class对象六种方法","slug":"Java/获取class对象六种方法","date":"2021-05-10T07:52:02.000Z","updated":"2021-05-10T14:43:22.913Z","comments":true,"path":"2021/05/10/Java/获取class对象六种方法/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/10/Java/%E8%8E%B7%E5%8F%96class%E5%AF%B9%E8%B1%A1%E5%85%AD%E7%A7%8D%E6%96%B9%E6%B3%95/","excerpt":"","text":"简述Class类是Java反射机制的入口，封装了一个类或接口的运行时信息，通过调用Class类的方法可以获取这些信息。Class类有如下特点：1、该类在java.lang包中，不需要引包2、该类被final修饰，不可被继承3、该类实现了Serializable接口4、该类的构造方法被private修饰，不能通过关键字new创建该类的对象 获取对应class类对象1、（建议）通过Class类静态forName(“类包名.类名”)2、类名.class获取Class类实例3、如果已创建了引用类型的对象，则可以通过调用对象中的getClass()方法获取Class类实例4、基本数据类型，可以通过包装类.TYPE/class获取Class类实例5、如果是数组，可以通过数组元素的类型[].class获取Class类实例6、可以通过调用子类Class实例的getSuperClass()方法获取其父类的Class类实例 1234567891011121314151617181920212223242526272829public class Test&#123; public static void main(String[] args) &#123; Class clazz = null; try &#123; //a.Class.forName(&quot;包.类&quot;) clazz = Class.forName(&quot;Student&quot;); System.out.println(clazz.getName()); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; //2.类名.class clazz = Student.class; System.out.println(clazz.getName()); //3.对象名.class clazz = new Student().getClass(); System.out.println(clazz.getName()); //4.基本数据类型对应的class对象：包装类.TYPE clazz = Integer.TYPE; System.out.println(clazz.getName()); clazz = Integer.class; System.out.println(clazz.getName()); //5.数组类型对应class：元素类型[].class clazz = String[].class; System.out.println(clazz.getName()); //6.某个类父类所对应的class对象 clazz = Student.class.getSuperclass(); System.out.println(clazz.getName()); &#125;&#125;","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"对象","slug":"对象","permalink":"https://xmmarlowe.github.io/tags/%E5%AF%B9%E8%B1%A1/"}],"author":"Marlowe"},{"title":"LockSupport简单用法及原理","slug":"并发/LockSupport简单用法及原理","date":"2021-05-10T03:20:48.000Z","updated":"2021-05-10T14:43:22.984Z","comments":true,"path":"2021/05/10/并发/LockSupport简单用法及原理/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/10/%E5%B9%B6%E5%8F%91/LockSupport%E7%AE%80%E5%8D%95%E7%94%A8%E6%B3%95%E5%8F%8A%E5%8E%9F%E7%90%86/","excerpt":"","text":"简介LockSupport是一个线程阻塞工具类，所有的方法都是静态方法，可以让线程在任意位置阻塞，当然阻塞之后肯定得有唤醒的方法。 作用接下面我来看看LockSupport有哪些常用的方法。主要有两类方法：park和unpark。 12345678public static void park(Object blocker); // 暂停当前线程public static void parkNanos(Object blocker, long nanos); // 暂停当前线程，不过有超时时间的限制public static void parkUntil(Object blocker, long deadline); // 暂停当前线程，直到某个时间public static void park(); // 无期限暂停当前线程public static void parkNanos(long nanos); // 暂停当前线程，不过有超时时间的限制public static void parkUntil(long deadline); // 暂停当前线程，直到某个时间public static void unpark(Thread thread); // 恢复当前线程public static Object getBlocker(Thread t); 为什么叫park呢，park英文意思为停车。我们如果把Thread看成一辆车的话，park就是让车停下，unpark就是让车启动然后跑起来。 我们写一个例子来看看这个工具类怎么用的。 123456789101112131415161718192021222324252627282930313233public class LockSupportDemo &#123; public static Object u = new Object(); static ChangeObjectThread t1 = new ChangeObjectThread(&quot;t1&quot;); static ChangeObjectThread t2 = new ChangeObjectThread(&quot;t2&quot;); public static class ChangeObjectThread extends Thread &#123; public ChangeObjectThread(String name) &#123; super(name); &#125; @Override public void run() &#123; synchronized (u) &#123; System.out.println(&quot;in &quot; + getName()); LockSupport.park(); if (Thread.currentThread().isInterrupted()) &#123; System.out.println(&quot;被中断了&quot;); &#125; System.out.println(&quot;继续执行&quot;); &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; t1.start(); Thread.sleep(1000L); t2.start(); Thread.sleep(3000L); t1.interrupt(); LockSupport.unpark(t2); t1.join(); t2.join(); &#125;&#125; 运行的结果如下： 12345in t1被中断了继续执行in t2继续执行 park和unpark 与 wait和notify的区别这儿park和unpark其实实现了wait和notify的功能，不过还是有一些差别的。 park不需要获取某个对象的锁 因为中断的时候park不会抛出InterruptedException异常，所以需要在park之后自行判断中断状态，然后做额外的处理 我们再来看看Object blocker，这是个什么东西呢？这其实就是方便在线程dump的时候看到具体的阻塞对象的信息。 1234567&quot;t1&quot; #10 prio=5 os_prio=31 tid=0x00007f95030cc800 nid=0x4e03 waiting on condition [0x00007000011c9000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:304) // `下面的这个信息` at com.wtuoblist.beyond.concurrent.demo.chapter3.LockSupportDemo$ChangeObjectThread.run(LockSupportDemo.java:23) // - locked &lt;0x0000000795830950&gt; (a java.lang.Object) 还有一个地方需要注意，相对于线程的stop和resume，park和unpark的先后顺序并不是那么严格。stop和resume如果顺序反了，会出现死锁现象。而park和unpark却不会。这又是为什么呢？还是看一个例子 12345678910111213141516171819202122232425262728293031323334public class LockSupportDemo &#123; public static Object u = new Object(); static ChangeObjectThread t1 = new ChangeObjectThread(&quot;t1&quot;); public static class ChangeObjectThread extends Thread &#123; public ChangeObjectThread(String name) &#123; super(name); &#125; @Override public void run() &#123; synchronized (u) &#123; System.out.println(&quot;in &quot; + getName()); try &#123; Thread.sleep(1000L); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; LockSupport.park(); if (Thread.currentThread().isInterrupted()) &#123; System.out.println(&quot;被中断了&quot;); &#125; System.out.println(&quot;继续执行&quot;); &#125; &#125; &#125; public static void main(String[] args) &#123; t1.start(); LockSupport.unpark(t1); System.out.println(&quot;unpark invoked&quot;); &#125;&#125; t1内部有休眠1s的操作，所以unpark肯定先于park的调用，但是t1最终仍然可以完结。这是因为park和unpark会对每个线程维持一个许可（boolean值） unpark调用时，如果当前线程还未进入park，则许可为true park调用时，判断许可是否为true，如果是true，则继续往下执行；如果是false，则等待，直到许可为true JDK文档描述 park描述 unpark描述 总结 park和unpark可以实现类似wait和notify的功能，但是并不和wait和notify交叉，也就是说unpark不会对wait起作用，notify也不会对park起作用。 park和unpark的使用不会出现死锁的情况 blocker的作用是在dump线程的时候看到阻塞对象的信息 参考LockSupport的用法及原理","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"LockSupport","slug":"LockSupport","permalink":"https://xmmarlowe.github.io/tags/LockSupport/"}],"author":"Marlowe"},{"title":"Java9-Java14新特性","slug":"Java/Java9-Java14新特性","date":"2021-05-10T03:17:03.000Z","updated":"2021-05-16T13:58:32.416Z","comments":true,"path":"2021/05/10/Java/Java9-Java14新特性/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/10/Java/Java9-Java14%E6%96%B0%E7%89%B9%E6%80%A7/","excerpt":"待完善…","text":"待完善… Java 9Java 10Java 11Java 12Java 13Java 14","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"JDK","slug":"JDK","permalink":"https://xmmarlowe.github.io/tags/JDK/"},{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"}],"author":"Marlowe"},{"title":"InnoDB引擎 - 行记录存储 - Compact 行格式","slug":"数据库/InnoDB引擎-行记录存储-Compact-行格式","date":"2021-05-09T08:28:43.000Z","updated":"2021-05-10T14:43:22.991Z","comments":true,"path":"2021/05/09/数据库/InnoDB引擎-行记录存储-Compact-行格式/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/09/%E6%95%B0%E6%8D%AE%E5%BA%93/InnoDB%E5%BC%95%E6%93%8E-%E8%A1%8C%E8%AE%B0%E5%BD%95%E5%AD%98%E5%82%A8-Compact-%E8%A1%8C%E6%A0%BC%E5%BC%8F/","excerpt":"待完善…","text":"待完善… 参考MySQL原理 - InnoDB引擎 - 行记录存储 - Compact 行格式","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"InnoDB","slug":"InnoDB","permalink":"https://xmmarlowe.github.io/tags/InnoDB/"},{"name":"Compact","slug":"Compact","permalink":"https://xmmarlowe.github.io/tags/Compact/"}],"author":"Marlowe"},{"title":"局部性原理和工作原理","slug":"操作系统/局部性原理和工作原理","date":"2021-05-09T03:10:59.000Z","updated":"2021-05-10T14:43:22.916Z","comments":true,"path":"2021/05/09/操作系统/局部性原理和工作原理/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/09/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%B1%80%E9%83%A8%E6%80%A7%E5%8E%9F%E7%90%86%E5%92%8C%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/","excerpt":"","text":"程序访问的局部性原理程序访问的局部性原理包括时间局部性和空间局部性。 空间局部性：在最近的未来要用到的信息（指令和数据），很可能与现在正在使用的信息在存储空间上是邻近的 时间局部性：在最近的未来要用到的信息，很可能是现在正在使用的信息高速缓冲技术是利用程序访问的局部性原理，把程序中正在使用的部分存放在一个高速的、容量较小的Cache中，使CPU的访存操作大多数针对Cache进行，从而大大提高程序的执行速度。 Cache的基本工作原理 Cache位于存储器层次结构的顶层，通常由SRAM构成。 Cache和主存都被分成若干大小相等的块（Cache块又称为Cache行），每块由若干字节组成，块的长度称为块长（Cache行长）。所以Cache中的块数要远少于主存中的块数，它仅保存主存中最活跃的若干块的副本。 CPU与Cache之间的数据交换以字为单位，而Cache与主存之间的数据交换则以Cahce块为单位。 当CPU发出读请求时，若访存地址在Cache中命中，就将此地址转换成Cache地址，直接对Cahce进行读操作，与主存无关；若访存地址在Cache中未命中，则需访问主存，并把此字所在的块一次性地从主存调入Cache，若此时Cache已满，则需根据某种 替换算法，用这个块替换Cache中原来的某块信息。 当CPU发出写请求时，若Cache命中，有可能会遇到Cache与主存中的内容不一致的问题，此时需要根据某种 写策略 解决这个问题。 Cahce的性能指标与Cahce有关的性能指标主要有：命中率，缺失率和平均访问时间。 1. 命中率H CPU欲访问的信息已在Cache中的比率 2. 缺失率M CPU欲访问的信息不在Cache中的比率 M=1−H 3. 平均访问时间 Ta 参考Cache —— 局部性原理和工作原理","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"cache","slug":"cache","permalink":"https://xmmarlowe.github.io/tags/cache/"}],"author":"Marlowe"},{"title":"ThreadLocal专题学习","slug":"并发/ThreadLocal专题学习","date":"2021-05-09T03:09:04.000Z","updated":"2021-05-23T04:41:23.851Z","comments":true,"path":"2021/05/09/并发/ThreadLocal专题学习/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/09/%E5%B9%B6%E5%8F%91/ThreadLocal%E4%B8%93%E9%A2%98%E5%AD%A6%E4%B9%A0/","excerpt":"通过ThreadLocal源码和相关问题专题学习…","text":"通过ThreadLocal源码和相关问题专题学习… ThreadLocal代码演示我们先看下ThreadLocal使用示例： 1234567891011121314151617181920212223public class ThreadLocalTest &#123; private List&lt;String&gt; messages = Lists.newArrayList(); public static final ThreadLocal&lt;ThreadLocalTest&gt; holder = ThreadLocal.withInitial(ThreadLocalTest::new); public static void add(String message) &#123; holder.get().messages.add(message); &#125; public static List&lt;String&gt; clear() &#123; List&lt;String&gt; messages = holder.get().messages; holder.remove(); System.out.println(&quot;size: &quot; + holder.get().messages.size()); return messages; &#125; public static void main(String[] args) &#123; ThreadLocalTest.add(&quot;一枝花算不算浪漫&quot;); System.out.println(holder.get().messages); ThreadLocalTest.clear(); &#125;&#125; 打印结果： 12[一枝花算不算浪漫]size: 0 ThreadLocal对象可以提供线程局部变量，每个线程Thread拥有一份自己的副本变量，多个线程互不干扰。 ThreadLocal的数据结构 Thread 类有一个类型为 ThreadLocal.ThreadLocalMap的实例变量threadLocals，也就是说每个线程有一个自己的ThreadLocalMap。 ThreadLocalMap有自己的独立实现，可以简单地将它的key视作ThreadLocal，value为代码中放入的值（实际上key并不是ThreadLocal本身，而是它的一个弱引用）。 每个线程在往ThreadLocal里放值的时候，都会往自己的ThreadLocalMap里存，读也是以ThreadLocal作为引用，在自己的map里找对应的key，从而实现了线程隔离。 ThreadLocalMap有点类似HashMap的结构，只是HashMap是由数组+链表实现的，而ThreadLocalMap中并没有链表结构。 我们还要注意Entry， 它的key是ThreadLocal&lt;?&gt; k ，继承自WeakReference， 也就是我们常说的弱引用类型。 GC 之后key是否为null？回应开头的那个问题， ThreadLocal 的key是弱引用，那么在ThreadLocal.get()的时候,发生GC之后，key是否是null？ 为了搞清楚这个问题，我们需要搞清楚Java的四种引用类型： 强引用： 我们常常new出来的对象就是强引用类型，只要强引用存在，垃圾回收器将永远不会回收被引用的对象，哪怕内存不足的时候 软引用： 使用SoftReference修饰的对象被称为软引用，软引用指向的对象在内存要溢出的时候被回收 弱引用： 使用WeakReference修饰的对象被称为弱引用，只要发生垃圾回收，若这个对象只被弱引用指向，那么就会被回收 虚引用： 虚引用是最弱的引用，在 Java 中使用 PhantomReference 进行定义。虚引用中唯一的作用就是用队列接收对象即将死亡的通知 接着再来看下代码，我们使用反射的方式来看看GC后ThreadLocal中的数据情况： 123456789101112131415161718192021222324252627282930313233343536373839404142public class ThreadLocalDemo &#123; public static void main(String[] args) throws NoSuchFieldException, IllegalAccessException, InterruptedException &#123; Thread t = new Thread(()-&gt;test(&quot;abc&quot;,false)); t.start(); t.join(); System.out.println(&quot;--gc后--&quot;); Thread t2 = new Thread(() -&gt; test(&quot;def&quot;, true)); t2.start(); t2.join(); &#125; private static void test(String s,boolean isGC) &#123; try &#123; new ThreadLocal&lt;&gt;().set(s); if (isGC) &#123; System.gc(); &#125; Thread t = Thread.currentThread(); Class&lt;? extends Thread&gt; clz = t.getClass(); Field field = clz.getDeclaredField(&quot;threadLocals&quot;); field.setAccessible(true); Object ThreadLocalMap = field.get(t); Class&lt;?&gt; tlmClass = ThreadLocalMap.getClass(); Field tableField = tlmClass.getDeclaredField(&quot;table&quot;); tableField.setAccessible(true); Object[] arr = (Object[]) tableField.get(ThreadLocalMap); for (Object o : arr) &#123; if (o != null) &#123; Class&lt;?&gt; entryClass = o.getClass(); Field valueField = entryClass.getDeclaredField(&quot;value&quot;); Field referenceField = entryClass.getSuperclass().getSuperclass().getDeclaredField(&quot;referent&quot;); valueField.setAccessible(true); referenceField.setAccessible(true); System.out.println(String.format(&quot;弱引用key:%s,值:%s&quot;, referenceField.get(o), valueField.get(o))); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 结果如下： 1234弱引用key:java.lang.ThreadLocal@433619b6,值:abc弱引用key:java.lang.ThreadLocal@418a15e3,值:java.lang.ref.SoftReference@bf97a12--gc后--弱引用key:null,值:def 如图所示，因为这里创建的ThreadLocal并没有指向任何值，也就是没有任何引用： 1new ThreadLocal&lt;&gt;().set(s); 所以这里在GC之后，key就会被回收，我们看到上面debug中的referent=null, 如果改动一下代码： 这个问题刚开始看，如果没有过多思考，弱引用，还有垃圾回收，那么肯定会觉得是null。 其实是不对的，因为题目说的是在做 ThreadLocal.get() 操作，证明其实还是有强引用存在的，所以 key 并不为 null，如下图所示，ThreadLocal的强引用仍然是存在的。 如果我们的强引用不存在的话，那么 key 就会被回收，也就是会出现我们 value 没被回收，key 被回收，导致 value 永远存在，出现内存泄漏。 ThreadLocal.set()方法源码详解 ThreadLocal中的set方法原理如上图所示，很简单，主要是判断ThreadLocalMap是否存在，然后使用ThreadLocal中的set方法进行数据处理。 代码如下： 123456789101112public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125;void createMap(Thread t, T firstValue) &#123; t.threadLocals = new `ThreadLocalMap`(this, firstValue);&#125; 主要的核心逻辑还是在ThreadLocalMap中的，一步步往下看，后面还有更详细的剖析。 ThreadLocalMap Hash算法既然是Map结构，那么ThreadLocalMap当然也要实现自己的hash算法来解决散列表数组冲突问题。 1int i = key.threadLocalHashCode &amp; (len-1); ThreadLocalMap中hash算法很简单，这里i就是当前key在散列表中对应的数组下标位置。 这里最关键的就是threadLocalHashCode值的计算，ThreadLocal中有一个属性为HASH_INCREMENT = 0x61c88647 12345678910111213141516171819202122public class ThreadLocal&lt;T&gt; &#123; private final int threadLocalHashCode = nextHashCode(); private static AtomicInteger nextHashCode = new AtomicInteger(); private static final int HASH_INCREMENT = 0x61c88647; private static int nextHashCode() &#123; return nextHashCode.getAndAdd(HASH_INCREMENT); &#125; static class ThreadLocalMap &#123; ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123; table = new Entry[INITIAL_CAPACITY]; int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); table[i] = new Entry(firstKey, firstValue); size = 1; setThreshold(INITIAL_CAPACITY); &#125; &#125;&#125; 每当创建一个ThreadLocal对象，这个ThreadLocal.nextHashCode 这个值就会增长 0x61c88647 。 这个值很特殊，它是斐波那契数也叫 黄金分割数。hash增量为这个数字，带来的好处就是 hash 分布非常均匀。 我们自己可以尝试下： 可以看到产生的哈希码分布很均匀，这里不去细纠斐波那契具体算法，感兴趣的可以自行查阅相关资料。 ThreadLocalMap Hash冲突 注明： 下面所有示例图中，绿色块Entry代表正常数据，灰色块代表Entry的key值为null，已被垃圾回收。白色块表示Entry为null。 虽然ThreadLocalMap中使用了黄金分割数来作为hash计算因子，大大减少了Hash冲突的概率，但是仍然会存在冲突。 HashMap中解决冲突的方法是在数组上构造一个链表结构，冲突的数据挂载到链表上，如果链表长度超过一定数量则会转化成红黑树。 而ThreadLocalMap中并没有链表结构，所以这里不能适用HashMap解决冲突的方式了。 如上图所示，如果我们插入一个value=27的数据，通过hash计算后应该落入第4个槽位中，而槽位4已经有了Entry数据。 此时就会线性向后查找，一直找到Entry为null的槽位才会停止查找，将当前元素放入此槽位中。当然迭代过程中还有其他的情况，比如遇到了Entry不为null且key值相等的情况，还有Entry中的key值为null的情况等等都会有不同的处理，后面会一一详细讲解。 这里还画了一个Entry中的key为null的数据（Entry=2的灰色块数据），因为key值是弱引用类型，所以会有这种数据存在。在set过程中，如果遇到了key过期的Entry数据，实际上是会进行一轮探测式清理操作的，具体操作方式后面会讲到。 ThreadLocalMap.set()详解ThreadLocalMap.set()原理图解看完了ThreadLocal hash算法后，我们再来看set是如何实现的。 往ThreadLocalMap中set数据（新增或者更新数据）分为好几种情况，针对不同的情况我们画图来说说明。 第一种情况： 通过hash计算后的槽位对应的Entry数据为空： 这里直接将数据放到该槽位即可。 第二种情况： 槽位数据不为空，key值与当前ThreadLocal通过hash计算获取的key值一致： 这里直接更新该槽位的数据。 第三种情况： 槽位数据不为空，往后遍历过程中，在找到Entry为null的槽位之前，没有遇到key过期的Entry： 遍历散列数组，线性往后查找，如果找到Entry为null的槽位，则将数据放入该槽位中，或者往后遍历过程中，遇到了key值相等的数据，直接更新即可。 第四种情况： 槽位数据不为空，往后遍历过程中，在找到Entry为null的槽位之前，遇到key过期的Entry，如下图，往后遍历过程中，一到了index=7的槽位数据Entry的key=null： 散列数组下标为7位置对应的Entry数据key为null，表明此数据key值已经被垃圾回收掉了，此时就会执行replaceStaleEntry()方法，该方法含义是替换过期数据的逻辑，以index=7位起点开始遍历，进行探测式数据清理工作。 初始化探测式清理过期数据扫描的开始位置：slotToExpunge = staleSlot = 7 以当前staleSlot开始 向前迭代查找，找其他过期的数据，然后更新过期数据起始扫描下标slotToExpunge。for循环迭代，直到碰到Entry为null结束。 如果找到了过期的数据，继续向前迭代，直到遇到Entry=null的槽位才停止迭代，如下图所示，slotToExpunge被更新为0： 以当前节点(index=7)向前迭代，检测是否有过期的Entry数据，如果有则更新slotToExpunge值。碰到null则结束探测。以上图为例slotToExpunge被更新为0。 上面向前迭代的操作是为了更新探测清理过期数据的起始下标slotToExpunge的值，这个值在后面会讲解，它是用来判断当前过期槽位staleSlot之前是否还有过期元素。 接着开始以staleSlot位置(index=7)向后迭代，如果找到了相同key值的Entry数据： 从当前节点staleSlot向后查找key值相等的Entry元素，找到后更新Entry的值并交换staleSlot元素的位置(staleSlot位置为过期元素)，更新Entry数据，然后开始进行过期Entry的清理工作，如下图所示： 向后遍历过程中，如果没有找到相同key值的Entry数据： 从当前节点staleSlot向后查找key值相等的Entry元素，直到Entry为null则停止寻找。通过上图可知，此时table中没有key值相同的Entry。 创建新的Entry，替换table[stableSlot]位置： 替换完成后也是进行过期元素清理工作，清理工作主要是有两个方法：expungeStaleEntry()和cleanSomeSlots()，具体细节后面会讲到，请继续往后看。 ThreadLocalMap.set()源码详解上面已经用图的方式解析了set()实现的原理，其实已经很清晰了，我们接着再看下源码： java.lang.ThreadLocal.ThreadLocalMap.set(): 1234567891011121314151617181920212223242526private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; `ThreadLocal`&lt;?&gt; k = e.get(); if (k == key) &#123; e.value = value; return; &#125; if (k == null) &#123; replaceStaleEntry(key, value, i); return; &#125; &#125; tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash();&#125; 这里会通过key来计算在散列表中的对应位置，然后以当前key对应的桶的位置向后查找，找到可以使用的桶。 123Entry[] tab = table;int len = tab.length;int i = key.threadLocalHashCode &amp; (len-1); 什么情况下桶才是可以使用的呢？ k = key 说明是替换操作，可以使用 碰到一个过期的桶，执行替换逻辑，占用过期桶 查找过程中，碰到桶中Entry=null的情况，直接使用 接着就是执行for循环遍历，向后查找，我们先看下nextIndex()、prevIndex()方法实现： 1234567private static int nextIndex(int i, int len) &#123; return ((i + 1 &lt; len) ? i + 1 : 0);&#125;private static int prevIndex(int i, int len) &#123; return ((i - 1 &gt;= 0) ? i - 1 : len - 1);&#125; 接着看剩下for循环中的逻辑： 遍历当前key值对应的桶中Entry数据为空，这说明散列数组这里没有数据冲突，跳出for循环，直接set数据到对应的桶中 如果key值对应的桶中Entry数据不为空 1 如果k = key，说明当前set操作是一个替换操作，做替换逻辑，直接返回 2 如果key = null，说明当前桶位置的Entry是过期数据，执行replaceStaleEntry()方法(核心方法)，然后返回 for循环执行完毕，继续往下执行说明向后迭代的过程中遇到了entry为null的情况 1 在Entry为null的桶中创建一个新的Entry对象 2 执行++size操作 调用cleanSomeSlots()做一次启发式清理工作，清理散列数组中Entry的key过期的数据 1 如果清理工作完成后，未清理到任何数据，且size超过了阈值(数组长度的2/3)，进行rehash()操作 2 rehash()中会先进行一轮探测式清理，清理过期key，清理完成后如果size &gt;= threshold - threshold / 4，就会执行真正的扩容逻辑(扩容逻辑往后看) 接着重点看下replaceStaleEntry()方法，replaceStaleEntry()方法提供替换过期数据的功能，我们可以对应上面第四种情况的原理图来再回顾下，具体代码如下： java.lang.ThreadLocal.ThreadLocalMap.replaceStaleEntry(): 123456789101112131415161718192021222324252627282930313233343536373839404142private void replaceStaleEntry(ThreadLocal&lt;?&gt; key, Object value, int staleSlot) &#123; Entry[] tab = table; int len = tab.length; Entry e; int slotToExpunge = staleSlot; for (int i = prevIndex(staleSlot, len); (e = tab[i]) != null; i = prevIndex(i, len)) if (e.get() == null) slotToExpunge = i; for (int i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) &#123; `ThreadLocal`&lt;?&gt; k = e.get(); if (k == key) &#123; e.value = value; tab[i] = tab[staleSlot]; tab[staleSlot] = e; if (slotToExpunge == staleSlot) slotToExpunge = i; cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); return; &#125; if (k == null &amp;&amp; slotToExpunge == staleSlot) slotToExpunge = i; &#125; tab[staleSlot].value = null; tab[staleSlot] = new Entry(key, value); if (slotToExpunge != staleSlot) cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);&#125; slotToExpunge表示开始探测式清理过期数据的开始下标，默认从当前的staleSlot开始。以当前的staleSlot开始，向前迭代查找，找到没有过期的数据，for循环一直碰到Entry为null才会结束。如果向前找到了过期数据，更新探测清理过期数据的开始下标为i，即slotToExpunge=i 12345678for (int i = prevIndex(staleSlot, len); (e = tab[i]) != null; i = prevIndex(i, len))&#123; if (e.get() == null)&#123; slotToExpunge = i; &#125;&#125; 接着开始从staleSlot向后查找，也是碰到Entry为null的桶结束。 如果迭代过程中，碰到k == key，这说明这里是替换逻辑，替换新数据并且交换当前staleSlot位置。如果slotToExpunge == staleSlot，这说明replaceStaleEntry()一开始向前查找过期数据时并未找到过期的Entry数据，接着向后查找过程中也未发现过期数据，修改开始探测式清理过期数据的下标为当前循环的index，即slotToExpunge = i。最后调用cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);进行启发式过期数据清理。 123456789101112if (k == key) &#123; e.value = value; tab[i] = tab[staleSlot]; tab[staleSlot] = e; if (slotToExpunge == staleSlot) slotToExpunge = i; cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); return;&#125; cleanSomeSlots()和expungeStaleEntry()方法后面都会细讲，这两个是和清理相关的方法，一个是过期key相关Entry的启发式清理(Heuristically scan)，另一个是过期key相关Entry的探测式清理。 如果k != key则会接着往下走，k == null说明当前遍历的Entry是一个过期数据，slotToExpunge == staleSlot说明，一开始的向前查找数据并未找到过期的Entry。如果条件成立，则更新slotToExpunge 为当前位置，这个前提是前驱节点扫描时未发现过期数据。 12if (k == null &amp;&amp; slotToExpunge == staleSlot) slotToExpunge = i; 往后迭代的过程中如果没有找到k == key的数据，且碰到Entry为null的数据，则结束当前的迭代操作。此时说明这里是一个添加的逻辑，将新的数据添加到table[staleSlot] 对应的slot中。 12tab[staleSlot].value = null;tab[staleSlot] = new Entry(key, value); 最后判断除了staleSlot以外，还发现了其他过期的slot数据，就要开启清理数据的逻辑： 12if (slotToExpunge != staleSlot) cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); ThreadLocalMap过期key的探测式清理流程上面我们有提及ThreadLocalMap的两种过期key数据清理方式：探测式清理和启发式清理。 我们先讲下探测式清理，也就是expungeStaleEntry方法，遍历散列数组，从开始位置向后探测清理过期数据，将过期数据的Entry设置为null，沿途中碰到未过期的数据则将此数据rehash后重新在table数组中定位，如果定位的位置已经有了数据，则会将未过期的数据放到最靠近此位置的Entry=null的桶中，使rehash后的Entry数据距离正确的桶的位置更近一些。操作逻辑如下： 如上图，set(27) 经过hash计算后应该落到index=4的桶中，由于index=4桶已经有了数据，所以往后迭代最终数据放入到index=7的桶中，放入后一段时间后index=5中的Entry数据key变为了null 如果再有其他数据set到map中，就会触发探测式清理操作。 如上图，执行探测式清理后，index=5的数据被清理掉，继续往后迭代，到index=7的元素时，经过rehash后发现该元素正确的index=4，而此位置已经已经有了数据，往后查找离index=4最近的Entry=null的节点(刚被探测式清理掉的数据：index=5)，找到后移动index= 7的数据到index=5中，此时桶的位置离正确的位置index=4更近了。 经过一轮探测式清理后，key过期的数据会被清理掉，没过期的数据经过rehash重定位后所处的桶位置理论上更接近i= key.hashCode &amp; (tab.len - 1)的位置。这种优化会提高整个散列表查询性能。 接着看下expungeStaleEntry()具体流程，我们还是以先原理图后源码讲解的方式来一步步梳理： 我们假设expungeStaleEntry(3) 来调用此方法，如上图所示，我们可以看到ThreadLocalMap中table的数据情况，接着执行清理操作： 第一步是清空当前staleSlot位置的数据，index=3位置的Entry变成了null。然后接着往后探测： 执行完第二步后，index=4的元素挪到index=3的槽位中。 继续往后迭代检查，碰到正常数据，计算该数据位置是否偏移，如果被偏移，则重新计算slot位置，目的是让正常数据尽可能存放在正确位置或离正确位置更近的位置 在往后迭代的过程中碰到空的槽位，终止探测，这样一轮探测式清理工作就完成了，接着我们继续看看具体实现源代码： 12345678910111213141516171819202122232425262728293031private int expungeStaleEntry(int staleSlot) &#123; Entry[] tab = table; int len = tab.length; tab[staleSlot].value = null; tab[staleSlot] = null; size--; Entry e; int i; for (i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) &#123; `ThreadLocal`&lt;?&gt; k = e.get(); if (k == null) &#123; e.value = null; tab[i] = null; size--; &#125; else &#123; int h = k.threadLocalHashCode &amp; (len - 1); if (h != i) &#123; tab[i] = null; while (tab[h] != null) h = nextIndex(h, len); tab[h] = e; &#125; &#125; &#125; return i;&#125; 这里我们还是以staleSlot=3来做示例说明，首先是将tab[staleSlot]槽位的数据清空，然后设置size-- 接着以staleSlot位置往后迭代，如果遇到k==null的过期数据，也是清空该槽位数据，然后size-- 1234567ThreadLocal&lt;?&gt; k = e.get();if (k == null) &#123; e.value = null; tab[i] = null; size--;&#125; 如果key没有过期，重新计算当前key的下标位置是不是当前槽位下标位置，如果不是，那么说明产生了hash冲突，此时以新计算出来正确的槽位位置往后迭代，找到最近一个可以存放entry的位置。 123456789int h = k.threadLocalHashCode &amp; (len - 1);if (h != i) &#123; tab[i] = null; while (tab[h] != null) h = nextIndex(h, len); tab[h] = e;&#125; 这里是处理正常的产生Hash冲突的数据，经过迭代后，有过Hash冲突数据的Entry位置会更靠近正确位置，这样的话，查询的时候效率才会更高。 ThreadLocalMap扩容机制在ThreadLocalMap.set()方法的最后，如果执行完启发式清理工作后，未清理到任何数据，且当前散列数组中Entry的数量已经达到了列表的扩容阈值(len*2/3)，就开始执行rehash()逻辑： 12if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash(); 接着看下rehash()具体实现： 12345678910111213141516private void rehash() &#123; expungeStaleEntries(); if (size &gt;= threshold - threshold / 4) resize();&#125;private void expungeStaleEntries() &#123; Entry[] tab = table; int len = tab.length; for (int j = 0; j &lt; len; j++) &#123; Entry e = tab[j]; if (e != null &amp;&amp; e.get() == null) expungeStaleEntry(j); &#125;&#125; 这里首先是会进行探测式清理工作，从table的起始位置往后清理，上面有分析清理的详细流程。清理完成之后，table中可能有一些key为null的Entry数据被清理掉，所以此时通过判断size &gt;= threshold - threshold / 4 也就是size &gt;= threshold* 3/4 来决定是否扩容。 我们还记得上面进行rehash()的阈值是size &gt;= threshold，所以当面试官套路我们ThreadLocalMap扩容机制的时候 我们一定要说清楚这两个步骤： 接着看看具体的resize()方法，为了方便演示，我们以oldTab.len=8来举例： 扩容后的tab的大小为oldLen * 2，然后遍历老的散列表，重新计算hash位置，然后放到新的tab数组中，如果出现hash冲突则往后寻找最近的entry为null的槽位，遍历完成之后，oldTab中所有的entry数据都已经放入到新的tab中了。重新计算tab下次扩容的阈值，具体代码如下： 123456789101112131415161718192021222324252627private void resize() &#123; Entry[] oldTab = table; int oldLen = oldTab.length; int newLen = oldLen * 2; Entry[] newTab = new Entry[newLen]; int count = 0; for (int j = 0; j &lt; oldLen; ++j) &#123; Entry e = oldTab[j]; if (e != null) &#123; `ThreadLocal`&lt;?&gt; k = e.get(); if (k == null) &#123; e.value = null; &#125; else &#123; int h = k.threadLocalHashCode &amp; (newLen - 1); while (newTab[h] != null) h = nextIndex(h, newLen); newTab[h] = e; count++; &#125; &#125; &#125; setThreshold(newLen); size = count; table = newTab;&#125; ThreadLocalMap.get()详解上面已经看完了set()方法的源码，其中包括set数据、清理数据、优化数据桶的位置等操作，接着看看get()操作的原理。 ThreadLocalMap.get()图解第一种情况： 通过查找key值计算出散列表中slot位置，然后该slot位置中的Entry.key和查找的key一致，则直接返回： 第二种情况： slot位置中的Entry.key和要查找的key不一致： 我们以get(ThreadLocal1)为例，通过hash计算后，正确的slot位置应该是4，而index=4的槽位已经有了数据，且key值不等于ThreadLocal1，所以需要继续往后迭代查找。 迭代到index=5的数据时，此时Entry.key=null，触发一次探测式数据回收操作，执行expungeStaleEntry()方法，执行完后，index 5,8的数据都会被回收，而index 6,7的数据都会前移，此时继续往后迭代，到index = 6的时候即找到了key值相等的Entry数据，如下图所示： ThreadLocalMap.get()源码详解java.lang.ThreadLocal.ThreadLocalMap.getEntry(): 12345678910111213141516171819202122232425private Entry getEntry(ThreadLocal&lt;?&gt; key) &#123; int i = key.threadLocalHashCode &amp; (table.length - 1); Entry e = table[i]; if (e != null &amp;&amp; e.get() == key) return e; else return getEntryAfterMiss(key, i, e);&#125;private Entry getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, Entry e) &#123; Entry[] tab = table; int len = tab.length; while (e != null) &#123; `ThreadLocal`&lt;?&gt; k = e.get(); if (k == key) return e; if (k == null) expungeStaleEntry(i); else i = nextIndex(i, len); e = tab[i]; &#125; return null;&#125; ThreadLocalMap过期key的启发式清理流程上面多次提及到ThreadLocalMap过期可以的两种清理方式：探测式清理(expungeStaleEntry())**、启发式清理(cleanSomeSlots())** 探测式清理是以当前Entry 往后清理，遇到值为null则结束清理，属于线性探测清理。 而启发式清理被作者定义为：Heuristically scan some cells looking for stale entries. 具体代码如下： 123456789101112131415private boolean cleanSomeSlots(int i, int n) &#123; boolean removed = false; Entry[] tab = table; int len = tab.length; do &#123; i = nextIndex(i, len); Entry e = tab[i]; if (e != null &amp;&amp; e.get() == null) &#123; n = len; removed = true; i = expungeStaleEntry(i); &#125; &#125; while ( (n &gt;&gt;&gt;= 1) != 0); return removed;&#125; InheritableThreadLocal我们使用ThreadLocal的时候，在异步场景下是无法给子线程共享父线程中创建的线程副本数据的。 为了解决这个问题，JDK中还有一个InheritableThreadLocal类，我们来看一个例子： 12345678910111213141516public class InheritableThreadLocalDemo &#123; public static void main(String[] args) &#123; ThreadLocal&lt;String&gt; ThreadLocal = new ThreadLocal&lt;&gt;(); ThreadLocal&lt;String&gt; inheritableThreadLocal = new InheritableThreadLocal&lt;&gt;(); ThreadLocal.set(&quot;父类数据:threadLocal&quot;); inheritableThreadLocal.set(&quot;父类数据:inheritableThreadLocal&quot;); new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;子线程获取父类`ThreadLocal`数据：&quot; + `ThreadLocal`.get()); System.out.println(&quot;子线程获取父类inheritableThreadLocal数据：&quot; + inheritableThreadLocal.get()); &#125; &#125;).start(); &#125;&#125; 打印结果： 12子线程获取父类`ThreadLocal`数据：null子线程获取父类inheritableThreadLocal数据：父类数据:inheritableThreadLocal 实现原理是子线程是通过在父线程中通过调用new Thread()方法来创建子线程，Thread#init方法在Thread的构造方法中被调用。在init方法中拷贝父线程数据到子线程中： 12345678910111213private void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc, boolean inheritThreadLocals) &#123; if (name == null) &#123; throw new NullPointerException(&quot;name cannot be null&quot;); &#125; if (inheritThreadLocals &amp;&amp; parent.inheritableThreadLocals != null) this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); this.stackSize = stackSize; tid = nextThreadID();&#125; 但InheritableThreadLocal仍然有缺陷，一般我们做异步化处理都是使用的线程池，而InheritableThreadLocal是在new Thread中的init()方法给赋值的，而线程池是线程复用的逻辑，所以这里会存在问题。 当然，有问题出现就会有解决问题的方案，阿里巴巴开源了一个TransmittableThreadLocal组件就可以解决这个问题，这里就不再延伸，感兴趣的可自行查阅资料。 参考ThreadLocal 关键字解析","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"ThreadLocal","slug":"ThreadLocal","permalink":"https://xmmarlowe.github.io/tags/ThreadLocal/"}],"author":"Marlowe"},{"title":"Java序列化与反序列化","slug":"Java/Java序列化与反序列化","date":"2021-05-09T02:36:43.000Z","updated":"2021-08-26T10:58:13.173Z","comments":true,"path":"2021/05/09/Java/Java序列化与反序列化/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/09/Java/Java%E5%BA%8F%E5%88%97%E5%8C%96%E4%B8%8E%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/","excerpt":"","text":"一些问题Java序列化，反序列化Java序列化指将Java对象转换为字节序列的过程，反序列化指将字节序列转换为目标对象的过程； 什么情况下需要序列化？当Java对象需要网络传输或者持久化到磁盘上时； 序列化的实现？让类实现Serializable接口，标注该类对象可以被序列化； Java 序列化中如果有些字段不想进行序列化，怎么办？对于不想进行序列化的变量，使用 transient 关键字修饰。 transient 关键字的作用是：阻止实例中那些用此关键字修饰的的变量序列化；当对象被反序列化时，被 transient 修饰的变量值不会被持久化和恢复。transient 只能修饰变量，不能修饰类和方法。 引言将 Java 对象序列化为二进制文件的 Java 序列化技术是 Java 系列技术中一个较为重要的技术点，在大部分情况下，开发人员只需要了解被序列化的类需要实现 Serializable 接口，使用 ObjectInputStream 和 ObjectOutputStream 进行对象的读写。然而在有些情况下，光知道这些还远远不够，文章列举了笔者遇到的一些真实情境，它们与 Java 序列化相关，通过分析情境出现的原因，使读者轻松牢记 Java 序列化中的一些高级认识。 1. 序列化 ID 问题情境： 两个客户端 A 和 B 试图通过网络传递对象数据，A 端将对象 C 序列化为二进制数据再传给 B，B 反序列化得到 C。 问题： C 对象的全类路径假设为 com.inout.Test，在 A 和 B 端都有这么一个类文件，功能代码完全一致。也都实现了 Serializable 接口，但是反序列化时总是提示不成功。 解决： 虚拟机是否允许反序列化，不仅取决于类路径和功能代码是否一致，一个非常重要的一点是两个类的序列化 ID 是否一致（就是 private static final long serialVersionUID = 1L） 。清单 1 中，虽然两个类的功能代码完全一致，但是序列化 ID 不同，他们无法相互序列化和反序列化。 清单 1. 相同功能代码不同序列化 ID 的类对比 1234567891011121314151617181920212223242526272829303132333435363738394041package com.inout; import java.io.Serializable; public class A implements Serializable &#123; private static final long serialVersionUID = 1L; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; &#125; package com.inout; import java.io.Serializable; public class A implements Serializable &#123; private static final long serialVersionUID = 2L; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; &#125; 序列化 ID 在 Eclipse 下提供了两种生成策略，一个是固定的 1L，一个是随机生成一个不重复的 long 类型数据（实际上是使用 JDK 工具生成），在这里有一个建议，如果没有特殊需求，就是用默认的 1L 就可以，这样可以确保代码一致时反序列化成功。那么随机生成的序列化 ID 有什么作用呢，有些时候，通过改变序列化 ID 可以用来限制某些用户的使用。 特性使用案例 读者应该听过 Façade 模式【提供一组统一的接口，使子系统更易用，可以解决：1.易用性（封装底层数据，对外暴露简单接口）；2.性能问题（需要访问3次的接口，用一个接口分装，解决交互性能问题）；3.解决分布式事务问题 （一次请求需要两个模块操作共同成功，共同失败，直接封装到一个接口解决问题）】，它是为应用程序提供统一的访问接口，案例程序中的 Client 客户端使用了该模式，案例程序结构图如图 1 所示。 案例程序结构 Client 端通过 Façade Object 才可以与业务逻辑对象进行交互。而客户端的 Façade Object 不能直接由 Client 生成，而是需要 Server 端生成，然后序列化后通过网络将二进制对象数据传给 Client，Client 负责反序列化得到 Façade 对象。该模式可以使得 Client 端程序的使用需要服务器端的许可，同时 Client 端和服务器端的 Façade Object 类需要保持一致。当服务器端想要进行版本更新时，只要将服务器端的 Façade Object 类的序列化 ID 再次生成，当 Client 端反序列化 Façade Object 就会失败，也就是强制 Client 端从服务器端获取最新程序。 2. 静态变量序列化情境 ：查看清单 2 的代码。 清单 2. 静态变量序列化问题代码 12345678910111213141516171819202122232425262728293031323334public class Test implements Serializable &#123; private static final long serialVersionUID = 1L; public static int staticVar = 5; public static void main(String[] args) &#123; try &#123; //初始时staticVar为5 ObjectOutputStream out = new ObjectOutputStream( new FileOutputStream(&quot;result.obj&quot;)); out.writeObject(new Test()); out.close(); //序列化后修改为10 Test.staticVar = 10; ObjectInputStream oin = new ObjectInputStream(new FileInputStream( &quot;result.obj&quot;)); Test t = (Test) oin.readObject(); oin.close(); //再读取，通过t.staticVar打印新的值 System.out.println(t.staticVar); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 清单 2 中的 main 方法，将对象序列化后，修改静态变量的数值，再将序列化对象读取出来，然后通过读取出来的对象获得静态变量的数值并打印出来。依照清单 2，这个 System.out.println(t.staticVar) 语句输出的是 10 还是 5 呢？ 最后的输出是 10，对于无法理解的读者认为，打印的 staticVar 是从读取的对象里获得的，应该是保存时的状态才对。之所以打印 10 的原因在于序列化时，并不保存静态变量，这其实比较容易理解，序列化保存的是对象的状态，静态变量属于类的状态，因此 序列化并不保存静态变量 。 3. 父类的序列化与 Transient 关键字情境： 一个子类实现了 Serializable 接口，它的父类都没有实现 Serializable 接口，序列化该子类对象，然后反序列化后输出父类定义的某变量的数值，该变量数值与序列化时的数值不同。 解决： 要想将父类对象也序列化，就需要让父类也实现 Serializable 接口 。如果父类不实现的话的，就 需要有默认的无参的构造函数 。在父类没有实现 Serializable 接口时，虚拟机是不会序列化父对象的，而一个 Java 对象的构造必须先有父对象，才有子对象，反序列化也不例外。所以反序列化时，为了构造父对象，只能调用父类的无参构造函数作为默认的父对象。因此当我们取父对象的变量值时，它的值是调用父类无参构造函数后的值。如果你考虑到这种序列化的情况，在父类无参构造函数中对变量进行初始化，否则的话，父类变量值都是默认声明的值，如 int 型的默认是 0，string 型的默认是 null。 Transient 关键字的作用是控制变量的序列化，在变量声明前加上该关键字，可以阻止该变量被序列化到文件中，在被反序列化后，transient 变量的值被设为初始值，如 int 型的是 0，对象型的是 null。 特性使用案例 我们熟悉使用 Transient 关键字可以使得字段不被序列化，那么还有别的方法吗？根据父类对象序列化的规则，我们可以将不需要被序列化的字段抽取出来放到父类中，子类实现 Serializable 接口，父类不实现，根据父类序列化规则，父类的字段数据将不被序列化，形成类图如图 2 所示。 案例程序类图 上图中可以看出，attr1、attr2、attr3、attr5 都不会被序列化，放在父类中的好处在于当有另外一个 Child 类时，attr1、attr2、attr3 依然不会被序列化，不用重复抒写 transient，代码简洁。 4. 对敏感字段加密情境： 服务器端给客户端发送序列化对象数据，对象中有一些数据是敏感的，比如密码字符串等，希望对该密码字段在序列化时，进行加密，而客户端如果拥有解密的密钥，只有在客户端进行反序列化时，才可以对密码进行读取，这样可以一定程度保证序列化对象的数据安全。 解决： 在序列化过程中，虚拟机会试图调用对象类里的 writeObject 和 readObject 方法，进行用户自定义的序列化和反序列化，如果没有这样的方法，则默认调用是 ObjectOutputStream 的 defaultWriteObject 方法以及 ObjectInputStream 的 defaultReadObject 方法。用户自定义的 writeObject 和 readObject 方法可以允许用户控制序列化的过程，比如可以在序列化的过程中动态改变序列化的数值。基于这个原理，可以在实际应用中得到使用，用于敏感字段的加密工作，清单 3 展示了这个过程。 清单 3. 静态变量序列化问题代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859private static final long serialVersionUID = 1L; private String password = &quot;pass&quot;; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125; private void writeObject(ObjectOutputStream out) &#123; try &#123; PutField putFields = out.putFields(); System.out.println(&quot;原密码:&quot; + password); password = &quot;encryption&quot;;//模拟加密 putFields.put(&quot;password&quot;, password); System.out.println(&quot;加密后的密码&quot; + password); out.writeFields(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; private void readObject(ObjectInputStream in) &#123; try &#123; GetField readFields = in.readFields(); Object object = readFields.get(&quot;password&quot;, &quot;&quot;); System.out.println(&quot;要解密的字符串:&quot; + object.toString()); password = &quot;pass&quot;;//模拟解密,需要获得本地的密钥 &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; try &#123; ObjectOutputStream out = new ObjectOutputStream( new FileOutputStream(&quot;result.obj&quot;)); out.writeObject(new Test()); out.close(); ObjectInputStream oin = new ObjectInputStream(new FileInputStream( &quot;result.obj&quot;)); Test t = (Test) oin.readObject(); System.out.println(&quot;解密后的字符串:&quot; + t.getPassword()); oin.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125; 在清单 3 的 writeObject 方法中，对密码进行了加密，在 readObject 中则对 password 进行解密，只有拥有密钥的客户端，才可以正确的解析出密码，确保了数据的安全。执行清单 3 后控制台输出如图 3 所示。 数据加密演示 特性使用案例 RMI 技术是完全基于 Java 序列化技术的，服务器端接口调用所需要的参数对象来至于客户端，它们通过网络相互传输。这就涉及 RMI 的安全传输的问题。一些敏感的字段，如用户名密码（用户登录时需要对密码进行传输），我们希望对其进行加密，这时，就可以采用本节介绍的方法在客户端对密码进行加密，服务器端进行解密，确保数据传输的安全性。 5. 序列化存储规则情境 ：问题代码如清单 4 所示。 清单 4. 存储规则问题代码 1234567891011121314151617181920ObjectOutputStream out = new ObjectOutputStream( new FileOutputStream(&quot;result.obj&quot;)); Test test = new Test(); //试图将对象两次写入文件 out.writeObject(test); out.flush(); System.out.println(new File(&quot;result.obj&quot;).length()); out.writeObject(test); out.close(); System.out.println(new File(&quot;result.obj&quot;).length()); ObjectInputStream oin = new ObjectInputStream(new FileInputStream( &quot;result.obj&quot;)); //从文件依次读出两个文件 Test t1 = (Test) oin.readObject(); Test t2 = (Test) oin.readObject(); oin.close(); //判断两个引用是否指向同一个对象 System.out.println(t1 == t2); 清单 4 中对同一对象两次写入文件，打印出写入一次对象后的存储大小和写入两次后的存储大小，然后从文件中反序列化出两个对象，比较这两个对象是否为同一对象。一般的思维是，两次写入对象，文件大小会变为两倍的大小，反序列化时，由于从文件读取，生成了两个对象，判断相等时应该是输入 false 才对，但是最后结果输出如图 4 所示。 示例程序输出 我们看到，第二次写入对象时文件只增加了 5 字节，并且两个对象是相等的，这是为什么呢？ 解答： Java 序列化机制为了节省磁盘空间，具有特定的存储规则，当写入文件的为同一对象时，并不会再将对象的内容进行存储，而只是再次存储一份引用，上面增加的 5 字节的存储空间就是新增引用和一些控制信息的空间。反序列化时，恢复引用关系，使得清单 3 中的 t1 和 t2 指向唯一的对象，二者相等，输出 true。该存储规则极大的节省了存储空间。 特性案例分析 查看清单 5 的代码。 清单 5. 案例代码 1234567891011121314ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream(&quot;result.obj&quot;));Test test = new Test();test.i = 1;out.writeObject(test);out.flush();test.i = 2;out.writeObject(test);out.close();ObjectInputStream oin = new ObjectInputStream(new FileInputStream( &quot;result.obj&quot;));Test t1 = (Test) oin.readObject();Test t2 = (Test) oin.readObject();System.out.println(t1.i);System.out.println(t2.i); 清单 5 的目的是希望将 test 对象两次保存到 result.obj 文件中，写入一次以后修改对象属性值再次保存第二次，然后从 result.obj 中再依次读出两个对象，输出这两个对象的 i 属性值。案例代码的目的原本是希望一次性传输对象修改前后的状态。 结果两个输出的都是 1， 原因就是第一次写入对象以后，第二次再试图写的时候，虚拟机根据引用关系知道已经有一个相同对象已经写入文件，因此只保存第二次写的引用，所以读取时，都是第一次保存的对象。读者在使用一个文件多次 writeObject 需要特别注意这个问题。 参考面试题 - Java序列化和反序列化","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"序列化","slug":"序列化","permalink":"https://xmmarlowe.github.io/tags/%E5%BA%8F%E5%88%97%E5%8C%96/"},{"name":"反序列化","slug":"反序列化","permalink":"https://xmmarlowe.github.io/tags/%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/"}],"author":"Marlowe"},{"title":"OOM有哪些情况，SOF有哪些情况","slug":"Java/OOM有哪些情况，SOF有哪些情况","date":"2021-05-09T02:12:27.000Z","updated":"2021-05-10T14:43:22.900Z","comments":true,"path":"2021/05/09/Java/OOM有哪些情况，SOF有哪些情况/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/09/Java/OOM%E6%9C%89%E5%93%AA%E4%BA%9B%E6%83%85%E5%86%B5%EF%BC%8CSOF%E6%9C%89%E5%93%AA%E4%BA%9B%E6%83%85%E5%86%B5/","excerpt":"","text":"OOM(OutOfMemory)1、全称为OutOfMemoryError异常，如果虚拟机在扩展栈时无法申请足够的内存空间，抛出它； 2、Java heap异常:java.lang.OutOfMemoryError:Java heap Space； 3、虚拟机栈和本地方法溢出； 4、运行时常量池溢出异常信息：java.lang.OutOfMemoryError:PermGen Space； 如果要向运行时常量池添加内容，简单的做法就是使用String.intern()这个Native方法； 5、方法区溢出，方法区用于存放class的相关信息，如类名，常量池，字段描述，方法描述等。 异常信息为：java.lang.OutofMemoryError:PermGen Space; 6、引起OOM主要有2个原因：内存泄露和内存溢出（即堆、栈溢出）； SOF(StackOverFlow)1、全称为StackOverFlowError异常； 2、如果线程请求的栈深度大于虚拟机所允许的深度，抛出该异常； 3、主要发生在递归调用中； 1. Java heap space当堆内存（Heap Space）没有足够空间存放新创建的对象时，就会抛出 java.lang.OutOfMemoryError:Javaheap space 错误（根据实际生产经验，可以对程序日志中的 OutOfMemoryError 配置关键字告警，一经发现，立即处理）。 原因分析Javaheap space 错误产生的常见原因可以分为以下几类： 1、请求创建一个超大对象，通常是一个大数组。 2、超出预期的访问量/数据量，通常是上游系统请求流量飙升，常见于各类促销/秒杀活动，可以结合业务流量指标排查是否有尖状峰值。 3、过度使用终结器（Finalizer），该对象没有立即被 GC。 4、内存泄漏（Memory Leak），大量对象引用没有释放，JVM 无法对其自动回收，常见于使用了 File 等资源没有回收。 解决方案针对大部分情况，通常只需要通过 -Xmx 参数调高 JVM 堆内存空间即可。如果仍然没有解决，可以参考以下情况做进一步处理： 1、如果是超大对象，可以检查其合理性，比如是否一次性查询了数据库全部结果，而没有做结果数限制。 2、如果是业务峰值压力，可以考虑添加机器资源，或者做限流降级。 3、如果是内存泄漏，需要找到持有的对象，修改代码设计，比如关闭没有释放的连接。 2. GC overhead limit exceeded当 Java 进程花费 98% 以上的时间执行 GC，但只恢复了不到 2% 的内存，且该动作连续重复了 5 次，就会抛出 java.lang.OutOfMemoryError:GC overhead limit exceeded 错误。简单地说，就是应用程序已经基本耗尽了所有可用内存， GC 也无法回收。 此类问题的原因与解决方案跟 Javaheap space 非常类似，可以参考上文。 3. Permgen space该错误表示永久代（Permanent Generation）已用满，通常是因为加载的 class 数目太多或体积太大。 原因分析永久代存储对象主要包括以下几类： 1、加载/缓存到内存中的 class 定义，包括类的名称，字段，方法和字节码； 2、常量池； 3、对象数组/类型数组所关联的 class； 4、JIT 编译器优化后的 class 信息。 PermGen 的使用量与加载到内存的 class 的数量/大小正相关。 解决方案根据 Permgen space 报错的时机，可以采用不同的解决方案，如下所示： 1、程序启动报错，修改 -XX:MaxPermSize 启动参数，调大永久代空间。 2、应用重新部署时报错，很可能是没有应用没有重启，导致加载了多份 class 信息，只需重启 JVM 即可解决。 3、运行时报错，应用程序可能会动态创建大量 class，而这些 class 的生命周期很短暂，但是 JVM 默认不会卸载 class，可以设置 -XX:+CMSClassUnloadingEnabled 和 -XX:+UseConcMarkSweepGC这两个参数允许 JVM 卸载 class。 如果上述方法无法解决，可以通过 jmap 命令 dump 内存对象 jmap-dump:format=b,file=dump.hprof ，然后利用 Eclipse MAT https://www.eclipse.org/mat 功能逐一分析开销最大的 classloader 和重复 class。 4. MetaspaceJDK 1.8 使用 Metaspace 替换了永久代（Permanent Generation），该错误表示 Metaspace 已被用满，通常是因为加载的 class 数目太多或体积太大。 此类问题的原因与解决方法跟 Permgenspace 非常类似，可以参考上文。需要特别注意的是调整 Metaspace 空间大小的启动参数为 -XX:MaxMetaspaceSize。 Unable to create new native thread每个 Java 线程都需要占用一定的内存空间，当 JVM 向底层操作系统请求创建一个新的 native 线程时，如果没有足够的资源分配就会报此类错误。 原因分析JVM 向 OS 请求创建 native 线程失败，就会抛出 Unableto createnewnativethread，常见的原因包括以下几类： 1、线程数超过操作系统最大线程数 ulimit 限制； 2、线程数超过 kernel.pid_max（只能重启）； 3、native 内存不足； 该问题发生的常见过程主要包括以下几步： 1、JVM 内部的应用程序请求创建一个新的 Java 线程； 2、JVM native 方法代理了该次请求，并向操作系统请求创建一个 native 线程； 3、操作系统尝试创建一个新的 native 线程，并为其分配内存； 4、如果操作系统的虚拟内存已耗尽，或是受到 32 位进程的地址空间限制，操作系统就会拒绝本次 native 内存分配； 5、JVM 将抛出 java.lang.OutOfMemoryError:Unableto createnewnativethread 错误。 解决方案1、升级配置，为机器提供更多的内存； 2、降低 Java Heap Space 大小； 3、修复应用程序的线程泄漏问题； 4、限制线程池大小； 5、使用 -Xss 参数减少线程栈的大小； 6、调高 OS 层面的线程最大数：执行 ulimia-a 查看最大线程数限制，使用 ulimit-u xxx 调整最大线程数限制。 ulimit -a …. 省略部分内容 ….. max user processes (-u) 16384 6. Out of swap space？该错误表示所有可用的虚拟内存已被耗尽。虚拟内存（Virtual Memory）由物理内存（Physical Memory）和交换空间（Swap Space）两部分组成。当运行时程序请求的虚拟内存溢出时就会报 Outof swap space? 错误。 原因分析该错误出现的常见原因包括以下几类： 1、地址空间不足； 2、物理内存已耗光； 3、应用程序的本地内存泄漏（native leak），例如不断申请本地内存，却不释放。 4、执行 jmap-histo:live 命令，强制执行 Full GC；如果几次执行后内存明显下降，则基本确认为 Direct ByteBuffer 问题。 解决方案根据错误原因可以采取如下解决方案： 1、升级地址空间为 64 bit； 2、使用 Arthas 检查是否为 Inflater/Deflater 解压缩问题，如果是，则显式调用 end 方法。 3、Direct ByteBuffer 问题可以通过启动参数 -XX:MaxDirectMemorySize 调低阈值。 4、升级服务器配置/隔离部署，避免争用。 Kill process or sacrifice child有一种内核作业（Kernel Job）名为 Out of Memory Killer，它会在可用内存极低的情况下“杀死”（kill）某些进程。OOM Killer 会对所有进程进行打分，然后将评分较低的进程“杀死”，具体的评分规则可以参考 Surviving the Linux OOM Killer。 不同于其他的 OOM 错误， Killprocessorsacrifice child 错误不是由 JVM 层面触发的，而是由操作系统层面触发的。 原因分析默认情况下，Linux 内核允许进程申请的内存总量大于系统可用内存，通过这种“错峰复用”的方式可以更有效的利用系统资源。 然而，这种方式也会无可避免地带来一定的“超卖”风险。例如某些进程持续占用系统内存，然后导致其他进程没有可用内存。此时，系统将自动激活 OOM Killer，寻找评分低的进程，并将其“杀死”，释放内存资源。 解决方案1、升级服务器配置/隔离部署，避免争用。 2、OOM Killer 调优。 8. Requested array size exceeds VM limitJVM 限制了数组的最大长度，该错误表示程序请求创建的数组超过最大长度限制。 JVM 在为数组分配内存前，会检查要分配的数据结构在系统中是否可寻址，通常为 Integer.MAX_VALUE-2。 此类问题比较罕见，通常需要检查代码，确认业务是否需要创建如此大的数组，是否可以拆分为多个块，分批执行。 9. Direct buffer memoryJava 允许应用程序通过 Direct ByteBuffer 直接访问堆外内存，许多高性能程序通过 Direct ByteBuffer 结合内存映射文件（Memory Mapped File）实现高速 IO。 原因分析Direct ByteBuffer 的默认大小为 64 MB，一旦使用超出限制，就会抛出 Directbuffer memory 错误。 解决方案1、Java 只能通过 ByteBuffer.allocateDirect 方法使用 Direct ByteBuffer，因此，可以通过 Arthas 等在线诊断工具拦截该方法进行排查。 2、检查是否直接或间接使用了 NIO，如 netty，jetty 等。 3、通过启动参数 -XX:MaxDirectMemorySize 调整 Direct ByteBuffer 的上限值。 4、检查 JVM 参数是否有 -XX:+DisableExplicitGC 选项，如果有就去掉，因为该参数会使 System.gc() 失效。 5、检查堆外内存使用代码，确认是否存在内存泄漏；或者通过反射调用 sun.misc.Cleaner 的 clean() 方法来主动释放被 Direct ByteBuffer 持有的内存空间。 6、内存容量确实不足，升级配置。 参考教你分析9种 OOM 常见原因及解决方案 OOM有哪些情况，SOF有哪些情况","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"OOM","slug":"OOM","permalink":"https://xmmarlowe.github.io/tags/OOM/"},{"name":"SOF","slug":"SOF","permalink":"https://xmmarlowe.github.io/tags/SOF/"}],"author":"Marlowe"},{"title":"如何在List种移除元素","slug":"Java/如何在List种移除元素","date":"2021-05-08T14:01:59.000Z","updated":"2021-08-26T14:02:52.235Z","comments":true,"path":"2021/05/08/Java/如何在List种移除元素/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/08/Java/%E5%A6%82%E4%BD%95%E5%9C%A8List%E7%A7%8D%E7%A7%BB%E9%99%A4%E5%85%83%E7%B4%A0/","excerpt":"","text":"Itr对象源码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667private class Itr implements Iterator&lt;E&gt; &#123; int cursor; // index of next element to return int lastRet = -1; // index of last element returned; -1 if no such int expectedModCount = modCount; Itr() &#123;&#125; public boolean hasNext() &#123; return cursor != size; &#125; @SuppressWarnings(&quot;unchecked&quot;) public E next() &#123; checkForComodification(); int i = cursor; if (i &gt;= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i &gt;= elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; return (E) elementData[lastRet = i]; &#125; public void remove() &#123; if (lastRet &lt; 0) throw new IllegalStateException(); checkForComodification(); try &#123; ArrayList.this.remove(lastRet); cursor = lastRet; lastRet = -1; expectedModCount = modCount; &#125; catch (IndexOutOfBoundsException ex) &#123; throw new ConcurrentModificationException(); &#125; &#125; @Override @SuppressWarnings(&quot;unchecked&quot;) public void forEachRemaining(Consumer&lt;? super E&gt; consumer) &#123; Objects.requireNonNull(consumer); final int size = ArrayList.this.size; int i = cursor; if (i &gt;= size) &#123; return; &#125; final Object[] elementData = ArrayList.this.elementData; if (i &gt;= elementData.length) &#123; throw new ConcurrentModificationException(); &#125; while (i != size &amp;&amp; modCount == expectedModCount) &#123; consumer.accept((E) elementData[i++]); &#125; // update once at end of iteration to reduce heap write traffic cursor = i; lastRet = i - 1; checkForComodification(); &#125; /**报错的地方*/ final void checkForComodification() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException(); &#125; &#125; 通过代码我们发现 Itr 是 ArrayList 中定义的一个私有内部类，在 next、remove方法中都会调用 checkForComodification 方法，该方法的作用是判断 modCount != expectedModCount是否相等，如果不相等则抛出ConcurrentModificationException异常。每次正常执行 remove 方法后，都会对执行 expectedModCount = modCount 赋值，保证两个值相等！ 那么问题基本上已经清晰了，在 foreach 循环中执行 list.remove(item);，对 list 对象的 modCount 值进行了修改，而 list 对象的迭代器的 expectedModCount 值未进行修改，因此抛出了ConcurrentModificationException 异常。 采用倒序移除123456789101112131415161718192021222324252627282930public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add(&quot;11&quot;); list.add(&quot;11&quot;); list.add(&quot;12&quot;); list.add(&quot;13&quot;); list.add(&quot;14&quot;); list.add(&quot;15&quot;); list.add(&quot;16&quot;); System.out.println(&quot;原始list元素：&quot;+ list.toString()); CopyOnWriteArrayList&lt;String&gt; copyList = new CopyOnWriteArrayList&lt;&gt;(list); //通过下表移除等于11的元素 for (int i = list.size() - 1; i &gt;= 0; i--) &#123; String item = list.get(i); if(&quot;11&quot;.equals(item)) &#123; list.remove(i); &#125; &#125; System.out.println(&quot;通过下表移除后的list元素：&quot;+ list.toString()); //通过对象移除等于11的元素 for (int i = copyList.size() - 1; i &gt;= 0; i--) &#123; String item = copyList.get(i); if(&quot;11&quot;.equals(item)) &#123; copyList.remove(item); &#125; &#125; System.out.println(&quot;通过对象移除后的list元素：&quot;+ list.toString()); &#125; 输出结果： 123原始list元素：[11, 11, 12, 13, 14, 15, 16] 通过下表移除后的list元素：[12, 13, 14, 15, 16] 通过对象移除后的list元素：[12, 13, 14, 15, 16] for的解决办法1234567891011121314151617181920public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add(&quot;11&quot;); list.add(&quot;11&quot;); list.add(&quot;12&quot;); list.add(&quot;13&quot;); list.add(&quot;14&quot;); list.add(&quot;15&quot;); list.add(&quot;16&quot;); System.out.println(&quot;原始list元素：&quot;+ list.toString()); CopyOnWriteArrayList&lt;String&gt; copyList = new CopyOnWriteArrayList&lt;&gt;(list); //通过对象移除等于11的元素 for (String item : copyList) &#123; if(&quot;11&quot;.equals(item)) &#123; copyList.remove(item); &#125; &#125; System.out.println(&quot;通过对象移除后的list元素：&quot;+ copyList.toString()); &#125; 输出结果： 12原始list元素：[11, 11, 12, 13, 14, 15, 16] 通过对象移除后的list元素：[12, 13, 14, 15, 16] 使用迭代器移除123456789101112131415161718192021public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add(&quot;11&quot;); list.add(&quot;11&quot;); list.add(&quot;12&quot;); list.add(&quot;13&quot;); list.add(&quot;14&quot;); list.add(&quot;15&quot;); list.add(&quot;16&quot;); System.out.println(&quot;原始list元素：&quot;+ list.toString()); //通过迭代器移除等于11的元素 Iterator&lt;String&gt; iterator = list.iterator(); while(iterator.hasNext()) &#123; String item = iterator.next(); if(&quot;11&quot;.equals(item)) &#123; iterator.remove(); &#125; &#125; System.out.println(&quot;通过迭代器移除后的list元素：&quot;+ list.toString()); &#125; 输出结果： 12原始list元素：[11, 11, 12, 13, 14, 15, 16] 通过迭代器移除后的list元素：[12, 13, 14, 15, 16] jdk1.8的写法12345678910111213141516public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add(&quot;11&quot;); list.add(&quot;11&quot;); list.add(&quot;12&quot;); list.add(&quot;13&quot;); list.add(&quot;14&quot;); list.add(&quot;15&quot;); list.add(&quot;16&quot;); System.out.println(&quot;原始list元素：&quot;+ list.toString()); //jdk1.8移除等于11的元素 list.removeIf(item -&gt; &quot;11&quot;.equals(item)); System.out.println(&quot;移除后的list元素：&quot;+ list.toString()); &#125; 输出结果： 12原始list元素：[11, 11, 12, 13, 14, 15, 16] 通过迭代器移除后的list元素：[12, 13, 14, 15, 16] 参考java中List元素移除元素的那些坑","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"List","slug":"List","permalink":"https://xmmarlowe.github.io/tags/List/"}],"author":"Marlowe"},{"title":"Java 中 a = a+b和a += b 的区别","slug":"Java/Java 中 a = a+b和a += b 的区别","date":"2021-05-08T13:54:40.000Z","updated":"2021-05-08T14:08:42.264Z","comments":true,"path":"2021/05/08/Java/Java 中 a = a+b和a += b 的区别/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/08/Java/Java%20%E4%B8%AD%20a%20=%20a+b%E5%92%8Ca%20+=%20b%20%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"区别 a += b执行实际过程实际是 是先计算出a的值，然后用一个temp对象存储，之后和b进行相加，然后将值赋值给a引用。 a = a+b 的执行过程则是先计算 a + b，然后再赋值给a引用，给a引用的时候如果 引用a 有计算过程，则会再次计算。 +=如果两边的操作数的精度不一样时会自动向低转化，而a = a+b则不会自动转化，需要手动进行强制类型转化。","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"}],"author":"Marlowe"},{"title":"Java对象创建的4种方式","slug":"Java/Java对象创建的4种方式","date":"2021-05-08T13:42:11.000Z","updated":"2021-05-08T14:08:42.268Z","comments":true,"path":"2021/05/08/Java/Java对象创建的4种方式/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/08/Java/Java%E5%AF%B9%E8%B1%A1%E5%88%9B%E5%BB%BA%E7%9A%844%E7%A7%8D%E6%96%B9%E5%BC%8F/","excerpt":"","text":"使用 new 关键字创建对象这是常用的创建对象的方法，语法格式如下：类名 对象名=new 类名()； 调用 java.lang.Class 或者 java.lang.reflect.Constuctor 类的 newlnstance() 实例方法在 Java 中，可以使用 java.lang.Class 或者 java.lang.reflect.Constuctor 类的 newlnstance() 实例方法来创建对象，代码格式如下：java.lang.Class Class 类对象名称=java.lang.Class.forName(要实例化的类全称);类名 对象名=(类名)Class类对象名称.newInstance(); 调用 java.lang.Class 类中的 forName() 方法时，需要将要实例化的类的全称（比如 com.mxl.package.Student）作为参数传递过去，然后再调用 java.lang.Class 类对象的 newInstance() 方法创建对象。 调用对象的 clone() 方法该方法不常用，使用该方法创建对象时，要实例化的类必须继承 java.lang.Cloneable 接口。 调用对象的 clone() 方法创建对象的语法格式如下：类名对象名=(类名)已创建好的类对象名.clone(); 调用 java.io.ObjectlnputStream 对象的 readObject() 方法示例代码1234567891011121314151617181920212223242526272829303132333435363738public class Student implements Cloneable&#123; //实现 Cloneable 接口 private String Name; //学生名字 private int age; //学生年龄 public Student(String name,int age) &#123; //构造方法 this.Name=name; this.age=age; &#125; public Student() &#123; this.Name=&quot;name&quot;; this.age=0; &#125; public String toString() &#123; return&quot;学生名字：&quot;+Name+&quot;，年龄：&quot;+age; &#125; public static void main(String[] args)throws Exception &#123; System.out.println(&quot;---------使用 new 关键字创建对象---------&quot;); //使用new关键字创建对象 Student student1=new Student(&quot;小刘&quot;,22); System.out.println(student1); System.out.println(&quot;-----------调用 java.lang.Class 的 newInstance() 方法创建对象-----------&quot;); //调用 java.lang.Class 的 newInstance() 方法创建对象 Class cl=Class.forName(&quot;Student&quot;); Student student2=(Student)cl.newInstance(); System.out.println(student2); System.out.println(&quot;-------------------调用对象的 clone() 方法创建对象----------&quot;); //调用对象的 clone() 方法创建对象 Student student3=(Student)student2.clone(); System.out.println(student3); &#125;&#125; 无论釆用哪种方式创建对象，Java 虚拟机在创建一个对象时都包含以下步骤： 给对象分配内存。 将对象的实例变量自动初始化为其变量类型的默认值。 初始化对象，给实例变量赋予正确的初始值。 注意： 每个对象都是相互独立的，在内存中占有独立的内存地址，并且每个对象都具有自己的生命周期，当一个对象的生命周期结束时，对象就变成了垃圾，由 Java 虚拟机自带的垃圾回收机制处理。","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"对象","slug":"对象","permalink":"https://xmmarlowe.github.io/tags/%E5%AF%B9%E8%B1%A1/"}],"author":"Marlowe"},{"title":"hashCode()与 equals()相关问题","slug":"Java/hashCode-与-equals-相关问题","date":"2021-05-08T13:29:19.000Z","updated":"2021-05-08T14:08:42.272Z","comments":true,"path":"2021/05/08/Java/hashCode-与-equals-相关问题/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/08/Java/hashCode-%E4%B8%8E-equals-%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/","excerpt":"","text":"hashCode()介绍hashCode() 的作用是获取哈希码，也称为散列码；它实际上是返回一个 int 整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode()定义在 JDK 的 Object 类中，这就意味着 Java 中的任何类都包含有 hashCode() 函数。另外需要注意的是： Object 的 hashcode 方法是本地方法，也就是用 c 语言或 c++ 实现的，该方法通常用来将对象的 内存地址 转换为整数之后返回。 1public native int hashCode(); 散列表存储的是键值对(key-value)，它的特点是：能根据“键”快速的检索出对应的“值”。这其中就利用到了散列码！（可以快速找到所需要的对象） 对于hashCode有以下几点约束： 在 Java 应用程序执行期间，在对同一对象多次调用 hashCode 方法时，必须一致地返回相同的整数，前提是将对象进行 equals 比较时所用的信息没有被修改； 如果两个对象 x.equals(y) 方法返回true，则x、y这两个对象的hashCode必须相等。 如果两个对象x.equals(y) 方法返回false，则x、y这两个对象的hashCode可以相等也可以不等。 但是，为不相等的对象生成不同整数结果可以提高哈希表的性能。 默认的hashCode是将内存地址转换为的hash值，重写过后就是自定义的计算方式；也可以通过System.identityHashCode(Object)来返回原本的hashCode。 12345678910111213public class HashCodeTest &#123; private int age; private String name; @Override public int hashCode() &#123; Object[] a = Stream.of(age, name).toArray(); int result = 1; for (Object element : a) &#123; result = 31 * result + (element == null ? 0 : element.hashCode()); &#125; return result; &#125;&#125; 推荐使用Objects.hash(Object… values)方法。相信看源码的时候，都看到计算hashCode都使用了31作为基础乘数， 为什么使用31呢？我比较赞同与理解result * 31 = (result&lt;&lt;5) - result。JVM底层可以自动做优化为位运算，效率很高；还有因为31计算的hashCode冲突较少，利于hash桶位的分布。 equals()介绍public boolean equals(Object obj);用于比较当前对象与目标对象是否相等，默认是比较引用是否指向同一对象。为public方法，子类可重写。 12345public class Object&#123; public boolean equals(Object obj) &#123; return (this == obj); &#125;&#125; 为什么要有 hashCode？我们以“HashSet 如何检查重复”为例子来说明为什么要有 hashCode？ 当你把对象加入 HashSet 时，HashSet 会先计算对象的 hashcode 值来判断对象加入的位置，同时也会与其他已经加入的对象的 hashcode 值作比较，如果没有相符的 hashcode，HashSet 会假设对象没有重复出现。但是如果发现有相同 hashcode 值的对象，这时会调用 equals() 方法来检查 hashcode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置。（摘自我的 Java 启蒙书《Head First Java》第二版）。这样我们就大大减少了 equals 的次数，相应就大大提高了执行速度。 为什么需要重写equals方法？因为如果不重写equals方法，当将自定义对象放到map或者set中时；如果这时两个对象的hashCode相同，就会调用equals方法进行比较，这个时候会调用Object中默认的equals方法，而默认的equals方法只是比较了两个对象的引用是否指向了同一个对象，显然大多数时候都不会指向，这样就会将重复对象存入map或者set中。这就 破坏了map与set不能存储重复对象的特性，会造成内存溢出 。 重写equals方法的几条约定 自反性：即x.equals(x)返回true，x不为null； 对称性：即x.equals(y)与y.equals(x）的结果相同，x与y不为null； 传递性：即x.equals(y)结果为true, y.equals(z)结果为true，则x.equals(z)结果也必须为true； 一致性：即x.equals(y)返回true或false，在未更改equals方法使用的参数条件下，多次调用返回的结果也必须一致。x与y不为null。 如果x不为null, x.equals(null)返回false。 建议equals及hashCode两个方法，需要重写时，两个都要重写，一般都是将自定义对象放至Set中，或者Map中的key时，需要重写这两个方法。 为什么重写 equals 时必须重写 hashCode 方法？如果两个对象相等，则 hashcode 一定也是相同的。两个对象相等,对两个对象分别调用 equals 方法都返回 true。但是，两个对象有相同的 hashcode 值，它们也不一定是相等的 。因此，equals 方法被覆盖过，则 hashCode 方法也必须被覆盖。 hashCode()的默认行为是对堆上的对象产生独特值。如果没有重写 hashCode()，则该 class 的两个对象无论如何都不会相等（即使这两个对象指向相同的数据） 为什么两个对象有相同的 hashcode 值，它们也不一定是相等的？因为 hashCode() 所使用的杂凑算法也许刚好会让多个对象传回相同的杂凑值。越糟糕的杂凑算法越容易碰撞，但这也与数据值域分布的特性有关（所谓碰撞也就是指的是不同的对象得到相同的 hashCode。 我们刚刚也提到了 HashSet,如果 HashSet 在对比的时候，同样的 hashcode 有多个对象，它会使用 equals() 来判断是否真的相同。也就是说 hashcode 只是用来缩小查找成本。","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"hashcode","slug":"hashcode","permalink":"https://xmmarlowe.github.io/tags/hashcode/"},{"name":"equals","slug":"equals","permalink":"https://xmmarlowe.github.io/tags/equals/"}],"author":"Marlowe"},{"title":"InnoDB崩溃恢复机制","slug":"数据库/InnoDB崩溃恢复机制","date":"2021-05-08T13:05:16.000Z","updated":"2021-05-08T14:08:16.644Z","comments":true,"path":"2021/05/08/数据库/InnoDB崩溃恢复机制/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/08/%E6%95%B0%E6%8D%AE%E5%BA%93/InnoDB%E5%B4%A9%E6%BA%83%E6%81%A2%E5%A4%8D%E6%9C%BA%E5%88%B6/","excerpt":"","text":"概述数据库系统与文件系统大的区别在于数据库能保证操作的原子性，一个操作要么不做要么都做，即使在数据库宕机的情况下，也不会出现操作一半的情况，这个就需要数据库的日志和一套完善的崩溃恢复机制来保证。下面简单介绍一下InnoDB的崩溃恢复流程。 相关概念 lsn: 可以理解为数据库从创建以来产生的redo日志量，这个值越大，说明数据库的更新越多，也可以理解为更新的时刻。此外，每个数据页上也有一个lsn，表示最后被修改时的lsn，值越大表示越晚被修改。比如，数据页A的lsn为100，数据页B的lsn为200，checkpoint lsn为150，系统lsn为300，表示当前系统已经更新到300，小于150的数据页已经被刷到磁盘上，因此数据页A的最新数据一定在磁盘上，而数据页B则不一定，有可能还在内存中。 redo日志: 现代数据库都需要写redo日志，例如修改一条数据，首先写redo日志，然后再写数据。在写完redo日志后，就直接给客户端返回成功。这样虽然看过去多写了一次盘，但是由于把对磁盘的随机写入(写数据)转换成了顺序的写入(写redo日志)，性能有很大幅度的提高。当数据库挂了之后，通过扫描redo日志，就能找出那些没有刷盘的数据页(在崩溃之前可能数据页仅仅在内存中修改了，但是还没来得及写盘)，保证数据不丢。 undo日志: 数据库还提供类似撤销的功能，当你发现修改错一些数据时，可以使用rollback指令回滚之前的操作。这个功能需要undo日志来支持。此外，现代的关系型数据库为了提高并发(同一条记录，不同线程的读取不冲突，读写和写读不冲突，只有同时写才冲突)，都实现了类似MVCC的机制，在InnoDB中，这个也依赖undo日志。为了实现统一的管理，与redo日志不同，undo日志在Buffer Pool中有对应的数据页，与普通的数据页一起管理，依据LRU规则也会被淘汰出内存，后续再从磁盘读取。与普通的数据页一样，对undo页的修改，也需要先写redo日志。 检查点: 英文名为checkpoint。数据库为了提高性能，数据页在内存修改后并不是每次都会刷到磁盘上。checkpoint之前的数据页保证一定落盘了，这样之前的日志就没有用了(由于InnoDB redolog日志循环使用，这时这部分日志就可以被覆盖)，checkpoint之后的数据页有可能落盘，也有可能没有落盘，所以checkpoint之后的日志在崩溃恢复的时候还是需要被使用的。InnoDB会依据脏页的刷新情况，定期推进checkpoint，从而减少数据库崩溃恢复的时间。检查点的信息在第一个日志文件的头部。 崩溃恢复: 用户修改了数据，并且收到了成功的消息，然而对数据库来说，可能这个时候修改后的数据还没有落盘，如果这时候数据库挂了，重启后，数据库需要从日志中把这些修改后的数据给捞出来，重新写入磁盘，保证用户的数据不丢。这个从日志中捞数据的过程就是崩溃恢复的主要任务，也可以成为数据库前滚。当然，在崩溃恢复中还需要回滚没有提交的事务，提交没有提交成功的事务。由于回滚操作需要undo日志的支持，undo日志的完整性和可靠性需要redo日志来保证，所以崩溃恢复先做redo前滚，然后做undo回滚。 数据库崩溃恢复过程下面看一下数据库崩溃恢复过程。整个过程都在引擎初始化阶段完成(innobase_init)，其中最主要的函数是innobase_start_or_create_for_mysql，innodb通过这个函数完成创建和初始化，包括崩溃恢复。首先来介绍一下数据库的前滚。 崩溃恢复相关参数解析innodb_fast_shutdowninnodb_fast_shutdown = 0。这个表示在MySQL关闭的时候，执行slow shutdown，不但包括日志的刷盘，数据页的刷盘，还包括数据的清理(purge)，ibuf的合并，buffer pool dump以及lazy table drop操作(如果表上有未完成的操作，即使执行了drop table且返回成功了，表也不一定立刻被删除)。 innodb_fast_shutdown = 1。这个是默认值，表示在MySQL关闭的时候，仅仅把日志和数据刷盘。 innodb_fast_shutdown = 2。这个表示关闭的时候，仅仅日志刷盘，其他什么都不做，就好像MySQL crash了一样。 这个参数值越大，MySQL关闭的速度越快，但是启动速度越慢，相当于把关闭时候需要做的工作挪到了崩溃恢复上。另外，如果MySQL要升级，建议使用第一种方式进行一次干净的shutdown。 innodb_force_recovery这个参数主要用来控制InnoDB启动时候做哪些工作，数值越大，做的工作越少，启动也更加容易，但是数据不一致的风险也越大。当MySQL因为某些不可控的原因不能启动时，可以设置这个参数，从1开始逐步递增，知道MySQL启动，然后使用SELECT INTO OUTFILE把数据导出，尽最大的努力减少数据丢失。 innodb_force_recovery = 0。这个是默认的参数，启动的时候会做所有的事情，包括redo日志应用，undo日志回滚，启动后台master和purge线程，ibuf合并。检测到了数据页损坏了，如果是系统表空间的，则会crash，用户表空间的，则打错误日志。 innodb_force_recovery = 1。如果检测到数据页损坏了，不会crash也不会报错(buf_page_io_complete)，启动的时候也不会校验表空间第一个数据页的正确性(fil_check_first_page)，表空间无法访问也继续做崩溃恢复(fil_open_single_table_tablespace、fil_load_single_table_tablespace)，ddl操作不能进行(check_if_supported_inplace_alter)，同时数据库也被不能进行写入操作(row_insert_for_mysql、row_update_for_mysql等)，所有的prepare事务也会被回滚(trx_resurrect_insert、trx_resurrect_update_in_prepared_state)。这个选项还是很常用的，数据页可能是因为磁盘坏了而损坏了，设置为1，能保证数据库正常启动。 innodb_force_recovery = 2。除了设置1之后的操作不会运行，后台的master和purge线程就不会启动了(srv_master_thread、srv_purge_coordinator_thread等)，当你发现数据库因为这两个线程的原因而无法启动时，可以设置。 innodb_force_recovery = 3。除了设置2之后的操作不会运行，undo回滚数据库也不会进行，但是回滚段依然会被扫描，undo链表也依然会被创建(trx_sys_init_at_db_start)。srv_read_only_mode会被打开。 innodb_force_recovery = 4。除了设置3之后的操作不会运行，ibuf的操作也不会运行(ibuf_merge_or_delete_for_page)，表信息统计的线程也不会运行(因为一个坏的索引页会导致数据库崩溃)(info_low、dict_stats_update等)。从这个选项开始，之后的所有选项，都会损坏数据，慎重使用。 innodb_force_recovery = 5。除了设置4之后的操作不会运行，回滚段也不会被扫描(recv_recovery_rollback_active)，undo链表也不会被创建，这个主要用在undo日志被写坏的情况下。 innodb_force_recovery = 6。除了设置5之后的操作不会运行，数据库前滚操作也不会进行，包括解析和应用(recv_recovery_from_checkpoint_start_func)。 总结InnoDB实现了一套完善的崩溃恢复机制，保证在任何状态下(包括在崩溃恢复状态下)数据库挂了，都能正常恢复，这个是与文件系统最大的差别。 参考mysql 崩溃恢复_超详细的MySQL数据库InnoDB崩溃恢复机制总结","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"InnoDB","slug":"InnoDB","permalink":"https://xmmarlowe.github.io/tags/InnoDB/"}],"author":"Marlowe"},{"title":"信号与信号量的区别","slug":"操作系统/信号与信号量的区别","date":"2021-05-08T07:46:04.000Z","updated":"2021-05-08T13:20:38.159Z","comments":true,"path":"2021/05/08/操作系统/信号与信号量的区别/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/08/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BF%A1%E5%8F%B7%E4%B8%8E%E4%BF%A1%E5%8F%B7%E9%87%8F%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"信号由用户、系统或者进程发送给目标进程的信息，以通知目标进程某个状态的改变或系统异常。 信号量信号量是一个特殊的变量，它的本质是计数器，信号量里面记录了临界资源的数目，有多少数目，信号量的值就为多少，进程对其访问都是原子操作（pv操作，p：占用资源，v：释放资源）。它的作用就是，调协进程对共享资源的访问，让一个临界区同一时间只有一个进程在访问它。 区别信号是通知进程产生了某个事件，信号量是用来同步进程的（用来调协进程对共享资源的访问的） 参考Linux系统编程————信号与信号量的区别","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"信号","slug":"信号","permalink":"https://xmmarlowe.github.io/tags/%E4%BF%A1%E5%8F%B7/"},{"name":"信号量","slug":"信号量","permalink":"https://xmmarlowe.github.io/tags/%E4%BF%A1%E5%8F%B7%E9%87%8F/"}],"author":"Marlowe"},{"title":"UDP如何实现可靠传输","slug":"计算机网络/UDP如何实现可靠传输","date":"2021-05-08T07:24:14.000Z","updated":"2021-05-08T13:20:38.163Z","comments":true,"path":"2021/05/08/计算机网络/UDP如何实现可靠传输/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/08/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/UDP%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%8F%AF%E9%9D%A0%E4%BC%A0%E8%BE%93/","excerpt":"","text":"概述UDP不属于连接协议，具有资源消耗少，处理速度快的优点，所以通常音频，视频和普通数据在传送时，使用UDP较多，因为即使丢失少量的包，也不会对接受结果产生较大的影响。 传输层无法保证数据的可靠传输，只能通过应用层来实现了。实现的方式可以参照tcp可靠性传输的方式，只是实现不在传输层，实现转移到了应用层。 最简单的方式是在应用层模仿传输层TCP的可靠性传输。下面不考虑拥塞处理，可靠UDP的简单设计。 1、添加seq/ack机制，确保数据发送到对端 2、添加发送和接收缓冲区，主要是用户超时重传。 3、添加超时重传机制。 详细说明： 送端发送数据时，生成一个随机seq=x，然后每一片按照数据大小分配seq。数据到达接收端后接收端放入缓存，并发送一个ack=x的包，表示对方已经收到了数据。发送端收到了ack包后，删除缓冲区对应的数据。时间到后，定时任务检查是否需要重传数据。 目前有如下开源程序利用udp实现了可靠的数据传输。分别为RUDP、RTP、UDT。 开源程序RUDP（Reliable User Datagram Protocol）RUDP 提供一组数据服务质量增强机制，如拥塞控制的改进、重发机制及淡化服务器算法等， 从而在包丢失和网络拥塞的情况下， RTP 客户机（实时位置）面前呈现的就是一个高质量的 RTP 流。在不干扰协议的实时特性的同时，可靠 UDP 的拥塞控制机制允许 TCP 方式下的流控制行为。 RTP（Real Time Protocol）RTP为数据提供了具有实时特征的端对端传送服务，如在组播或单播网络服务下的交互式视频音频或模拟数据。 应用程序通常在 UDP 上运行 RTP 以便使用其多路结点和校验服务；这两种协议都提供了传输层协议的功能。但是 RTP 可以与其它适合的底层网络或传输协议一起使用。如果底层网络提供组播方式，那么 RTP 可以使用该组播表传输数据到多个目的地。 RTP 本身并没有提供按时发送机制或其它服务质量（QoS）保证，它依赖于底层服务去实现这一过程。 RTP 并不保证传送或防止无序传送，也不确定底层网络的可靠性。 RTP 实行有序传送， RTP 中的序列号允许接收方重组发送方的包序列，同时序列号也能用于决定适当的包位置，例如：在视频解码中，就不需要顺序解码。 UDT（UDP-based Data Transfer Protocol）基于UDP的数据传输协议（UDP-basedData Transfer Protocol，简称UDT）是一种互联网数据传输协议。UDT的主要目的是支持高速广域网上的海量数据传输，而互联网上的标准数据传输协议TCP在高带宽长距离网络上性能很差。 顾名思义，UDT建于UDP之上，并引入新的拥塞控制和数据可靠性控制机制。UDT是面向连接的双向的应用层协议。它同时支持可靠的数据流传输和部分可靠的数据报传输。由于UDT完全在UDP上实现，它也可以应用在除了高速数据传输之外的其它应用领域，例如点到点技术（P2P），防火墙穿透，多媒体数据传输等等。 参考UDP如何实现可靠传输","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"UDP","slug":"UDP","permalink":"https://xmmarlowe.github.io/tags/UDP/"},{"name":"可靠传输","slug":"可靠传输","permalink":"https://xmmarlowe.github.io/tags/%E5%8F%AF%E9%9D%A0%E4%BC%A0%E8%BE%93/"}],"author":"Marlowe"},{"title":"Redis过期数据删除策略","slug":"NoSQL/Redis过期数据删除策略","date":"2021-05-08T07:10:53.000Z","updated":"2021-05-08T13:20:38.155Z","comments":true,"path":"2021/05/08/NoSQL/Redis过期数据删除策略/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/08/NoSQL/Redis%E8%BF%87%E6%9C%9F%E6%95%B0%E6%8D%AE%E5%88%A0%E9%99%A4%E7%AD%96%E7%95%A5/","excerpt":"","text":"如果假设你设置了一批 key 只能存活 1 分钟，那么 1 分钟后，Redis 是怎么对这批 key 进行删除的呢？ 常用的过期数据的删除策略就两个（重要！自己造缓存轮子的时候需要格外考虑的东西）： 惰性删除： 只会在取出 key 的时候才对数据进行过期检查。这样对 CPU 最友好，但是可能会造成太多过期 key 没有被删除。 定期删除： 每隔一段时间抽取一批 key 执行删除过期 key 操作。并且，Redis 底层会通过限制删除操作执行的时长和频率来减少删除操作对 CPU 时间的影响。定期删除对内存更加友好，惰性删除对 CPU 更加友好。两者各有千秋，所以 Redis 采用的是 定期删除+惰性/懒汉式删除。 但是，仅仅通过给 key 设置过期时间还是有问题的。因为还是可能存在定期删除和惰性删除漏掉了很多过期 key 的情况。这样就导致大量过期 key 堆积在内存里，然后就 Out of memory 了。 怎么解决这个问题呢？答案就是： Redis 内存淘汰机制。","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"}],"author":"Marlowe"},{"title":"Redis和memcached的区别和使用场景","slug":"NoSQL/Redis和memcached的区别和使用场景","date":"2021-05-08T06:58:20.000Z","updated":"2021-05-08T13:20:38.151Z","comments":true,"path":"2021/05/08/NoSQL/Redis和memcached的区别和使用场景/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/08/NoSQL/Redis%E5%92%8Cmemcached%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF/","excerpt":"","text":"共同点 都是基于内存的数据库，一般都用来当做缓存使用。 都有过期策略。 两者的性能都非常高。 区别 Redis和Memcache都是将数据存放在内存中，都是内存数据库。不过memcache还可用于缓存其他东西，例如图片、视频等等； Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，hash等数据结构的存储； 虚拟内存–Redis当物理内存用完时，可以将一些很久没用到的value 交换到磁盘； 过期策略–memcache在set时就指定，例如set key1 0 0 8,即永不过期。Redis可以通过例如expire 设定，例如expire name 10； 分布式–设定memcache集群，利用magent做一主多从;redis可以做一主多从。都可以一主一从； 存储数据安全–memcache挂掉后，数据没了；redis可以定期保存到磁盘（持久化）； 灾难恢复–memcache挂掉后，数据不可恢复; redis数据丢失后可以通过aof恢复； Redis支持数据的备份，即master-slave模式的数据备份； 应用场景不一样：Redis出来作为NoSQL数据库使用外，还能用做消息队列、数据堆栈和数据缓存等；Memcached适合于缓存SQL语句、数据集、用户临时性数据、延迟查询数据和session等。 使用场景 如果有持久方面的需求或对数据类型和处理有要求的应该选择redis。 如果 简单的key/value 存储 应该选择memcached。 参考redis和memcached的区别和使用场景 说一下 Redis 和 Memcached 的区别和共同点","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"},{"name":"memcached","slug":"memcached","permalink":"https://xmmarlowe.github.io/tags/memcached/"}],"author":"Marlowe"},{"title":"海量数据Top K问题","slug":"算法与数据结构/海量数据Top K问题","date":"2021-05-07T05:40:02.000Z","updated":"2021-05-08T07:08:40.555Z","comments":true,"path":"2021/05/07/算法与数据结构/海量数据Top K问题/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/07/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AETop%20K%E9%97%AE%E9%A2%98/","excerpt":"问题引入：10亿个数中找出最大的10000个数（top K问题）","text":"问题引入：10亿个数中找出最大的10000个数（top K问题） Top K 问题在大规模数据处理中，经常会遇到的一类问题：在海量数据中找出出现频率最高的前k个数，或者从海量数据中找出最大的前k个数，这类问题通常被称为top K问题。例如，在搜索引擎中，统计搜索最热门的10个查询词；在歌曲库中统计下载最高的前10首歌等。 针对top K类问题，通常比较好的方案是分治+Trie树/hash+小顶堆，即先将数据集按照Hash方法分解成多个小数据集，然后使用Trie树或者Hash统计每个小数据集中的query词频，之后用小顶堆求出每个数据集中出现频率最高的前K个数，最后在所有top K中求出最终的top K。 解决的几种方法假设场景为：1亿个数中找出最大的1000个数 直接排序最容易想到的方法是将数据全部排序，然后在排序后的集合中进行查找，最快的排序算法的时间复杂度一般为O（nlogn），如快速排序。但是在32位的机器上，每个float类型占4个字节，1亿个浮点数就要占用400MB的存储空间，对于一些可用内存小于400M的计算机而言，很显然是不能一次将全部数据读入内存进行排序的。其实即使内存能够满足要求（我机器内存都是8GB），该方法也并不高效，因为题目的目的是寻找出最大的1000个数即可，而排序却是将所有的元素都排序了，做了很多的无用功。 局部淘汰法第二种方法为局部淘汰法，该方法与排序方法类似，用一个容器保存前1000个数，然后将剩余的所有数字——与容器内的最小数字相比，如果所有后续的元素都比容器内的1000个数还小，那么容器内这个1000个数就是最大1000个数。如果某一后续元素比容器内最小数字大，则删掉容器内最小元素，并将该元素插入容器，最后遍历完这1亿个数，得到的结果容器中保存的数即为最终结果了。此时的时间复杂度为O（n+m^2），其中m为容器的大小，即1000。 分治法第三种方法是分治法，将1亿个数据分成100份，每份100万个数据，找到每份数据中最大的1000个，最后在剩下的100*1000个数据里面找出最大的1000个。如果100万数据选择足够理想，那么可以过滤掉1亿数据里面99%的数据。100万个数据里面查找最大的1000个数据的方法如下：用快速排序的方法。 Hash法第四种方法是Hash法。如果这1亿个数里面有很多重复的数，先通过Hash法，把这1亿个数字去重复，这样如果重复率很高的话，会减少很大的内存用量，从而缩小运算空间，然后通过分治法或最小堆法查找最大的1000个数。 最小堆第五种方法采用最小堆。首先读入前1000个数来创建大小为1000的最小堆，建堆的时间复杂度为O（mlogm）（m为数组的大小即为1000），然后遍历后续的数字，并于堆顶（最小）数字进行比较。如果比最小的数小，则继续读取后续数字；如果比堆顶数字大，则替换堆顶元素并重新调整堆为最小堆。整个过程直至1亿个数全部遍历完为止。然后输出当前堆中的所有1000个数字。该算法的时间复杂度为O（nmlogm），空间复杂度是1000（常数）。 分场景方法选择实际上，最优的解决方案应该是最符合实际设计需求的方案，在时间应用中，可能有足够大的内存，那么直接将数据扔到内存中一次性处理即可，也可能机器有多个核，这样可以采用多线程处理整个数据集。 下面针对不同的应用场景，分析了适合相应应用场景的解决方案。 单机+单核+足够大内存如果需要查找10亿个查询次（每个占8B）中出现频率最高的10个，考虑到每个查询词占8B，则10亿个查询次所需的内存大约是10^9 * 8B=8GB内存。如果有这么大内存，直接在内存中对查询次进行排序，顺序遍历找出10个出现频率最大的即可。这种方法简单快速，使用。然后，也可以先用HashMap求出每个词出现的频率，然后求出频率最大的10个词。 单机+多核+足够大内存这时可以直接在内存中使用Hash方法将数据划分成n个partition，每个partition交给一个线程处理，线程的处理逻辑同（1）类似，最后一个线程将结果归并。 该方法存在一个瓶颈会明显影响效率，即数据倾斜。每个线程的处理速度可能不同，快的线程需要等待慢的线程，最终的处理速度取决于慢的线程。而针对此问题，解决的方法是，将数据划分成c×n个partition（c&gt;1），每个线程处理完当前partition后主动取下一个partition继续处理，知道所有数据处理完毕，最后由一个线程进行归并。 单机+单核+受限内存这种情况下，需要将原数据文件切割成一个一个小文件，如次啊用hash(x)%M，将原文件中的数据切割成M小文件，如果小文件仍大于内存大小，继续采用Hash的方法对数据文件进行分割，知道每个小文件小于内存大小，这样每个文件可放到内存中处理。采用（1）的方法依次处理每个小文件。 多机+受限内存这种情况，为了合理利用多台机器的资源，可将数据分发到多台机器上，每台机器采用（3）中的策略解决本地的数据。可采用hash+socket方法进行数据分发。 重点讲下最小堆算法 在几千亿个数据中如何获取10000个最大的数？ 一个复杂度比较低的算法就是利用最小堆算法，它的思想就是：先建立一个容量为K的最小堆，然后遍历这几千亿个数，如果对于遍历到的数大于最小堆的根节点，那么这个数入堆，并且调整最小堆的结构，遍历完成以后，最小堆的数字就是这几千亿个数中最大的K个数了。 先来介绍一下最小堆：最小堆（小根堆）是一种数据结构，它首先是一颗完全二叉树，并且，它所有父节点的值小于或等于两个子节点的值。最小堆的存储结构（物理结构）实际上是一个数组。 因为它是一个完全二叉树，对于下标小于 数组.length/2 - 1 时有叶子节点 ， 对于下标为i（基0），其左节点下标为2i + 1，右节点下标为2i + 2。 最小堆如图所示，对于每个非叶子节点的数值，一定不大于孩子节点的数值。这样可用含有K个节点的最小堆来保存K个目前的最大值(当然根节点是其中的最小数值)。 每次有数据输入的时候可以先与根节点比较。若不大于根节点，则舍弃；否则用新数值替换根节点数值。并进行最小堆的调整。 代码实现：创建堆的复杂度是O(N)，调整最小堆的时间复杂度为O(logK)，因此Top K算法(问题)时间复杂度为O(NlogK)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283class TopK &#123; //创建堆 int[] createHeap(int a[], int k) &#123; int[] result = new int[k]; for (int i = 0; i &lt; k; i++) &#123; result[i] = a[i]; &#125; //完全二叉树的数组表示中，下标小于等于result.length / 2 - 1才有子节点 for (int i = result.length / 2 - 1;i &gt;= 0;i--)&#123; heapify(i,result); &#125; return result; &#125; void heapify(int i,int[] result)&#123; int left = 2 * i + 1; int right = 2 * i + 2; int smallest = i; if (left &lt; result.length &amp;&amp; result[left] &lt; result[i])&#123; smallest = left; &#125; if (right &lt; result.length &amp;&amp; result[right] &lt; result[smallest])&#123; smallest = right; &#125; if (smallest == i)&#123; return; &#125; else &#123; int temp = result[i]; result[i] = result[smallest]; result[smallest] = temp; &#125; heapify(smallest,result); &#125; //调整堆 void filterDown(int a[], int value) &#123; a[0] = value; int parent = 0; while(parent &lt; a.length)&#123; int left = 2*parent+1; int right = 2*parent+2; int smallest = parent; if(left &lt; a.length &amp;&amp; a[parent] &gt; a[left])&#123; smallest = left; &#125; if(right &lt; a.length &amp;&amp; a[smallest] &gt; a[right])&#123; smallest = right; &#125; if(smallest == parent)&#123; break; &#125;else&#123; int temp = a[parent]; a[parent] = a[smallest]; a[smallest] = temp; parent = smallest; &#125; &#125; &#125; //遍历数组，并且调整堆 int[] findTopKByHeap(int input[], int k) &#123; int heap[] = this.createHeap(input, k); for(int i=k;i&lt;input.length;i++)&#123; if(input[i]&gt;heap[0])&#123; this.filterDown(heap, input[i]); &#125; &#125; return heap; &#125; public static void main(String[] args) &#123; int a[] = &#123; 100,101,5,4,88,89,845,45,8,4,5,8,452,1,5,8,4,5,8,4,588,44444,88888,777777,100000&#125;; int result[] = new TopK().findTopKByHeap(a, 5); for (int temp : result) &#123; System.out.println(temp); &#125; &#125;&#125; 参考10亿个数中找出最大的10000个数（top K问题）","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"https://xmmarlowe.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Top K","slug":"Top-K","permalink":"https://xmmarlowe.github.io/tags/Top-K/"}],"author":"Marlowe"},{"title":"进程调度算法","slug":"操作系统/进程调度算法","date":"2021-05-07T05:10:56.000Z","updated":"2021-05-09T01:28:49.798Z","comments":true,"path":"2021/05/07/操作系统/进程调度算法/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/07/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/","excerpt":"","text":"介绍无论是在批处理系统还是分时系统中，用户进程数一般都多于处理机数、这将导致它们互相争夺处理机。另外，系统进程也同样需要使用处理机。这就要求进程调度程序按一定的策略，动态地把处理机分配给处于就绪队列中的某一个进程，以使之执行。 7种调度算法先来先服务调度算法（FCFS）先来先服务(FCFS)调度算法是一种最简单的调度算法，该算法既可用于作业调度，也可用于进程调度。当在作业调度中采用该算法时，每次调度都是从后备作业队列中选择一个或多个最先进入该队列的作业，将它们调入内存，为它们分配资源、创建进程，然后放入就绪队列。在进程调度中采用FCFS算法时，则每次调度是从就绪队列中选择一个最先进入该队列的进程，为之分配处理机，使之投入运行。该进程一直运行到完成或发生某事件而阻塞后才放弃处理机。 执行时间与调度之后执行顺序： 短作业优先(SJF)的调度算法从就绪队列中选出⼀个估计运⾏时间最短的进程为之分配资源，使它⽴即执⾏并⼀直执⾏到完成或发⽣某事件⽽被阻塞放弃占⽤ CPU 时再重新调度。 优先级调度为每个流程分配优先级，⾸先执⾏具有最⾼优先级的进程，依此类推。具有相同优先级的进程以 FCFS ⽅式执⾏。可以根据内存要求，时间要求或任何其他资源要求来确定优先级 时间⽚轮转调度算法(RR)时间⽚轮转调度是⼀种最古⽼，最简单，最公平且使⽤最⼴的算法，⼜称 RR(Round robin)调度。每个进程被分配⼀个时间段，称作它的时间⽚，即该进程允许运⾏的时间。 时间片为4，到期后切换下一个进程： 最短剩余时间优先最短剩余时间是针对最短进程优先增加了抢占机制的版本。在这种情况下，进程调度总是选择预期剩余时间最短的进程。当一个进程加入到就绪队列时，他可能比当前运行的进程具有更短的剩余时间，因此只要新进程就绪，调度程序就能可能抢占当前正在运行的进程。像最短进程优先一样，调度程序正在执行选择函数是必须有关于处理时间的估计，并且存在长进程饥饿的危险。 高响应比优先调度算法根据比率：R=(w+s)/s （R为响应比，w为等待处理的时间，s为预计的服务时间） 如果该进程被立即调用，则R值等于归一化周转时间（周转时间和服务时间的比率）。R最小值为1.0，只有第一个进入系统的进程才能达到该值。调度规则为：当前进程完成或被阻塞时，选择R值最大的就绪进程，它说明了进程的年龄。当偏向短作业时，长进程由于得不到服务，等待时间不断增加，从而增加比值，最终在竞争中赢了短进程。和最短进程优先、最短剩余时间优先一样，使用最高响应比策略需要估计预计服务时间。 ​ 高响应比优先调度算法主要用于作业调度，该算法是对FCFS调度算法和SJF调度算法的一种综合平衡，同时考虑每个作业的等待时间和估计的运行时间。在每次进行作业调度时，先计算后备作业队列中每个作业的响应比，从中选出响应比最高的作业投入运行。 根据公式可知： 当作业的等待时间相同时，则要求服务时间越短，其响应比越高，有利于短作业。 当要求服务时间相同时，作业的响应比由其等待时间决定，等待时间越长，其响应比越高，因而它实现的是先来先服务。 对于长作业，作业的响应比可以随等待时间的增加而提高，当其等待时间足够长时，其响应比便可升到很高，从而也可获得处理机。克服了饥饿状态，兼顾了长作业。 多级反馈队列调度算法前⾯介绍的⼏种进程调度的算法都有⼀定的局限性。如短进程优先的调度算法，仅照顾了短进程⽽忽略了⻓进程 。多级反馈队列调度算法既能使⾼优先级的作业得到响应⼜能使短作业（进程）迅速完成。，因⽽它是⽬前被公认的⼀种较好的进程调度算法，UNIX 操作系统采取的便是这种调度算法。 多级反馈队列调度算法的实现思想如下： 应设置多个就绪队列，并为各个队列赋予不同的优先级，第1级队列的优先级最高，第2级队列次之，其余队列的优先级逐次降低。 赋予各个队列中进程执行时间片的大小也各不相同，在优先级越高的队列中，每个进程的运行时间片就越小。例如，第2级队列的时间片要比第1级队列的时间片长一倍， ……第i+1级队列的时间片要比第i级队列的时间片长一倍。 当一个新进程进入内存后，首先将它放入第1级队列的末尾，按FCFS原则排队等待调度。当轮到该进程执行时，如它能在该时间片内完成，便可准备撤离系统；如果它在一个时间片结束时尚未完成，调度程序便将该进程转入第2级队列的末尾，再同样地按FCFS 原则等待调度执行；如果它在第2级队列中运行一个时间片后仍未完成，再以同样的方法放入第3级队列……如此下去，当一个长进程从第1级队列依次降到第 n 级队列后，在第 n 级队列中便釆用时间片轮转的方式运行。 仅当第1级队列为空时，调度程序才调度第2级队列中的进程运行；仅当第1 ~ (i-1)级队列均为空时，才会调度第i级队列中的进程运行。如果处理机正在执行第i级队列中的某进程时，又有新进程进入优先级较高的队列（第 1 ~ (i-1)中的任何一个队列），则此时新进程将抢占正在运行进程的处理机，即由调度程序把正在运行的进程放回到第i级队列的末尾，把处理机分配给新到的更高优先级的进程。 参考操作系统之进程调度算法","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"进程","slug":"进程","permalink":"https://xmmarlowe.github.io/tags/%E8%BF%9B%E7%A8%8B/"},{"name":"调度","slug":"调度","permalink":"https://xmmarlowe.github.io/tags/%E8%B0%83%E5%BA%A6/"}],"author":"Marlowe"},{"title":"Redis6.0 相关问题","slug":"NoSQL/Redis6-0 相关问题","date":"2021-05-07T04:55:58.000Z","updated":"2021-05-07T14:49:06.138Z","comments":true,"path":"2021/05/07/NoSQL/Redis6-0 相关问题/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/07/NoSQL/Redis6-0%20%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/","excerpt":"","text":"Redis6.0之前的版本真的是单线程吗？Redis在处理客户端的请求时，包括获取 (socket 读)、解析、执行、内容返回 (socket 写) 等都由一个顺序串行的主线程处理，这就是所谓的“单线程”。但如果严格来讲从Redis4.0之后并不是单线程，除了主线程外，它也有后台线程在处理一些较为缓慢的操作，例如清理脏数据、无用连接的释放、大 key 的删除等等。 Redis6.0之前为什么一直不使用多线程？官方曾做过类似问题的回复：使用Redis时，几乎不存在CPU成为瓶颈的情况， Redis主要受限于内存和网络。例如在一个普通的Linux系统上，Redis通过使用pipelining每秒可以处理100万个请求，所以如果应用程序主要使用O(N)或O(log(N))的命令，它几乎不会占用太多CPU。 使用了单线程后，可维护性高。多线程模型虽然在某些方面表现优异，但是它却引入了程序执行顺序的不确定性，带来了并发读写的一系列问题，增加了系统复杂度、同时可能存在线程切换、甚至加锁解锁、死锁造成的性能损耗。Redis通过AE事件模型以及IO多路复用等技术，处理性能非常高，因此没有必要使用多线程。单线程机制使得 Redis 内部实现的复杂度大大降低，Hash 的惰性 Rehash、Lpush 等等 “线程不安全” 的命令都可以无锁进行。 Redis6.0 之后为何引入了多线程？Redis6.0 引入多线程主要是为了提高网络 IO 读写性能，因为这个算是 Redis 中的一个性能瓶颈（Redis 的瓶颈主要受限于内存和网络）。 虽然，Redis6.0 引入了多线程，但是 Redis 的多线程只是在网络数据的读写这类耗时操作上使用了， 执行命令仍然是单线程顺序执行。因此，你也不需要担心线程安全问题。 Redis将所有数据放在内存中，内存的响应时长大约为100纳秒，对于小数据包，Redis服务器可以处理80,000到100,000 QPS，这也是Redis处理的极限了，对于80%的公司来说，单线程的Redis已经足够使用了。 但随着越来越复杂的业务场景，有些公司动不动就上亿的交易量，因此需要更大的QPS。常见的解决方案是在分布式架构中对数据进行分区并采用多个服务器，但该方案有非常大的缺点，例如要管理的Redis服务器太多，维护代价大；某些适用于单个Redis服务器的命令不适用于数据分区；数据分区无法解决热点读/写问题；数据偏斜，重新分配和放大/缩小变得更加复杂等等。 从Redis自身角度来说，因为读写网络的read/write系统调用占用了Redis执行期间大部分CPU时间，瓶颈主要在于网络的 IO 消耗, 优化主要有两个方向: 提高网络 IO 性能，典型的实现比如使用 DPDK 来替代内核网络栈的方式 使用多线程充分利用多核，典型的实现比如 Memcached。 协议栈优化的这种方式跟 Redis 关系不大，支持多线程是一种最有效最便捷的操作方式。所以总结起来，redis支持多线程主要就是两个原因： 可以充分利用服务器 CPU 资源，目前主线程只能利用一个核 多线程任务可以分摊 Redis 同步 IO 读写负荷 Redis6.0默认是否开启了多线程？Redis6.0 的多线程默认是禁用的，只使用主线程。 如需开启需要修改 redis 配置文件 redis.conf ： 1io-threads-do-reads yes Redis6.0多线程开启时，线程数如何设置？开启多线程后，还需要设置线程数，否则是不生效的。同样需要修改 redis 配置文件 redis.conf : 1io-threads 4 #官网建议4核的机器建议设置为2或3个线程，8核的建议设置为6个线程 Redis6.0多线程的实现机制？ 流程简述如下： 1、主线程负责接收建立连接请求，获取 socket 放入全局等待读处理队列 2、主线程处理完读事件之后，通过 RR(Round Robin) 将这些连接分配给这些 IO 线程 3、主线程阻塞等待 IO 线程读取 socket 完毕 4、主线程通过单线程的方式执行请求命令，请求数据读取并解析完成，但并不执行 5、主线程阻塞等待 IO 线程将数据回写 socket 完毕 6、解除绑定，清空等待队列 该设计有如下特点： 1、IO 线程要么同时在读 socket，要么同时在写，不会同时读或写 2、IO 线程只负责读写 socket 解析命令，不负责命令处理 开启多线程后，是否会存在线程并发安全问题？从上面的实现机制可以看出，Redis的多线程部分只是用来处理网络数据的读写和协议解析，执行命令仍然是单线程顺序执行。所以我们不需要去考虑控制 key、lua、事务，LPUSH/LPOP 等等的并发及线程安全问题。 Redis6.0的多线程和Memcached多线程模型进行对比前些年memcached 是各大互联网公司常用的缓存方案，因此redis 和 memcached 的区别基本成了面试官缓存方面必问的面试题，最近几年memcached用的少了，基本都是 redis。不过随着Redis6.0加入了多线程特性，类似的问题可能还会出现，接下来我们只针对多线程模型来简单比较一下。 如上图所示：Memcached 服务器采用 master-woker 模式进行工作，服务端采用 socket 与客户端通讯。主线程、工作线程 采用 pipe管道进行通讯。主线程采用 libevent 监听 listen、accept 的读事件，事件响应后将连接信息的数据结构封装起来，根据算法选择合适的工作线程，将连接任务携带连接信息分发出去，相应的线程利用连接描述符建立与客户端的socket连接 并进行后续的存取数据操作。 Redis6.0与Memcached多线程模型对比： 相同点： 都采用了 master线程-worker 线程的模型 不同点： Memcached 执行主逻辑也是在 worker 线程里，模型更加简单，实现了真正的线程隔离，符合我们对线程隔离的常规理解。而 Redis 把处理逻辑交还给 master 线程，虽然一定程度上增加了模型复杂度，但也解决了线程并发安全等问题。 Redis线程中经常提到IO多路复用，如何理解？这是IO模型的一种，即经典的Reactor设计模式，有时也称为异步阻塞IO。 多路指的是多个socket连接，复用指的是复用一个线程。多路复用主要有三种技术：select，poll，epoll。epoll是最新的也是目前最好的多路复用技术。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络IO的时间消耗），且Redis在内存中操作数据的速度非常快（内存内的操作不会成为这里的性能瓶颈），主要以上两点造就了Redis具有很高的吞吐量。 参考Redis 6.0 新特性-多线程连环13问！","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"},{"name":"多线程","slug":"多线程","permalink":"https://xmmarlowe.github.io/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"author":"Marlowe"},{"title":"Redis 单线程模型","slug":"NoSQL/Redis-单线程模型","date":"2021-05-06T14:44:45.000Z","updated":"2021-05-07T05:20:51.279Z","comments":true,"path":"2021/05/06/NoSQL/Redis-单线程模型/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/06/NoSQL/Redis-%E5%8D%95%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"Redis单线程模型文件事件处理器 Redis基于Reactor模式开发了网络事件处理器,这个处理器就叫做文件事件处理器(file event handler).这个文件事件处理器是单线程的,所以Redis才叫做单线程的模型,文件事件处理器采用了IO多路复用机制同时监听多个socket,根据socket上的事件来选择对应的事件处理器来处理这个事件; 如果被监听的socket准备好执行accept,read,write,close等事件/操作的时候,跟事件/操作对应的文件事件就会产生,这个时候文件事件处理器就会调用之前关联好的事件处理器来处理这个事件; 文件事件处理器是单线程模式运行的,但是通过IO多路复用机制监听多个socket,可以实现高性能的网络通信模型.又可以跟内部其他单线程的模块进行对接,保证了Redis内部的线程模型的简单性; 文件事件处理器的结构包含4个部分 ①.多个socket;②.IO多路复用程序;③.文件事件分派器;④.事件处理器(命令请求处理器,命令回复处理器,连接应答处理器等等); 多个socket可能并发的产生不同的操作,每个操作对应不同的文件事件,但是IO多路复用程序会监听多个socket,会将socket放入一个队列中排队,然后每次从队列中取出一个socket给事件分派器,事件分派器再把socket分派给对应的事件处理器去处理; 当一个socket的事件被处理完之后,IO多路复用程序才会将队列中的下一个socket取出交给事件分派器.文件事件分派器再根据socket当前产生的事件来选择对应的事件处理器来处理; 文件事件 当socket变得可读时(比如客户端对Redis执行write操作或者close操作),或者有新的可以应答的socket出现时(客户端对Redis执行connect操作),socket就会产生一个”AE_READABLE”事件; 当socket变得可写的时候(客户端对Redis执行read操作),socket就会产生一个”AE_WRITABLE”事件; IO多路复用程序可以同时监听”AE_READABLE”和”AE_WRITABLE”两种事件,要是一个socket同时产生了”AE_READABLE”和”AE_WRITABLE”两种事件,那么文件事件分派器会优先处理”AE_READABLE”事件,然后才是”AE_WRITABLE”事件; 常用的文件事件处理器 如果是客户端要连接Redis,那么会为socket关联连接应答处理器; 如果是客户端要写数据到Redis,那么会为socket关联命令请求处理器; 如果是客户端要从Redis中读取数据(Redis发送数据给客户端),那么会为socket关联命令回复处理器; 客户端与Redis通信的一次流程如图: 说明: ①.在Redis启动及初始化的时候,Redis会(预先)将连接应答处理器跟”AE_READABLE”事件关联起来,接着如果一个客户端向Redis发起连接,此时就会产生一个”AE_READABLE”事件,然后由连接应答处理器来处理跟客户端建立连接,创建客户端对应的socket,同时将这个socket的”AE_READABLE”事件跟命令请求处理器关联起来;②.当客户端向Redis发起请求的时候(不管是读请求还是写请求,都一样),首先就会在之前创建的客户端对应的socket上产生一个”AE_READABLE”事件,然后IO多路复用程序会监听到在之前创建的客户端对应的socket上产生了一个”AE_READABLE”事件,接着把这个socket放入一个队列中排队,然后由文件事件分派器从队列中获取socket交给对应的命令请求处理器来处理(因为之前在Redis启动并进行初始化的时候就已经预先将”AE_READABLE”事件跟命令请求处理器关联起来了).之后命令请求处理器就会从之前创建的客户端对应的socket中读取请求相关的数据,然后在自己的内存中进行执行和处理;③.当客户端请求处理完成,Redis这边也准备好了给客户端的响应数据之后,就会(预先)将socket的”AE_WRITABLE”事件跟命令回复处理器关联起来,当客户端这边准备好读取响应数据时,就会在之前创建的客户端对应的socket上产生一个”AE_WRITABLE”事件,然后IO多路复用程序会监听到在之前创建的客户端对应的socket上产生了一个”AE_WRITABLE”事件,接着把这个socket放入一个队列中排队,然后由文件事件分派器从队列中获取socket交给对应的命令回复处理器来处理(因为之前在Redis这边准备好给客户端的响应数据之后就已经预先将”AE_WRITABLE”事件跟命令回复处理器关联起来了),之后命令回复处理器就会向之前创建的客户端对应的socket输出/写入准备好的响应数据,最终返回给客户端,供客户端来读取;④.当命令回复处理器将准备好的响应数据写完之后,就会删除之前创建的客户端对应的socket上的”AE_WRITABLE”事件和命令回复处理器的关联关系; 为什么Redis单线程模型也能效率这么高? 纯内存操作; 核心是基于非阻塞的IO多路复用机制; 底层使用C语言实现,一般来说,C 语言实现的程序”距离”操作系统更近,执行速度相对会更快; 单线程同时也避免了多线程的上下文频繁切换问题,预防了多线程可能产生的竞争问题; 参考Redis单线程模型","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"}],"author":"Marlowe"},{"title":"Linux常用命令总结","slug":"操作系统/Linux常用命令总结","date":"2021-05-06T14:20:52.000Z","updated":"2021-06-01T09:01:17.947Z","comments":true,"path":"2021/05/06/操作系统/Linux常用命令总结/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/06/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/","excerpt":"","text":"目录切换命令 cd usr： 切换到该目录下 usr 目录 cd ..（或cd../）： 切换到上一层目录 cd /： 切换到系统根目录 cd ~： 切换到用户主目录 cd -： 切换到上一个操作所在目录 目录的操作命令(增删改查) mkdir 目录名称： 增加目录。 ls/ll（ll 是 ls -l 的别名，ll 命令可以看到该目录下的所有目录和文件的详细信息）：查看目录信息。 find 目录 参数： 寻找目录（查）。示例：① 列出当前目录及子目录下所有文件和文件夹: find .；② 在/home目录下查找以.txt 结尾的文件名:find /home -name “.txt” ,忽略大小写: find /home -iname “.txt” ；③ 当前目录及子目录下查找所有以.txt 和.pdf 结尾的文件:find . ( -name “.txt” -o -name “.pdf” )或find . -name “.txt” -o -name “.pdf”。 mv 目录名称 新目录名称： 修改目录的名称（改）。注意：mv 的语法不仅可以对目录进行重命名而且也可以对各种文件，压缩包等进行 重命名的操作。mv 命令用来对文件或目录重新命名，或者将文件从一个目录移到另一个目录中。后面会介绍到 mv 命令的另一个用法。 mv 目录名称 目录的新位置： 移动目录的位置—剪切（改）。注意：mv 语法不仅可以对目录进行剪切操作，对文件和压缩包等都可执行剪切操作。另外 mv 与 cp 的结果不同，mv 好像文件“搬家”，文件个数并未增加。而 cp 对文件进行复制，文件个数增加了。 cp -r 目录名称 目录拷贝的目标位置： 拷贝目录（改），-r 代表递归拷贝 。注意：cp 命令不仅可以拷贝目录还可以拷贝文件，压缩包等，拷贝文件和压缩包时不 用写-r 递归。 rm [-rf] 目录: 删除目录（删）。注意：rm 不仅可以删除目录，也可以删除其他文件或压缩包，为了增强大家的记忆， 无论删除任何目录或文件，都直接使用rm -rf 目录/文件/压缩包。 文件的操作命令(增删改查) touch 文件名称: 文件的创建（增）。 cat/more/less/tail 文件名称： 文件的查看（查） 。命令 tail -f 文件 可以对某个文件进行动态监控，例如 tomcat 的日志文件， 会随着程序的运行，日志会变化，可以使用 tail -f catalina-2016-11-11.log 监控 文 件的变化 。 vim 文件： 修改文件的内容（改）。vim 编辑器是 Linux 中的强大组件，是 vi 编辑器的加强版，vim 编辑器的命令和快捷方式有很多，但此处不一一阐述，大家也无需研究的很透彻，使用 vim 编辑修改文件的方式基本会使用就可以了。在实际开发中，使用 vim 编辑器主要作用就是修改配置文件，下面是一般步骤： vim 文件——&gt;进入文件—–&gt;命令模式——&gt;按i进入编辑模式—–&gt;编辑文件 ——-&gt;按Esc进入底行模式—–&gt;输入：wq/q! （输入 wq 代表写入内容并退出，即保存；输入 q!代表强制退出不保存）。 rm -rf 文件： 删除文件（删）。 压缩文件的操作命令打包并压缩文件Linux 中的打包文件一般是以.tar 结尾的，压缩的命令一般是以.gz 结尾的。而一般情况下打包和压缩是一起进行的，打包并压缩后的文件的后缀名一般.tar.gz。 命令：tar -zcvf 打包压缩后的文件名 要打包压缩的文件 ，其中： z：调用 gzip 压缩命令进行压缩 c：打包文件 v：显示运行过程 f：指定文件名 比如：假如 test 目录下有三个文件分别是：aaa.txt bbb.txt ccc.txt，如果我们要打包 test 目录并指定压缩后的压缩包名称为 test.tar.gz 可以使用命令：tar -zcvf test.tar.gz aaa.txt bbb.txt ccc.txt 或 tar -zcvf test.tar.gz /test/ 解压压缩包命令：tar [-xvf] 压缩文件 其中：x：代表解压 示例： 将 /test 下的 test.tar.gz 解压到当前目录下可以使用命令：tar -xvf test.tar.gz 将 /test 下的 test.tar.gz 解压到根目录/usr 下:tar -xvf test.tar.gz -C /usr（- C 代表指定解压的位置） Linux 的权限命令操作系统中每个文件都拥有特定的权限、所属用户和所属组。权限是操作系统用来限制资源访问的机制，在 Linux 中权限一般分为读(readable)、写(writable)和执行(excutable)，分为三组。分别对应文件的属主(owner)，属组(group)和其他用户(other)，通过这样的机制来限制哪些用户、哪些组可以对特定的文件进行什么样的操作。 通过 ls -l 命令我们可以 查看某个目录下的文件或目录的权限 示例：在随意某个目录下ls -l 第一列的内容的信息解释如下： 下面将详细讲解文件的类型、Linux 中权限以及文件有所有者、所在组、其它组具体是什么？ 文件的类型： d： 代表目录 -： 代表文件 l： 代表软链接（可以认为是 window 中的快捷方式） Linux 中权限分为以下几种： r：代表权限是可读，r 也可以用数字 4 表示 w：代表权限是可写，w 也可以用数字 2 表示 x：代表权限是可执行，x 也可以用数字 1 表示 文件和目录权限的区别： 对文件和目录而言，读写执行表示不同的意义。 对于文件： 权限名称 可执行操作 r 可以使用 cat 查看文件的内容 w 可以修改文件的内容 x 可以将其运行为二进制文件 对于目录：|权限名称|可执行操作||:–:|:–:||r|可以查看目录下列表||w|可以创建和删除目录下文件||x|可以使用 cd 进入目录| 需要注意的是： 超级用户可以无视普通用户的权限，即使文件目录权限是 000，依旧可以访问。 在 linux 中的每个用户必须属于一个组，不能独立于组外。在 linux 中每个文件有所有者、所在组、其它组的概念。 所有者(u)： 一般为文件的创建者，谁创建了该文件，就天然的成为该文件的所有者，用 ls ‐ahl 命令可以看到文件的所有者 也可以使用 chown 用户名 文件名来修改文件的所有者 。 文件所在组(g)： 当某个用户创建了一个文件后，这个文件的所在组就是该用户所在的组用 ls ‐ahl命令可以看到文件的所有组也可以使用 chgrp 组名 文件名来修改文件所在的组。 其它组(o)： 除开文件的所有者和所在组的用户外，系统的其它用户都是文件的其它组。 修改文件/目录的权限的命令：chmod 示例：修改/test 下的 aaa.txt 的权限为文件所有者有全部权限，文件所有者所在的组有读写权限，其他用户只有读的权限。 chmod u=rwx,g=rw,o=r aaa.txt 或者 chmod 764 aaa.txt 补充一个比较常用的东西: 假如我们装了一个 zookeeper，我们每次开机到要求其自动启动该怎么办？ 新建一个脚本 zookeeper为新建的脚本 zookeeper 添加可执行权限，命令是:chmod +x zookeeper把 zookeeper 这个脚本添加到开机启动项里面，命令是：chkconfig –add zookeeper如果想看看是否添加成功，命令是：chkconfig –list Linux 用户管理Linux 系统是一个多用户多任务的分时操作系统，任何一个要使用系统资源的用户，都必须首先向系统管理员申请一个账号，然后以这个账号的身份进入系统。 用户的账号一方面可以帮助系统管理员对使用系统的用户进行跟踪，并控制他们对系统资源的访问；另一方面也可以帮助用户组织文件，并为用户提供安全性保护。 Linux 用户管理相关命令: useradd 选项 用户名:添加用户账号 userdel 选项 用户名:删除用户帐号 usermod 选项 用户名:修改帐号 passwd 用户名:更改或创建用户的密码 passwd -S 用户名 :显示用户账号密码信息 passwd -d 用户名: 清除用户密码 useradd 命令用于 Linux 中创建的新的系统用户。useradd可用来建立用户帐号。帐号建好之后，再用passwd设定帐号的密码．而可用userdel删除帐号。使用useradd指令所建立的帐号，实际上是保存在 /etc/passwd文本文件中。 passwd命令用于设置用户的认证信息，包括用户密码、密码过期时间等。系统管理者则能用它管理系统用户的密码。只有管理者可以指定用户名称，一般用户只能变更自己的密码。 Linux 系统用户组的管理每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理。不同 Linux 系统对用户组的规定有所不同，如 Linux 下的用户属于与它同名的用户组，这个用户组在创建用户时同时创建。 用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对/etc/group文件的更新。 Linux 系统用户组的管理相关命令: groupadd 选项 用户组 :增加一个新的用户组 groupdel 用户组:要删除一个已有的用户组 groupmod 选项 用户组 : 修改用户组的属性 其他常用命令 pwd： 显示当前所在位置 sudo + 其他命令：以系统管理者的身份执行指令，也就是说，经由 sudo 所执行的指令就好像是 root 亲自执行。 awk、sed、grep更适合的方向： grep 更适合单纯的查找或匹配文本 sed 更适合编辑匹配到的文本 awk 更适合格式化文本，对文本进行较复杂格式处理 grep 要搜索的字符串 要搜索的文件 –color： 搜索命令，–color 代表高亮显示 sed 1sed [-hnV][-e&lt;script&gt;][-f&lt;script文件&gt;][文本文件] awk 123awk [选项参数] &#x27;script&#x27; var=value file(s)或awk [选项参数] -f scriptfile var=value file(s) ps -ef/ps -aux： 这两个命令都是查看当前系统正在运行进程，两者的区别是展示格式不同。如果想要查看特定的进程可以使用这样的格式：ps aux|grep redis （查看包括 redis 字符串的进程），也可使用 pgrep redis -a。 注意：如果直接用 ps（（Process Status））命令，会显示所有进程的状态，通常结合 grep 命令查看某进程的状态。 kill -9 进程的pid： 杀死进程（-9 表示强制终止。）先用 ps 查找进程，然后用 kill 杀掉 网络通信命令： 查看当前系统的网卡信息：ifconfig 查看与某台机器的连接情况：ping 查看当前系统的端口使用：netstat -an 查看磁盘信息命令： df -hl：查看磁盘剩余空间 df -h：查看每个根路径的分区大小 du -sh [目录名]：返回该目录的大小 du -sm [文件夹]：返回该文件夹总M数 du -h [目录名]：查看指定文件夹下的所有文件大小（包含子文件夹） du(disk usage)df 以磁盘分区为单位查看文件系统，可以获取硬盘被占用了多少空间，目前还剩下多少空间等信息。 net-tools 和 iproute2 ： net-tools起源于 BSD 的 TCP/IP 工具箱，后来成为老版本 LinuxLinux 中配置网络功能的工具。但自 2001 年起，Linux 社区已经对其停止维护。同时，一些 Linux 发行版比如 Arch Linux 和 CentOS/RHEL 7 则已经完全抛弃了 net-tools，只支持iproute2。linux ip 命令类似于 ifconfig，但功能更强大，旨在替代它。更多详情请阅读如何在 Linux 中使用 IP 命令和示例 shutdown： shutdown -h now： 指定现在立即关机；shutdown +5 “System will shutdown after 5 minutes”：指定 5 分钟后关机，同时送出警告信息给登入用户。 reboot： reboot： 重开机。reboot -w： 做个重开机的模拟（只有纪录并不会真的重开机）。 参考Linux 基本命令","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://xmmarlowe.github.io/tags/Linux/"},{"name":"命令","slug":"命令","permalink":"https://xmmarlowe.github.io/tags/%E5%91%BD%E4%BB%A4/"}],"author":"Marlowe"},{"title":"进程状态与僵尸进程、孤儿进程","slug":"操作系统/进程状态与僵尸进程、孤儿进程","date":"2021-05-06T12:18:12.000Z","updated":"2021-05-07T05:20:42.945Z","comments":true,"path":"2021/05/06/操作系统/进程状态与僵尸进程、孤儿进程/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/06/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E7%8A%B6%E6%80%81%E4%B8%8E%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B%E3%80%81%E5%AD%A4%E5%84%BF%E8%BF%9B%E7%A8%8B/","excerpt":"","text":"进程状态一个进程的生命周期可以划分为一组状态，这些状态刻画了整个进程。进程状态即体现一个进程的生命状态 一般来说，进程有五种状态： 创建状态： 进程在创建时需要申请一个空白PCB，向其中填写控制和管理进程的信息，完成资源分配。如果创建工作无法完成，比如资源无法满足，就无法被调度运行，把此时进程所处状态称为创建状态 就绪状态： 进程已经准备好，已分配到所需资源，只要分配到CPU就能够立即运行 执行状态： 进程处于就绪状态被调度后，进程进入执行状态 阻塞状态： 正在执行的进程由于某些事件（I/O请求，申请缓存区失败）而暂时无法运行，进程受到阻塞。在满足请求时进入就绪状态等待系统调用 终止状态： 进程结束，或出现错误，或被系统终止，进入终止状态。无法再执行 这五种状态的转换如图： 上面所说的是一个概念性质的，而具体在Linux里，进程的状态是如何定义的？在Linux内核里，进程有时候也叫做任务，下面是状态在kernel源码里的定义： 123456789101112131415/** The task state array is a strange &quot;bitmap&quot; of* reasons to sleep. Thus &quot;running&quot; is zero, and* you can test for combinations of others with* simple bit tests.*/static const char * const task_state_array[] = &#123;&quot;R (running)&quot;, /* 0 */&quot;S (sleeping)&quot;, /* 1 */&quot;D (disk sleep)&quot;, /* 2 */&quot;T (stopped)&quot;, /* 4 */&quot;t (tracing stop)&quot;, /* 8 */&quot;X (dead)&quot;, /* 16 */&quot;Z (zombie)&quot;, /* 32 */&#125;; 这些状态的具体含义是： R运行状态(running): 并不意味着进程一定在运行中，它表明进程要么是在运行中要么在运行队列 里。 S睡眠状态(sleeping): 意味着进程在等待事件完成（这里的睡眠有时候也叫做可中断睡眠 (interruptible sleep))。 D磁盘休眠状态(Disk sleep): 有时候也叫不可中断睡眠状态（uninterruptible sleep），在这个状态的 进程通常会等待IO的结束。 T停止状态(stopped)： 可以通过发送 SIGSTOP 信号给进程来停止（T）进程。这个被暂停的进程可 以通过发送 SIGCONT 信号让进程继续运行。 X死亡状态(dead)： 这个状态只是一个返回状态，你不会在任务列表里看到这个状态。 Z僵死状态(zombie)： 下文具体了解 父进程与子进程在学习接下来的内容之前，需要对父进程和子进程有一个清晰的认识 在Linux里，除了进程0（即PID=0的进程）以外的所有进程都是由其他进程使用系统调用fork创建的，这里调用fork创建新进程的进程即为父进程，而相对应的为其创建出的进程则为子进程，因而除了进程0以外的进程都只有一个父进程，但一个进程可以有多个子进程。 fork函数包含在unistd.h库中，其最主要的特点是，调用一次，返回两次，当父进程fork()创建子进程失败时，fork()返回-1，当父进程fork()创建子进程成功时，此时，父进程会返回子进程的pid，而子进程返回的是0。所以可以根据返回值的不同让父进程和子进程执行不同的代码 如上图所示，当fork()函数调用后，父进程中的变量pid赋值成子进程的pid(pid&gt;0)，所以父进程会执行else里的代码，打印出”This is the parent”，而子进程的变量pid赋值成0，所以子进程执行if(pid == 0)里的代码，打印出”This is the child” 现在我们知道，在Linux中，正常情况下，子进程是通过父进程创建的，子进程再创建新的子进程。但是子进程的结束和父进程的运行是一个异步过程，即父进程永远无法预测子进程到底什么时候结束。当一个进程完成它的工作终止之后，它的父进程需要调用wait()或者waitpid()系统调用取得子进程的终止状态 知道了这些，我们再来了解两种特殊的进程。 僵尸进程简介一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵死进程。 上文中提到的进程的僵死状态Z(zombie)就是僵尸进程对应的状态 我们可以写一个程序来查看一下僵尸进程： 1234567891011121314151617181920212223242526#include&lt;stdio.h&gt;#include&lt;unistd.h&gt;#include&lt;stdlib.h&gt;int main()&#123; printf(&quot;pid = %d\\n&quot;,getpid()); pid_t pid = fork(); if(pid &lt; 0)&#123; printf(&quot;fork error\\n&quot;); return -1; &#125;else if(pid == 0)&#123; //这段代码只有子进程能够运行到，因为在子进程中fork的返回值为0 printf(&quot;This is the child!pid = %d\\n&quot;,getpid()); sleep(5); exit(0); //退出进程 &#125;else if(pid &gt; 0)&#123; //这段代码只有父进程能运行到 printf(&quot;This is the parent!pid = %d\\n&quot;,getpid()); &#125; //当fork成功时下面的代码父子进程都会运行到 while(1)&#123; printf(&quot;-------------pid = %d\\n&quot;,getpid()); sleep(1); &#125; return 0;&#125; 程序的运行结果： 12345678ubuntu@VM-0-7-ubuntu:~/c_practice$ ./zombie pid = 24816This is the parent!pid = 24816-------------pid = 24816This is the child!pid = 24817-------------pid = 24816-------------pid = 24816..... 在程序开始运行时立即查看进程： (这里我分别运行了两次，分别使用ps -ef和ps -aux查看了进程状态，所以两次的进程PID是不同的) 1234567ubuntu@VM-0-7-ubuntu:~$ ps -ef | grep -v grep | grep zombieubuntu 23797 15818 0 14:53 pts/0 00:00:00 ./zombieubuntu 23798 23797 0 14:53 pts/0 00:00:00 ./zombieubuntu@VM-0-7-ubuntu:~$ ps -aux | grep -v grep | grep zombieubuntu 24288 0.0 0.0 4352 648 pts/0 S+ 14:56 0:00 ./zombieubuntu 24289 0.0 0.0 4352 80 pts/0 S+ 14:56 0:00 ./zombie 在进程运行五秒后再次查看进程： 1234567ubuntu@VM-0-7-ubuntu:~$ ps -ef | grep -v grep | grep zombieubuntu 23797 15818 0 14:53 pts/0 00:00:00 ./zombieubuntu 23798 23797 0 14:53 pts/0 00:00:00 [zombie] &lt;defunct&gt;ubuntu@VM-0-7-ubuntu:~$ ps -aux | grep -v grep | grep zombieubuntu 24288 0.0 0.0 4352 648 pts/0 S+ 14:56 0:00 ./zombieubuntu 24289 0.0 0.0 0 0 pts/0 Z+ 14:56 0:00 [zombie] &lt;defunct&gt; 可以看出当进程运行五秒后，子进程状态变成Z，就是僵死状态，子进程就成了僵尸进程 其实，僵尸进程是有危害的。进程的退出状态必须被维持下去，因为它要告诉关心它的进程（父进程），你交给我的任务，我办的怎么样了。可父进程如果一直不读取，那子进程就一直处于Z状态。维护退出状态本身就是要用数据维护，也属于进程基本信息，所以保存在task_struct(PCB)中，换句话说，当一个进程一直处于Z状态，那么它的PCB也就一直都要被维护。因为PCB本身就是一个结构体会占用空间，僵尸进程也就会造成资源浪费，所以我们应该避免僵尸进程的产生。 孤儿进程简介孤儿进程则是指当一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。 代码示例： 123456789101112131415161718192021222324#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;unistd.h&gt;#include&lt;errno.h&gt;int main()&#123; pid_t pid; pid = fork(); if(pid &lt; 0)&#123; perror(&quot;fork error&quot;); exit(1); &#125; if(pid == 0)&#123; printf(&quot;This is the child!\\n&quot;); printf(&quot;pid = %d,ppid = %d\\n&quot;,getpid(),getppid());//父进程退出前的pid和ppid sleep(5); printf(&quot;\\npid = %d,ppid = %d\\n&quot;,getpid(),getppid());//父进程退出后的pid和ppid &#125;else&#123; printf(&quot;This is the father!\\n&quot;); sleep(1); printf(&quot;father process is exited!\\n&quot;); &#125; return 0;&#125; 运行结果： 1234567ubuntu@VM-0-7-ubuntu:~/c_practice$ ./orphan This is the father!This is the child!pid = 2338,ppid = 2337father process is exited!ubuntu@VM-0-7-ubuntu:~/c_practice$ pid = 2338,ppid = 1 我们可以看到结果和我们预见的是一样的，孤儿进程在父进程退出后会被init进程领养，直到自己运行结束为止。这个程序很容易理解,先输出子进程的pid和父进程的pid，再然后子进程开始睡眠父进程退出，这时候子进程变成孤儿进程，再次输出时，该进程的父进程变为init 孤儿进程由于有init进程循环的wait()回收资源，因此并没有什么危害 问题及危害僵尸进程unix提供了一种机制可以保证只要父进程想知道子进程结束时的状态信息， 就可以得到。这种机制就是: 在每个进程退出的时候,内核释放该进程所有的资源,包括打开的文件,占用的内存等。 但是仍然为其保留一定的信息(包括进程号the process ID,退出状态the termination status of the process,运行时间the amount of CPU time taken by the process等)。直到父进程通过wait / waitpid来取时才释放。 但这样就导致了问题，如果进程不调用wait / waitpid的话， 那么保留的那段信息就不会释放，其进程号就会一直被占用，但是系统所能使用的进程号是有限的，如果大量的产生僵死进程，将因为没有可用的进程号而导致系统不能产生新的进程. 此即为僵尸进程的危害，应当避免。 孤儿进程孤儿进程是没有父进程的进程，孤儿进程这个重任就落到了init进程身上， init进程就好像是一个民政局，专门负责处理孤儿进程的善后工作。每当出现一个孤儿进程的时候，内核就把孤 儿进程的父进程设置为init，而init进程会循环地wait()它的已经退出的子进程。这样，当一个孤儿进程凄凉地结束了其生命周期的时候，init进程就会代表党和政府出面处理它的一切善后工作。因此孤儿进程并不会有什么危害。 任何一个子进程(init除外)在exit()之后，并非马上就消失掉，而是留下一个称为僵尸进程(Zombie)的数据结构，等待父进程处理。 这是每个子进程在结束时都要经过的阶段。如果子进程在exit()之后，父进程没有来得及处理，这时用ps命令就能看到子进程的状态是“Z”。如果父进程能及时 处理，可能用ps命令就来不及看到子进程的僵尸状态，但这并不等于子进程不经过僵尸状态。 如果父进程在子进程结束之前退出，则子进程将由init接管。init将会以父进程的身份对僵尸状态的子进程进行处理。 僵尸进程危害场景 例如有个进程，它定期的产 生一个子进程，这个子进程需要做的事情很少，做完它该做的事情之后就退出了，因此这个子进程的生命周期很短，但是，父进程只管生成新的子进程，至于子进程 退出之后的事情，则一概不闻不问，这样，系统运行上一段时间之后，系统中就会存在很多的僵死进程，倘若用ps命令查看的话，就会看到很多状态为Z的进程。 严格地来说，僵死进程并不是问题的根源，罪魁祸首是产生出大量僵死进程的那个父进程。因此，当我们寻求如何消灭系统中大量的僵死进程时，答案就是把产生大 量僵死进程的那个元凶枪毙掉（也就是通过kill发送SIGTERM或者SIGKILL信号啦）。枪毙了元凶进程之后，它产生的僵死进程就变成了孤儿进程，这些孤儿进程会被init进程接管，init进程会wait()这些孤儿进程，释放它们占用的系统进程表中的资源，这样，这些已经僵死的孤儿进程 就能瞑目而去了。 僵尸进程解决办法通过信号机制子进程退出时向父进程发送SIGCHILD信号，父进程处理SIGCHILD信号。在信号处理函数中调用wait进行处理僵尸进程。 代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;stdlib.h&gt;#include &lt;signal.h&gt;static void sig_child(int signo);int main()&#123; pid_t pid; //创建捕捉子进程退出信号 signal(SIGCHLD,sig_child); pid = fork(); if (pid &lt; 0) &#123; perror(&quot;fork error:&quot;); exit(1); &#125; else if (pid == 0) &#123; printf(&quot;I am child process,pid id %d.I am exiting.\\n&quot;,getpid()); exit(0); &#125; printf(&quot;I am father process.I will sleep two seconds\\n&quot;); //等待子进程先退出 sleep(2); //输出进程信息 system(&quot;ps -o pid,ppid,state,tty,command&quot;); printf(&quot;father process is exiting.\\n&quot;); return 0;&#125;static void sig_child(int signo)&#123; pid_t pid; int stat; //处理僵尸进程 while ((pid = waitpid(-1, &amp;stat, WNOHANG)) &gt;0) printf(&quot;child %d terminated.\\n&quot;, pid);&#125; 测试结果如下所示： fork两次原理是将子进程成为孤儿进程，从而其的父进程变为init进程，通过init进程可以处理僵尸进程。 代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;int main()&#123; pid_t pid; //创建第一个子进程 pid = fork(); if (pid &lt; 0) &#123; perror(&quot;fork error:&quot;); exit(1); &#125; //第一个子进程 else if (pid == 0) &#123; //子进程再创建子进程 printf(&quot;I am the first child process.pid:%d\\tppid:%d\\n&quot;,getpid(),getppid()); pid = fork(); if (pid &lt; 0) &#123; perror(&quot;fork error:&quot;); exit(1); &#125; //第一个子进程退出 else if (pid &gt;0) &#123; printf(&quot;first procee is exited.\\n&quot;); exit(0); &#125; //第二个子进程 //睡眠3s保证第一个子进程退出，这样第二个子进程的父亲就是init进程里 sleep(3); printf(&quot;I am the second child process.pid: %d\\tppid:%d\\n&quot;,getpid(),getppid()); exit(0); &#125; //父进程处理第一个子进程退出 if (waitpid(pid, NULL, 0) != pid) &#123; perror(&quot;waitepid error:&quot;); exit(1); &#125; exit(0); return 0;&#125; 测试结果如下图所示： 参考进程3.0——进程状态与僵尸进程、孤儿进程 孤儿进程与僵尸进程[总结]","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"进程","slug":"进程","permalink":"https://xmmarlowe.github.io/tags/%E8%BF%9B%E7%A8%8B/"},{"name":"僵尸进程","slug":"僵尸进程","permalink":"https://xmmarlowe.github.io/tags/%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B/"},{"name":"孤儿进程","slug":"孤儿进程","permalink":"https://xmmarlowe.github.io/tags/%E5%AD%A4%E5%84%BF%E8%BF%9B%E7%A8%8B/"}],"author":"Marlowe"},{"title":"Redis分布式锁","slug":"NoSQL/Redis分布式锁","date":"2021-05-06T08:55:54.000Z","updated":"2021-05-06T14:47:22.417Z","comments":true,"path":"2021/05/06/NoSQL/Redis分布式锁/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/06/NoSQL/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/","excerpt":"","text":"问题描述随着业务发展的需要，原单体单机部署的系统被演化成分布式集群系统后，由于分布式系统多线程、多进程并且分布在不同机器上，这将使原单机部署情况下的并发控制锁策略失效，单纯的Java API并不能提供分布式锁的能力。为了解决这个问题就需要一种跨JVM的互斥机制来控制共享资源的访问，这就是分布式锁要解决的问题！ 分布式锁主流的实现方案 基于数据库实现分布式锁 基于缓存（Redis等） 基于Zookeeper 每一种分布式锁解决方案都有各自的优缺点： 性能：redis最高 可靠性：zookeeper最高 使用redis实现分布式锁redis:命令1# set sku:1:info “OK” NX PX 10000 EX second：设置键的过期时间为 second 秒。 SET key value EX second 效果等同于 SETEX key second value 。PX millisecond：设置键的过期时间为 millisecond 毫秒。 SET key value PX millisecond 效果等同于 PSETEX key millisecond value 。NX：只在键不存在时，才对键进行设置操作。 SET key value NX 效果等同于 SETNX key value 。XX：只在键已经存在时，才对键进行设置操作。 多个客户端同时获取锁（setnx） 获取成功，执行业务逻辑{从db获取数据，放入缓存}，执行完成释放锁（del） 其他客户端等待重试 代码测试12345678910111213141516171819202122232425262728@GetMapping(&quot;testLock&quot;)public void testLock()&#123; //1获取锁，setne Boolean lock = redisTemplate.opsForValue().setIfAbsent(&quot;lock&quot;, &quot;111&quot;); //2获取锁成功、查询num的值 if(lock)&#123; Object value = redisTemplate.opsForValue().get(&quot;num&quot;); //2.1判断num为空return if(StringUtils.isEmpty(value))&#123; return; &#125; //2.2有值就转成成int int num = Integer.parseInt(value+&quot;&quot;); //2.3把redis的num加1 redisTemplate.opsForValue().set(&quot;num&quot;, ++num); //2.4释放锁，del redisTemplate.delete(&quot;lock&quot;); &#125;else&#123; //3获取锁失败、每隔0.1秒再获取 try &#123; Thread.sleep(100); testLock(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 重启，服务集群，通过网关压力测试：ab -n 1000 -c 100 http://192.168.140.1:8080/test/testLock 查看redis中num的值： 基本实现。问题：setnx刚好获取到锁，业务逻辑出现异常，导致锁无法释放解决：设置过期时间，自动释放锁。 优化之设置锁的过期时间设置过期时间有两种方式： 首先想到通过expire设置过期时间（缺乏原子性：如果在setnx和expire之间出现异常，锁也无法释放） 在set时指定过期时间（推荐） 设置过期时间： 问题： 可能会释放其他服务器的锁。 场景： 如果业务逻辑的执行时间是7s。执行流程如下 index1业务逻辑没执行完，3秒后锁被自动释放。 index2获取到锁，执行业务逻辑，3秒后锁被自动释放。 index3获取到锁，执行业务逻辑 index1业务逻辑执行完成，开始调用del释放锁，这时释放的是index3的锁，导致index3的业务只执行1s就被别人释放。最终等于没锁的情况。 解决： setnx获取锁时，设置一个指定的唯一值（例如：uuid）；释放前获取这个值，判断是否自己的锁 优化之UUID防误删 问题： 删除操作缺乏原子性。场景： index1执行删除时，查询到的lock值确实和uuid相等uuid=v1set(lock,uuid)； index1执行删除前，lock刚好过期时间已到，被redis自动释放在redis中没有了lock，没有了锁。 index2获取了lockindex2线程获取到了cpu的资源，开始执行方法uuid=v2set(lock,uuid)； index1执行删除，此时会把index2的lock删除index1 因为已经在方法中了，所以不需要重新上锁。index1有执行的权限。index1已经比较完成了，这个时候，开始执行 删除的index2的锁！ 优化之LUA脚本保证删除的原子性1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@GetMapping(&quot;testLockLua&quot;)public void testLockLua() &#123; //1 声明一个uuid ,将做为一个value 放入我们的key所对应的值中 String uuid = UUID.randomUUID().toString(); //2 定义一个锁：lua 脚本可以使用同一把锁，来实现删除！ String skuId = &quot;25&quot;; // 访问skuId 为25号的商品 100008348542 String locKey = &quot;lock:&quot; + skuId; // 锁住的是每个商品的数据 // 3 获取锁 Boolean lock = redisTemplate.opsForValue().setIfAbsent(locKey, uuid, 3, TimeUnit.SECONDS); // 第一种： lock 与过期时间中间不写任何的代码。 // redisTemplate.expire(&quot;lock&quot;,10, TimeUnit.SECONDS);//设置过期时间 // 如果true if (lock) &#123; // 执行的业务逻辑开始 // 获取缓存中的num 数据 Object value = redisTemplate.opsForValue().get(&quot;num&quot;); // 如果是空直接返回 if (StringUtils.isEmpty(value)) &#123; return; &#125; // 不是空 如果说在这出现了异常！ 那么delete 就删除失败！ 也就是说锁永远存在！ int num = Integer.parseInt(value + &quot;&quot;); // 使num 每次+1 放入缓存 redisTemplate.opsForValue().set(&quot;num&quot;, String.valueOf(++num)); /*使用lua脚本来锁*/ // 定义lua 脚本 String script = &quot;if redis.call(&#x27;get&#x27;, KEYS[1]) == ARGV[1] then return redis.call(&#x27;del&#x27;, KEYS[1]) else return 0 end&quot;; // 使用redis执行lua执行 DefaultRedisScript&lt;Long&gt; redisScript = new DefaultRedisScript&lt;&gt;(); redisScript.setScriptText(script); // 设置一下返回值类型 为Long // 因为删除判断的时候，返回的0,给其封装为数据类型。如果不封装那么默认返回String 类型， // 那么返回字符串与0 会有发生错误。 redisScript.setResultType(Long.class); // 第一个要是script 脚本 ，第二个需要判断的key，第三个就是key所对应的值。 redisTemplate.execute(redisScript, Arrays.asList(locKey), uuid); &#125; else &#123; // 其他线程等待 try &#123; // 睡眠 Thread.sleep(1000); // 睡醒了之后，调用方法。 testLockLua(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; Lua 脚本详解： 项目中正确使用： 定义key，key应该是为每个sku定义的，也就是每个sku有一把锁。String locKey =”lock:”+skuId; // 锁住的是每个商品的数据Boolean lock = redisTemplate.opsForValue().setIfAbsent(locKey, uuid,3,TimeUnit.SECONDS); 总结1、加锁 1234// 1. 从redis中获取锁,set k1 v1 px 20000 nxString uuid = UUID.randomUUID().toString();Boolean lock = this.redisTemplate.opsForValue() .setIfAbsent(&quot;lock&quot;, uuid, 2, TimeUnit.SECONDS); 2、使用lua释放锁 12345678// 2. 释放锁 delString script = &quot;if redis.call(&#x27;get&#x27;, KEYS[1]) == ARGV[1] then return redis.call(&#x27;del&#x27;, KEYS[1]) else return 0 end&quot;;// 设置lua脚本返回的数据类型DefaultRedisScript&lt;Long&gt; redisScript = new DefaultRedisScript&lt;&gt;();// 设置lua脚本返回类型为LongredisScript.setResultType(Long.class);redisScript.setScriptText(script);redisTemplate.execute(redisScript, Arrays.asList(&quot;lock&quot;),uuid); 3、重试 12Thread.sleep(500);testLock(); 为了确保分布式锁可用，我们至少要确保锁的实现同时满足以下四个条件： 互斥性。在任意时刻，只有一个客户端能持有锁。 不会发生死锁。即使有一个客户端在持有锁的期间崩溃而没有主动解锁，也能保证后续其他客户端能加锁。 解铃还须系铃人。加锁和解锁必须是同一个客户端，客户端自己不能把别人加的锁给解了。 加锁和解锁必须具有原子性。","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"}],"author":"Marlowe"},{"title":"URI和URL的区别是什么?","slug":"计算机网络/URI和URL的区别是什么","date":"2021-05-06T08:29:14.000Z","updated":"2021-05-06T14:47:22.421Z","comments":true,"path":"2021/05/06/计算机网络/URI和URL的区别是什么/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/06/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/URI%E5%92%8CURL%E7%9A%84%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88/","excerpt":"","text":"URI(Uniform Resource Identifier) 是统一资源标志符，可以唯一标识一个资源。 URL(Uniform Resource Location) 是统一资源定位符，可以提供该资源的路径。它是一种具体的 URI，即 URL 可以用来标识一个资源，而且还指明了如何 locate 这个资源。 URI的作用像身份证号一样，URL的作用更像家庭住址一样。URL是一种具体的URI，它不仅唯一标识资源，而且还提供了定位该资源的信息。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"URL","slug":"URL","permalink":"https://xmmarlowe.github.io/tags/URL/"},{"name":"URI","slug":"URI","permalink":"https://xmmarlowe.github.io/tags/URI/"}],"author":"Marlowe"},{"title":"Java学习推荐书籍","slug":"随笔/Java学习推荐书籍","date":"2021-05-05T09:09:18.000Z","updated":"2021-05-05T14:55:09.555Z","comments":true,"path":"2021/05/05/随笔/Java学习推荐书籍/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/05/%E9%9A%8F%E7%AC%94/Java%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E4%B9%A6%E7%B1%8D/","excerpt":"列举一些接下来要读的书单…","text":"列举一些接下来要读的书单… 总《Java编程思想》《Java核心技术》卷1卷2《Go语言实战》 or《Go In Action》《算法导论》 不适合初学者《算法》第四版 红色《TCP/IP详解》《计算机网络 自顶向下》《私房菜》 工具书《Unix环境高级编程》《Spring实战》《Spring Boot实战》《Spring技术内幕》 很难《MySQL必知必会》 涵盖 《SQL必知必会》的内容《高性能MySQL》《重构 改善既有代码的设计》 需要一些经验，拔高内容 参考bilibili Video 必读计算机编程好书推荐！程序员小伙搬出了他的书架！","categories":[{"name":"随笔","slug":"随笔","permalink":"https://xmmarlowe.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"学习","slug":"学习","permalink":"https://xmmarlowe.github.io/tags/%E5%AD%A6%E4%B9%A0/"},{"name":"书籍","slug":"书籍","permalink":"https://xmmarlowe.github.io/tags/%E4%B9%A6%E7%B1%8D/"}],"author":"Marlowe"},{"title":"海量数据下，如何快速查找一条记录？","slug":"NoSQL/海量数据下，如何快速查找一条记录？","date":"2021-05-04T14:27:41.000Z","updated":"2021-05-05T14:45:39.215Z","comments":true,"path":"2021/05/04/NoSQL/海量数据下，如何快速查找一条记录？/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/04/NoSQL/%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E4%B8%8B%EF%BC%8C%E5%A6%82%E4%BD%95%E5%BF%AB%E9%80%9F%E6%9F%A5%E6%89%BE%E4%B8%80%E6%9D%A1%E8%AE%B0%E5%BD%95%EF%BC%9F/","excerpt":"","text":"1、使用布隆过滤器，快速过滤不存在的记录。 使用Redis的bitmap结构来实现布隆过滤器。 2、在Redis中建立数据缓存。将我们对Redis使用场景的理解尽量表达出来。 以普通字符串的形式来存储，(userld -&gt; user.json)。 以一个hash来存储一条记录(userld key-&gt; username field-&gt; ，userAge-&gt;)。以一个整的hash来存储所有的数据，Userlnfo-&gt; field就用userld ，value就用user.jison。一个hash最多能支持2^32-1(40多个亿)个键值对。 缓存击穿: 对不存在的数据也建立key。这些key都是经过布隆过滤器过滤的，所以一般不会太多。 缓存过期: 将热点数据设置成永不过期，定期重建缓存。使用分布式锁重建缓存。 3、查询优化。 按槽位分配数据。 自己实现槽位计算，找到记录应该分配在哪台机器上，然后直接去目标机器上找。","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"}],"author":"Marlowe"},{"title":"Redis 如何配置Key的过期时间？他的实现原理是什么？","slug":"NoSQL/Redis-如何配置Key的过期时间？他的实现原理是什么？","date":"2021-05-04T13:11:54.000Z","updated":"2021-05-06T10:58:18.322Z","comments":true,"path":"2021/05/04/NoSQL/Redis-如何配置Key的过期时间？他的实现原理是什么？/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/04/NoSQL/Redis-%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AEKey%E7%9A%84%E8%BF%87%E6%9C%9F%E6%97%B6%E9%97%B4%EF%BC%9F%E4%BB%96%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/","excerpt":"","text":"Redis设置key的过期时间 EXPIRE SETEX 实现原理定期删除每隔一段时间， 执行一次删除过期key的操作。 懒汉式删除当使用get、getset等指令 去获取数据时，判断key是否过期。 过期后，就先把key删除，再执行后面的操作。 Redis是将两种方式结合来使用。 定期删除：平衡执行频率和执行时长。 定期删除时会遍历每个datapase(默认16个),检查当前库中指定个数的key(默认是20个)。随机抽查这些key,如果有过期的，就删除。 程序中有一个全局变量记录到扫描到了哪个数据库。|","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"}],"author":"Marlowe"},{"title":"如何设计一个分布式锁？如何对锁的性能进行优化？","slug":"并发/如何设计一个分布式锁？如何对锁的性能进行优化？","date":"2021-05-04T12:49:09.000Z","updated":"2021-05-04T14:38:59.918Z","comments":true,"path":"2021/05/04/并发/如何设计一个分布式锁？如何对锁的性能进行优化？/","link":"","permalink":"https://xmmarlowe.github.io/2021/05/04/%E5%B9%B6%E5%8F%91/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%EF%BC%9F%E5%A6%82%E4%BD%95%E5%AF%B9%E9%94%81%E7%9A%84%E6%80%A7%E8%83%BD%E8%BF%9B%E8%A1%8C%E4%BC%98%E5%8C%96%EF%BC%9F/","excerpt":"","text":"分布式锁的本质就是在所有进程都能访问到的一个地方，设置一一个锁资源， 让这些进程都来竞争锁资源。数据库、zookeeper, Redis。 通常对于分布式锁，会要求响应快、性能高、与业务无关。 Redis实现分布式锁 SETNX key value: 当key不存在时，就将key设置为value,并返回1。如果key存在，就返回0。 EXPIRE key locktime: 设置key的有效时长。 DEL key: 删除。 GETSET key value: 先GET, 再SET， 先返回key对应的值，如果没有就返回空。然后再将key设 置成value。 简单的分布式锁SETNX加锁， DEL解锁。 问题: 如果获取到锁的进程执行失败，他就永远不会主动解锁，那这个锁就被锁死了。 解决方法： 给锁设置过期时长。 问题： SETNX 和EXPIRE并不是原子性的，所以获取到锁的进程有可能还没有执行EXPIRE指令，就挂了，这时锁还是会被锁死。 解决方法： 将锁的内容设置为过期时间(客户端时间+过期时长),SETNX获取锁失败时，拿这个时间跟当前时间比对，如果是过期的锁，就先删除锁，再重新上锁。 问题： 在高并发场景下，会产生多个进程同时拿到锁的情况。 解决方法： SETNX失败后，获取锁上的时间戳，然后用GETSET, 将自己的过期时间更新上去，并获取旧值。如果这个旧值，跟之前获得的时间戳是不一致的，就表示这个锁已经被其他进程占用了，自己就要放弃竞争锁。 123456789101112131415161718192021public boolean tryLock(RedisConnection conn)&#123; long nowTime = System.currentTimeMills(); long expireTime = nowTime + 1000; if(conn.SETNX(&quot;mykey&quot;,expireTime) == 1)&#123; // 给锁设置过期时间 conn.EXPIRE(&quot;mykey&quot;,1000); return true; &#125;else&#123; // 类似CAS操作 long oldVal = conn.get(&quot;mykey&quot;); if(oldVal != null &amp;&amp; oldVal &lt; nowTime&gt;)&#123; long currentVal = conn.GETSET(&quot;mykey&quot;,expireTime); if(oldVal == currentVal)&#123; conn.EXPIRE(&quot;mykey&quot;,1000); return true; &#125; return false; &#125; return false; &#125;&#125; 分析一下： 上面各种优化的根本问题在于SETNX和EXPIRE两个指令 无法保证原子性。Redis2.6提供了 直接执行Lua脚本的方式，通过Lua脚本来保证原子性。redission。","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"锁","slug":"锁","permalink":"https://xmmarlowe.github.io/tags/%E9%94%81/"}],"author":"Marlowe"},{"title":"MySQL redo log、binlog、undo log 区别与作用","slug":"数据库/MySQL-redo-log、binlog、undo-log-区别与作用","date":"2021-04-29T14:06:33.000Z","updated":"2021-08-26T10:09:02.607Z","comments":true,"path":"2021/04/29/数据库/MySQL-redo-log、binlog、undo-log-区别与作用/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/29/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL-redo-log%E3%80%81binlog%E3%80%81undo-log-%E5%8C%BA%E5%88%AB%E4%B8%8E%E4%BD%9C%E7%94%A8/","excerpt":"日志系统主要有redo log(重做日志)和binlog(归档日志)。redo log是InnoDB存储引擎层的日志，binlog是MySQL Server层记录的日志， 两者都是记录了某些操作的日志(不是所有)自然有些重复（但两者记录的格式不同）。","text":"日志系统主要有redo log(重做日志)和binlog(归档日志)。redo log是InnoDB存储引擎层的日志，binlog是MySQL Server层记录的日志， 两者都是记录了某些操作的日志(不是所有)自然有些重复（但两者记录的格式不同）。 MySQL逻辑架构 重做日志（redo log）作用确保事务的持久性。防止在发生故障的时间点，尚有脏页未写入磁盘，在重启mysql服务的时候，根据redo log进行重做，从而达到事务的持久性这一特性。 内容物理格式的日志，记录的是物理数据页面的修改的信息，其redo log是顺序写入redo log file的物理文件中去的。 什么时候产生事务开始之后就产生redo log，redo log的落盘并不是随着事务的提交才写入的，而是在事务的执行过程中，便开始写入redo log文件中。 什么时候释放当对应事务的脏页写入到磁盘之后，redo log的使命也就完成了，重做日志占用的空间就可以重用（被覆盖）。 对应的物理文件默认情况下，对应的物理文件位于数据库的data目录下的ib_logfile1&amp;ib_logfile2 innodb_log_group_home_dir 指定日志文件组所在的路径，默认./ ，表示在数据库的数据目录下。 innodb_log_files_in_group 指定重做日志文件组中文件的数量，默认2 关于文件的大小和数量，由以下两个参数配置 innodb_log_file_size 重做日志文件的大小。 innodb_mirrored_log_groups 指定了日志镜像文件组的数量，默认1 其他很重要一点，redo log是什么时候写盘的？前面说了是在事务开始之后逐步写盘的。 之所以说重做日志是在事务开始之后逐步写入重做日志文件，而不一定是事务提交才写入重做日志缓存，原因就是，重做日志有一个缓存区Innodb_log_buffer，Innodb_log_buffer的默认大小为8M(这里设置的16M),Innodb存储引擎先将重做日志写入innodb_log_buffer中。 然后会通过以下三种方式将innodb日志缓冲区的日志刷新到磁盘 Master Thread 每秒一次执行刷新Innodb_log_buffer到重做日志文件。 每个事务提交时会将重做日志刷新到重做日志文件。 当重做日志缓存可用空间 少于一半时，重做日志缓存被刷新到重做日志文件 由此可以看出，重做日志通过不止一种方式写入到磁盘，尤其是对于第一种方式，Innodb_log_buffer到重做日志文件是Master Thread线程的定时任务。 因此重做日志的写盘，并不一定是随着事务的提交才写入重做日志文件的，而是随着事务的开始，逐步开始的。 另外引用《MySQL技术内幕 Innodb 存储引擎》（page37）上的原话： 即使某个事务还没有提交，Innodb存储引擎仍然每秒会将重做日志缓存刷新到重做日志文件。 这一点是必须要知道的，因为这可以很好地解释再大的事务的提交（commit）的时间也是很短暂的。 回滚日志（undo log）作用保存了事务发生之前的数据的一个版本，可以用于回滚，同时可以提供多版本并发控制下的读（MVCC），也即非锁定读。 内容逻辑格式的日志，在执行undo的时候，仅仅是将数据从逻辑上恢复至事务之前的状态，而不是从物理页面上操作实现的，这一点是不同于redo log的。 什么时候产生事务开始之前，将当前是的版本生成undo log，undo 也会产生 redo 来保证undo log的可靠性 什么时候释放当事务提交之后，undo log并不能立马被删除，而是放入待清理的链表，由purge线程判断是否由其他事务在使用undo段中表的上一个事务之前的版本信息，决定是否可以清理undo log的日志空间。 对应的物理文件MySQL5.6之前，undo表空间位于共享表空间的回滚段中，共享表空间的默认的名称是ibdata，位于数据文件目录中。 MySQL5.6之后，undo表空间可以配置成独立的文件，但是提前需要在配置文件中配置，完成数据库初始化后生效且不可改变undo log文件的个数如果初始化数据库之前没有进行相关配置，那么就无法配置成独立的表空间了。 关于MySQL5.7之后的独立undo 表空间配置参数如下 innodb_undo_directory = /data/undospace/ –undo独立表空间的存放目录 innodb_undo_logs = 128 –回滚段为128KB innodb_undo_tablespaces = 4 –指定有4个undo log文件 如果undo使用的共享表空间，这个共享表空间中又不仅仅是存储了undo的信息，共享表空间的默认为与MySQL的数据目录下面，其属性由参数innodb_data_file_path配置。 其他undo是在事务开始之前保存的被修改数据的一个版本，产生undo日志的时候，同样会伴随类似于保护事务持久化机制的redolog的产生。 默认情况下undo文件是保持在共享表空间的，也即ibdatafile文件中，当数据库中发生一些大的事务性操作的时候，要生成大量的undo信息，全部保存在共享表空间中的。 因此共享表空间可能会变的很大，默认情况下，也就是undo 日志使用共享表空间的时候，被“撑大”的共享表空间是不会也不能自动收缩的。 因此，mysql5.7之后的“独立undo 表空间”的配置就显得很有必要了。 二进制日志（binlog）作用用于复制，在主从复制中，从库利用主库上的binlog进行重播，实现主从同步。用于数据库的基于时间点的还原。 内容逻辑格式的日志，可以简单认为就是执行过的事务中的sql语句。 但又不完全是sql语句这么简单，而是包括了执行的sql语句（增删改）反向的信息，也就意味着delete对应着delete本身和其反向的insert；update对应着update执行前后的版本的信息；insert对应着delete和insert本身的信息。 在使用mysqlbinlog解析binlog之后一些都会真相大白。 因此可以基于binlog做到类似于oracle的闪回功能，其实都是依赖于binlog中的日志记录。 什么时候产生事务提交的时候，一次性将事务中的sql语句（一个事物可能对应多个sql语句）按照一定的格式记录到binlog中。 这里与redo log很明显的差异就是redo log并不一定是在事务提交的时候刷新到磁盘，redo log是在事务开始之后就开始逐步写入磁盘。 因此对于事务的提交，即便是较大的事务，提交（commit）都是很快的，但是在开启了bin_log的情况下，对于较大事务的提交，可能会变得比较慢一些。 这是因为binlog是在事务提交的时候一次性写入的造成的，这些可以通过测试验证。 什么时候释放binlog的默认是保持时间由参数expire_logs_days配置，也就是说对于非活动的日志文件，在生成时间超过expire_logs_days配置的天数之后，会被自动删除。 对应的物理文件配置文件的路径为log_bin_basename，binlog日志文件按照指定大小，当日志文件达到指定的最大的大小之后，进行滚动更新，生成新的日志文件。 对于每个binlog日志文件，通过一个统一的index文件来组织。 其他二进制日志的作用之一是还原数据库的，这与redo log很类似，很多人混淆过，但是两者有本质的不同。 作用不同： redo log是保证事务的持久性的，是事务层面的，binlog作为还原的功能，是数据库层面的（当然也可以精确到事务层面的），虽然都有还原的意思，但是其保护数据的层次是不一样的。 内容不同： redo log是物理日志，是数据页面的修改之后的物理记录，binlog是逻辑日志，可以简单认为记录的就是sql语句 另外，两者日志产生的时间，可以释放的时间，在可释放的情况下清理机制，都是完全不同的。 恢复数据时候的效率，基于物理日志的redo log恢复数据的效率要高于语句逻辑日志的binlog。 关于事务提交时，redo log和binlog的写入顺序，为了保证主从复制时候的主从一致（当然也包括使用binlog进行基于时间点还原的情况），是要严格一致的，MySQL通过两阶段提交过程来完成事务的一致性的，也即redo log和binlog的一致性的，理论上是先写redo log，再写binlog，两个日志都提交成功（刷入磁盘），事务才算真正的完成。 redo log日志模块redo log是InnoDB存储引擎层的日志，又称重做日志文件，用于记录事务操作的变化，记录的是数据修改之后的值，不管事务是否提交都会记录下来。在实例和介质失败（media failure）时，redo log文件就能派上用场，如数据库掉电，InnoDB存储引擎会使用redo log恢复到掉电前的时刻，以此来保证数据的完整性。 在一条更新语句进行执行的时候，InnoDB引擎会把更新记录写到redo log日志中，然后更新内存，此时算是语句执行完了，然后在空闲的时候或者是按照设定的更新策略将redo log中的内容更新到磁盘中，这里涉及到WAL即Write Ahead logging技术，他的关键点是先写日志，再写磁盘。 有了redo log日志，那么在数据库进行异常重启的时候，可以根据redo log日志进行恢复，也就达到了crash-safe。 redo log日志的大小是固定的，即记录满了以后就从头循环写。 该图展示了一组4个文件的redo log日志，checkpoint之前表示擦除完了的，即可以进行写的，擦除之前会更新到磁盘中，write pos是指写的位置，当write pos和checkpoint相遇的时候表明redo log已经满了，这个时候数据库停止进行数据库更新语句的执行，转而进行redo log日志同步到磁盘中。 bin log日志模块bin log是属于MySQL Server层面的，又称为归档日志，属于逻辑日志，是以二进制的形式记录的是这个语句的原始逻辑，依靠bin log是没有crash-safe能力的 redo log和binlog区别 redo log是属于innoDB层面，binlog属于MySQL Server层面的，这样在数据库用别的存储引擎时可以达到一致性的要求。 redo log是物理日志，记录该数据页更新的内容；binlog是逻辑日志，记录的是这个更新语句的原始逻辑 redo log是循环写，日志空间大小固定；binlog是追加写，是指一份写到一定大小的时候会更换下一个文件，不会覆盖。 binlog可以作为恢复数据使用，主从复制搭建，redo log作为异常宕机或者介质故障后的数据恢复使用。 参考MySQL中的六种日志文件 MySQL日志系统：redo log、binlog、undo log 区别与作用","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"redo log","slug":"redo-log","permalink":"https://xmmarlowe.github.io/tags/redo-log/"},{"name":"binlog","slug":"binlog","permalink":"https://xmmarlowe.github.io/tags/binlog/"},{"name":"undo log","slug":"undo-log","permalink":"https://xmmarlowe.github.io/tags/undo-log/"}],"author":"Marlowe"},{"title":"SQL执行过慢，如何排查以及调优","slug":"数据库/SQL执行过慢，如何排查以及调优","date":"2021-04-29T13:37:24.000Z","updated":"2021-08-26T00:26:11.292Z","comments":true,"path":"2021/04/29/数据库/SQL执行过慢，如何排查以及调优/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/29/%E6%95%B0%E6%8D%AE%E5%BA%93/SQL%E6%89%A7%E8%A1%8C%E8%BF%87%E6%85%A2%EF%BC%8C%E5%A6%82%E4%BD%95%E6%8E%92%E6%9F%A5%E4%BB%A5%E5%8F%8A%E8%B0%83%E4%BC%98/","excerpt":"","text":"导致SQL执行慢的原因 硬件问题。如网络速度慢，内存不足，I/O吞吐量小，磁盘空间满了等。 没有索引或者索引失效。（一般在互联网公司，DBA会在半夜把表锁了，重新建立一遍索引，因为当你删除某个数据的时候，索引的树结构就不完整了。所以互联网公司的数据做的是假删除，一是为了做数据分析,二是为了不破坏索引 ） 数据过多（分库分表） 服务器调优及各个参数设置（调整my.cnf） 查询出的数据量过大（可以采用多次查询，其他的方法降低数据量） 锁或者死锁(这也是查询慢最常见的问题，是程序设计的缺陷) sp_lock,sp_who,活动的用户查看,原因是读写竞争资源。 返回了不必要的行和列 分析原因时，一定要找切入点 先观察，开启慢查询日志，设置相应的阈值（比如超过3秒就是慢SQL），在生产环境跑上个一天过后，看看哪些SQL比较慢。 Explain和慢SQL分析。比如SQL语句写的烂，索引没有或失效，关联查询太多（有时候是设计缺陷或者不得以的需求）等等。 Show Profile是比Explain更近一步的执行细节，可以查询到执行每一个SQL都干了什么事，这些事分别花了多少秒。 找DBA或者运维对MySQL进行服务器的参数调优。 解析： (1)、explain出来的各种item的意义 id: 每个被独立执行的操作的标志，表示对象被操作的顺序。一般来说， id 值大，先被执行；如果 id 值相同，则顺序从上到下。 select_type： 查询中每个 select 子句的类型。 A：simple：表示不需要union操作或者不包含子查询的简单select查询。有连接查询时，外层的查询为simple，且只有一个B：primary：一个需要union操作或者含有子查询的select，位于最外层的单位查询的select_type即为primary。且只有一个C：union：union连接的两个select查询，第一个查询是dervied派生表，除了第一个表外，第二个以后的表select_type都是unionD：dependent union：与union一样，出现在union 或union all语句中，但是这个查询要受到外部查询的影响E：union result：包含union的结果集，在union和union all语句中,因为它不需要参与查询，所以id字段为nullF：subquery：除了from字句中包含的子查询外，其他地方出现的子查询都可能是subqueryG：dependent subquery：与dependent union类似，表示这个subquery的查询要受到外部表查询的影响H：derived：from字句中出现的子查询，也叫做派生表，其他数据库中可能叫做内联视图或嵌套select table: 名字，被操作的对象名称，通常的表名(或者别名)，但是也有其他格式。 显示的查询表名，如果查询使用了别名，那么这里显示的是别名，如果不涉及对数据表的操作，那么这显示为null，如果显示为尖括号括起来的&lt;derived N&gt;就表示这个是临时表，后边的N就是执行计划中的id，表示结果来自于这个查询产生。如果是尖括号括起来的&lt;union M,N&gt;，与&lt;derived N&gt;类似，也是一个临时表，表示这个结果来自于union查询的id为M,N的结果集。 partitions: 匹配的分区信息。 type: join 类型。 依次从好到差：system，const，eq_ref，ref，fulltext，ref_or_null，unique_subquery，index_subquery，range，index_merge，index，ALL，除了all之外，其他的type都可以使用到索引，除了index_merge之外，其他的type只可以用到一个索引A：system：表中只有一行数据或者是空表，且只能用于myisam和memory表。如果是Innodb引擎表，type列在这个情况通常都是all或者indexB：const：使用唯一索引或者主键，返回记录一定是1行记录的等值where条件时，通常type是const。其他数据库也叫做唯一索引扫描C：eq_ref：出现在要连接过个表的查询计划中，驱动表只返回一行数据，且这行数据是第二个表的主键或者唯一索引，且必须为not null，唯一索引和主键是多列时，只有所有的列都用作比较时才会出现eq_refD：ref：不像eq_ref那样要求连接顺序，也没有主键和唯一索引的要求，只要使用相等条件检索时就可能出现，常见与辅助索引的等值查找。或者多列主键、唯一索引中，使用第一个列之外的列作为等值查找也会出现，总之，返回数据不唯一的等值查找就可能出现。E：fulltext：全文索引检索，要注意，全文索引的优先级很高，若全文索引和普通索引同时存在时，mysql不管代价，优先选择使用全文索引F：ref_or_null：与ref方法类似，只是增加了null值的比较。实际用的不多。G：unique_subquery：用于where中的in形式子查询，子查询返回不重复值唯一值H：index_subquery：用于in形式子查询使用到了辅助索引或者in常数列表，子查询可能返回重复值，可以使用索引将子查询去重。I：range：索引范围扫描，常见于使用&gt;,&lt;,is null,between ,in ,like等运算符的查询中。J：index_merge：表示查询使用了两个以上的索引，最后取交集或者并集，常见and ，or的条件使用了不同的索引，官方排序这个在ref_or_null之后，但是实际上由于要读取所个索引，性能可能大部分时间都不如rangeK：index：索引全表扫描，把索引从头到尾扫一遍，常见于使用索引列就可以处理不需要读取数据文件的查询、可以使用索引排序或者分组的查询。L：all：这个就是全表扫描数据文件，然后再在server层进行过滤返回符合要求的记录。 possible_keys： 列出可能会用到的索引。 key: 实际用到的索引。 查询真正使用到的索引，select_type为index_merge时，这里可能出现两个以上的索引，其他的select_type这里只会出现一个。 key_len: 用到的索引键的平均长度，单位为字节。 用于处理查询的索引长度，如果是单列索引，那就整个索引长度算进去，如果是多列索引，那么查询不一定都能使用到所有的列，具体使用到了多少个列的索引，这里就会计算进去，没有使用到的列，这里不会计算进去。留意下这个列的值，算一下你的多列索引总长度就知道有没有使用到所有的列了。要注意，mysql的ICP特性使用到的索引不会计入其中。另外，key_len只计算where条件用到的索引长度，而排序和分组就算用到了索引，也不会计算到key_len中。 ref: 表示本行被操作的对象的参照对象，可能是一个常量用 const 表示，也可能是其他表的key 指向的对象，比如说驱动表的连接列。 如果是使用的常数等值查询，这里会显示const，如果是连接查询，被驱动表的执行计划这里会显示驱动表的关联字段，如果是条件使用了表达式或者函数，或者条件列发生了内部隐式转换，这里可能显示为func rows: 估计每次需要扫描的行数。 这里是执行计划中估算的扫描行数，不是精确值 filtered: rows*filtered/100 表示该步骤最后得到的行数(估计值)。 使用explain extended时会出现这个列，5.7之后的版本默认就有这个字段，不需要使用explain extended了。这个字段表示存储引擎返回的数据在server层过滤后，剩下多少满足查询的记录数量的比例，注意是百分比，不是具体记录数。 extra: 重要的补充信息。 这个列可以显示的信息非常多，有几十种，常用的有A：distinct：在select部分使用了distinc关键字B：no tables used：不带from字句的查询或者From dual查询C：使用not in()形式子查询或not exists运算符的连接查询，这种叫做反连接。即，一般连接查询是先查询内表，再查询外表，反连接就是先查询外表，再查询内表。D：using filesort：排序时无法使用到索引时，就会出现这个。常见于order by和group by语句中E：using index：查询时不需要回表查询，直接通过索引就可以获取查询的数据。F：using join buffer（block nested loop），using join buffer（batched key accss）：5.6.x之后的版本优化关联查询的BNL，BKA特性。主要是减少内表的循环数量以及比较顺序地扫描查询。G：using sort_union，using_union，using intersect，using sort_intersection：using intersect：表示使用and的各个索引的条件时，该信息表示是从处理结果获取交集using union：表示使用or连接各个使用索引的条件时，该信息表示从处理结果获取并集using sort_union和using sort_intersection：与前面两个对应的类似，只是他们是出现在用and和or查询信息量大时，先查询主键，然后进行排序合并后，才能读取记录并返回。H：using temporary：表示使用了临时表存储中间结果。临时表可以是内存临时表和磁盘临时表，执行计划中看不出来，需要查看status变量，used_tmp_table，used_tmp_disk_table才能看出来。I：using where：表示存储引擎返回的记录并不是所有的都满足查询条件，需要在server层进行过滤。查询条件中分为限制条件和检查条件，5.6之前，存储引擎只能根据限制条件扫描数据并返回，然后server层根据检查条件进行过滤再返回真正符合查询的数据。5.6.x之后支持ICP特性，可以把检查条件也下推到存储引擎层，不符合检查条件和限制条件的数据，直接不读取，这样就大大减少了存储引擎扫描的记录数量。extra列显示using index conditionJ：firstmatch(tb_name)：5.6.x开始引入的优化子查询的新特性之一，常见于where字句含有in()类型的子查询。如果内表的数据量比较大，就可能出现这个K：loosescan(m..n)：5.6.x之后引入的优化子查询的新特性之一，在in()类型的子查询中，子查询返回的可能有重复记录时，就可能出现这个。 除了这些之外，还有很多查询数据字典库，执行计划过程中就发现不可能存在结果的一些提示信息。 (2)、profile的意义以及使用场景 Profile 用来分析 sql 性能的消耗分布情况。当用 explain 无法解决慢 SQL 的时候，需要用profile 来对 sql 进行更细致的分析，找出 sql 所花的时间大部分消耗在哪个部分，确认 sql的性能瓶颈。 (3)、explain 中的索引问题 Explain 结果中，一般来说，要看到尽量用 index(type 为 const、 ref 等， key 列有值)，避免使用全表扫描(type 显式为 ALL)。比如说有 where 条件且选择性不错的列，需要建立索引。被驱动表的连接列，也需要建立索引。被驱动表的连接列也可能会跟 where 条件列一起建立联合索引。当有排序或者 group by 的需求时，也可以考虑建立索引来达到直接排序和汇总的需求。 如何进行sql调优 查看sql是否涉及多表的联表或者子查询，如果有，看是否能进行业务拆分，相关字段冗余或者合并成临时表（业务和算法的优化）。 涉及连表的查询，是否能进行分表查询，单表查询之后的结果进行字段整合。 如果以上两种都不能操作，非要链表查询，那么考虑对相对应的查询条件做索引。加快查询速度。 针对数量大的表进行历史表分离（如交易流水表）。 数据库主从分离，读写分离，降低读写针对同一表同时的压力，至于主从同步，mysql有自带的binlog实现 主从同步。 explain分析sql语句，查看执行计划，分析索引是否用上，分析扫描行数等等。 查看mysql执行日志，看看是否有其他方面的问题。 SQL调优思路总结从根本上来说，查询慢是占用sql内存比较多，那么可以从这方面去酌手考虑。 参考sql执行慢的原因有哪些，如何进行sql优化？ SQL优化 | sql执行过长的时间，如何优化?","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://xmmarlowe.github.io/tags/SQL/"},{"name":"调优","slug":"调优","permalink":"https://xmmarlowe.github.io/tags/%E8%B0%83%E4%BC%98/"}],"author":"Marlowe"},{"title":"初识HTTP2.0","slug":"计算机网络/初识HTTP2-0","date":"2021-04-29T13:02:52.000Z","updated":"2021-04-29T14:35:23.886Z","comments":true,"path":"2021/04/29/计算机网络/初识HTTP2-0/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E5%88%9D%E8%AF%86HTTP2-0/","excerpt":"HTTP/2是HTTP协议自1999年HTTP 1.1发布后的首个更新，主要基于SPDY协议。由互联网工程任务组(IETF) 的Hypertext Transfer Protocol Bis (httpbis) 工作小组进行开发。该组织于2014年12月将HTTP/2标准提议递交至IESG进行讨论，于2015年2月17日被批准。HTTP/2标准于2015年5月以RFC 7540正式发表。","text":"HTTP/2是HTTP协议自1999年HTTP 1.1发布后的首个更新，主要基于SPDY协议。由互联网工程任务组(IETF) 的Hypertext Transfer Protocol Bis (httpbis) 工作小组进行开发。该组织于2014年12月将HTTP/2标准提议递交至IESG进行讨论，于2015年2月17日被批准。HTTP/2标准于2015年5月以RFC 7540正式发表。 简介HTTP/2（超文本传输协议第2版，最初命名为HTTP 2.0），是HTTP协议的的第二个主要版本，使用于万维网。HTTP/2是HTTP协议自1999年HTTP 1.1发布后的首个更新，主要基于SPDY协议（是Google开发的基于TCP的应用层协议，用以最小化网络延迟，提升网络速度，优化用户的网络使用体验）。 相对于HTTP/1.1的改进二进制分帧HTTP/2（超文本传输协议第2版，最初命名为HTTP 2.0），是HTTP协议的的第二个主要版本，使用于万维网。HTTP/2是HTTP协议自1999年HTTP 1.1发布后的首个更新，主要基于SPDY协议（是Google开发的基于TCP的应用层协议，用以最小化网络延迟，提升网络速度，优化用户的网络使用体验）。 在二进制分帧层中，HTTP/2 会将所有传输的信息分割为更小的消息和帧(frame)，并对它们采用二进制格式的编码。这种单连接多资源的方式，减少了服务端的压力，使得内存占用更少，连接吞吐量更大。而且,TCP连接数的减少使得网络拥塞状况得以改善，同时慢启动时间的减少，使拥塞和丢包恢复速度更快。 多路复用多路复用允许同时通过单一的HTTP/2.0连接发起多重 的请求-响应消息。在HTTP1.1协议中,浏览器客户端在同一时间，针对同一域名下的请求有一定数量的限制， 超过了这个限制的请求就会被阻塞。而多路复用允许同时通过单一的HTTP2.0连接发起多重的“请求-响应”消息。 HTTP2的请求的TCP的connection一旦建立，后续请求以stream的方式发送。每个stream的基本组成单位是frame (二进制帧)。客户端和服务器可以把HTTP消息分解为互不依赖的帧，然后乱序发送，最后再在另一端把它们重新组合起来。 也就是说，HTTP2.0 通信都在一个连接 上完成，这个连接可以承载任意数量的双向数据流。就好比，我请求一个页面baidu.com。页面上所有的资源请求都是客户端与服务器上的一条TCP上请求和响应的! header压缩HTTP/1.1的header带有大量信息，而且每次都要重复发送。HTTP/2 为了减少这部分开销，采用了HPACK头部压缩算法对Header进行压缩。 服务端推送简单来讲就是当用户的浏览器和服务器在建立连接后，服务器主动将一些资源推送给浏览器并缓存起来的机制。有了缓存，当浏览器想要访问已缓存的资源的时候就可以直接从缓存中读取了。 一些问题HTTP/2为什么是二进制？比起像HTTP/1.x这样的文本协议，二进制协议解析起来更高效、“线上”更紧凑，更重要的是错误更少。 为什么 HTTP/2 需要多路传输?HTTP/1.x 有个问题叫线端阻塞(head-of-line blocking), 它是指一个连接(connection)一次只提交一个请求的效率比较高, 多了就会变慢。 HTTP/1.1 试过用流水线(pipelining)来解决这个问题, 但是效果并不理想(数据量较大或者速度较慢的响应, 会阻碍排在他后面的请求). 此外, 由于网络媒介(intermediary )和服务器不能很好的支持流水线, 导致部署起来困难重重。而多路传输(Multiplexing)能很好的解决这些问题, 因为它能同时处理多个消息的请求和响应; 甚至可以在传输过程中将一个消息跟另外一个掺杂在一起。所以客户端只需要一个连接就能加载一个页面。 消息头为什么需要压缩?假定一个页面有80个资源需要加载（这个数量对于今天的Web而言还是挺保守的）, 而每一次请求都有1400字节的消息头（着同样也并不少见，因为Cookie和引用等东西的存在）, 至少要7到8个来回去“在线”获得这些消息头。这还不包括响应时间——那只是从客户端那里获取到它们所花的时间而已。这全都由于TCP的慢启动机制，它会基于对已知有多少个包，来确定还要来回去获取哪些包 – 这很明显的限制了最初的几个来回可以发送的数据包的数量。相比之下，即使是头部轻微的压缩也可以是让那些请求只需一个来回就能搞定——有时候甚至一个包就可以了。这种开销是可以被节省下来的，特别是当你考虑移动客户端应用的时候，即使是良好条件下，一般也会看到几百毫秒的来回延迟。 服务器推送的好处是什么？当浏览器请求一个网页时，服务器将会发回HTML，在服务器开始发送JavaScript、图片和CSS前，服务器需要等待浏览器解析HTML和发送所有内嵌资源的请求。服务器推送服务通过“推送”那些它认为客户端将会需要的内容到客户端的缓存中，以此来避免往返的延迟。 参考HTTP 2.0 和 HTTP 1.1 相比有哪些优势呢？ HTTP 2.0与HTTP 1.1区别","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://xmmarlowe.github.io/tags/HTTP/"}],"author":"Marlowe"},{"title":"as-if-serial规则和happens-before规则","slug":"并发/as-if-serial规则和happens-before规则","date":"2021-04-28T12:41:41.000Z","updated":"2021-04-28T13:53:59.283Z","comments":true,"path":"2021/04/28/并发/as-if-serial规则和happens-before规则/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/28/%E5%B9%B6%E5%8F%91/as-if-serial%E8%A7%84%E5%88%99%E5%92%8Chappens-before%E8%A7%84%E5%88%99/","excerpt":"我们知道为了提高并行度，优化程序性能，编译器和处理器会对代码进行指令重排序。但为了不改变程序的执行结果，尽可能地提高程序执行的并行度，我们需要了解as-if-serial规则和happens-before规则。","text":"我们知道为了提高并行度，优化程序性能，编译器和处理器会对代码进行指令重排序。但为了不改变程序的执行结果，尽可能地提高程序执行的并行度，我们需要了解as-if-serial规则和happens-before规则。 as-if-serial规则as-if-serial语义的意思指：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。 编译器、runtime和处理器都必须遵守as-if-serial语义。为了遵守as-if-serial语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。 但是，如果操作之间不存在数据依赖关系，这些操作可能被编译器和处理器重排序。示例代码如下： 123int a=1;int b=2;int c=a+b; a和c之间存在数据依赖关系，同时b和c之间也存在数据依赖关系。因此在最终执行的指令序列中，c不能被重排序到A和B的前面（c排到a和b的前面，程序的结果将会被改变）。但a和b之间没有数据依赖关系，编译器和处理器可以重排序a和b之间的执行顺序。 happens-before（先行发生）规则定义JMM可以通过happens-before关系向程序员提供跨线程的内存可见性保证（如果A线程的写操作a与B线程的读操作b之间存在happens-before关系，尽管a操作和b操作在不同的线程中执行，但JMM向程序员保证a操作将对b操作可见）。具体的定义为： 如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。 两个操作之间存在happens-before关系，并不意味着Java平台的具体实现必须要按照happens-before关系指定的顺序来执行。如果重排序之后的执行结果，与按happens-before关系来执行的结果一致，那么JMM允许这种重排序。 八大规则|规则| 解释||:—-:|:—-:|:—-:||程序次序规则| 在一个线程内，代码按照书写的控制流顺序执行||管程锁定规则| 一个 unlock 操作先行发生于后面对同一个锁的 lock 操作||volatile 变量规则| volatile 变量的写操作先行发生于后面对这个变量的读操作||线程启动规则| Thread 对象的 start() 方法先行发生于此线程的每一个动作||线程终止规则| 线程中所有的操作都先行发生于对此线程的终止检测(通过 Thread.join() 方法结束、 Thread.isAlive() 的返回值检测)||线程中断规则| 对线程 interrupt() 方法调用优先发生于被中断线程的代码检测到中断事件的发生 (通过 Thread.interrupted() 方法检测)||对象终结规则| 一个对象的初始化完成(构造函数执行结束)先行发生于它的 finalize() 方法的开始||传递性| 如果操作 A 先于 操作 B 发生，操作 B 先于 操作 C 发生，那么操作 A 先于 操作 C| as-if-serial规则和happens-before规则的区别 as-if-serial语义保证单线程内程序的执行结果不被改变，happens-before关系保证正确同步的多线程程序的执行结果不被改变。 as-if-serial语义给编写单线程程序的程序员创造了一个幻觉：单线程程序是按程序的顺序来执行的。happens-before关系给编写正确同步的多线程程序的程序员创造了一个幻觉：正确同步的多线程程序是按happens-before指定的顺序来执行的。 as-if-serial语义和happens-before这么做的目的，都是为了在不改变程序执行结果的前提下，尽可能地提高程序执行的并行度。 参考Java并发理论（二）：as-if-serial规则和happens-before规则详解","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"as-if-serial","slug":"as-if-serial","permalink":"https://xmmarlowe.github.io/tags/as-if-serial/"},{"name":"happens-before","slug":"happens-before","permalink":"https://xmmarlowe.github.io/tags/happens-before/"}],"author":"Marlowe"},{"title":"对象在内存中的内存布局","slug":"Java/对象在内存中的内存布局","date":"2021-04-28T07:27:19.000Z","updated":"2021-04-28T13:53:59.240Z","comments":true,"path":"2021/04/28/Java/对象在内存中的内存布局/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/28/Java/%E5%AF%B9%E8%B1%A1%E5%9C%A8%E5%86%85%E5%AD%98%E4%B8%AD%E7%9A%84%E5%86%85%E5%AD%98%E5%B8%83%E5%B1%80/","excerpt":"","text":"对象的内存布局HotSpot虚拟机中，对象在内存中存储的布局可以分为三块区域：对象头（Header）、实例数据（Instance Data）和对齐填充（Padding）。 从上面的这张图里面可以看出，对象在内存中的结构主要包含以下几个部分： Mark Word(标记字段)：对象的Mark Word部分占4个字节，其内容是一系列的标记位，比如轻量级锁的标记位，偏向锁标记位等等。 Klass Pointer（Class对象指针）：Class对象指针的大小也是4个字节，其指向的位置是对象对应的Class对象（其对应的元数据对象）的内存地址 对象实际数据：这里面包括了对象的所有成员变量，其大小由各个成员变量的大小决定，比如：byte和boolean是1个字节，short和char是2个字节，int和float是4个字节，long和double是8个字节，reference是4个字节 对齐：最后一部分是对齐填充的字节，按8个字节填充。 对象头Mark Word（标记字段）HotSpot虚拟机的对象头包括两部分信息，第一部分是“Mark Word”，用于存储对象自身的运行时数据， 如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等等，这部分数据的长度在32位和64位的虚拟机（暂 不考虑开启压缩指针的场景）中分别为32个和64个Bits，官方称它为“Mark Word”。对象需要存储的运行时数据很多，其实已经超出了32、64位Bitmap结构所能记录的限度，但是对象头信息是与对象自身定义的数据无关的额 外存储成本，考虑到虚拟机的空间效率，Mark Word被设计成一个非固定的数据结构以便在极小的空间内存储尽量多的信息，它会根据对象的状态复用自己的存储空间。例如在32位的HotSpot虚拟机 中对象未被锁定的状态下，Mark Word的32个Bits空间中的25Bits用于存储对象哈希码（HashCode），4Bits用于存储对象分代年龄，2Bits用于存储锁标志 位，1Bit固定为0，在其他状态（轻量级锁定、重量级锁定、GC标记、可偏向）下对象的存储内容如下表所示。 但是如果对象是数组类型，则需要三个机器码，因为JVM虚拟机可以通过Java对象的元数据信息确定Java对象的大小，但是无法从数组的元数据来确认数组的大小，所以用一块来记录数组长度。 对象头信息是与对象自身定义的数据无关的额外存储成本，但是考虑到虚拟机的空间效率，Mark Word被设计成一个非固定的数据结构以便在极小的空间内存存储尽量多的数据，它会根据对象的状态复用自己的存储空间，也就是说，Mark Word会随着程序的运行发生变化，变化状态如下（32位虚拟机）： HotSpot虚拟机对象头Mark Word： 存储内容 标志位 状态 对象哈希码、对象分代年龄 01 未锁定 指向锁记录的指针 00 轻量级锁定 指向重量级锁的指针 10 膨胀（重量级锁定） 空，不需要记录信息 11 GC标记 偏向线程ID、偏向时间戳、对象分代年龄 01 可偏向 注意偏向锁、轻量级锁、重量级锁等都是jdk 1.6以后引入的。 其中轻量级锁和偏向锁是Java 6 对 synchronized 锁进行优化后新增加的，稍后我们会简要分析。这里我们主要分析一下重量级锁也就是通常说synchronized的对象锁，锁标识位为10，其中指针指向的是monitor对象（也称为管程或监视器锁）的起始地址。每个对象都存在着一个 monitor 与之关联，对象与其 monitor 之间的关系有存在多种实现方式，如monitor可以与对象一起创建销毁或当线程试图获取对象锁时自动生成，但当一个 monitor 被某个线程持有后，它便处于锁定状态。在Java虚拟机(HotSpot)中，monitor是由ObjectMonitor实现的，其主要数据结构如下（位于HotSpot虚拟机源码ObjectMonitor.hpp文件，C++实现的） 123456789101112131415161718ObjectMonitor() &#123; _header = NULL; _count = 0; //记录个数 _waiters = 0, _recursions = 0; _object = NULL; _owner = NULL; _WaitSet = NULL; //处于wait状态的线程，会被加入到_WaitSet _WaitSetLock = 0 ; _Responsible = NULL ; _succ = NULL ; _cxq = NULL ; FreeNext = NULL ; _EntryList = NULL ; //处于等待锁block状态的线程，会被加入到该列表 _SpinFreq = 0 ; _SpinClock = 0 ; OwnerIsThread = 0 ; &#125; ObjectMonitor中有两个队列，_WaitSet 和 _EntryList，用来保存ObjectWaiter对象列表( 每个等待锁的线程都会被封装成ObjectWaiter对象)，_owner指向持有ObjectMonitor对象的线程，当多个线程同时访问一段同步代码时，首先会进入 _EntryList 集合，当线程获取到对象的monitor 后进入 _Owner 区域并把monitor中的owner变量设置为当前线程同时monitor中的计数器count加1，若线程调用 wait() 方法，将释放当前持有的monitor，owner变量恢复为null，count自减1，同时该线程进入 WaitSe t集合中等待被唤醒。若当前线程执行完毕也将释放monitor(锁)并复位变量的值，以便其他线程进入获取monitor(锁)。如下图所示: 由此看来，monitor对象存在于每个Java对象的对象头中(存储的指针的指向)，synchronized锁便是通过这种方式获取锁的，也是为什么Java中任意对象可以作为锁的原因，同时也是notify/notifyAll/wait等方法存在于顶级对象Object中的原因(关于这点稍后还会进行分析)，ok~，有了上述知识基础后，下面我们将进一步分析synchronized在字节码层面的具体语义实现。 对象头的另外一部分是类型指针，即是对象指向它的类的元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。并不是所有的虚拟机实现都必须在对象数据上保留类型指针，换句话说查找对象的元数据信息并不一定要经过对象本身。另外，如果对象是一个Java数组，那在对象头中还必须有一块用于记录数组长度的数据，因为虚拟机可以通过普通Java对象的元数据信息确定Java对象的大小，但是从数组的元数据中无法确定数组的大小。以下是HotSpot虚拟机markOop.cpp中的C++代码（注释）片段，它描述了32bits下MarkWord的存储状态： 12345678// Bit-format of an object header (most significant first, big endian layout below): // // 32 bits: // -------- // hash:25 ------------&gt;| age:4 biased_lock:1 lock:2 (normal object) // JavaThread*:23 epoch:2 age:4 biased_lock:1 lock:2 (biased object) // size:32 ------------------------------------------&gt;| (CMS free block) // PromotedObject*:29 ----------&gt;| promo_bits:3 -----&gt;| (CMS promoted object) 实例数据（Instance Data）接下来实例数据部分是对象真正存储的有效信息，也既是我们在程序代码里面所定义的各种类型的字段内容，无论是从父类继承下来的，还是在子类中定义的都需要记录下来。 这部分的存储顺序会受到虚拟机分配策略参数（FieldsAllocationStyle）和字段在Java源码中定义顺序的影响。HotSpot虚拟机 默认的分配策略为longs/doubles、ints、shorts/chars、bytes/booleans、oops（Ordinary Object Pointers），从分配策略中可以看出，相同宽度的字段总是被分配到一起。在满足这个前提条件的情况下，在父类中定义的变量会出现在子类之前。如果 CompactFields参数值为true（默认为true），那子类之中较窄的变量也可能会插入到父类变量的空隙之中。 对齐填充（Padding）第三部分对齐填充并不是必然存在的，也没有特别的含义，它仅仅起着占位符的作用。由于HotSpot VM的自动内存管理系统要求对象起始地址必须是8字节的整数倍，换句话说就是对象的大小必须是8字节的整数倍。对象头正好是8字节的倍数（1倍或者2倍），因此当对象实例数据部分没有对齐的话，就需要通过对齐填充来补全。 对象的创建过程见本站Java对象创建的过程 对象的访问定位Java是通过虚拟机栈中的局部变量表中的reference数据来操作Java堆上的具体对象。但reference只是虚拟机规范中规定指向一个对象的引用，它并没有定义这个引用通过何种方式去定位、访问堆中的对象的具体位置，所以对象访问方法也取决于虚拟机的实现而定的。目前主流的访问方式有使用句柄和直接指针两种。 句柄访问 如果使用句柄访问，Java堆中将会划分出一块儿内存作为句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据和对象类型数据的具体地址信息。实际上是采用了句柄池这样一个中间介质进行了两次指针定位，有效的避免了对象的移动或改变直接导致reference本身发生改变。句柄访问方式如下图所示： 使用句柄访问最大的好处就是reference中存储的是稳定的句柄地址，在对象回收过程中或者其它对象需要移动的时，只会改变句柄中的实例数据的指针，而reference本身不需要做任何修改。 直接指针访问如果使用直接指针访问，那么Java堆对象的布局必须考虑如何放置访问类型的数据的相关信息，而reference中存储的直接就是对象地址，而不再是句柄地址信息，相当于在reference与对象地址信息直接少了句柄池这样一个中间地址，reference中直接存储的就是对象地址。 这种定位方式也就导致了在对象被移动时，reference本身必须发生改变。但是我们都知道，使用句柄访问方式时，相当于进行了两次指针定位，而直接指针访问方式恰好节省了这一次指针定位的时间开销，由于对象的访问在Java中非常的频繁，时间开销的减少也是一种可观的执行成本。例如，常见的HotSpot虚拟机就使用的是直接指针访问方式。 示例在Hotspot JVM中，32位机器下，Integer对象的大小是int的几倍？我们都知道在Java语言规范已经规定了int的大小是4个字节，那么Integer对象的大小是多少呢？要知道一个对象的大小，那么必须需要知道对象在虚拟机中的结构是怎样的，根据上面的图，那么我们可以得出Integer的对象的结构如下： Integer只有一个int类型的成员变量value，所以其对象实际数据部分的大小是4个字节，然后再在后面填充4个字节达到8字节的对齐，所以可以得出Integer对象的大小是16个字节。 因此，我们可以得出Integer对象的大小是原生的int类型的4倍。 关于对象的内存结构，需要注意数组的内存结构和普通对象的内存结构稍微不同，因为数据有一个长度length字段，所以在对象头后面还多了一个int类型的length字段，占4个字节，接下来才是数组中的数据，如下图： Object o = new Object()在内存中占了多少字节?想要知道 Object o = new Object();在内存中占用了多少字节，可以使用如下方法直观的看到。 maven中添加依赖12345&lt;dependency&gt; &lt;groupId&gt;org.openjdk.jol&lt;/groupId&gt; &lt;artifactId&gt;jol-core&lt;/artifactId&gt; &lt;version&gt;0.9&lt;/version&gt;&lt;/dependency&gt; 写一个测试类123456public class ObjectLayOutTest &#123; public static void main(String[] args) &#123; Object o = new Object(); System.out.println(ClassLayout.parseInstance(o).toPrintable()); &#125;&#125; 结果 可以直观的看到 new Object()在内存中占用16个字节。为什么是16个字节呢，就需要了解对象在内存中的存储布局。 MarkWord：对象头，8字节。包括了对象的hashCode、对象的分代年龄、锁标志位等。结构如下图所示： classPointer：对象指向它的类元素的指针。在不开启对象指针压缩的情况下是8字节。压缩后变为4字节，默认压缩。 1通过命令：java -XX:+PrintCommandLineFlags -version 查看classPointer是否开启压缩 padding ：用于对象在内存中占用的字节数不能被8整除的情况下，进行补充。 因此，Object o = new Object()在内存中占的字节数计算如下： markword 8字节，因为java默认使用了calssPointer压缩，classpointer 4字节，padding 4字节 因此是16字节。如果没开启classpointer默认压缩，markword 8字节，classpointer 8字节，padding 0字节 也是16字节。 参考java对象在内存中的结构（HotSpot虚拟机）对象的访问定位","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"内存布局","slug":"内存布局","permalink":"https://xmmarlowe.github.io/tags/%E5%86%85%E5%AD%98%E5%B8%83%E5%B1%80/"}],"author":"Marlowe"},{"title":"为什么AQS底层是CAS + volatile","slug":"并发/为什么AQS底层是CAS-volatile","date":"2021-04-28T07:13:44.000Z","updated":"2021-05-17T08:46:31.390Z","comments":true,"path":"2021/04/28/并发/为什么AQS底层是CAS-volatile/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/28/%E5%B9%B6%E5%8F%91/%E4%B8%BA%E4%BB%80%E4%B9%88AQS%E5%BA%95%E5%B1%82%E6%98%AFCAS-volatile/","excerpt":"","text":"CAS操作和volatile简述CAS操作CAS是什么？CAS是compare and swap的缩写，从字面上理解就是比较并更新；主要是通过 处理器的指令 来保证操作的原子性 。 CAS 操作包含三个操作数： 内存位置（V） 预期原值（A） 更新值(B) 简单来说： 从内存位置V上取到存储的值，将值和预期值A进行比较，如果值和预期值A的结果相等，那么我们就把新值B更新到内存位置V上，如果不相等，那么就重复上述操作直到成功为止。 例如：JDK中的 unsafe 类中的 compareAndSwapInt 方法： 1unsafe.compareAndSwapInt(this, stateOffset, expect, update); stateOffset 变量值在内存中存放的位置； expect 期望值； update 更新值； CAS的优点CAS是一种无锁化编程，是一种非阻塞的轻量级的乐观锁；相比于synchronized阻塞式的重量级的悲观锁来说，性能会好很多 。 但是注意： synchronized关键字在不断的优化下（锁升级优化等），性能也变得十分的好。 volatile 关键字volatile是什么？volatile是java虚拟机提供的一种轻量级同步机制。 volatile的作用 可以保证被volatile修饰的变量的读写具有原子性，不保证复合操作（i++操作等）的原子性； 禁止指令重排序； 被volatile修饰的的变量修改后，可以马上被其它线程感知到，保证可见性； CAS + volatile = 同步代码块总述同步代码块的实现原理： 使用 volatile 关键字修饰一个int类型的同步标志位state，初始值为0； 加锁/释放锁时使用CAS操作对同步标志位state进行更新； 加锁成功，同步标志位值为 1，加锁状态； 释放锁成功，同步标志位值为0，初始状态； 加锁实现加锁流程图 加锁代码123456789101112131415161718192021222324252627282930313233343536373839404142 ** * 加锁，非公平方式获取锁 */public final void lock() &#123; while (true) &#123; // CAS操作更新同步标志位 if (compareAndSetState(0, 1)) &#123; // 将独占锁的拥有者设置为当前线程 exclusiveOwnerThread = Thread.currentThread(); System.out.println(Thread.currentThread() + &quot; lock success ! set lock owner is current thread . &quot; + &quot;state：&quot; + state); try &#123; // 睡眠一小会，模拟更加好的效果 Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // 跳出循环 break; &#125; else &#123; // TODO 如果同步标志位是1，并且锁的拥有者是当前线程的话，则可以设置重入，但本方法暂未实现 if (1 == state &amp;&amp; Thread.currentThread() == exclusiveOwnerThread) &#123; // 进行设置重入锁 &#125; System.out.println(Thread.currentThread() + &quot; lock fail ! If the owner of the lock is the current thread,&quot; + &quot; the reentrant lock needs to be set；else Adds the current thread to the blocking queue .&quot;); // 将线程阻塞，并将其放入阻塞列表 parkThreadList.add(Thread.currentThread()); LockSupport.park(this); // 线程被唤醒后会执行此处，并且继续执行此 while 循环 System.out.println(Thread.currentThread() + &quot; The currently blocking thread is awakened !&quot;); &#125; &#125;&#125; 锁释放实现释放锁流程图 释放锁代码123456789101112131415161718192021222324252627282930/** * 释放锁 * * @return */public final boolean unlock() &#123; // 判断锁的拥有者是否为当前线程 if (Thread.currentThread() != exclusiveOwnerThread) &#123; throw new IllegalMonitorStateException(&quot;Lock release failed ! The owner of the lock is not &quot; + &quot;the current thread.&quot;); &#125; // 将同步标志位设置为0，初始未加锁状态 state = 0; // 将独占锁的拥有者设置为 null exclusiveOwnerThread = null; System.out.println(Thread.currentThread() + &quot; Release the lock successfully, and then wake up &quot; + &quot;the thread node in the blocking queue ! state：&quot; + state); if (parkThreadList.size() &gt; 0) &#123; // 从阻塞列表中获取阻塞的线程 Thread thread = parkThreadList.get(0); // 唤醒阻塞的线程 LockSupport.unpark(thread); // 将唤醒的线程从阻塞列表中移除 parkThreadList.remove(0); &#125; return true;&#125; 完整代码如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161import sun.misc.Unsafe;import java.lang.reflect.Field;import java.util.ArrayList;import java.util.List;import java.util.concurrent.locks.LockSupport;/** * @PACKAGE_NAME: com.lyl.thread6 * @ClassName: AqsUtil * @Description: 使用 CAS + volatile 同步标志位 = 实现 迷你版AQS ; * &lt;p&gt; * &lt;p&gt; * 注意：本类只简单实现了基本的非公平方式的独占锁的获取与释放; 像重入锁、公平方式获取锁、共享锁等都暂未实现 * &lt;p/&gt; * @Date: 2021-01-15 10:55 * @Author: [ 木子雷 ] 公众号 **/public class AqsUtil &#123; /** * 同步标志位 */ private volatile int state = 0; /** * 独占锁拥有者 */ private transient Thread exclusiveOwnerThread; /** * JDK中的rt.jar中的Unsafe类提供了硬件级别的原子性操作 */ private static final Unsafe unsafe; /** * 存放阻塞线程的列表 */ private static List&lt;Thread&gt; parkThreadList = new ArrayList&lt;&gt;(); /** * 同步标志位 的“起始地址”偏移量 */ private static final long stateOffset; static &#123; try &#123; unsafe = getUnsafe(); // 获取 同步标志位status 的“起始地址”偏移量 stateOffset = unsafe.objectFieldOffset(AqsUtil.class.getDeclaredField(&quot;state&quot;)); &#125; catch (NoSuchFieldException e) &#123; throw new Error(e); &#125; &#125; /** * 通过反射 获取 Unsafe 对象 * * @return */ private static Unsafe getUnsafe() &#123; try &#123; Field field = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;); field.setAccessible(true); return (Unsafe) field.get(null); &#125; catch (Exception e) &#123; return null; &#125; &#125; /** * 加锁，非公平方式获取锁 */ public final void lock() &#123; while (true) &#123; if (compareAndSetState(0, 1)) &#123; // 将独占锁的拥有者设置为当前线程 exclusiveOwnerThread = Thread.currentThread(); System.out.println(Thread.currentThread() + &quot; lock success ! set lock owner is current thread . &quot; + &quot;state：&quot; + state); try &#123; // 睡眠一小会，模拟更加好的效果 Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // 跳出循环 break; &#125; else &#123; // TODO 如果同步标志位是1，并且锁的拥有者是当前线程的话，则可以设置重入，但本方法暂未实现 if (1 == state &amp;&amp; Thread.currentThread() == exclusiveOwnerThread) &#123; // 进行设置重入锁 &#125; System.out.println(Thread.currentThread() + &quot; lock fail ! If the owner of the lock is the current thread,&quot; + &quot; the reentrant lock needs to be set；else Adds the current thread to the blocking queue .&quot;); // 将线程阻塞，并将其放入阻塞队列 parkThreadList.add(Thread.currentThread()); LockSupport.park(this); // 线程被唤醒后会执行此处，并且继续执行此 while 循环 System.out.println(Thread.currentThread() + &quot; The currently blocking thread is awakened !&quot;); &#125; &#125; &#125; /** * 释放锁 * * @return */ public final boolean unlock() &#123; if (Thread.currentThread() != exclusiveOwnerThread) &#123; throw new IllegalMonitorStateException(&quot;Lock release failed ! The owner of the lock is not &quot; + &quot;the current thread.&quot;); &#125; // 将同步标志位设置为0，初始未加锁状态 state = 0; // 将独占锁的拥有者设置为 null exclusiveOwnerThread = null; System.out.println(Thread.currentThread() + &quot; Release the lock successfully, and then wake up &quot; + &quot;the thread node in the blocking queue ! state：&quot; + state); if (parkThreadList.size() &gt; 0) &#123; // 从阻塞列表中获取阻塞的线程 Thread thread = parkThreadList.get(0); // 唤醒阻塞的线程 LockSupport.unpark(thread); // 将唤醒的线程从阻塞列表中移除 parkThreadList.remove(0); &#125; return true; &#125; /** * 使用CAS 安全的更新 同步标志位 * * @param expect * @param update * @return */ public final boolean compareAndSetState(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, stateOffset, expect, update); &#125;&#125; 测试运行测试代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;/** * @PACKAGE_NAME: com.lyl.thread6 * @ClassName: SynCodeBlock * @Description: 简单的测试 * @Date: 2021-01-15 10:26 * @Author: [ 木子雷 ] 公众号 **/public class SynCodeBlock &#123; public static void main(String[] args) &#123; // 10 个线程的固定线程池 ExecutorService logWorkerThreadPool = Executors.newFixedThreadPool(10); AqsUtil aqsUtil = new AqsUtil(); int i = 10; while (i &gt; 0) &#123; logWorkerThreadPool.execute(new Runnable() &#123; @Override public void run() &#123; test(aqsUtil); &#125; &#125;); --i; &#125; &#125; public static void test(AqsUtil aqsUtil) &#123; // 加锁 aqsUtil.lock(); try &#123; System.out.println(&quot;正常的业务处理&quot;); &#125; finally &#123; // 释放锁 aqsUtil.unlock(); &#125; &#125;&#125; 运行结果123例如上面测试程序启动了10个线程同时执行同步代码块，可能此时只有线程 thread-2 获取到了锁，其余线程由于没有获取到锁被阻塞进入到了阻塞列表中；当获取锁的线程释放了锁后，会唤醒阻塞列表中的线程，并且是按照进入列表的顺序被唤醒；此时被唤醒的线程会再次去尝试获取锁，如果此时有新线程同时尝试获取锁，那么此时也存在竞争了，这就是非公平方式抢占锁（不会按照申请锁的顺序获取锁）。 扩展上面的代码中没有实现线程自旋操作，下面看看该怎么实现呢？ 首先说说为什么需要自旋操作因为在某些场景下，同步资源的锁定时间很短，如果没有获取到锁的线程，为了这点时间就进行阻塞的话，就有些得不偿失了；因为进入阻塞时会进行线程上下文的切换，这个消耗是很大的； 使线程进行自旋的话就很大可能会避免阻塞时的线程上下文切换的消耗；并且一般情况下都会设置一个线程自旋的次数，超过这个次数后，线程还未获取到锁的话，也要将其阻塞了，防止线程一直自旋下去白白浪费CPU资源。 代码如下 参考根据AQS原理使用CAS + volatile实现同步代码块","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"AQS","slug":"AQS","permalink":"https://xmmarlowe.github.io/tags/AQS/"},{"name":"volatile","slug":"volatile","permalink":"https://xmmarlowe.github.io/tags/volatile/"},{"name":"CAS","slug":"CAS","permalink":"https://xmmarlowe.github.io/tags/CAS/"}],"author":"Marlowe"},{"title":"Synchronized与ReentrantLock","slug":"并发/Synchronized与ReentrantLock","date":"2021-04-28T06:19:05.000Z","updated":"2021-06-01T02:45:32.498Z","comments":true,"path":"2021/04/28/并发/Synchronized与ReentrantLock/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/28/%E5%B9%B6%E5%8F%91/Synchronized%E4%B8%8EReentrantLock/","excerpt":"Java 里面，最基本的互斥同步手段就是 synchronized 关键字，这是一种块结构( Block Structured ）的同步语法。还有就是 Java 类库中新提供了 java. util.concurrent 包，其中的 java.util.concurrent.locks.Lock 接口便成了 Java 另一 全新的互斥 同步手段。","text":"Java 里面，最基本的互斥同步手段就是 synchronized 关键字，这是一种块结构( Block Structured ）的同步语法。还有就是 Java 类库中新提供了 java. util.concurrent 包，其中的 java.util.concurrent.locks.Lock 接口便成了 Java 另一 全新的互斥 同步手段。 Synchronized 被 synchronized 修饰的同步块对同一条线程来说是可重人的 这意味着同一线程反复进入同步块也不会出现自己把自己锁死的情况。 被synchronized 修饰的同步块在持有锁的线程执行完毕并释放锁之前，会元条件地阻塞后面其他线程的进入 意味着无法像处理某些数据库中 的锁那样，强制已获取锁的线程释放锁；也无法强制正在等待锁的线程中断等待或超时退出。 三种使⽤⽅式 修饰实例⽅法: 作⽤于当前对象实例加锁，进⼊同步代码前要获得当前对象实例的锁 修饰静态⽅法: 也就是给当前类加锁，会作⽤于类的所有对象实例，因为静态成员不属于任何⼀个实例对象，是类成员（ static 表明这是该类的⼀个静态资源，不管new了多少个对象，只有⼀份）。所以如果⼀个线程A调⽤⼀个实例对象的⾮静态 synchronized ⽅法，⽽线程B需要调⽤这个实例对象所属类的静态 synchronized ⽅法，是允许的，不会发⽣互斥现象，因为访问静态synchronized ⽅法占⽤的锁是当前类的锁，⽽访问⾮静态 synchronized ⽅法占⽤的锁是当前实例对象锁。 修饰代码块： 指定加锁对象，对给定对象加锁，进⼊同步代码库前要获得给定对象的锁。 总结： synchronized 关键字加到 static 静态⽅法和 synchronized(class)代码块上都是是给 Class类上锁。synchronized 关键字加到实例⽅法上是给对象实例上锁。尽量不要使⽤synchronized(String a) 因为JVM中，字符串常量池具有缓存功能！ 底层实现作用于对象的时候 当synchronized作用于对象时候（即代码块方式），JVM会使用字节码monitorenter，monitorexit来进行同步代码块区分: 123456 4: monitorenter 5: getstatic #3 // Field java/lang/System.out:Ljava/io/PrintStream; 8: sipush 66611: invokevirtual #4 // Method java/io/PrintStream.println:(I)V14: aload_115: monitorexit 在执行 monitorenter 指令时，首先要去尝试获取对象的锁 如果这个对象没被锁定，或者当前线程已经持有了那个对象的锁，就把锁的计数器的值增加一，而在执行 monitorexit 指令时会将锁计数器的值减一，一旦计数器的值为零，锁随即就被释放了 如果获取对象锁失败，那当前线程就应当被阻塞等待，直到请求锁定的对象被持有它的线程释放为止。 锁的优化Java HotSpot 虚拟机中，每个对象都有对象头（包括 class 指针和 Mark Word）。Mark Word 平时存储这个对象的 哈希码 、 分代年龄 ，当加锁时，这些信息就根据情况被替换为 标记位 、 线程锁记录指 针 、 重量级锁指针 、 线程ID 等内容。 高效并发是从 JDK 升级到 JDK 后一项重要的改进项， Hotspot 虚拟机开发团队在这个版本上花费了大 的资源去实现各种锁优化技术，如适应性自旋（ Adaptive Spinning锁消除（ Lock Elimination ）、锁膨胀（ Lock Coarsening 、轻量级锁（ Lightweight Locking）、偏向锁（ Biased Locking ）等，这些技术都是为了在线程之间更高效地共享数据及解决竞争问题，从而提高程序的执行效率。 对象头 Mark Word 锁之间的转换 偏向锁轻量级锁在无竞争的情况下使用 CAS 操作去代替使用互斥量，而偏向锁在无竞争的情况下会把整个同步都会消除掉。 偏向锁中的“偏”，就是偏心的“偏”、偏袒的“偏” 它的意思是这个锁会偏向于第一个获得它的线程，如果在接下来的执行过程中，该锁一直没有被其他的线程获取，则持有偏向锁的线程将永远不需要再进行同步。 一旦出现另外一个线程去尝试获取这个锁的情况，偏向模式就马上宣告结束。根据锁对象目前是否处于被锁定的状态决定是否撤销偏向（偏向模式设置为“ 0”），撤销后标志位恢复到未锁定（标志位为“01 ”）或轻量级锁定（标志位为“00 ”）的状态 注意： 撤销偏向锁这个过程中所有线程需要暂停（STW） 访问对象的 hashCode 时候，如果对象处于偏向锁，也会撤销偏向锁，并且转换为重量级锁 如果对象虽然被多个线程访问，但没有竞争，这时偏向了线程 T1 的对象仍有机会重新偏向 T2，重偏向会重置对象的 Thread ID 撤销偏向和重偏向都是批量进行的，以类为单位 如果撤销偏向到达某个阈值，整个类的所有对象都会变为不可偏向的 可以主动使用 -XX:-UseBiasedLocking 禁用偏向锁 轻量级锁倘若偏向锁失败，虚拟机并不会立即升级为重量级锁，它还会尝试使用一种称为轻量级锁的优化手段(1.6之后加入的)。轻量级锁不是为了代替重量级锁，它的本意是在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗，因为使用轻量级锁时，不需要申请互斥量。另外，轻量级锁的加锁和解锁都用到了CAS操作。 加锁：在代码进入同步块的时候，如果此同步对象没有被锁定（锁标志位为“01 ”状态），那么虚拟机就会在当前线程栈帧中创建一个名字为Lock Record的空间，用于存储当前对象的MarK Word的拷贝（方便后期比较），虚拟机将会使用CAS把对象的Mark Word更新为指向Lock Recod的指针。 转化之前的对象头 转换之后的对象头 如果这个更新操作成功了，则代表这个对象获得了这个对象的锁，锁状态变成轻量级锁的“00”。如果失败了，则代表有多个线程正在竞争这个对象的锁，这个时候虚拟机再检查对象的Mark Word的指针是不是指向了当前线程存的Lock Record，如果是则直接进入同步代码块（锁重入）。如果不是则代表锁已经被其他线程占用了。当线程数量两个及以上时候，则可能进行锁膨胀。 解锁将Lock Recod存的Mark Word替换回去，同样是使用CAS操作，假如能够成功替换，那整个同步过程就顺利完成了；如果替换失败，则说明有其他线程尝试过获取该锁，就要在释放锁的同时，唤醒被挂起的线程。 轻量级锁能提升程序同步性能的依据是“对于绝大部分的锁，在整个同步周期内都是不存在竞争的”这一经验法则 如果没有竞争，轻量级锁便通过 CAS 操作成功避免了使用互斥量的开销；但如果确实存在锁竞争，除了互斥量的本身开销 ，还额外发生了CAS作的开销 因此在有竞争的情况下，轻量级锁反而会比传统的重量级锁更慢。 锁膨胀如果在尝试加轻量级锁的过程中，CAS 操作无法成功，这时一种情况就是有其它线程为此对象加上了轻量级锁（有竞争），如果出现两条及以上的线程争用同一个锁的情况，后来的那条会自旋（循环等待）一定次数来等待锁，如果还是获取不到锁，这时需要进行锁膨胀，将轻量级锁变为重量级锁。 重量级锁重量级锁竞争的时候，还可以使用自旋来进行优化，如果当前线程自旋成功（即这时候持锁线程已经退出了同步块，释放了锁），这时当前线程就可以避免阻塞。 在 Java 6 之后自旋锁是自适应的，比如对象刚刚的一次自旋操作成功过，那么认为这次自旋成功的可能性会高，就多自旋几次；反之，就少自旋甚至不自旋，总之，比较智能。 synchronized的其他优化 减少上锁时间： 同步代码块中尽量短 减少锁的粒度： 将一个锁拆分为多个锁提高并发度 锁粗化： 多次循环进入同步块不如同步块内多次循环 另外 JVM 可能会做如下优化，把多次 append 的加锁操作粗化为一次（因为都是对同一个对象加锁，没必要重入多次） 锁消除： JVM 会进行代码的逃逸分析，例如某个加锁对象是方法内局部变量，不会被其它线程所访问到，这时候就会被即时编译器忽略掉所有同步操作。 读写分离： CopyOnWriteArrayList、ConyOnWriteSet等 ReentrantLock重人锁（ ReentrantLock ）是 Lock 接口最常见的一种实现，顾名思义，它与 synchronized样是可重人的 在基本用法上， ReentrantLock 也与 synchronized 很相似，只是代码写法上稍有区别而已 不过， ReentrantLock synchronized 相比增加了一些高级功能，主要有以下 等待可中断、可实现公平锁及锁可以绑定多个条件 等待可中断： 是指当持有锁的线程长期不释放锁的时候，正在等待的线程可以选择放弃等待，改为处理其他事情 可中断特性对处理执行时间非常长的同步块很有帮助 公平锁： 是指多个线程在等待同一个锁时，必须按照申请锁的时间顺序来依次获得锁；而非公平锁则不保证这一点，在锁被释放时，任何－个等待锁的线程都有机会获得锁 synchronized 中的锁是非公平的， ReentrantLock 默认情况下也是非公平的，但可以通过带布尔值的构造函数要求使用公平锁 不过一旦使用了公平锁，将会导致 ReentrantLock 的性能急剧下降，会明显 吞吐量 锁绑定多个条件： 是指一个 ReentrantLock 象可以同时绑定多个 Condition 对象synchronized 中，锁对象的 wait() 跟的 notify()或者 notifyAll ()方法配合可以实现一个隐含的条件，如果要和多于一个的条件关联的时候，就不得不额外添加一个锁；而 ReentrantLock 则无须这样做，多次调用 newCondition（）方法即可 Synchronized 和 ReentrantLock 的对比① 两者都是可重入锁 两者都是可重入锁。“可重入锁”概念是：自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果不可锁重入的话，就会造成死锁。同一个线程每次获取锁，锁的计数器都自增1，所以要等到锁的计数器下降为0时才能释放锁。 ② synchronized 依赖于 JVM 而 ReenTrantLock 依赖于 API synchronized 是依赖于 JVM 实现的，前面我们也讲到了 虚拟机团队在 JDK1.6 为 synchronized 关键字进行了很多优化，但是这些优化都是在虚拟机层面实现的，并没有直接暴露给我们。ReenTrantLock 是 JDK 层面实现的（也就是 API 层面，需要 lock() 和 unlock 方法配合 try/finally 语句块来完成），所以我们可以通过查看它的源代码，来看它是如何实现的。 ③ ReenTrantLock 比 synchronized 增加了一些高级功能 相比synchronized，ReenTrantLock增加了一些高级功能。主要来说主要有三点：①等待可中断；②可实现公平锁；③可实现选择性通知（锁可以绑定多个条件） ReenTrantLock提供了一种能够中断等待锁的线程的机制，通过lock.lockInterruptibly()来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。 ReenTrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。 ReenTrantLock默认情况是非公平的，可以通过 ReenTrantLock类的ReentrantLock(boolean fair) 构造方法来制定是否是公平的。 synchronized关键字与wait()和notify/notifyAll()方法相结合可以实现等待/通知机制，ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition() 方法。Condition是JDK1.5之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用notify/notifyAll()方法进行通知时，被通知的线程是由 JVM 选择的，用ReentrantLock类结合Condition实例可以实现“选择性通知” ， 这个功能非常重要，而且是Condition接口默认提供的。而synchronized关键字就相当于整个Lock对象中只有一个Condition实例，所有的线程都注册在它一个身上。如果执行notifyAll()方法的话就会通知所有处于等待状态的线程这样会造成很大的效率问题，而Condition实例的signalAll()方法 只会唤醒注册在该Condition实例中的所有等待线程。 如果你想使用上述功能，那么选择ReenTrantLock是一个不错的选择。 ④ 性能已不是选择标准 在JDK1.6之前，synchronized 的性能是比 ReenTrantLock 差很多。具体表示为：synchronized 关键字吞吐量随线程数的增加，下降得非常严重。而ReenTrantLock 基本保持一个比较稳定的水平。我觉得这也侧面反映了， synchronized 关键字还有非常大的优化余地。后续的技术发展也证明了这一点，我们上面也讲了在 JDK1.6 之后 JVM 团队对 synchronized 关键字做了很多优化。JDK1.6 之后，synchronized 和 ReenTrantLock 的性能基本是持平了。所以网上那些说因为性能才选择 ReenTrantLock 的文章都是错的！JDK1.6之后，性能已经不是选择synchronized和ReenTrantLock的影响因素了！而且虚拟机在未来的性能改进中会更偏向于原生的synchronized，所以还是提倡在synchronized能满足你的需求的情况下，优先考虑使用synchronized关键字来进行同步！优化后的synchronized和ReenTrantLock一样，在很多地方都是用到了CAS操作。 图解 参考Synchronized与ReentrantLock 好文推荐：带你探索ReentrantLock源码的快乐","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"Synchronized","slug":"Synchronized","permalink":"https://xmmarlowe.github.io/tags/Synchronized/"},{"name":"ReentrantLock","slug":"ReentrantLock","permalink":"https://xmmarlowe.github.io/tags/ReentrantLock/"}],"author":"Marlowe"},{"title":"两个线程交替打印数字和字母","slug":"并发/两个线程交替打印数字和字母","date":"2021-04-28T05:58:53.000Z","updated":"2021-04-28T13:53:59.322Z","comments":true,"path":"2021/04/28/并发/两个线程交替打印数字和字母/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/28/%E5%B9%B6%E5%8F%91/%E4%B8%A4%E4%B8%AA%E7%BA%BF%E7%A8%8B%E4%BA%A4%E6%9B%BF%E6%89%93%E5%8D%B0%E6%95%B0%E5%AD%97%E5%92%8C%E5%AD%97%E6%AF%8D/","excerpt":"使用LockSupport的 park() 和 unpark() 方法 使用wait()和notify()方法","text":"使用LockSupport的 park() 和 unpark() 方法 使用wait()和notify()方法 使用LockSupport的 park() 和 unpark() 方法代码如下： 123456789101112131415161718192021222324252627282930313233public class LockSupportTest &#123; static Thread t1 = null; static Thread t2 = null; public static void main(String[] args) &#123; char[] a1 = &quot;1234567&quot;.toCharArray(); char[] a2 = &quot;ABCDEFG&quot;.toCharArray(); t1 = new Thread(() -&gt; &#123; for (char c : a1) &#123; System.out.println(c); // 叫醒t2 LockSupport.unpark(t2); // t1阻塞，当前线程阻塞 LockSupport.park(); &#125; &#125;); t2 = new Thread(() -&gt; &#123; for (char c : a2) &#123; // t2阻塞 LockSupport.park(); System.out.println(c); // 叫醒t1 LockSupport.unpark(t1); &#125; &#125;); t1.start(); t2.start(); &#125;&#125; 使用wait()和notify()方法代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243public class WaitNotifyTest &#123; public static void main(String[] args) &#123; final Object o = new Object(); char[] a1 = &quot;1234567&quot;.toCharArray(); char[] a2 = &quot;ABCDEFG&quot;.toCharArray(); new Thread(() -&gt; &#123; synchronized (o) &#123; for (char c : a1) &#123; System.out.println(c); try &#123; o.notify(); // 让出锁 o.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; // 必须唤醒，不然程序无法终止 o.notify(); &#125; &#125;, &quot;t1&quot;).start(); new Thread(() -&gt; &#123; synchronized (o) &#123; for (char c : a2) &#123; System.out.println(c); try &#123; o.notify(); // 让出锁 o.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; // 必须唤醒，不然程序无法终止 o.notify(); &#125; &#125;, &quot;t2&quot;).start(); &#125;&#125; 结果： 123456789101112131415161A2B3C4D5E6F7GProcess finished with exit code 0","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"线程","slug":"线程","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B/"}],"author":"Marlowe"},{"title":"Java如何开启线程?怎么保证线程安全?","slug":"并发/Java如何开启线程-怎么保证线程安全","date":"2021-04-26T13:57:17.000Z","updated":"2021-04-26T15:01:56.380Z","comments":true,"path":"2021/04/26/并发/Java如何开启线程-怎么保证线程安全/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/26/%E5%B9%B6%E5%8F%91/Java%E5%A6%82%E4%BD%95%E5%BC%80%E5%90%AF%E7%BA%BF%E7%A8%8B-%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8/","excerpt":"","text":"线程和进程的区别进程是操作系统进行资源分配的最小单元。线程是操作系统进行任务调度分配的最小单元，线程隶属于进程。 如何开启线程？ 继承Thread类,重写run方法。 实现Runnable接口， 实现run方法。 实现Callable接口， 实现call方法。通过FutureTask创建一个线程，获取到线程执行的返回值。 通过线程池来开启线程。 怎么保证线程安全？加锁 JVM提供的锁，也就是Synchronized关键字。 JDK提供的各种锁。","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"线程安全","slug":"线程安全","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8/"},{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"线程","slug":"线程","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B/"}],"author":"Marlowe"},{"title":"聊聊对关系型数据库和非关系型数据库的理解","slug":"数据库/聊聊对关系型数据库和非关系型数据库的理解","date":"2021-04-25T03:28:55.000Z","updated":"2021-04-25T14:18:30.493Z","comments":true,"path":"2021/04/25/数据库/聊聊对关系型数据库和非关系型数据库的理解/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/25/%E6%95%B0%E6%8D%AE%E5%BA%93/%E8%81%8A%E8%81%8A%E5%AF%B9%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E5%92%8C%E9%9D%9E%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E7%90%86%E8%A7%A3/","excerpt":"","text":"关系型数据库关系型数据库最典型的数据结构是表，由二维表及其之间的联系所组成的一个数据组织 优点 易于维护：都是使用表结构，格式一致； 使用方便：SQL语言通用，可用于复杂查询； 复杂操作：支持SQL，可用于一个表以及多个表之间非常复杂的查询。 缺点 读写性能比较差，尤其是海量数据的高效率读写； 固定的表结构，灵活度稍欠； 高并发读写需求，传统关系型数据库来说，硬盘I/O是一个很大的瓶颈。 非关系型数据库非关系型数据库严格上不是一种数据库，应该是一种数据结构化存储方法的集合，可以是文档或者键值对等。 优点 格式灵活：存储数据的格式可以是key,value形式、文档形式、图片形式等等，文档形式、图片形式等等，使用灵活，应用场景广泛，而关系型数据库则只支持基础类型。 速度快：nosql可以使用硬盘或者随机存储器作为载体，而关系型数据库只能使用硬盘； 高扩展性； 成本低：nosql数据库部署简单，基本都是开源软件。 缺点 不提供sql支持，学习和使用成本较高； 无事务处理； 数据结构相对复杂，复杂查询方面稍欠。 区别1. 数据存储方式不同关系型和非关系型数据库的主要差异是数据存储的方式。关系型数据天然就是表格式的，因此存储在数据表的行和列中。数据表可以彼此关联协作存储，也很容易提取数据。 与其相反，非关系型数据不适合存储在数据表的行和列中，而是大块组合在一起。非关系型数据通常存储在数据集中，就像文档、键值对或者图结构。你的数据及其特性是选择数据存储和提取方式的首要影响因素。 2. 扩展方式不同SQL和NoSQL数据库最大的差别可能是在扩展方式上，要支持日益增长的需求当然要扩展。 要支持更多并发量，SQL数据库是纵向扩展，也就是说提高处理能力，使用速度更快速的计算机，这样处理相同的数据集就更快了。 因为数据存储在关系表中，操作的性能瓶颈可能涉及很多个表，这都需要通过提高计算机性能来客服。虽然SQL数据库有很大扩展空间，但最终肯定会达到纵向扩展的上限。而NoSQL数据库是横向扩展的。 而非关系型数据存储天然就是分布式的，NoSQL数据库的扩展可以通过给资源池添加更多普通的数据库服务器(节点)来分担负载。 3. 对事务性的支持不同如果数据操作需要高事务性或者复杂数据查询需要控制执行计划，那么传统的SQL数据库从性能和稳定性方面考虑是你的最佳选择。SQL数据库支持对事务原子性细粒度控制，并且易于回滚事务。 虽然NoSQL数据库也可以使用事务操作，但稳定性方面没法和关系型数据库比较，所以它们真正闪亮的价值是在操作的扩展性和大数据量处理方面。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[],"author":"Marlowe"},{"title":"同一线程组下的线程，一个线程的异常会影响其他线程运行么？","slug":"操作系统/同一线程组下的线程，一个线程的异常会影响其他线程运行么？","date":"2021-04-23T14:33:15.000Z","updated":"2021-04-24T15:05:29.966Z","comments":true,"path":"2021/04/23/操作系统/同一线程组下的线程，一个线程的异常会影响其他线程运行么？/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/23/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%90%8C%E4%B8%80%E7%BA%BF%E7%A8%8B%E7%BB%84%E4%B8%8B%E7%9A%84%E7%BA%BF%E7%A8%8B%EF%BC%8C%E4%B8%80%E4%B8%AA%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%BC%82%E5%B8%B8%E4%BC%9A%E5%BD%B1%E5%93%8D%E5%85%B6%E4%BB%96%E7%BA%BF%E7%A8%8B%E8%BF%90%E8%A1%8C%E4%B9%88%EF%BC%9F/","excerpt":"","text":"当一个线程抛出OOM异常后，它所占据的内存资源会全部被释放掉，从而不会影响其他线程的运行！ 一个线程溢出后，进程里的其他线程还能照常运行。 同一线程组下的线程，一个线程出现异常不会影响其他线程的运行。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"线程","slug":"线程","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B/"},{"name":"进程","slug":"进程","permalink":"https://xmmarlowe.github.io/tags/%E8%BF%9B%E7%A8%8B/"}],"author":"Marlowe"},{"title":"线程崩溃必会使进程崩溃吗？","slug":"操作系统/线程崩溃必会使进程崩溃吗？","date":"2021-04-23T14:30:18.000Z","updated":"2021-04-23T14:39:37.116Z","comments":true,"path":"2021/04/23/操作系统/线程崩溃必会使进程崩溃吗？/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/23/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E7%BA%BF%E7%A8%8B%E5%B4%A9%E6%BA%83%E5%BF%85%E4%BC%9A%E4%BD%BF%E8%BF%9B%E7%A8%8B%E5%B4%A9%E6%BA%83%E5%90%97%EF%BC%9F/","excerpt":"","text":"结论：线程崩溃不一定导致进程崩溃。 线程崩溃的本质就是内存出错。而内存出错有时不会引起其他线程出错的，因为崩溃的线程，也就是出错的内存有时侯没有被其他线程访问，也就不会产生问题，但有时候会打乱其他线程的内存。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"线程","slug":"线程","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B/"},{"name":"进程","slug":"进程","permalink":"https://xmmarlowe.github.io/tags/%E8%BF%9B%E7%A8%8B/"}],"author":"Marlowe"},{"title":"Object类中常用方法","slug":"Java/Object类中常用方法","date":"2021-04-23T14:04:31.000Z","updated":"2021-05-08T13:37:35.010Z","comments":true,"path":"2021/04/23/Java/Object类中常用方法/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/23/Java/Object%E7%B1%BB%E4%B8%AD%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95/","excerpt":"","text":"取得对象信息的方法：toString()该方法在打印对象时被调用，将对象信息变为字符串返回，默认输出对象地址。 编译器默认调用toString()方法输出对象，但输出的是对象的地址，我们并不能看懂它的意思。那么就要通过重写Object类的toString()方法来输出对象属性信息。 对象相等判断方法：equals()public boolean equals(Object obj);用于比较当前对象与目标对象是否相等，默认是比较引用是否指向同一对象。为public方法，子类可重写。 12345public class Object&#123; public boolean equals(Object obj) &#123; return (this == obj); &#125;&#125; 为什么需要重写equals方法？ 因为如果不重写equals方法，当将自定义对象放到map或者set中时；如果这时两个对象的hashCode相同，就会调用equals方法进行比较，这个时候会调用Object中默认的equals方法，而默认的equals方法只是比较了两个对象的引用是否指向了同一个对象，显然大多数时候都不会指向，这样就会将重复对象存入map或者set中。这就 破坏了map与set不能存储重复对象的特性，会造成内存溢出 。 重写equals方法的几条约定： 自反性：即x.equals(x)返回true，x不为null； 对称性：即x.equals(y)与y.equals(x）的结果相同，x与y不为null； 传递性：即x.equals(y)结果为true, y.equals(z)结果为true，则x.equals(z)结果也必须为true； 一致性：即x.equals(y)返回true或false，在未更改equals方法使用的参数条件下，多次调用返回的结果也必须一致。x与y不为null。 如果x不为null, x.equals(null)返回false。 建议equals及hashCode两个方法，需要重写时，两个都要重写，一般都是将自定义对象放至Set中，或者Map中的key时，需要重写这两个方法。 对象签名:hashCode()该方法用来返回其所在对象的物理地址（哈希码值），常会和equals方法同时重写，确保相等的两个对象拥有相等的hashCode。 public native int hashCode();这是一个public的方法，所以 子类可以重写 它。这个方法返回当前对象的hashCode值，这个值是一个整数范围内的（-2^31 ~ 2^31 - 1）数字。 对于hashCode有以下几点约束： 在 Java 应用程序执行期间，在对同一对象多次调用 hashCode 方法时，必须一致地返回相同的整数，前提是将对象进行 equals 比较时所用的信息没有被修改； 如果两个对象 x.equals(y) 方法返回true，则x、y这两个对象的hashCode必须相等。 如果两个对象x.equals(y) 方法返回false，则x、y这两个对象的hashCode可以相等也可以不等。 但是，为不相等的对象生成不同整数结果可以提高哈希表的性能。 默认的hashCode是将内存地址转换为的hash值，重写过后就是自定义的计算方式；也可以通过System.identityHashCode(Object)来返回原本的hashCode。 12345678910111213public class HashCodeTest &#123; private int age; private String name; @Override public int hashCode() &#123; Object[] a = Stream.of(age, name).toArray(); int result = 1; for (Object element : a) &#123; result = 31 * result + (element == null ? 0 : element.hashCode()); &#125; return result; &#125;&#125; 推荐使用Objects.hash(Object… values)方法。相信看源码的时候，都看到计算hashCode都使用了31作为基础乘数， 为什么使用31呢？我比较赞同与理解result * 31 = (result&lt;&lt;5) - result。JVM底层可以自动做优化为位运算，效率很高；还有因为31计算的hashCode冲突较少，利于hash桶位的分布。 getClass()public final native ClassgetClass()：这是一个public的方法，我们可以直接通过对象调用。 类加载的第一阶段类的加载就是将.class文件加载到内存，并生成一个java.lang.Class对象的过程。getClass()方法就是获取这个对象，这是当前类的对象在运行时类的所有信息的集合。这个方法是反射三种方式之一。 反射三种方式： 对象的getClass() 类名.class Class.forName() clone()protected native Object clone() throws CloneNotSupportedException; 此方法返回当前对象的一个副本。 这是一个protected方法，提供给子类重写。但需要实现Cloneable接口，这是一个标记接口，如果没有实现，当调用object.clone()方法，会抛出CloneNotSupportedException。 1234567891011121314151617181920public class CloneTest implements Cloneable &#123; private int age; private String name; //省略get、set、构造函数等 @Override protected CloneTest clone() throws CloneNotSupportedException &#123; return (CloneTest) super.clone(); &#125; public static void main(String[] args) throws CloneNotSupportedException &#123; CloneTest cloneTest = new CloneTest(23, &quot;9龙&quot;); CloneTest clone = cloneTest.clone(); System.out.println(clone == cloneTest); System.out.println(cloneTest.getAge()==clone.getAge()); System.out.println(cloneTest.getName()==clone.getName()); &#125;&#125;//输出结果//false//true//true 从输出我们看见，clone的对象是一个新的对象；但原对象与clone对象的 String类型 的name却是同一个引用，这表明，super.clone方法对成员变量如果是引用类型，进行是浅拷贝。 那什么是浅拷贝？对应的深拷贝？ 浅拷贝：拷贝的是引用。 深拷贝：新开辟内存空间，进行值拷贝。 那如果我们要进行深拷贝怎么办呢？看下面的例子。 12345678910111213141516171819202122232425262728293031323334353637383940414243class Person implements Cloneable&#123; private int age; private String name; //省略get、set、构造函数等 @Override protected Person clone() throws CloneNotSupportedException &#123; Person person = (Person) super.clone(); //name通过new开辟内存空间 person.name = new String(name); return person; &#125;&#125;public class CloneTest implements Cloneable &#123; private int age; private String name; //增加了person成员变量 private Person person; //省略get、set、构造函数等 @Override protected CloneTest clone() throws CloneNotSupportedException &#123; CloneTest clone = (CloneTest) super.clone(); clone.person = person.clone(); return clone; &#125; public static void main(String[] args) throws CloneNotSupportedException &#123; CloneTest cloneTest = new CloneTest(23, &quot;9龙&quot;); Person person = new Person(22, &quot;路飞&quot;); cloneTest.setPerson(person); CloneTest clone = cloneTest.clone(); System.out.println(clone == cloneTest); System.out.println(cloneTest.getAge() == clone.getAge()); System.out.println(cloneTest.getName() == clone.getName()); Person clonePerson = clone.getPerson(); System.out.println(person == clonePerson); System.out.println(person.getName() == clonePerson.getName()); &#125;&#125;//输出结果//false//true//true//false//false 可以看到，即使成员变量是引用类型，我们也实现了深拷贝。 如果成员变量是引用类型，想实现深拷贝，则成员变量也要实现Cloneable接口，重写clone方法。 wait()/ wait(long)/ wait(long,int)这三个方法是用来 线程间通信用 的，作用是 阻塞当前线程 ，等待其他线程调用notify()/notifyAll()方法将其唤醒。这些方法都是public final的，不可被重写。 注意： 此方法只能在当前线程获取到对象的锁监视器之后才能调用，否则会抛出IllegalMonitorStateException异常。 调用wait方法，线程会将锁监视器进行释放；而Thread.sleep，Thread.yield()并不会释放锁 。 wait方法会一直阻塞，直到其他线程调用当前对象的notify()/notifyAll()方法将其唤醒；而wait(long)是等待给定超时时间内（单位毫秒），如果还没有调用notify()/nofiyAll()会自动唤醒；waite(long,int)如果第二个参数大于0并且小于999999，则第一个参数+1作为超时时间； 1234567891011121314151617public final void wait() throws InterruptedException &#123; wait(0); &#125; public final native void wait(long timeout) throws InterruptedException;public final void wait(long timeout, int nanos) throws InterruptedException &#123; if (timeout &lt; 0) &#123; throw new IllegalArgumentException(&quot;timeout value is negative&quot;); &#125; if (nanos &lt; 0 || nanos &gt; 999999) &#123; throw new IllegalArgumentException( &quot;nanosecond timeout value out of range&quot;); &#125; if (nanos &gt; 0) &#123; timeout++; &#125; wait(timeout); &#125; notify()/notifyAll()前面说了， 如果当前线程获得了当前对象锁，调用wait方法，将锁释放并阻塞；这时另一个线程获取到了此对象锁，并调用此对象的notify()/notifyAll()方法将之前的线程唤醒。 这些方法都是public final的，不可被重写。 public final native void notify(); 随机唤醒之前在当前对象上调用wait方法的一个线程 public final native void notifyAll(); 唤醒所有之前在当前对象上调用wait方法的线程 finalize()protected void finalize() throws Throwable ; 此方法是在垃圾回收之前，JVM会调用此方法来清理资源。此方法可能会将对象重新置为可达状态，导致JVM无法进行垃圾回收。 我们知道java相对于C++很大的优势是程序员不用手动管理内存，内存由jvm管理；如果我们的引用对象在堆中没有引用指向他们时，当内存不足时，JVM会自动将这些对象进行回收释放内存，这就是我们常说的垃圾回收。但垃圾回收没有讲述的这么简单。 finalize()方法具有如下4个特点： 永远不要主动调用某个对象的finalize()方法，该方法由垃圾回收机制自己调用； finalize()何时被调用，是否被调用具有不确定性； 当JVM执行可恢复对象的finalize()可能会将此对象重新变为可达状态； 当JVM执行finalize()方法时出现异常，垃圾回收机制不会报告异常，程序继续执行。","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"Object","slug":"Object","permalink":"https://xmmarlowe.github.io/tags/Object/"}],"author":"Marlowe"},{"title":"broadcast hash join和sort merge join","slug":"数据库/broadcast-hash-join和sort-merge-join","date":"2021-04-23T06:01:46.000Z","updated":"2021-04-23T14:23:09.528Z","comments":true,"path":"2021/04/23/数据库/broadcast-hash-join和sort-merge-join/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/23/%E6%95%B0%E6%8D%AE%E5%BA%93/broadcast-hash-join%E5%92%8Csort-merge-join/","excerpt":"","text":"Join背景介绍Join是数据库查询永远绕不开的话题，传统查询SQL技术总体可以分为简单操作（过滤操作-where、排序操作-limit等），聚合操作-groupby以及Join操作等。其中Join操作是最复杂、代价最大的操作类型，也是OLAP场景中使用相对较多的操作。因此很有必要对其进行深入研究。 另外，从业务层面来讲，用户在数仓建设的时候也会涉及Join使用的问题。通常情况下，数据仓库中的表一般会分为“低层次表”和“高层次表”。 所谓“低层次表”，就是数据源导入数仓之后直接生成的表，单表列值较少，一般可以明显归为维度表或事实表，表和表之间大多存在外健依赖，所以查询起来会遇到大量Join运算，查询效率很差。而“高层次表”是在“低层次表”的基础上加工转换而来，通常做法是使用SQL语句将需要Join的表预先进行合并形成“宽表”，在宽表上的查询不需要执行大量Join，效率很高。但宽表缺点是数据会有大量冗余，且相对生成较滞后，查询结果可能并不及时。 为了获得时效性更高的查询结果，大多数场景都需要进行复杂的Join操作。Join操作之所以复杂，主要是通常情况下其时间空间复杂度高，且有很多算法，在不同场景下需要选择特定算法才能获得最好的优化效果。本文将介绍SparkSQL所支持的几种常见的Join算法及其适用场景。 Join常见分类以及基本实现机制当前SparkSQL支持三种Join算法：shuffle hash join、broadcast hash join以及sort merge join。其中前两者归根到底都属于hash join，只不过在hash join之前需要先shuffle还是先broadcast。其实，hash join算法来自于传统数据库，而shuffle和broadcast是大数据的皮（分布式），两者一结合就成了大数据的算法了。因此可以说，大数据的根就是传统数据库。既然hash join是“内核”，那就刨出来看看，看完把“皮”再分析一下。 hash join先来看看这样一条SQL语句：select * from order,item where item.id = order.i_id，很简单一个Join节点，参与join的两张表是item和order，join key分别是item.id以及order.i_id。现在假设这个Join采用的是hash join算法，整个过程会经历三步： 确定Build Table以及Probe Table：这个概念比较重要，Build Table使用join key构建Hash Table，而Probe Table使用join key进行探测，探测成功就可以join在一起。通常情况下，小表会作为Build Table，大表作为Probe Table。此事例中item为Build Table，order为Probe Table。 构建Hash Table：依次读取Build Table（item）的数据，对于每一行数据根据join key(item.id)进行hash，hash到对应的Bucket，生成hash table中的一条记录。数据缓存在内存中，如果内存放不下需要dump到外存。 探测：再依次扫描Probe Table（order）的数据，使用相同的hash函数映射Hash Table中的记录，映射成功之后再检查join条件（item.id = order.i_id），如果匹配成功就可以将两者join在一起。 基本流程可以参考上图，这里有两个小问题需要关注： hash join性能如何？很显然，hash join基本都只扫描两表一次，可以认为o(a+b)，较之最极端的笛卡尔集运算a*b，不知甩了多少条街。 为什么Build Table选择小表？道理很简单，因为构建的Hash Table最好能全部加载在内存，效率最高；这也决定了hash join算法只适合至少一个小表的join场景，对于两个大表的join场景并不适用。 上文说过，hash join是传统数据库中的单机join算法，在分布式环境下需要经过一定的分布式改造，就是尽可能利用分布式计算资源进行并行化计算，提高总体效率。hash join分布式改造一般有两种经典方案： broadcast hash join：将其中一张小表广播分发到另一张大表所在的分区节点上，分别并发地与其上的分区记录进行hash join。broadcast适用于小表很小，可以直接广播的场景。 shuffler hash join：一旦小表数据量较大，此时就不再适合进行广播分发。这种情况下，可以根据join key相同必然分区相同的原理，将两张表分别按照join key进行重新组织分区，这样就可以将join分而治之，划分为很多小join，充分利用集群资源并行化。 下面分别进行详细讲解。 broadcast hash join如下图所示，broadcast hash join可以分为两步： broadcast阶段：将小表广播分发到大表所在的所有主机。广播算法可以有很多，最简单的是先发给driver，driver再统一分发给所有executor；要不就是基于BitTorrent的TorrentBroadcast。 hash join阶段：在每个executor上执行单机版hash join，小表映射，大表试探。 SparkSQL规定broadcast hash join执行的基本条件为被广播小表必须小于参数spark.sql.autoBroadcastJoinThreshold，默认为10M。 shuffle hash join在大数据条件下如果一张表很小，执行join操作最优的选择无疑是broadcast hash join，效率最高。但是一旦小表数据量增大，广播所需内存、带宽等资源必然就会太大，broadcast hash join就不再是最优方案。此时可以按照join key进行分区，根据key相同必然分区相同的原理，就可以将大表join分而治之，划分为很多小表的join，充分利用集群资源并行化。如下图所示，shuffle hash join也可以分为两步： shuffle阶段：分别将两个表按照join key进行分区，将相同join key的记录重分布到同一节点，两张表的数据会被重分布到集群中所有节点。这个过程称为shuffle。 hash join阶段：每个分区节点上的数据单独执行单机hash join算法。 看到这里，可以初步总结出来如果两张小表join可以直接使用单机版hash join；如果一张大表join一张极小表，可以选择broadcast hash join算法；而如果是一张大表join一张小表，则可以选择shuffle hash join算法；那如果是两张大表进行join呢？ sort merge joinSparkSQL对两张大表join采用了全新的算法－sort-merge join，如下图所示，整个过程分为三个步骤： shuffle阶段：将两张大表根据join key进行重新分区，两张表数据会分布到整个集群，以便分布式并行处理。 sort阶段：对单个分区节点的两表数据，分别进行排序。 merge阶段：对排好序的两张分区表数据执行join操作。join操作很简单，分别遍历两个有序序列，碰到相同join key就merge输出，否则取更小一边。如下图所示： 经过上文的分析，很明显可以得出来这几种Join的代价关系：cost(broadcast hash join) &lt; cost(shuffle hash join) &lt; cost(sort merge join)，数据仓库设计时最好避免大表与大表的join查询，SparkSQL也可以根据内存资源、带宽资源适量将参数spark.sql.autoBroadcastJoinThreshold调大，让更多join实际执行为broadcast hash join。 参考broadcast hash join和sort merge join","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"join","slug":"join","permalink":"https://xmmarlowe.github.io/tags/join/"}],"author":"Marlowe"},{"title":"操作系统之内存管理","slug":"操作系统/操作系统之内存管理","date":"2021-04-22T11:58:34.000Z","updated":"2021-04-23T14:26:19.817Z","comments":true,"path":"2021/04/22/操作系统/操作系统之内存管理/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/22/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/","excerpt":"","text":"虚拟地址转换为物理地址 第一步，先将虚拟地址转换为逻辑地址： 根据程序中的虚拟地址得出其所在的段，然后加上段偏移，我们就能得到一个逻辑地址 第二步，再将逻辑地址转换为线型地址： 有了逻辑地址之后，我们需要将逻辑地址转换为线型地址（因为线型地址是逻辑地址转换到物理地址的一个中间层），只需要把逻辑地址加上段的基地址就能生成一个线型地址 第三步，再将线型地址再转换为物理地址： 如果启用了分页机制，那么就需要将需要找到段中对应页的地址，然后再找到页内偏移地址，最后得到物理地址 如果没有启用分页机制，那么线型地址直接就是物理地址了 常见的几种内存管理机制简单分为连续分配管理方式和非连续分配管理方式这两种。连续分配管理方式是指为一个用户程序分配一个连续的内存空间，常见的如块式管理。同样地，非连续分配管理方式允许一个程序使用的内存分布在离散或者说不相邻的内存中，常见的如页式管理、段式管理、段页式管理。 块式管理: 远古时代的计算机操系统的内存管理方式。将内存分为几个固定大小的块，每个块中只包含一个进程。 如果程序运行需要内存的话，操作系统就分配给它一块，如果程序运行只需要很小的空间的话，分配的这块内存很大一部分几乎被浪费了。这些在每个块中未被利用的空间，我们称之为碎片。 页式管理: 把主存分为大小相等且固定的一页一页的形式，页较小，相对相比于块式管理的划分力度更大，提高了内存利用率,减少了碎片。页式管理通过页表对应逻辑地址和物理地址。 段式管理: 页式管理虽然提高了内存利用率，但是页式管理其中的页实际并无任何实际意义。段式管理把主存分为一段段的，每一段的空间又要比一页的空间小很多。但是，最重要的是段是有实际意义的，每个段定义了一组逻辑信息，例如，有主程序段MAIN、子程序段X、数据段D及栈段S等。段式管理通过段表对应逻辑地址和物理地址。 段页式管理: 结合了段式管理和页式管理的优点。简单来说段页式管理机制就是把主存先分成若干段，每个段又分成若干页，也就是说段页式管理机制中段与段之间以及段的内部的都是离散的。 参考操作系统之内存管理 常见的几种内存管理机制","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"内存管理","slug":"内存管理","permalink":"https://xmmarlowe.github.io/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"}],"author":"Marlowe"},{"title":"Java并发之Unsafe类","slug":"并发/Java并发之Unsafe类","date":"2021-04-21T14:07:17.000Z","updated":"2021-04-21T14:58:33.218Z","comments":true,"path":"2021/04/21/并发/Java并发之Unsafe类/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/21/%E5%B9%B6%E5%8F%91/Java%E5%B9%B6%E5%8F%91%E4%B9%8BUnsafe%E7%B1%BB/","excerpt":"","text":"Unsafe是位于sun.misc包下的一个类，主要提供一些用于执行低级别、不安全操作的方法，如直接访问系统内存资源、自主管理内存资源等，这些方法在提升Java运行效率、增强Java语言底层资源操作能力方面起到了很大的作用。但由于Unsafe类使Java语言拥有了类似C语言指针一样操作内存空间的能力，这无疑也增加了程序发生相关指针问题的风险。在程序中过度、不正确使用Unsafe类会使得程序出错的概率变大，使得Java这种安全的语言变得不再“安全”，因此对Unsafe的使用一定要慎重。 这个类尽管里面的方法都是 public 的，但是并没有办法使用它们，JDK API 文档也没有提供任何关于这个类的方法的解释。总而言之，对于 Unsafe 类的使用都是受限制的，只有授信的代码才能获得该类的实例，当然 JDK 库里面的类是可以随意使用的。 参考Java并发之Unsafe类","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"Unsafe","slug":"Unsafe","permalink":"https://xmmarlowe.github.io/tags/Unsafe/"}],"author":"Marlowe"},{"title":"继承Thread和实现Runnable的区别","slug":"并发/继承Thread和实现Runnable的区别","date":"2021-04-19T14:42:00.000Z","updated":"2021-05-14T06:36:08.825Z","comments":true,"path":"2021/04/19/并发/继承Thread和实现Runnable的区别/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/19/%E5%B9%B6%E5%8F%91/%E7%BB%A7%E6%89%BFThread%E5%92%8C%E5%AE%9E%E7%8E%B0Runnable%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"当使用继承的时候，主要是为了不必重新开发，并且在不必了解实现细节的情况下拥有了父类我所需要的特征。它也有一个很大的缺点，那就是如果我们的类已经从一个类继承（如小程序必须继承自 Applet 类），则无法再继承 Thread 类， Java只能单继承，因此如果是采用继承Thread的方法，那么在以后进行代码重构的时候可能会遇到问题，因为你无法继承别的类了，在其他的方面，两者之间并没什么太大的区别。 implement Runnable是面向接口，扩展性等方面比继承Thread好。 使用 Runnable 接口来实现多线程使得我们能够在一个类中包容所有的代码，有利于封装，它的缺点在于，我们只能使用一套代码，若想创建多个线程并使各个线程执行不同的代码，则仍必须额外创建类，如果这样的话，在大多数情况下也许还不如直接用多个类分别继承 Thread 来得紧凑。","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"Thread","slug":"Thread","permalink":"https://xmmarlowe.github.io/tags/Thread/"},{"name":"Runnable","slug":"Runnable","permalink":"https://xmmarlowe.github.io/tags/Runnable/"}],"author":"Marlowe"},{"title":"多线程Future的用法","slug":"并发/多线程Future的用法","date":"2021-04-17T08:49:20.000Z","updated":"2021-04-19T12:10:56.916Z","comments":true,"path":"2021/04/17/并发/多线程Future的用法/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/17/%E5%B9%B6%E5%8F%91/%E5%A4%9A%E7%BA%BF%E7%A8%8BFuture%E7%9A%84%E7%94%A8%E6%B3%95/","excerpt":"","text":"在并发编程时，一般使用runnable，然后扔给线程池完事，这种情况下不需要线程的结果。 所以run的返回值是void类型。 如果是一个多线程协作程序，比如斐波那契数列，1，1，2，3，5，8…使用多线程来计算。但后者需要前者的结果，就需要用callable接口了。callable用法和runnable一样，只不过调用的是call方法，该方法有一个泛型返回值类型，你可以任意指定。 线程是属于异步计算模型，所以你不可能直接从别的线程中得到函数返回值。这时候，Future就出场了。Futrue可以监视目标线程调用call的情况，当你调用Future的get()方法以获得结果时，当前线程就开始阻塞，直接call方法结束返回结果。 下面三段简单的代码可以很简明的揭示这个意思： runnable接口实现的没有返回值的并发编程。 callable实现的存在返回值的并发编程。（call的返回值String受泛型的影响） 同样是callable，使用Future获取返回值。 参考多线程Future的用法","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"Future","slug":"Future","permalink":"https://xmmarlowe.github.io/tags/Future/"}],"author":"Marlowe"},{"title":"Redis哨兵模式","slug":"NoSQL/Redis哨兵模式","date":"2021-04-17T07:43:36.000Z","updated":"2021-04-25T13:52:51.401Z","comments":true,"path":"2021/04/17/NoSQL/Redis哨兵模式/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/17/NoSQL/Redis%E5%93%A8%E5%85%B5%E6%A8%A1%E5%BC%8F/","excerpt":"主从切换技术的方法是：当主服务器宕机后，需要手动把一台从服务器切换为主服务器，这就需要人工干预，费事费力，还会造成一段时间内服务不可用。这不是一种推荐的方式，更多时候，我们优先考虑哨兵模式…","text":"主从切换技术的方法是：当主服务器宕机后，需要手动把一台从服务器切换为主服务器，这就需要人工干预，费事费力，还会造成一段时间内服务不可用。这不是一种推荐的方式，更多时候，我们优先考虑哨兵模式… 概述哨兵模式是一种特殊的模式，首先Redis提供了哨兵的命令，哨兵是一个独立的进程，作为进程，它会独立运行。其原理是哨兵通过发送命令，等待Redis服务器响应，从而监控运行的多个Redis实例。 这里的哨兵有两个作用 通过发送命令，让Redis服务器返回监控其运行状态，包括主服务器和从服务器。 当哨兵监测到master宕机，会自动将slave切换成master，然后通过发布订阅模式通知其他的从服务器，修改配置文件，让它们切换主机。 然而一个哨兵进程对Redis服务器进行监控，可能会出现问题，为此，我们可以使用多个哨兵进行监控。各个哨兵之间还会进行监控，这样就形成了多哨兵模式。 用文字描述一下故障切换（failover） 的过程。假设主服务器宕机，哨兵1先检测到这个结果，系统并不会马上进行failover过程，仅仅是哨兵1主观的认为主服务器不可用，这个现象成为主观下线。 当后面的哨兵也检测到主服务器不可用，并且数量达到一定值时，那么哨兵之间就会进行一次投票，投票的结果由一个哨兵发起，进行failover操作。切换成功后，就会通过发布订阅模式，让各个哨兵把自己监控的从服务器实现切换主机，这个过程称为客观下线。 这样对于客户端而言，一切都是透明的。 工作原理三个定时任务一、每10秒每个 sentinel 对master 和 slave 执行info 命令 :该命令第一个是用来发现slave节点,第二个是确定主从关系。 二、每2秒每个 sentinel 通过 master 节点的 channel(名称为sentinel:hello) 交换信息(pub/sub):用来交互对节点的看法(后面会介绍的节点主观下线和客观下线)以及自身信息。 三、每1秒每个 sentinel 对其他 sentinel 和 redis 执行 ping 命令,用于心跳检测,作为节点存活的判断依据。 主观下线和客观下线一、主观下线SDOWN:subjectively down,直接翻译的为”主观”失效,即当前sentinel实例认为某个redis服务为”不可用”状态。 二、客观下线 ODOWN:objectively down,直接翻译为”客观”失效,即多个sentinel实例都认为master处于”SDOWN”状态,那么此时master将处于ODOWN,ODOWN可以简单理解为master已经被集群确定为”不可用”,将会开启故障转移机制。 主从切换时,kill掉Redis主节点,然后查看 sentinel 日志,如下: 发现有类似 sdown 和 odown 的日志.在结合我们配置 sentinel 时的配置文件来看: 12#监控的IP 端口号 名称 sentinel通过投票后认为mater宕机的数量，此处为至少2个sentinel monitor mymaster 192.168.14.101 6379 2 最后的 2 表示投票数,也就是说当一台 sentinel 发现一个 Redis 服务无法 ping 通时,就标记为 主观下线 sdown;同时另外的 sentinel 服务也发现该 Redis 服务宕机,也标记为 主观下线,当多台 sentinel (大于等于2,上面配置的最后一个)时,都标记该Redis服务宕机,这时候就变为客观下线了,然后进行故障转移。 故障转移故障转移是由 sentinel 领导者节点来完成的(只需要一个sentinel节点),关于 sentinel 领导者节点的选取也是每个 sentinel 向其他 sentinel 节点发送我要成为领导者的命令,超过半数sentinel 节点同意,并且也大于quorum ,那么他将成为领导者,如果有多个sentinel都成为了领导者,则会过段时间再进行选举。 sentinel 领导者节点选举出来后,会通过如下几步进行故障转移: 一、从 slave 节点中选出一个合适的 节点作为新的master节点.这里的合适包括如下几点: 选择 slave-priority(slave节点优先级,也即priority最小的)最高的slave节点,如果存在则返回,不存在则继续下一步判断。 选择复制偏移量最大的 slave 节点(复制的最完整),如果存在则返回,不存在则继续。 选择runId最小的slave节点(启动最早的节点) 二、对上面选出来的 slave 节点执行 slaveof no one 命令让其成为新的 master 节点。 三、向剩余的 slave 节点发送命令,让他们成为新master 节点的 slave 节点,复制规则和前面设置的 parallel-syncs 参数有关。 四、更新原来master 节点配置为 slave 节点,并保持对其进行关注,一旦这个节点重新恢复正常后,会命令它去复制新的master节点信息.(注意:原来的master节点恢复后是作为slave的角色) 可以从 sentinel 日志中出现的几个消息来进行查看故障转移: +switch-master: 表示切换主节点(从节点晋升为主节点) +sdown: 主观下线 +odown: 客观下线 +convert-to-slave: 切换从节点(原主节点降为从节点)","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"}],"author":"Marlowe"},{"title":"Redis主从复制","slug":"NoSQL/Redis主从复制","date":"2021-04-17T07:43:23.000Z","updated":"2021-04-22T06:31:27.513Z","comments":true,"path":"2021/04/17/NoSQL/Redis主从复制/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/17/NoSQL/Redis%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/","excerpt":"为了分担读压力，Redis支持主从复制，Redis的主从结构可以采用一主多从或者级联结构，Redis主从复制可以根据是否是全量分为全量同步和增量同步…","text":"为了分担读压力，Redis支持主从复制，Redis的主从结构可以采用一主多从或者级联结构，Redis主从复制可以根据是否是全量分为全量同步和增量同步… 概念主从复制： 指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点（Master/Leader），后者称为从节点（Slave/Follower）， 数据的复制是单向的！只能由主节点复制到从节点（主节点以写为主、从节点以读为主）。 主从复制的作用默认情况下，每台Redis服务器都是主节点，一个主节点可以有0个或者多个从节点，但每个从节点只能由一个主节点。 作用 解释 数据冗余 主从复制实现了数据的热备份 故障恢复 当主节点故障时，从节点可以暂时替代主节点提供服务式 负载均衡 由主节点进行写操作，从节点进行读操作，分担服务器的负载；尤其是在多读少写的场景下，通过多个从节点分担负载，提高并发量 高可用基石 主从复制还是哨兵和集群能够实施的基础 全量复制与增量复制全量复制 Redis 全量复制一般发生在Slave初始化阶段，这时 Slave 需要将 Master 上的所有数据都复制一份。具体步骤如下： 从服务器连接主服务器，发送SYNC命令； 主服务器接收到sYNC命名后，开始执行BGSAVE命令生成RDB文件并使用缓冲区记录此后执行的所有写命令； 主服务器BGSAVE执行完后，向所有从服务器发送快照文件，并在发送期间继续记录被执行的写命令； 从服务器收到快照文件后丢弃所有旧数据，载入收到的快照； 主服务器快照发送完毕后开始向从服务器发送缓冲区中的写命令； 从服务器完成对快照的载入，开始接收命令请求，并执行来自主服务器缓冲区的写命令。 增量复制 Redis 增量复制是指 Slave 初始化后开始正常工作时主服务器发生的写操作同步到从服务器的过程。 增量复制的过程主要是主服务器每执行一个写命令就会向从服务器发送相同的写命令，从服务器接收并执行收到的写命令。 为什么要搭建集群？一般来说，要将Redis运用于工程项目中，只使用一台Redis是万万不能的（宕机），原因如下： 从结构上，单个Redis服务器会发生单点故障，并且一台服务器需要处理所有的请求负载，压力较大； 从容量上，单个Redis服务器内存容量有限，就算一台Redis服务器内存容量为256G，也不能将所有 内存用作Redis存储内存，一般来说，单台Redis大使用内存不应该超过20G。复制原理 当启动一个 slave node 的时候，它会发送一个 psync 命令给 master node。如果这是 slave node 初次连接到 master node，那么会触发一次 full resynchronization 全量复制。此时 master 会启动一个后台线程，开始生成一份 RDB 快照文件，同时还会将从客户端 client 新收到的所有写命令缓存在内存中。RDB 文件生成完毕后， master 会将这个 RDB 发送给 slave，slave 会先写入本地磁盘，然后再从本地磁盘加载到内存中，接着 master 会将内存中缓存的写命令发送到 slave，slave 也会同步这些数据；如果slave node 跟 master node 有网络故障，断开了连接，会自动重连，连接之后 master node 仅会复制给 slave 部分缺少的数据。 过程原理 当从库和主库建立MS关系后，会向主数据库发送SYNC命令 主库接收到SYNC命令后会开始在后台保存快照(RDB持久化过程)， 并将期间接收到的 写命令缓存起来 当快照完成后，主Redis会将快照文件和所有缓存的写命令发送给从Redis 从Redis接收到后，会载入快照文件并且执行收到的缓存的命令 之后，主Redis每当接收到写命令时就会将命令发送从Redis，从而保证数据的一致 主从复制优缺点优点 支持主从复制，主机会自动将数据同步到从机，可以进行读写分离 为了分载Master的读操作压力，Slave服务器可以为客户端提供只读操作的服务，写服务仍然必须由Master来完成 Slave同样可以接受其它Slaves的连接和同步请求，这样可以有效的分载Master的同步压力。(层层连接) Master Server是以非阻塞的方式为Slaves提供服务。所以在Master-Slave同步期间，客户端仍然可以提交查询或修改请求。 Slave Server同样是以非阻塞的方式完成数据同步。在同步期间，如果有客户端提交查询请求，Redis则返回同步之前的数据 缺点 Redis不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的IP才能恢复。 主机宕机，宕机前有部分数据未能及时同步到从机，切换IP后还会引入数据不一致的问题，降低了系统的可用性。 Redis较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。 层层链路上一个M链接下一个 S！ 环境配置只配置从库，不用配置主库! 1234567891011127.0.0.1:6379&gt; info replication # 查看当前库的信息 # Replication role:master # 角色 master connected_slaves:0 # 没有从机master_replid:b63c90e6c501143759cb0e7f450bd1eb0c70882amaster_replid2:0000000000000000000000000000000000000000master_repl_offset:0 second_repl_offset:-1repl_backlog_active:0 repl_backlog_size:1048576repl_backlog_first_byte_offset:0 repl_backlog_histlen:0 复制3个配置文件，然后修改对应的信息 12341端口2pid 名字3log文件名字4dump.rdb 名字 修改完毕之后，启动我们的3个redis服务器，可以通过进程信息查看！ 默认情况下，每台Redis服务器都是主节点；我们一般情况下只用配置从就好了！认老大！ 一主 （79）二从（80，81） 真实的从主配置应该在配置文件中配置，这样的话是永久的，我们这里用的是命令，暂时的！主机可以写，从机不能写只能读！主机中的所有信息和数据，都会自动从机保存！测试：主机断开连接，从机依旧连接到主机的，但是没有写操作，这个候，主机如果回来了，从机依旧可以直接获取到主机写的信息！如果是使用命令行，来配置的主从，这个时候如果重启了，就会变回主机！只要变为从机，立马就会从 主机中获取值！ 参考请你谈谈Redis主从复制的理解？","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"}],"author":"Marlowe"},{"title":"","slug":"NoSQL/如何保证缓存和数据库数据的一致性？","date":"2021-04-17T07:30:13.752Z","updated":"2021-08-25T15:37:33.972Z","comments":true,"path":"2021/04/17/NoSQL/如何保证缓存和数据库数据的一致性？/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/17/NoSQL/%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E7%BC%93%E5%AD%98%E5%92%8C%E6%95%B0%E6%8D%AE%E5%BA%93%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%EF%BC%9F/","excerpt":"title: 如何保证缓存和数据库数据的一致性？author: Marlowedate: 2021-04-17 15:30:13tags: 缓存 数据库categories: NoSQL","text":"title: 如何保证缓存和数据库数据的一致性？author: Marlowedate: 2021-04-17 15:30:13tags: 缓存 数据库categories: NoSQL 如何保证缓存和数据库数据的一致性？当我们对数据进行修改的时候，到底是先删缓存，还是先写数据库?1、如果先删缓存，再写数据库: 在高并发场景下，当第一个线程删除了缓存，还没有来得及写数据库，第二个线程来读取数据，会发现缓存中的数据为空，那就会去读数据库中的数据(旧值，脏数据)，读完之后，把读到的结果写入缓存(此时，第一个线程已经将新的值写到缓存里面了)，这样缓存中的值就会被覆盖为修改前的脏数据。 总结: 在这种方式下，通常要求写操作不会太频繁。 解决方案: 先操作缓存，但是不删除缓存。将缓存修改为一个特殊值(-999)。客户端读缓存时，发现是默认值，就休眠一小会，再去查一次Redis。 问题： 1. 特殊值对业务有侵入。2. 休眠时间， 可能会多次重复，对性能有影响。 延时双删：先删除缓存，然后再写数据库，休眠一小会，再次删除缓存。 问题： 1. 如果数据写操作很频繁， 同样还是会有脏数据的问题。 2、先写数据库，再删缓存: 如果数据库写完了之后， 缓存删除失败，数据就会不一致。 总结: 始终只能保证一定时间内的最终一致性。 解决方案: 给缓存设置一个过期时间。 问题: 过期时间内，缓存数据不会更新。 引入MQ，保证原子操作。 解决方案: 将热点数据缓存设置为永不过期，但是在value当中写入一个逻辑上的过期时间，另外起一个后台线程，扫描这些key,对于已逻辑上过期的缓存，进行删除。 不是严格要求缓存+数据库必须一致性一般来说，就是如果你的系统不是严格要求缓存+数据库必须一致性的话，缓存可以稍微的跟数据库偶尔有不一致的情况，最好不要做这个方案，读请求和写请求串行化，串到一个内存队列里去，这样就可以保证一定不会出现不一致的情况 串行化之后，就会导致系统的吞吐量会大幅度的降低，用比正常情况下多几倍的机器去支撑线上的一个请求。 严格要求缓存+数据库必须一致性将不一致分为三种情况： 数据库有数据，缓存没有数据； 数据库有数据，缓存也有数据，数据不相等； 数据库没有数据，缓存有数据。 在讨论这三种情况之前，先说明一下使用缓存的策略，叫做 Cache Aside Pattern。简而言之就是 1. 首先尝试从缓存读取，读到数据则直接返回；如果读不到，就读数据库，并将数据会写到缓存，并返回。2. 需要更新数据时，先更新数据库，然后把缓存里对应的数据失效掉（删掉）。 第一种数据库有数据，缓存没有数据： 在读数据的时候，会自动把数据库的数据写到缓存，因此不一致自动消除. 第二种数据库有数据，缓存也有数据，数据不相等： 数据最终变成了不相等，但他们之前在某一个时间点一定是相等的（不管你使用懒加载还是预加载的方式，在缓存加载的那一刻，它一定和数据库一致）。这种不一致，一定是由于你更新数据所引发的。前面我们讲了更新数据的策略，先更新数据库，然后删除缓存。因此，不一致的原因，一定是数据库更新了，但是删除缓存失败了。 第三种数据库没有数据，缓存有数据， 情况和第二种类似，你把数据库的数据删了，但是删除缓存的时候失败了。 因此，最终的结论是，需要解决的不一致，产生的原因是更新数据库成功，但是删除缓存失败。 解决方案大概有以下几种： 对删除缓存进行重试，数据的一致性要求越高，我越是重试得快。 定期全量更新，简单地说，就是我定期把缓存全部清掉，然后再全量加载。 给所有的缓存一个失效期。 第三种方案可以说是一个大杀器，任何不一致，都可以靠失效期解决，失效期越短，数据一致性越高。但是失效期越短，查数据库就会越频繁。因此失效期应该根据业务来定。 并发不高的情况： 读: 读redis-&gt;没有，读mysql-&gt;把mysql数据写回redis，有的话直接从redis中取； 写: 写mysql-&gt;成功，再写redis； 并发高的情况： 读: 读redis-&gt;没有，读mysql-&gt;把mysql数据写回redis，有的话直接从redis中取； 写：异步话，先写入redis的缓存，就直接返回；定期或特定动作将数据保存到mysql，可以做到多次更新，一次保存；","categories":[],"tags":[]},{"title":"布隆过滤器","slug":"NoSQL/布隆过滤器","date":"2021-04-17T02:44:43.000Z","updated":"2021-08-26T10:28:05.688Z","comments":true,"path":"2021/04/17/NoSQL/布隆过滤器/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/17/NoSQL/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/","excerpt":"","text":"简介布隆过滤器（Bloom Filter）是1970年由布隆提出的。它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都比一般的算法要好的多，缺点是有一定的误识别率和删除困难。 在讲述布隆过滤器的原理之前，我们先思考一个问题，如果想要判断一个元素是否存在，你通常会怎么做？一般的做法都是将其保存起来然后通过比较确认，一共会有如下几种情况： 如果使用线性表或者数组存储，则查找的时间复杂度为 O(n)。 如果使用树存储，则查找的时间复杂度为 O(logn)。 如果使用哈希表存储，则查找的时间复杂度为 O(log(n/m))，m 为哈希分桶数。 对于上述三种情况我相信大部分读者都倾向于哈希表，因为其时间复杂度最低（在极端情况下时间复杂度可以为 O(1) ），但是哈希表也有缺陷，例如存储容量占比高，考虑到负载因子的存在，通常存储空间都不会被用完。当然无论是哈希表、树、线性表，一旦元素的数量极多时，查询的速度会变得很慢，而且占用的空间也会大到无法想象。那么有办法解决没有呢？答案是有，布隆过滤器就是解决该问题的利器。 设计思想布隆过滤器是一个由 一个长度为 M 比特的位数组（bit array）与 K 个哈希函数（hash function） 组成的数据结构。布隆过滤器主要用于用于检索一个元素是否在一个集合中。 位数组中的元素初始值都是 0 ，所有哈希函数可以把输入的数据均匀低散列。图例如下： 当要插入一个元素时，将其输入 K 个哈希函数，产生 K 个哈希值，同时以这些哈希值作为位数组的下标，将这些下标对应的比特值设置为 1。 当要查询一个元素时，同样是将其输入 K 个哈希函数，产生 K 个哈希值，然后检查这些哈希值中对应的比特值。如果有任意一个比特值为 0，则表明该元素一定不存在，如果所有比特值都是 1，则表明该元素可能存在，为什么不是一定存在呢？因为一个比特值为 1 有可能会受到其他元素的影响。所以 布隆过滤器是用于检测一个元素是否一定不存在或者有可能存在。 加入我们有一个布隆过滤器长度为 10，有 3 个哈希函数。这时我们我们将 ”死“插入到布隆过滤器中，经过三个哈希函数得到的哈希值为 3、6、9，则如下： 现在我们再存一个值：”磕“，假设得到的哈希值为 1 6 8，如下： 我们再查下 ”Redis“，假设返回的哈希值为 1 5 7，得到的比特值为 1 0 0 ，所以我们可以很确切地说”Redis“这个值一定不存在，如果查询 “Java” 得到的哈希值为 1 6 9，比特值为 1 1 1，那么我们是否可以说一定存在呢？答案是不可以，只能说 “Java” 这个值有可能存在。因为随着数据的增多，越来越多位置的比特值被设置为 1，有可能存在某个值从来没有被存储，但是哈希函数返回的位值都为 1 。 优缺点优点 不需要存储数据，只用比特表示，因此在空间占用率上有巨大的优势 检索效率高，插入和查询的时间复杂度都为 O(K)（K 表示哈希函数的个数） 哈希函数之间相互独立，可以在硬件指令层次并行计算，因此效率较高。 缺点 存在不确定的因素，无法判断一个元素是否一定存在，所以不适合要求 100% 准确率的场景 只能插入和查询元素，不能删除元素。 实例关于布隆过滤器，我们不需要自己实现，Guava 已经帮助我们实现了，使用起来非常简单。 引入 pom 12345&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;25.1-jre&lt;/version&gt;&lt;/dependency&gt; 使用 123456789101112public static void main(String... args)&#123; /** * 创建一个插入对象为一亿，误报率为0.01%的布隆过滤器 */ BloomFilter&lt;CharSequence&gt; bloomFilter = BloomFilter.create(Funnels.stringFunnel(Charset.forName(&quot;utf-8&quot;)), 100000000, 0.0001); bloomFilter.put(&quot;死&quot;); bloomFilter.put(&quot;磕&quot;); bloomFilter.put(&quot;Redis&quot;); System.out.println(bloomFilter.mightContain(&quot;Redis&quot;)); System.out.println(bloomFilter.mightContain(&quot;Java&quot;)); &#125; 参考【死磕 Redis】—– 布隆过滤器","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"过滤器","slug":"过滤器","permalink":"https://xmmarlowe.github.io/tags/%E8%BF%87%E6%BB%A4%E5%99%A8/"}],"author":"Marlowe"},{"title":"红黑树相比于BST和AVL树有什么优点？","slug":"算法与数据结构/红黑树相比于BST和AVL树有什么优点？","date":"2021-04-16T13:35:31.000Z","updated":"2021-04-22T06:54:04.864Z","comments":true,"path":"2021/04/16/算法与数据结构/红黑树相比于BST和AVL树有什么优点？/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/16/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E7%BA%A2%E9%BB%91%E6%A0%91%E7%9B%B8%E6%AF%94%E4%BA%8EBST%E5%92%8CAVL%E6%A0%91%E6%9C%89%E4%BB%80%E4%B9%88%E4%BC%98%E7%82%B9%EF%BC%9F/","excerpt":"","text":"红黑树是牺牲了严格的高度平衡的优越条件为代价，它只要求部分地达到平衡要求，降低了对旋转的要求，从而提高了性能。红黑树能够以O(log2 n)的时间复杂度进行搜索、插入、删除操作。此外，由于它的设计，任何不平衡都会在三次旋转之内解决。当然，还有一些更好的，但实现起来更复杂的数据结构能够做到一步旋转之内达到平衡，但红黑树能够给我们一个比较“便宜”的解决方案。 相比于BST，因为红黑树可以能确保树的最长路径不大于两倍的最短路径的长度，所以可以看出它的查找效果是有最低保证的。在最坏的情况下也可以保证O(logN)的，这是要好于二叉查找树的。因为二叉查找树最坏情况可以让查找达到O(N)。 红黑树的算法时间复杂度和AVL相同，但统计性能比AVL树更高，所以在插入和删除中所做的后期维护操作肯定会比红黑树要耗时好多，但是他们的查找效率都是O(logN)，所以红黑树应用还是高于AVL树的. 实际上插入 AVL 树和红黑树的速度取决于你所插入的数据.如果你的数据分布较好,则比较宜于采用 AVL树(例如随机产生系列数),但是如果你想处理比较杂乱的情况,则红黑树是比较快的。","categories":[{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"https://xmmarlowe.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"BST","slug":"BST","permalink":"https://xmmarlowe.github.io/tags/BST/"},{"name":"AVL","slug":"AVL","permalink":"https://xmmarlowe.github.io/tags/AVL/"},{"name":"红黑树","slug":"红黑树","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%A2%E9%BB%91%E6%A0%91/"}],"author":"Marlowe"},{"title":"Java内存模型-JMM","slug":"Java/Java内存模型-JMM","date":"2021-04-16T13:19:30.000Z","updated":"2021-04-22T07:23:46.027Z","comments":true,"path":"2021/04/16/Java/Java内存模型-JMM/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/16/Java/Java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B-JMM/","excerpt":"","text":"介绍Java内存模型（Java Memery Model）用来屏蔽掉各种硬件和操作系统的内存访问差异。以至于让Java在各中平台下都能达到一致的内存访问效果。 简单的说，JMM 定义了一套在多线程读写共享数据时（成员变量、数组）时，对数据的可见性、有序性、和原子性的规则和保障. 从硬件角度来看。因为处理器的运算速度很快，比如做一个递增操作，就需要从内存中拿值，操作后再放回内存。这样的I/O是无法避免的，但这I/O速度和处理器的运算速度就不是一个数量级，所以为了解决这个问题，就对每个处理器加一个高速缓存（Cache）来作为处理器与内存之间的缓冲。把需要使用的数据复制到缓存中，运算完成之后在从缓存同步到内存。这样处理器就不用等待缓慢的内存读写了。 Java内存模型图 主存与工作内存的一些交互指令 操作 作用对象 解释 lock 主内存 把一个变量标识为一条线程独占的状态 unlock 主内存 把一个处于锁定状态的变量释放出来，释放后才可被其他线程锁定 read 主内存 把一个变量的值从主内存传输到线程工作内存中，以便 load 操作使用 load 工作内存 把 read 操作从主内存中得到的变量值放入工作内存中 use 工作内存 把工作内存中一个变量的值传递给执行引擎，每当虚拟机遇到一个需要使用到变量值的字节码指令时将会执行这个操作 assign 工作内存 把一个从执行引擎接收到的值赋接收到的值赋给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作 store 工作内存 把工作内存中的一个变量的值传送到主内存中，以便 write 操作 write 工作内存 把 store 操作从工作内存中得到的变量的值放入主内存的变量中 例子： 对于volatile型变量的特殊规则关键字 volatile 是 Java 虚拟机提供的最轻量级的同步机制。保存了线程可见性与防止指令重排。 一个变量被定义为volatile的特性1.保证此变量对所有线程的可见性。但是对变量的操作如果不是原子操作，那么并发情况下不安全。如果不满足以下条件就，需要加锁保证并发安全。 运算结果并不依赖变量当前值 能够确保只有单一的线程修改变量的值 变量不需要与其他的状态变量共同参与不变约束 2.禁止指令重排序优化。通过插入内存屏障保证一致性。 对于long和double型变量的特殊规则Java 要求对于主内存和工作内存之间的八个操作都是原子性的，但是对于 64 位的数据类型，有一条宽松的规定：允许虚拟机将没有被 volatile 修饰的 64 位数据的读写操作划分为两次 32 位的操作来进行，即允许虚拟机实现选择可以不保证 64 位数据类型的 load、store、read 和 write 这 4 个操作的原子性。这就是 long 和 double 的非原子性协定。 原子性、可见性与有序性原子性(Atomicity)由 Java 内存模型来直接保证的原子性变量操作包括 read、load、assign、use、store 和 write。大致可以认为基本数据类型的操作是原子性的。同时 lock 和 unlock 可以保证更大范围操作的原子性。而 synchronize 同步块操作的原子性是用更高层次的字节码指令 monitorenter 和 monitorexit 来隐式操作的。 可见性(Visibility)是指当一个线程修改了共享变量的值，其他线程也能够立即得知这个通知。主要操作细节就是修改值后将值同步至主内存(volatile 值使用前都会从主内存刷新)，除了 volatile 还有 synchronize 和 final 可以保证可见性。同步块的可见性是由“对一个变量执行unlock 操作之前，必须先把此变量同步会主内存中( store、write 操作)”这条规则获得。而 final 可见性是指：被 final 修饰的字段在构造器中一旦完成，并且构造器没有把 “this” 的引用传递出去( this 引用逃逸是一件很危险的事情，其他线程有可能通过这个引用访问到“初始化了一半”的对象)，那在其他线程中就能看见 final 字段的值。 有序性(Ordering)如果在被线程内观察，所有操作都是有序的；如果在一个线程中观察另一个线程，所有操作都是无序的。前半句指“线程内表现为串行的语义”，后半句是指“指令重排”现象和“工作内存与主内存同步延迟”现象。Java 语言通过 volatile 和 synchronize 两个关键字来保证线程之间操作的有序性。volatile 自身就禁止指令重排，而 synchronize 则是由“一个变量在同一时刻只允许一条线程对其进行 lock 操作”这条规则获得，这条规则决定了持有同一个锁的两个同步块只能串行的进入。 先行发生原则也就是 happens-before 原则。这个原则是判断数据是否存在竞争、线程是否安全的主要依据。先行发生是 Java 内存模型中定义的两项操作之间的偏序关系。 天然的先行发生关系|规则| 解释||:—-:|:—-:|:—-:||程序次序规则| 在一个线程内，代码按照书写的控制流顺序执行||管程锁定规则| 一个 unlock 操作先行发生于后面对同一个锁的 lock 操作||volatile 变量规则| volatile 变量的写操作先行发生于后面对这个变量的读操作||线程启动规则| Thread 对象的 start() 方法先行发生于此线程的每一个动作||线程终止规则| 线程中所有的操作都先行发生于对此线程的终止检测(通过 Thread.join() 方法结束、 Thread.isAlive() 的返回值检测)||线程中断规则| 对线程 interrupt() 方法调用优先发生于被中断线程的代码检测到中断事件的发生 (通过 Thread.interrupted() 方法检测)||对象终结规则| 一个对象的初始化完成(构造函数执行结束)先行发生于它的 finalize() 方法的开始||传递性| 如果操作 A 先于 操作 B 发生，操作 B 先于 操作 C 发生，那么操作 A 先于 操作 C| 参考Java内存模型JMM","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"JMM","slug":"JMM","permalink":"https://xmmarlowe.github.io/tags/JMM/"}],"author":"Marlowe"},{"title":"线程间通信问题","slug":"并发/线程间通信问题","date":"2021-04-16T13:15:10.000Z","updated":"2021-04-22T07:28:28.102Z","comments":true,"path":"2021/04/16/并发/线程间通信问题/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/16/%E5%B9%B6%E5%8F%91/%E7%BA%BF%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E9%97%AE%E9%A2%98/","excerpt":"","text":"wait/notify/notifyAll wait()、notify/notifyAll() 方法是Object的本地final方法，无法被重写。 wait()使当前线程阻塞，前提是 必须先获得锁，一般配合synchronized 关键字使用，即，一般在synchronized 同步代码块里使用 wait()、notify/notifyAll() 方法。 由于 wait()、notify/notifyAll() 在synchronized 代码块执行，说明当前线程一定是获取了锁的。 当线程执行wait()方法时候，会释放当前的锁，然后让出CPU，进入等待状态。 只有当 notify/notifyAll() 被执行时候，才会唤醒一个或多个正处于等待状态的线程，然后继续往下执行，直到执行完synchronized 代码块的代码或是中途遇到wait() ，再次释放锁。 也就是说，notify/notifyAll() 的执行只是唤醒沉睡的线程，而不会立即释放锁，锁的释放要看代码块的具体执行情况。所以在编程中，尽量在使用了notify/notifyAll() 后立即退出临界区，以唤醒其他线程让其获得锁 wait() 需要被try catch包围，以便发生异常中断也可以使wait等待的线程唤醒。 notify 和wait 的顺序不能错，如果A线程先执行notify方法，B线程在执行wait方法，那么B线程是无法被唤醒的。 notify 和 notifyAll的区别notify方法只唤醒一个等待（对象的）线程并使该线程开始执行。所以如果有多个线程等待一个对象，这个方法只会唤醒其中一个线程，选择哪个线程取决于操作系统对多线程管理的实现。notifyAll 会唤醒所有等待(对象的)线程，尽管哪一个线程将会第一个处理取决于操作系统的实现。如果当前情况下有多个线程需要被唤醒，推荐使用notifyAll 方法。比如在生产者-消费者里面的使用，每次都需要唤醒所有的消费者或是生产者，以判断程序是否可以继续往下执行。 在多线程中要测试某个条件的变化，使用if 还是while？要注意，notify唤醒沉睡的线程后，线程会接着上次的执行继续往下执行。所以在进行条件判断时候，可以先把 wait 语句忽略不计来进行考虑；显然，要确保程序一定要执行，并且要保证程序直到满足一定的条件再执行，要使用while进行等待，直到满足条件才继续往下执行。 Volatile见站内文章volatile关键字 countDownLatch、CyclicBarrier、Semaphore见站内文章countDownLatch、CyclicBarrier、Semaphore 参考Java多线程学习之wait、notify/notifyAll 详解","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"线程","slug":"线程","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B/"},{"name":"通信","slug":"通信","permalink":"https://xmmarlowe.github.io/tags/%E9%80%9A%E4%BF%A1/"}],"author":"Marlowe"},{"title":"Redis跳跃表","slug":"NoSQL/Redis跳跃表","date":"2021-04-16T12:39:08.000Z","updated":"2021-04-17T14:58:35.358Z","comments":true,"path":"2021/04/16/NoSQL/Redis跳跃表/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/16/NoSQL/Redis%E8%B7%B3%E8%B7%83%E8%A1%A8/","excerpt":"Redis跳跃表相关问题…","text":"Redis跳跃表相关问题… 什么是跳跃表跳跃表是一种有序的数据结构，它通过在每个节点中维持多个指向其他的几点指针，从而达到快速访问队尾目的。跳跃表的效率可以和平衡树想媲美了，最关键是它的实现相对于平衡树来说，代码的实现上简单很多。 跳跃表用在哪里说真的，跳跃表在 Redis 中使用不是特别广泛，只用在了两个地方：一、是实现有序集合键。二、是集群节点中用作内部数据结构。 跳跃表原理我们先来看一下一张完整的跳跃表的图。 跳跃表的 level 是如何定义的？跳跃表 level 层级完全是随机的。一般来说，层级越多，访问节点的速度越快。 跳跃表的插入首先我们需要插入几个数据。链表开始时是空的。插入 level = 3，key = 1当我们插入 level = 3，key = 1 时，结果如下：插入 level = 1，key = 2当继续插入 level = 1，key = 2 时，结果如下插入 level = 2，key = 3当继续插入 level = 2，key = 3 时，结果如下插入 level = 3，key = 5当继续插入 level = 3，key = 5 时，结果如下插入 level = 1，key = 66当继续插入 level = 1，key = 66 时，结果如下插入 level = 2，key = 100当继续插入 level = 2，key = 100 时，结果如下上述便是跳跃表插入原理，关键点就是层级–使用抛硬币的方式，感觉还真是挺随机的。每个层级最末端节点指向都是为 null，表示该层级到达末尾，可以往下一级跳。 跳跃表的查询现在我们要找键为 66 的节点的值。那跳跃表是如何进行查询的呢？ 跳跃表的查询是从顶层往下找，那么会先从第顶层开始找，方式就是循环比较，如过顶层节点的下一个节点为空说明到达末尾，会跳到第二层，继续遍历，直到找到对应节点。 如下图所示红色框内，我们带着键 66 和 1 比较，发现 66 大于 1。继续找顶层的下一个节点，发现 66 也是大于五的，继续遍历。由于下一节点为空，则会跳到 level 2。 上层没有找到 66，这时跳到 level 2 进行遍历，但是这里有一个点需要注意，遍历链表不是又重新遍历。而是从 5 这个节点继续往下找下一个节点。如下，我们遍历了 level 3 后，记录下当前处在 5 这个节点，那接下来遍历是 5 往后走，发现 100 大于目标 66，所以还是继续下沉。 当到 level 1 时，发现 5 的下一个节点恰恰好是 66 ，就将结果直接返回。 跳跃表删除跳跃表的删除和查找类似，都是一级一级找到相对应的节点，然后将 next 对象指向下下个节点，完全和链表类似。 现在我们来删除 66 这个节点，查找 66 节点和上述类似。 接下来是断掉 5 节点 next 的 66 节点，然后将它指向 100 节点。如上就是跳跃表的删除操作了，和我们平时接触的链表是一致的。当然，跳跃表的修改，也是和删除查找类似，只不过是将值修改罢了，就不继续介绍了。 参考面试准备 – Redis 跳跃表","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"}],"author":"Marlowe"},{"title":"假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如何将它们全部找出来？","slug":"NoSQL/假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如何将它们全部找出来？","date":"2021-04-16T12:34:16.000Z","updated":"2021-04-17T14:58:35.394Z","comments":true,"path":"2021/04/16/NoSQL/假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如何将它们全部找出来？/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/16/NoSQL/%E5%81%87%E5%A6%82Redis%E9%87%8C%E9%9D%A2%E6%9C%891%E4%BA%BF%E4%B8%AAkey%EF%BC%8C%E5%85%B6%E4%B8%AD%E6%9C%8910w%E4%B8%AAkey%E6%98%AF%E4%BB%A5%E6%9F%90%E4%B8%AA%E5%9B%BA%E5%AE%9A%E7%9A%84%E5%B7%B2%E7%9F%A5%E7%9A%84%E5%89%8D%E7%BC%80%E5%BC%80%E5%A4%B4%E7%9A%84%EF%BC%8C%E5%A6%82%E4%BD%95%E5%B0%86%E5%AE%83%E4%BB%AC%E5%85%A8%E9%83%A8%E6%89%BE%E5%87%BA%E6%9D%A5%EF%BC%9F/","excerpt":"","text":"使用keys指令可以扫出指定模式的key列表。 对方接着追问：如果这个redis正在给线上的业务提供服务，那使用keys指令会有什么问题？ 这个时候你要回答redis关键的一个特性：redis的单线程的。keys指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用scan指令， scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长。","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"}],"author":"Marlowe"},{"title":"Redis事务的CAS(check-and-set)","slug":"NoSQL/Redis事务的CAS-check-and-set","date":"2021-04-16T12:28:46.000Z","updated":"2021-04-22T07:36:13.177Z","comments":true,"path":"2021/04/16/NoSQL/Redis事务的CAS-check-and-set/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/16/NoSQL/Redis%E4%BA%8B%E5%8A%A1%E7%9A%84CAS-check-and-set/","excerpt":"和众多其它数据库一样，Redis作为NoSQL数据库也同样提供了事务机制。在Redis中，MULTI/EXEC/DISCARD/WATCH这四个命令是我们实现事务的基石。","text":"和众多其它数据库一样，Redis作为NoSQL数据库也同样提供了事务机制。在Redis中，MULTI/EXEC/DISCARD/WATCH这四个命令是我们实现事务的基石。 Redis中事务的实现特征： 在事务中的所有命令都将会被串行化的顺序执行，事务执行期间，Redis不会再为其它客户端的请求提供任何服务，从而保证了事物中的所有命令被原子的执行。 和关系型数据库中的事务相比，在Redis事务中如果有某一条命令执行失败，其后的命令仍然会被继续执行。 我们可以通过MULTI命令开启一个事务，有关系型数据库开发经验的人可以将其理解为”BEGIN TRANSACTION”语句。在该语句之后执行的命令都将被视为事务之内的操作，最后我们可以通过执行EXEC/DISCARD命令来提交/回滚该事务内的所有操作。这两个Redis命令可被视为等同于关系型数据库中的COMMIT/ROLLBACK语句。 在事务开启之前，如果客户端与服务器之间出现通讯故障并导致网络断开，其后所有待执行的语句都将不会被服务器执行。然而如果网络中断事件是发生在客户端执行EXEC命令之后，那么该事务中的所有命令都会被服务器执行。 当使用Append-Only模式时，Redis会通过调用系统函数write将该事务内的所有写操作在本次调用中全部写入磁盘。然而如果在写入的过程中出现系统崩溃，如电源故障导致的宕机，那么此时也许只有部分数据被写入到磁盘，而另外一部分数据却已经丢失。Redis服务器会在重新启动时执行一系列必要的一致性检测，一旦发现类似问题，就会立即退出并给出相应的错误提示。此时，我们就要充分利用Redis工具包中提供的redis-check-aof工具，该工具可以帮助我们定位到数据不一致的错误，并将已经写入的部分数据进行回滚。修复之后我们就可以再次重新启动Redis服务器了。","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"}],"author":"Marlowe"},{"title":"为什么redis需要把所有数据放到内存中?","slug":"NoSQL/为什么redis需要把所有数据放到内存中","date":"2021-04-16T12:24:56.000Z","updated":"2021-04-17T14:58:35.364Z","comments":true,"path":"2021/04/16/NoSQL/为什么redis需要把所有数据放到内存中/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/16/NoSQL/%E4%B8%BA%E4%BB%80%E4%B9%88redis%E9%9C%80%E8%A6%81%E6%8A%8A%E6%89%80%E6%9C%89%E6%95%B0%E6%8D%AE%E6%94%BE%E5%88%B0%E5%86%85%E5%AD%98%E4%B8%AD/","excerpt":"","text":"Redis为了达到最快的读写速度将数据都读到内存中，并通过异步的方式将数据写入磁盘。所以redis具有快速和数据持久化的特征。如果不将数据放在内存中，磁盘I/O速度为严重影响redis的性能。在内存越来越便宜的今天，redis将会越来越受欢迎。 如果设置了最大使用的内存，则数据已有记录数达到内存限值后不能继续插入新值。","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"}],"author":"Marlowe"},{"title":"Redis内存淘汰机制","slug":"NoSQL/Redis内存淘汰机制","date":"2021-04-16T12:20:38.000Z","updated":"2021-08-20T15:49:53.351Z","comments":true,"path":"2021/04/16/NoSQL/Redis内存淘汰机制/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/16/NoSQL/Redis%E5%86%85%E5%AD%98%E6%B7%98%E6%B1%B0%E6%9C%BA%E5%88%B6/","excerpt":"","text":"Redis 提供 6 种数据淘汰策略 volatile-lru（least recently used）： 从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl： 从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random： 从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru（least recently used）： 当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的） allkeys-random： 从数据集（server.db[i].dict）中任意选择数据淘汰 no-eviction： 禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。这个应该没人使用吧！ 4.0 版本后增加以下两种 volatile-lfu（least frequently used）： 从已设置过期时间的数据集(server.db[i].expires)中挑选最不经常使用的数据淘汰 allkeys-lfu（least frequently used）： 当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的 key 淘汰策略详解redis过期键的删除策略如果一个键是过期的，那它到了过期时间之后是不是马上就从内存中被被删除呢？ 如果回答yes，你自己走还是面试官送你？ 如果不是，那过期后到底什么时候被删除呢？？是个什么操作？ 三种不同的删除策略 定时删除 - 总结：对CPU不友好，用处理器性能换取存储空间（拿时间换空间） 惰性删除 - 总结：对memory不友好，用存储空间换取处理器性能（拿空间换时间） 上面两种方案都走极端 - 定期删除 - 定期抽样key，判断是否过期（存在漏网之鱼） 定时删除Redis不可能时时刻刻遍历所有被设置了生存时间的key，来检测数据是否已经到达过期时间，然后对它进行删除。 立即删除能保证内存中数据的最大新鲜度，因为它保证过期键值会在过期后马上被删除，其所占用的内存也会随之释放。但是立即删除对cpu是最不友好的。因为删除操作会占用cpu的时间，如果刚好碰上了cpu很忙的时候，比如正在做交集或排序等计算的时候，就会给cpu造成额外的压力，让CPU心累，时时需要删除，忙死。 这会产生大量的性能消耗，同时也会影响数据的读取操作。 惰性删除数据到达过期时间，不做处理。等下次访问该数据时， 如果未过期，返回数据； 发现已过期，删除，返回不存在。 惰性删除策略的缺点是，它对内存是最不友好的。 如果一个键已经过期，而这个键又仍然保留在数据库中，那么只要这个过期键不被删除，它所占用的内存就不会释放。 在使用惰性删除策略时，如果数据库中有非常多的过期键，而这些过期键又恰好没有被访问到的话，那么它们也许永远也不会被删除（除非用户手动执行FLUSHDB），我们甚至可以将这种情况看作是一种内存泄漏 – 无用的垃圾数据占用了大量的内存，而服务器却不会自己去释放它们，这对于运行状态非常依赖于内存的Redis服务器来说，肯定不是一个好消息。 定期删除定期删除策略是前两种策略的折中： 定期删除策略每隔一段时间执行一次删除过期键操作，并通过限制删除操作执行的时长和频率来减少删除操作对CPU时间的影响。 周期性轮询Redis库中的时效性数据，来用随机抽取的策略，利用过期数据占比的方式控制删除频度 特点1：CPU性能占用设置有峰值，检测频度可自定义设置 特点2：内存压力不是很大，长期占用内存的冷数据会被持续清理 总结：周期性抽查存储空间（随机抽查，重点抽查） 举例： redis默认每个100ms检查，是否有过期的key，有过期key则删除。注意：redis不是每隔100ms将所有的key检查一次而是随机抽取进行检查(如果每隔100ms，全部key进行检查，redis直接进去ICU)。因此，如果只采用定期删除策略，会导致很多key到时间没有删除。 定期删除策略的难点是确定删除操作执行的时长和频率:如果删除操作执行得太频繁，或者执行的时间太长，定期删除策略就会退化成定时删除策略，以至于将CPU时间过多地消耗在删除过期键上面。如果删除操作执行得太少，或者执行的时间太短，定期删除策略又会和惰性删除束略一样，出现浪费内存的情况。因此，如果采用定期删除策略的话，服务器必须根据情况，合理地设置删除操作的执行时长和执行频率。 上述步骤都过堂了，还有漏洞吗？ 定期删除时，从来没有被抽查到 惰性删除时，也从来没有被点中使用过 上述2步骤====&gt;大量过期的key堆积在内存中，导致redis内存空间紧张或者很快耗尽 必须要有一个更好的兜底方案 内存淘汰策略登场（Redis 6.0.8版本） noeviction：不会驱逐任何key volatile-lfu：对所有设置了过期时间的key使用LFU算法进行删除 volatile-Iru：对所有设置了过期时间的key使用LRU算法进行删除 volatile-random：对所有设置了过期时间的key随机删除 volatile-ttl：删除马上要过期的key allkeys-lfu：对所有key使用LFU算法进行删除 allkeys-Iru：对所有key使用LRU算法进行删除 allkeys-random：对所有key随机删除 上面总结 2*4得8 2个维度 过期键中筛选 所有键中筛选 4个方面 LRU LFU random ttl（Time To Live） 8个选项 如何配置，修改 命令 config set maxmemory-policy noeviction config get maxmemory 配置文件 - 配置文件redis.conf的maxmemory-policy参数","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"}],"author":"Marlowe"},{"title":"Java反射相关知识点","slug":"Java/Java反射相关知识点","date":"2021-04-16T08:00:16.000Z","updated":"2021-04-22T07:43:39.141Z","comments":true,"path":"2021/04/16/Java/Java反射相关知识点/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/16/Java/Java%E5%8F%8D%E5%B0%84%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E7%82%B9/","excerpt":"","text":"什么是反射？反射是在运行状态中，对于任意一个类， 都能够知道这个类的所有属性和方法；对于任意一个对象， 都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为 Java 语言的反射机制。 哪里用到反射机制？ JDBC中，利用反射 (Class.forName(xxx)) 动态加载了数据库驱动程序。 Web服务器中利用反射调用了Sevlet的服务方法。 Eclispe等开发工具利用反射动态刨析对象的类型与结构，动态提示对象的属性和方法。 很多框架都用到反射机制，注入属性，调用方法，如Spring。 什么叫对象序列化，什么是反序列化，实现对象序列化需要做哪些工作？ 对象序列化： 将对象中的数据编码为字节序列的过程。 反序列化： 将对象的编码字节重新反向解码为对象的过程。 JAVA提供了API实现了对象的序列化和反序列化的功能，使用这些API时需要遵守如下约定： 被序列化的对象类型需要实现序列化接口，此接口是标志接口，没有声明任何的抽象方法，JAVA编译器识别这个接口，自动的为这个类添加序列化和反序列化方法。 为了保持序列化过程的稳定，建议在类中添加序列化版本号。 不想让字段放在硬盘上就加transient 以下情况需要使用 Java 序列化： 想把的内存中的对象状态保存到一个文件中或者数据库中时候； 想用套接字在网络上传送对象的时候； 想通过RMI（远程方法调用）传输对象的时候。 反射机制的优缺点？ 优点： 可以动态执行，在运行期间根据业务功能动态执行方法、访问属性，最大限度发挥了java的灵活性。 缺点： 让我们在运行时有了分析操作类的能力，这同样也增加了安全问题。比如可以无视泛型参数的安全检查（泛型参数的安全检查发生在编译时）。另外，反射的性能也要稍差点，不过，对于框架来说实际是影响不大的。 Java反射机制的作用 在运行时判断任意一个对象所属的类 在运行时构造任意一个类的对象 在运行时判断任意一个类所具有的成员变量和方法 在运行时调用任意一个对象的方法 获取 Class 对象的四种方式如果我们动态获取到这些信息，我们需要依靠 Class 对象。Class 类对象将一个类的方法、变量等信息告诉运行的程序。Java 提供了四种方式获取 Class 对象: 1. 知道具体类的情况下可以使用： 1Class alunbarClass = TargetObject.class; 但是我们一般是不知道具体类的，基本都是通过遍历包下面的类来获取 Class 对象，通过此方式获取 Class 对象不会进行初始化。 2. 通过 Class.forName()传入类的路径获取： 1Class alunbarClass1 = Class.forName(&quot;cn.javaguide.TargetObject&quot;); 3.通过对象实例instance.getClass()获取： 12TargetObject o = new TargetObject();Class alunbarClass2 = o.getClass(); 4.通过类加载器xxxClassLoader.loadClass()传入类路径获取: 1class clazz = ClassLoader.LoadClass(&quot;cn.javaguide.TargetObject&quot;); 通过类加载器获取 Class 对象不会进行初始化，意味着不进行包括初始化等一些列步骤，静态块和静态对象不会得到执行 反射的一些基本操作简单用代码演示一下反射的一些操作! 1.创建一个我们要使用反射操作的类 TargetObject。 1234567891011121314151617package cn.javaguide;public class TargetObject &#123; private String value; public TargetObject() &#123; value = &quot;JavaGuide&quot;; &#125; public void publicMethod(String s) &#123; System.out.println(&quot;I love &quot; + s); &#125; private void privateMethod() &#123; System.out.println(&quot;value is &quot; + value); &#125;&#125; 2.使用反射操作这个类的方法以及参数 12345678910111213141516171819202122232425262728293031323334353637383940414243package cn.javaguide;import java.lang.reflect.Field;import java.lang.reflect.InvocationTargetException;import java.lang.reflect.Method;public class Main &#123; public static void main(String[] args) throws ClassNotFoundException, NoSuchMethodException, IllegalAccessException, InstantiationException, InvocationTargetException, NoSuchFieldException &#123; /** * 获取TargetObject类的Class对象并且创建TargetObject类实例 */ Class&lt;?&gt; tagetClass = Class.forName(&quot;cn.javaguide.TargetObject&quot;); TargetObject targetObject = (TargetObject) tagetClass.newInstance(); /** * 获取所有类中所有定义的方法 */ Method[] methods = tagetClass.getDeclaredMethods(); for (Method method : methods) &#123; System.out.println(method.getName()); &#125; /** * 获取指定方法并调用 */ Method publicMethod = tagetClass.getDeclaredMethod(&quot;publicMethod&quot;, String.class); publicMethod.invoke(targetObject, &quot;JavaGuide&quot;); /** * 获取指定参数并对参数进行修改 */ Field field = tagetClass.getDeclaredField(&quot;value&quot;); //为了对类中的参数进行修改我们取消安全检查 field.setAccessible(true); field.set(targetObject, &quot;JavaGuide&quot;); /** * 调用 private 方法 */ Method privateMethod = tagetClass.getDeclaredMethod(&quot;privateMethod&quot;); //为了调用private方法我们取消安全检查 privateMethod.setAccessible(true); privateMethod.invoke(targetObject); &#125;&#125; 输出内容： 1234publicMethodprivateMethodI love JavaGuidevalue is JavaGuide 参考Java反射常见面试题","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"反射","slug":"反射","permalink":"https://xmmarlowe.github.io/tags/%E5%8F%8D%E5%B0%84/"}],"author":"Marlowe"},{"title":"Redis 对比 MySQL，为什么 redis 是快的？","slug":"NoSQL/Redis-对比-MySQL，为什么-redis-是快的？","date":"2021-04-15T15:00:53.000Z","updated":"2021-04-21T05:31:42.888Z","comments":true,"path":"2021/04/15/NoSQL/Redis-对比-MySQL，为什么-redis-是快的？/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/15/NoSQL/Redis-%E5%AF%B9%E6%AF%94-MySQL%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88-redis-%E6%98%AF%E5%BF%AB%E7%9A%84%EF%BC%9F/","excerpt":"","text":"1.Redis是基于内存存储的，MySQL是基于磁盘存储的 2.Redis存储的是k-v格式的数据。时间复杂度是O(1),常数阶，而MySQL引擎的底层实现是B+Tree，时间复杂度是O(logn)，对数阶。Redis会比MySQL快一点点。 3.MySQL数据存储是存储在表中，查找数据时要先对表进行全局扫描或者根据索引查找，这涉及到磁盘的查找，磁盘查找如果是按条点查找可能会快点，但是顺序查找就比较慢；而Redis不用这么麻烦，本身就是存储在内存中，会根据数据在内存的位置直接取出。 4.Redis是单线程的多路复用IO，单线程避免了线程切换的开销，而多路复用IO避免了IO等待的开销，在多核处理器下提高处理器的使用效率可以对数据进行分区，然后每个处理器处理不同的数据。 参考Redis为什么会比MySQL快？","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"},{"name":"MySQL","slug":"MySQL","permalink":"https://xmmarlowe.github.io/tags/MySQL/"}],"author":"Marlowe"},{"title":"设计模式-装饰器","slug":"设计模式/设计模式-装饰器","date":"2021-04-15T13:36:54.000Z","updated":"2021-04-22T07:47:09.032Z","comments":true,"path":"2021/04/15/设计模式/设计模式-装饰器/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/15/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%A3%85%E9%A5%B0%E5%99%A8/","excerpt":"装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能，同时又不改变其结构。这种类型的设计模式属于结构型模式，它是作为现有的类的一个包装。","text":"装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能，同时又不改变其结构。这种类型的设计模式属于结构型模式，它是作为现有的类的一个包装。 介绍意图： 动态地给一个对象添加一些额外的职责。就增加功能来说，装饰器模式相比生成子类更为灵活。 主要解决： 一般的，我们为了扩展一个类经常使用继承方式实现，由于继承为类引入静态特征，并且随着扩展功能的增多，子类会很膨胀。 何时使用： 在不想增加很多子类的情况下扩展类。 如何解决： 将具体功能职责划分，同时继承装饰者模式。 关键代码： Component 类充当抽象角色，不应该具体实现。 修饰类引用和继承 Component 类，具体扩展类重写父类方法。 应用实例： 孙悟空有 72 变，当他变成”庙宇”后，他的根本还是一只猴子，但是他又有了庙宇的功能。 不论一幅画有没有画框都可以挂在墙上，但是通常都是有画框的，并且实际上是画框被挂在墙上。在挂在墙上之前，画可以被蒙上玻璃，装到框子里；这时画、玻璃和画框形成了一个物体。 使用场景： 扩展一个类的功能。 动态增加功能，动态撤销。 优缺点及注意优点装饰类和被装饰类可以独立发展，不会相互耦合，装饰模式是继承的一个替代模式，装饰模式可以动态扩展一个实现类的功能。 缺点多层装饰比较复杂。 注意可代替继承。 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546package test;/** * @program: leecode1 * @description: 设计模式-装饰器模式 * @author: Marlowe * @create: 2021-04-15 21:44 **/public class DecoratorPattern &#123; public static void main(String[] args) &#123; new RobotDecorator(new FirstRobot()).doMoreThing(); &#125;&#125;interface Robot &#123; void doSomething();&#125;class FirstRobot implements Robot &#123; @Override public void doSomething() &#123; System.out.println(&quot;对话&quot;); System.out.println(&quot;唱歌&quot;); &#125;&#125;class RobotDecorator implements Robot &#123; private Robot robot; public RobotDecorator(Robot robot) &#123; this.robot = robot; &#125; @Override public void doSomething() &#123; robot.doSomething(); &#125; public void doMoreThing() &#123; robot.doSomething(); System.out.println(&quot;做饭&quot;); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"结构型模式","slug":"结构型模式","permalink":"https://xmmarlowe.github.io/tags/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F/"},{"name":"装饰器","slug":"装饰器","permalink":"https://xmmarlowe.github.io/tags/%E8%A3%85%E9%A5%B0%E5%99%A8/"}],"author":"Marlowe"},{"title":"线程上下文切换","slug":"操作系统/线程上下文切换","date":"2021-04-14T14:12:07.000Z","updated":"2021-04-14T14:34:42.811Z","comments":true,"path":"2021/04/14/操作系统/线程上下文切换/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/14/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E7%BA%BF%E7%A8%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E5%88%87%E6%8D%A2/","excerpt":"","text":"简介多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。 概括来说就是： 当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。 上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。 Linux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。 上下文切换的原因多线程编程中，我们知道线程间的上下文切换会导致性能问题，那么是什么原因造成的线程间的上下文切换。我们先看一下线程的生命周期，从中看一下找找答案。 线程的五种状态我们都非常清楚：NEW、RUNNABLE、RUNNING、BLOCKED、DEAD，对应的Java中的六种状态分别为：NEW、RUNABLE、BLOCKED、WAINTING、TIMED_WAITING、TERMINADTED。 图中，一个线程从RUNNABLE到RUNNING的过程就是线程的上下文切换，RUNNING状态到BLOCKED、再到RUNNABLE、再从RUNNABLE到RUNNING的过程就是一个上下文切换的过程。 一个线程从RUNNING转为BLOCKED状态时，我们叫做线程的暂停，线程暂停了，这个处理器就会有别的线程来占用，操作系统就会保存相应的上下文，为了这个线程以后再进入RUNNABLE状态时可以接着之前的执行进度继续执行。当线程从BLOCKED状态进入到RUNNABLE时，也就是线程的唤醒，此时线程将获取上次保存的上下文信息。 我们看到，多线程的上下文切换实际上就是多线程两个运行状态的相互切换导致的。 我们知道两种情况可以导致上下文切换： 一种是程序本身触发的切换，这种我们一般称为自发性上下文切换。 另一种是系统或者虚拟机导致的上下文切换，我们称之为非自发性上下文切换。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"上下文","slug":"上下文","permalink":"https://xmmarlowe.github.io/tags/%E4%B8%8A%E4%B8%8B%E6%96%87/"}],"author":"Marlowe"},{"title":"操作系统IO模型","slug":"操作系统/操作系统IO模型","date":"2021-04-12T14:55:40.000Z","updated":"2021-04-14T12:42:09.016Z","comments":true,"path":"2021/04/12/操作系统/操作系统IO模型/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/12/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9FIO%E6%A8%A1%E5%9E%8B/","excerpt":"IO模型的分类：主要有同步IO、异步IO、阻塞IO、非阻塞IO…","text":"IO模型的分类：主要有同步IO、异步IO、阻塞IO、非阻塞IO… 操作系统的IO交互模型现代的操作系统对于存储空间都有一套访问限制控制，所以将存储空间分成了用户空间和内核空间。用户空间负责给应用程序使用，应用程序可以访问用户空间内的数据，但是不可以访问内核空间中的数据；而内核程序可以访问计算机的所有存储空间，包括用户空间、内核空间以及硬件设备上的数据。所以当应用程序需要访问硬件设备上的数据或者是内核空间的数据时，就必须要通过内核空间的程序来实现。所以内核空间对外也提供了很多的函数，提供给了应用程序使用，让应用程序可以通过内核程序来访问想要的数据。 整体的IO交互模型如下图示： 下面就以应用程序需要从网卡中读取数据为例，整体IO交互流程主要分成如下几个步骤： 1、应用程序调用内核提供的函数发起请求数据（请求内核函数） 2、内核访问网卡存储空间获取数据（内核获取数据） 3、内核将获取的到数据复制到用户空间（内核复制数据） 4、应用程序从用户空间中获取需要的数据（应用程序获取数据） 操作系统的IO模型IO的类型同步IO应用程序调用内核函数到最终应用程序从用户空间中获取数据的整个流程是需要用户线程一次性完成的那么就是同步IO 异步IO应用程序调用内核函数请求获取数据和最终从用户空间中拿到数据不是一次性完成的，而是先请求数据，等数据全部准备好了之后再获取的就是异步IO 阻塞IO应用程序调用内核函数请求数据，如果此时还没有数据，那么应用程序就一直等待着，直到成功拿到数据为止，此时应用程序线程是一直处于等待状态的，那么就是阻塞IO 非阻塞IO应用程序调用内核函数请求数据，如果此时还没有数据，那么应用程序就不等待先去处理其他事情，过一会再重新尝试请求，直到成功拿到数据为止，此时应用程序不会一直处于等待状态，那么就是非阻塞IO 操作系统IO模型操作系统的IO模型也主要分成同步IO和异步IO两大类，而同步IO又分成了阻塞和非阻塞等类，异步IO不会出现阻塞IO情况，所以异步IO肯定是非阻塞的IO，操作系统IO模型主要分成如下几种类型 tips：操作系统给应用程序提供了recv函数，该函数用于从socket套接字中接收数据，默认情况下会等到网络数据接收完成并复制到用户空间之后才返回结果或者失败之后返回结果，可以通过flags参数设置如果没有数据的话立即返回结果 同步阻塞IO应用程序调用操作系统的recv函数，recv函数默认会等待数据接收完成并复制到用户空间之后返回结果，而如果数据没有准备好的话，那么应用程序就一直处于等待状态，直到有数据返回，此时应用程序的线程处于阻塞状态，无法执行其他操作。 同步非阻塞IO应用程序调用操作系统的recv函数，recv函数设置flags值为立即返回，那么如果内核发现没有数据时就立即返回，应用程序得到结果之后不再等待，而是先处理其他业务，然后轮训不断尝试获取数据，直到数据成功返回，此时应用程序不处于阻塞状态，可以先处理其他操作。 同步多路复用IO应用程序先调用操作系统的select函数或者poll函数或者epoll函数，这几个函数的作用是监听网络套接字上的数据状态，如果有数据可读，那么就通知应用程序，此时应用程序再调用recv函数来读取数据，此时肯定是可以读取到数据的。可以发现多路复用IO的特点是不需要尝试获取数据，而是先开启另外一个线程来监控数据的状态，等到有数据的时候再同步获取数据，而在没数据的时候也是不需要等待的。多路复用IO调用select函数之后也会阻塞进程，但是不会真正的IO操作线程没有被阻塞，所以实质上是同步非阻塞IO。 同步信号驱动IO通过调用sigaction函数注册信号函数，等内核数据准备好了之后会执行信号函数通知应用程序，应用程序此时再调用recv函数同步的获取数据。信号驱动IO和异步IO有点类似，都是异步通知，不同的是信号驱动IO的真正读取数据的操作还是同步操作的。 异步非阻塞IO通过调用aio_read函数，那么内核会先将数据读取好，并且复制到用户空间之后，再执行回调函数通知应用程序，此时应用程序就可以直接从用户空间中读取数据，而不需要再从内核中读取数据了。 总结IO操作主要可以分成两个阶段：1、数据准备阶段；2、数据从内核空间复制到用户空间阶段 而阻塞IO、非阻塞IO、多路复用IO和信号驱动IO只是在第一个阶段不同，而第二个阶段是相同的，都是需要阻塞当前线程等待数据复制完成，虽然阻塞的时间足够短，所以只要需要执行第二阶段的都是属于同步IO； 而异步IO模型的第一阶段和第二阶段都是内核主动完成，再两个阶段都不会阻塞当前线程去处理其他事情。 参考整理操作系统IO模型","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"IO","slug":"IO","permalink":"https://xmmarlowe.github.io/tags/IO/"}],"author":"Marlowe"},{"title":"SpringMVC执行流程及工作原理","slug":"Spring/SpringMVC执行流程及工作原理","date":"2021-04-11T02:09:43.000Z","updated":"2021-08-26T11:25:44.403Z","comments":true,"path":"2021/04/11/Spring/SpringMVC执行流程及工作原理/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/11/Spring/SpringMVC%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%8F%8A%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/","excerpt":"","text":"图解SpringMVC执行流程: SpringMVC执行流程 SpringMVC执行流程: 用户发送请求至前端控制器DispatcherServlet DispatcherServlet收到请求调用处理器映射器HandlerMapping。 处理器映射器根据请求url找到具体的处理器，生成处理器执行链HandlerExecutionChain(包括处理器对象和处理器拦截器)一并返回给DispatcherServlet。 DispatcherServlet根据处理器Handler获取处理器适配器HandlerAdapter执行HandlerAdapter处理一系列的操作，如：参数封装，数据格式转换，数据验证等操作 执行处理器Handler(Controller，也叫页面控制器)。 Handler执行完成返回ModelAndView HandlerAdapter将Handler执行结果ModelAndView返回到DispatcherServlet DispatcherServlet将ModelAndView传给ViewReslover视图解析器 ViewReslover解析后返回具体View DispatcherServlet对View进行渲染视图（即将模型数据model填充至视图中）。 DispatcherServlet响应用户。 组件说明 DispatcherServlet：前端控制器。用户请求到达前端控制器，它就相当于mvc模式中的c，dispatcherServlet是整个流程控制的中心，由它调用其它组件处理用户的请求，dispatcherServlet的存在降低了组件之间的耦合性,系统扩展性提高。由框架实现 HandlerMapping：处理器映射器。HandlerMapping负责根据用户请求的url找到Handler即处理器，springmvc提供了不同的映射器实现不同的映射方式，根据一定的规则去查找,例如：xml配置方式，实现接口方式，注解方式等。由框架实现 Handler：处理器。Handler 是继DispatcherServlet前端控制器的后端控制器，在DispatcherServlet的控制下Handler对具体的用户请求进行处理。由于Handler涉及到具体的用户业务请求，所以一般情况需要程序员根据业务需求开发Handler。 HandlAdapter：处理器适配器。通过HandlerAdapter对处理器进行执行，这是适配器模式的应用，通过扩展适配器可以对更多类型的处理器进行执行。由框架实现。 ModelAndView是springmvc的封装对象，将model和view封装在一起。 ViewResolver：视图解析器。ViewResolver负责将处理结果生成View视图，ViewResolver首先根据逻辑视图名解析成物理视图名即具体的页面地址，再生成View视图对象，最后对View进行渲染将处理结果通过页面展示给用户。 View:是springmvc的封装对象，是一个接口, springmvc框架提供了很多的View视图类型，包括：jspview，pdfview,jstlView、freemarkerView、pdfView等。一般情况下需要通过页面标签或页面模版技术将模型数据通过页面展示给用户，需要由程序员根据业务需求开发具体的页面。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/categories/Spring/"}],"tags":[{"name":"SpringMVC","slug":"SpringMVC","permalink":"https://xmmarlowe.github.io/tags/SpringMVC/"}],"author":"Marlowe"},{"title":"虚拟内存和局部性原理","slug":"操作系统/虚拟内存和局部性原理","date":"2021-04-10T11:22:21.000Z","updated":"2021-05-21T01:51:26.049Z","comments":true,"path":"2021/04/10/操作系统/虚拟内存和局部性原理/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/10/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E5%92%8C%E5%B1%80%E9%83%A8%E6%80%A7%E5%8E%9F%E7%90%86/","excerpt":"","text":"什么是虚拟内存(Virtual Memory)?这个在我们平时使用电脑特别是 Windows 系统的时候太常见了。很多时候我们使用点开了很多占内存的软件，这些软件占用的内存可能已经远远超出了我们电脑本身具有的物理内存。为什么可以这样呢？ 正是因为 虚拟内存 的存在，通过 虚拟内存 可以让程序可以拥有超过系统物理内存大小的可用内存空间。另外，虚拟内存为每个进程提供了一个一致的、私有的地址空间，它让每个进程产生了一种自己在独享主存的错觉（每个进程拥有一片连续完整的内存空间）。 这样会更加有效地管理内存并减少出错。 虚拟内存是计算机系统内存管理的一种技术，我们可以手动设置自己电脑的虚拟内存。不要单纯认为虚拟内存只是“使用硬盘空间来扩展内存“的技术。虚拟内存的重要意义是它定义了一个连续的虚拟地址空间，并且 把内存扩展到硬盘空间。 局部性原理局部性原理是虚拟内存技术的基础，正是因为程序运行具有局部性原理，才可以只装入部分程序到内存就开始运行。 早在 1968 年的时候，就有人指出我们的程序在执行的时候往往呈现局部性规律，也就是说在某个较短的时间段内，程序执行局限于某一小部分，程序访问的存储空间也局限于某个区域。 局部性原理表现在以下两个方面： 时间局部性： 如果程序中的某条指令一旦执行，不久以后该指令可能再次执行；如果某数据被访问过，不久以后该数据可能再次被访问。产生时间局部性的典型原因，是由于在程序中存在着大量的循环操作。 空间局部性： 一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也将被访问，即程序在一段时间内所访问的地址，可能集中在一定的范围之内，这是因为指令通常是顺序存放、顺序执行的，数据也一般是以向量、数组、表等形式簇聚存储的。时间局部性是通过将近来使用的指令和数据保存到高速缓存存储器中，并使用高速缓存的层次结构实现。空间局部性通常是使用较大的高速缓存，并将预取机制集成到高速缓存控制逻辑中实现。虚拟内存技术实际上就是建立了 “内存一外存”的两级存储器的结构，利用局部性原理实现髙速缓存。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"虚拟内存","slug":"虚拟内存","permalink":"https://xmmarlowe.github.io/tags/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98/"}],"author":"Marlowe"},{"title":"IO多路复用","slug":"操作系统/IO多路复用","date":"2021-04-10T11:04:10.000Z","updated":"2021-05-03T12:36:34.745Z","comments":true,"path":"2021/04/10/操作系统/IO多路复用/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/10/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/IO%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/","excerpt":"","text":"举例现实场景来理解我们试想一下这样的现实场景: 一个餐厅同时有100位客人到店，当然到店后第一件要做的事情就是点菜。但是问题来了，餐厅老板为了节约人力成本目前只有一位大堂服务员拿着唯一的一本菜单等待客人进行服务。 方法A 无论有多少客人等待点餐，服务员都把仅有的一份菜单递给其中一位客人，然后站在客人身旁等待这个客人完成点菜过程。在记录客人点菜内容后，把点菜记录交给后堂厨师。然后是第二位客人。。。。然后是第三位客人。很明显，只有脑袋被门夹过的老板，才会这样设置服务流程。因为随后的80位客人，再等待超时后就会离店(还会给差评)。 方法B 老板马上新雇佣99名服务员，同时印制99本新的菜单。每一名服务员手持一本菜单负责一位客人(关键不只在于服务员，还在于菜单。因为没有菜单客人也无法点菜)。在客人点完菜后，记录点菜内容交给后堂厨师(当然为了更高效，后堂厨师最好也有100名)。这样每一位客人享受的就是VIP服务咯，当然客人不会走，但是人力成本可是一个大头哦(亏死你)。 方法C 当客人到店后，自己申请一本菜单。想好自己要点的才后，就呼叫服务员。服务员站在自己身边后记录客人的菜单内容。将菜单递给厨师的过程也要进行改进，并不是每一份菜单记录好以后，都要交给后堂厨师。服务员可以记录号多份菜单后，同时交给厨师就行了。那么这种方式，对于老板来说人力成本是最低的；对于客人来说，虽然不再享受VIP服务并且要进行一定的等待，但是这些都是可接受的；对于服务员来说，基本上她的时间都没有浪费，基本上被老板压杆了最后一滴油水。 到店情况: 并发量。到店情况不理想时，一个服务员一本菜单，当然是足够了。所以不同的老板在不同的场合下，将会灵活选择服务员和菜单的配置. 客人: 客户端请求 点餐内容: 客户端发送的实际数据 老板: 操作系统 人力成本: 系统资源 菜单: 文件状态描述符。操作系统对于一个进程能够同时持有的文件状态描述符的个数是有限制的，在linux系统中$ulimit -n查看这个限制值，当然也是可以(并且应该)进行内核参数调整的。 服务员: 操作系统内核用于IO操作的线程(内核线程) 厨师: 应用程序线程(当然厨房就是应用程序进程咯) 餐单传递方式: 包括了阻塞式和非阻塞式两种。 方法A: 阻塞式/非阻塞式 同步IO 方法B: 使用线程进行处理的 阻塞式/非阻塞式 同步IO 方法C: 阻塞式/非阻塞式 多路复用IO 多路复用IO实现目前流程的多路复用IO实现主要包括四种: select、poll、epoll、kqueue。 多路复用IO技术最适用的是“高并发”场景，所谓高并发是指1毫秒内至少同时有上千个连接请求准备好。其他情况下多路复用IO技术发挥不出来它的优势。另一方面，使用JAVA NIO进行功能实现，相对于传统的Socket套接字实现要复杂一些，所以实际应用中，需要根据自己的业务需求进行技术选择。 IO多路复用工作模式epoll 的描述符事件有两种触发模式: LT(level trigger)和 ET(edge trigger)。 LT 模式当 epoll_wait() 检测到描述符事件到达时，将此事件通知进程，进程可以不立即处理该事件，下次调用 epoll_wait() 会再次通知进程。是默认的一种模式，并且同时支持 Blocking 和 No-Blocking。 ET 模式和 LT 模式不同的是，通知之后进程必须立即处理事件，下次再调用 epoll_wait() 时不会再得到事件到达的通知。 很大程度上减少了 epoll 事件被重复触发的次数，因此效率要比 LT 模式高。只支持 No-Blocking，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。 应用场景很容易产生一种错觉认为只要用 epoll 就可以了，select 和 poll 都已经过时了，其实它们都有各自的使用场景。 select 应用场景select 的 timeout 参数精度为 1ns，而 poll 和 epoll 为 1ms，因此 select 更加适用于实时要求更高的场景，比如核反应堆的控制。 select 可移植性更好，几乎被所有主流平台所支持。 poll 应用场景poll 没有最大描述符数量的限制，如果平台支持并且对实时性要求不高，应该使用 poll 而不是 select。 需要同时监控小于 1000 个描述符，就没有必要使用 epoll，因为这个应用场景下并不能体现 epoll 的优势。 需要监控的描述符状态变化多，而且都是非常短暂的，也没有必要使用 epoll。因为 epoll 中的所有描述符都存储在内核中，造成每次需要对描述符的状态改变都需要通过 epoll_ctl() 进行系统调用，频繁系统调用降低效率。并且epoll 的描述符存储在内核，不容易调试。 epoll 应用场景只需要运行在 Linux 平台上，并且有非常大量的描述符需要同时轮询，而且这些连接最好是长连接。 select、poll、epoll有什么区别？他们是NIO中多路复用的三种实现机制，是由Linux操作系统提供的。 用户空间和内核空间:操作系统为了保护系统安全，将内核划分为两个部分，一个是用户空间，一个是内核空间。用户空间不能直接访问底层的硬件设备，必须通过内核空间。 文件描述符File Descriptor(FD):是一个抽象的概念，形式上是一个整数，实际上是一个索引值。指向内核中为每个进程维护进程所打开的文件的记录表。当程序打开一个文件或者创建一个文件时，内核就会向进程返回一个FD。Unix,Linux select机制: 会维护一个FD的结合 fd_set。将fd_set从用户空间复制到内核空间，激活socket。x64 2048(数组大小) fd_set是一个数组结构。 Poll机制: 和selecter机制是差不多的， 把fd_ set结构进行了优化，FD集合的大小就突破了操作系统的限制。pollfd结构来代替fd_set, 通过链表实现的。 EPoll: Event Poll.Epoll不再扫描所有的FD，只将用户关心的FD的事件存放到内核的一一个事件表当中。 简单总结 操作方式 底层实现 最大连接数 IO效率 select 遍历 数组 受限于内核 一般 poll 遍历 链表 无上限 一般 epoll 事件回调 红黑树 无上限 高 Java的NIO当中使用的是那种机制？ 可以查看 DefaultSelectorProvider源码。在windows 下，WindowsSelectorProvider。而Linux下，根据Linux的内核版本，2.6版本以上，就是EPollSelectorProvider, 否则就是 默认的PollSelectorProvider。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"IO","slug":"IO","permalink":"https://xmmarlowe.github.io/tags/IO/"}],"author":"Marlowe"},{"title":"RESTful相关面试题","slug":"春招面试/RESTful相关面试题","date":"2021-04-10T10:43:48.000Z","updated":"2021-08-17T15:26:04.673Z","comments":true,"path":"2021/04/10/春招面试/RESTful相关面试题/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/10/%E6%98%A5%E6%8B%9B%E9%9D%A2%E8%AF%95/RESTful%E7%9B%B8%E5%85%B3%E9%9D%A2%E8%AF%95%E9%A2%98/","excerpt":"RESTful也可以称为“面向资源编程”。","text":"RESTful也可以称为“面向资源编程”。 谈谈对RESTful规范的理解 RESTful是一套编写接口的协议，协议规定如何编写，以及如何设置返回值，状态码等信息。 最显著的特点： RESTful：给用户一个url，根据method不同在后端做不同的处理，比如post 创建数据、get 获取数据、put和patch 修改数据、delete 删除数据。 no REST：给调用者很多url，每个url代表一个功能，比如：add_user/delte_user/edit_user/","categories":[{"name":"春招面试","slug":"春招面试","permalink":"https://xmmarlowe.github.io/categories/%E6%98%A5%E6%8B%9B%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"RESTful","slug":"RESTful","permalink":"https://xmmarlowe.github.io/tags/RESTful/"}],"author":"Marlowe"},{"title":"缺页中断及页面置换算法","slug":"操作系统/缺页中断及页面置换算法","date":"2021-04-10T03:05:43.000Z","updated":"2021-04-10T11:52:31.216Z","comments":true,"path":"2021/04/10/操作系统/缺页中断及页面置换算法/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/10/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E7%BC%BA%E9%A1%B5%E4%B8%AD%E6%96%AD%E5%8F%8A%E9%A1%B5%E9%9D%A2%E7%BD%AE%E6%8D%A2%E7%AE%97%E6%B3%95/","excerpt":"","text":"缺页中断在请求分页系统中，可以通过查询页表中的状态位来确定所要访问的页面是否存在于内存中。每当所要访问的页面不在内存时，会产生一次缺页中断，此时操作系统会根据页表中的外存地址在外存中找到所缺的一页，将其调入内存。 缺页本身是一种中断，与一般的中断一样，需要经过4个处理步骤： 保护CPU现场 分析中断原因 转入缺页中断处理程序进行处理 恢复CPU现场，继续执行 但是缺页中断时由于所要访问的页面不存在与内存时，有硬件所产生的一种特殊的中断，因此，与一般的中断存在区别： 在指令执行期间产生和处理缺页中断信号 一条指令在执行期间，可能产生多次缺页中断 缺页中断返回时，执行产生中断的那一条指令，而一般的中断返回时，执行下一条指令 页面置换算法进程运行过程中，如果发生缺页中断，而此时内存中有没有空闲的物理块是，为了能够把所缺的页面装入内存，系统必须从内存中选择一页调出到磁盘的对换区。但此时应该把那个页面换出，则需要根据一定的页面置换算法（Page Replacement Algorithm)来确定。 1. 先进先出置换算法（First In First Out, FIFO)置换最先调入内存的页面，即置换在内存中驻留时间最久的页面。按照进入内存的先后次序排列成队列，从队尾进入，从队首删除。但是该算法会淘汰经常访问的页面，不适应进程实际运行的规律，目前已经很少使用。 2.最近最久未使用置换算法（Least Recently Used， LRU)置换最近一段时间以来最长时间未访问过的页面。根据程序局部性原理，刚被访问的页面，可能马上又要被访问；而较长时间内没有被访问的页面，可能最近不会被访问。 LRU算法普偏地适用于各种类型的程序，但是系统要时时刻刻对各页的访问历史情况加以记录和更新，开销太大，因此LRU算法必须要有硬件的支持。 Belady异常 一般来说，分配给进程的物理块越多，运行时的缺页次数应该越少，使用FIFO时，可能存在相反情况，分配4个物理块的缺页竟然比3个物理块的缺页次数还多！ 3.最佳置换（Optimal, OPT)置换以后不再被访问，或者在将来最迟才回被访问的页面，缺页中断率最低。但是该算法需要依据以后各业的使用情况，而当一个进程还未运行完成是，很难估计哪一个页面是以后不再使用或在最长时间以后才会用到的页面。所以该算法是不能实现的。但该算法仍然有意义，作为很亮其他算法优劣的一个标准。 4. 最少使用页面置换算法(Least Frequently Used, LFU)该置换算法选择在之前时期使用最少的页面作为淘汰页。 参考缺页中断及页面置换算法","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"缺页中断","slug":"缺页中断","permalink":"https://xmmarlowe.github.io/tags/%E7%BC%BA%E9%A1%B5%E4%B8%AD%E6%96%AD/"},{"name":"页面置换算法","slug":"页面置换算法","permalink":"https://xmmarlowe.github.io/tags/%E9%A1%B5%E9%9D%A2%E7%BD%AE%E6%8D%A2%E7%AE%97%E6%B3%95/"}],"author":"Marlowe"},{"title":"多线程和多进程及其应用场景","slug":"操作系统/多线程和多进程及其应用场景","date":"2021-04-09T14:45:58.000Z","updated":"2021-04-14T13:27:51.154Z","comments":true,"path":"2021/04/09/操作系统/多线程和多进程及其应用场景/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/09/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%A4%9A%E8%BF%9B%E7%A8%8B%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/","excerpt":"","text":"多进程多线程的区别 进程是分配资源的基本单位；线程是系统调度和分派的基本单位。 属于同一进程的线程，堆是共享的，栈是私有的。 属于同一进程的所有线程都具有相同的地址空间。 多进程的优点：①编程相对容易；通常不需要考虑锁和同步资源的问题。 ②更强的容错性：比起多线程的一个好处是一个进程崩溃了不会影响其他进程。 ③有内核保证的隔离：数据和错误隔离。 对于使用如C/C++这些语言编写的本地代码，错误隔离是非常有用的：采用多进程架构的程序一般可以做到一定程度的自恢复；（master守护进程监控所有worker进程，发现进程挂掉后将其重启）。 多线程的优点：①创建速度快，方便高效的数据共享 共享数据：多线程间可以共享同一虚拟地址空间；多进程间的数据共享就需要用到共享内存、信号量等IPC技术。②较轻的上下文切换开销,不用切换地址空间，不用更改寄存器，不用刷新TLB。 ③提供非均质的服务。如果全都是计算任务，但每个任务的耗时不都为1s，而是1ms-1s之间波动；这样，多线程相比多进程的优势就体现出来，它能有效降低“简单任务被复杂任务压住”的概率。 应用场景1. 多进程应用场景 nginx主流的工作模式是多进程模式（也支持多线程模型） 几乎所有的web server服务器服务都有多进程的，至少有一个守护进程配合一个worker进程，例如apached,httpd等等以d结尾的进程包括init.d本身就是0级总进程，所有你认知的进程都是它的子进程； chrome浏览器也是多进程方式。 （原因：①可能存在一些网页不符合编程规范，容易崩溃，采用多进程一个网页崩溃不会影响其他网页；而采用多线程会。②网页之间互相隔离，保证安全，不必担心某个网页中的恶意代码会取得存放在其他网页中的敏感信息。） redis也可以归类到“多进程单线程”模型（平时工作是单个进程，涉及到耗时操作如持久化或aof重写时会用到多个进程） 2. 多线程应用场景 线程间有数据共享，并且数据是需要修改的（不同任务间需要大量共享数据或频繁通信时）。 提供非均质的服务（有优先级任务处理）事件响应有优先级。 单任务并行计算，在非CPU Bound的场景下提高响应速度，降低时延。 与人有IO交互的应用，良好的用户体验（键盘鼠标的输入，立刻响应） 案例：桌面软件，响应用户输入的是一个线程，后台程序处理是另外的线程memcached 3. 如何选择？①需要频繁创建销毁的优先用线程（进程的创建和销毁开销过大）这种原则最常见的应用就是Web服务器了，来一个连接建立一个线程，断了就销毁线程，要是用进程，创建和销毁的代价是很难承受的。 ②需要进行大量计算的优先使用线程（CPU频繁切换）所谓大量计算，当然就是要耗费很多CPU，切换频繁了，这种情况下线程是最合适的。这种原则最常见的是图像处理、算法处理。 ③强相关的处理用线程，弱相关的处理用进程什么叫强相关、弱相关？理论上很难定义，给个简单的例子就明白了。一般的Server需要完成如下任务：消息收发、消息处理。“消息收发”和“消息处理”就是弱相关的任务，而“消息处理”里面可能又分为“消息解码”、“业务处理”，这两个任务相对来说相关性就要强多了。因此“消息收发”和“消息处理”可以分进程设计，“消息解码”、“业务处理”可以分线程设计。当然这种划分方式不是一成不变的，也可以根据实际情况进行调整。 ④可能要扩展到多机分布的用进程，多核分布的用线程 ⑤都满足需求的情况下，用你最熟悉、最拿手的方式至于“数据共享、同步”、“编程、调试”、“可靠性”这几个维度的所谓的“复杂、简单”应该怎么取舍，我只能说：没有明确的选择方法。但我可以告诉你一个选择原则：如果多进程和多线程都能够满足要求，那么选择你最熟悉、最拿手的那个。 实际应用中基本上都是“进程+线程”的结合方式，千万不要真的陷入一种非此即彼的误区。 参考多线程和多进程及其应用场景","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"线程","slug":"线程","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B/"},{"name":"进程","slug":"进程","permalink":"https://xmmarlowe.github.io/tags/%E8%BF%9B%E7%A8%8B/"}],"author":"Marlowe"},{"title":"进程间的通信方式","slug":"操作系统/进程间的通信方式","date":"2021-04-09T14:13:41.000Z","updated":"2021-04-20T14:14:10.757Z","comments":true,"path":"2021/04/09/操作系统/进程间的通信方式/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/09/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E7%9A%84%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F/","excerpt":"进程间的7种通信方式概述…","text":"进程间的7种通信方式概述… 进程间通信的概念每个进程各自有不同的用户地址空间，任何一个进程的全局变量在另一个进程中都看不到，所以进程之间要交换数据必须通过内核，在内核中开辟一块缓冲区，进程1把数据从用户空间拷到内核缓冲区，进程2再从内核缓冲区把数据读走，内核提供的这种机制称为进程间通信（IPC，InterProcess Communication） 进程间通信模型 进程间通信的七种方式管道/匿名管道(Pipes)一句话介绍大白话来说，就是只能在父子间单向传递数据的方式，数据放在内核缓冲区，像FIFO的方式循环队列来存取。管道单独构成一种文件系统，并且只存在与内存中。 特点 管道是半双工的，数据只能向一个方向流动；需要双方通信时，需要建立起两个管道。 只能用于父子进程或者兄弟进程之间(具有亲缘关系的进程); 单独构成一种独立的文件系统：管道对于管道两端的进程而言，就是一个文件，但它不是普通的文件，它不属于某种文件系统，而是自立门户，单独构成一种文件系统，并且只存在与内存中。 数据的读出和写入：一个进程向管道中写的内容被管道另一端的进程读出。写入的内容每次都添加在管道缓冲区的末尾，并且每次都是从缓冲区的头部读出数据。 进程间管道通信模型 实质管道的实质是一个内核缓冲区，进程以先进先出的方式从缓冲区存取数据，管道一端的进程顺序的将数据写入缓冲区，另一端的进程则顺序的读出数据。该缓冲区可以看做是一个循环队列，读和写的位置都是自动增长的，不能随意改变，一个数据只能被读一次，读出来以后在缓冲区就不复存在了。当缓冲区读空或者写满时，有一定的规则控制相应的读进程或者写进程进入等待队列，当空的缓冲区有新数据写入或者满的缓冲区有数据读出来时，就唤醒等待队列中的进程继续读写。 局限性 只支持单向数据流； 只能用于具有亲缘关系的进程之间； 没有名字； 管道的缓冲区是有限的（管道制存在于内存中，在管道创建时，为缓冲区分配一个页面大小）； 管道所传送的是无格式字节流，这就要求管道的读出方和写入方必须事先约定好数据的格式，比如多少字节算作一个消息（或命令、或记录）等等； 有名管道(Names Pipes)一句话介绍与匿名管道一样，但是存在有一个名字，这个名字就是文件路径，存在于文件系统中，但是内容还是在内存，这样就可以非父子进程通信了。 介绍匿名管道，由于没有名字，只能用于亲缘关系的进程间通信。为了克服这个缺点，提出了有名管道。有名管道不同于匿名管道之处在于它提供了一个路径名与之关联，以有名管道的文件形式存在于文件系统中，这样，即使与有名管道的创建进程不存在亲缘关系的进程，只要可以访问该路径，就能够彼此通过有名管道相互通信，因此，通过有名管道不相关的进程也能交换数据。值的注意的是，有名管道严格遵循先进先出(first in first out),对匿名管道及有名管道的读总是从开始处返回数据，对它们的写则把数据添加到末尾。它们不支持诸如lseek()等文件定位操作。有名管道的名字存在于文件系统中，内容存放在内存中。 匿名管道和有名管道总结 管道是特殊类型的文件，在满足先入先出的原则条件下可以进行读写，但不能进行定位读写。 匿名管道是单向的，只能在有亲缘关系的进程间通信；有名管道以磁盘文件的方式存在，可以实现本机任意两个进程通信。 无名管道阻塞问题：无名管道无需显示打开，创建时直接返回文件描述符，在读写时需要确定对方的存在，否则将退出。如果当前进程向无名管道的一端写数据，必须确定另一端有某一进程。如果写入无名管道的数据超过其最大值，写操作将阻塞，如果管道中没有数据，读操作将阻塞，如果管道发现另一端断开，将自动退出。 有名管道阻塞问题：有名管道在打开时需要确实对方的存在，否则将阻塞。即以读方式打开某管道，在此之前必须一个进程以写方式打开管道，否则阻塞。此外，可以以读写（O_RDWR）模式打开有名管道，即当前进程读，当前进程写，不会阻塞。 信号信号是Linux系统中用于进程间互相通信或者操作的一种机制，信号可以在任何时候发给某一进程，而无需知道该进程的状态。如果该进程当前并未处于执行状态，则该信号就有内核保存起来，知道该进程回复执行并传递给它为止。如果一个信号被进程设置为阻塞，则该信号的传递被延迟，直到其阻塞被取消是才被传递给进程。 SIGINT：程序终止信号。程序运行过程中，按Ctrl+C键将产生该信号。 信号的生命周期 消息队列(Message Queuing) 消息队列是存放在内核中的消息链表，每个消息队列由消息队列标识符表示。 与管道（无名管道：只存在于内存中的文件；命名管道：存在于实际的磁盘介质或者文件系统）不同的是消息队列存放在内核中，只有在内核重启(即，操作系统重启)或者显示地删除一个消息队列时，该消息队列才会被真正的删除。 另外与管道不同的是，消息队列在某个进程往一个队列写入消息之前，并不需要另外某个进程在该队列上等待消息的到达。 特点 消息队列是消息的链表,具有特定的格式,存放在内存中并由消息队列标识符标识. 消息队列允许一个或多个进程向它写入与读取消息. 管道和消息队列的通信数据都是先进先出的原则。 消息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取.比FIFO更有优势。 消息队列克服了信号承载信息量少，管道只能承载无格式字 节流以及缓冲区大小受限等缺。 目前主要有两种类型的消息队列：POSIX消息队列以及System V消息队列，系统V消息队列目前被大量使用。系统V消息队列是随内核持续的，只有在内核重起或者人工删除时，该消息队列才会被删除。 信号量(Semaphores)一句话介绍信号量是⼀个计数器，⽤于多进程对共享数据的访问，信号量的意图在于进程间同步。这种通信⽅式主要⽤于解决与同步相关的问题并避免竞争条件。 实际过程信号量是一个计数器，用于多进程对共享数据的访问，信号量的意图在于进程间同步。 为了获得共享资源，进程需要执行下列操作： 创建一个信号量：这要求调用者指定初始值，对于二值信号量来说，它通常是1，也可是0。 等待一个信号量：该操作会测试这个信号量的值，如果小于0，就阻塞。也称为P操作。 挂出一个信号量：该操作将信号量的值加1，也称为V操作。 为了正确地实现信号量，信号量值的测试及减1操作应当是原子操作。为此，信号量通常是在内核中实现的。 信号量与普通整型变量的区别 信号量是非负整型变量，除了初始化之外，它只能通过两个标准原子操作：wait(semap) , signal(semap) ; 来进行访问； 操作也被成为PV原语（P来源于荷兰语proberen”测试”，V来源于荷兰语verhogen”增加”，P表示通过的意思，V表示释放的意思），而普通整型变量则可以在任何语句块中被访问； 信号量与互斥量之间的区别： 互斥量用于线程的互斥，信号量用于线程的同步。这是互斥量和信号量的根本区别，也就是互斥和同步之间的区别。 互斥： 是指某一资源同时只允许一个访问者对其进行访问，具有唯一性和排它性。但互斥无法限制访问者对资源的访问顺序，即访问是无序的。 同步： 是指在互斥的基础上（大多数情况），通过其它机制实现访问者对资源的有序访问。在大多数情况下，同步已经实现了互斥，特别是所有写入资源的情况必定是互斥的。少数情况是指可以允许多个访问者同时访问资源 互斥量值只能为0/1，信号量值可以为非负整数。也就是说，一个互斥量只能用于一个资源的互斥访问，它不能实现多个资源的多线程互斥问题。信号量可以实现多个同类资源的多线程互斥和同步。当信号量为单值信号量是，也可以完成一个资源的互斥访问。 互斥量的加锁和解锁必须由同一线程分别对应使用，信号量可以由一个线程释放，另一个线程得到。 两个进程使用一个二值信号量 两个进程所以用一个Posix有名二值信号量 一个进程两个线程共享基于内存的信号量 共享内存(Shared memory) 使得多个进程可以可以直接读写同一块内存空间，是最快的可用IPC形式。是针对其他通信机制运行效率较低而设计的。 为了在多个进程间交换信息，内核专门留出了一块内存区，可以由需要访问的进程将其映射到自己的私有地址空间。进程就可以直接读写这一块内存而不需要进行数据的拷贝，从而大大提高效率。 由于多个进程共享一段内存，因此需要依靠某种同步机制（如信号量）来达到进程间的同步及互斥。 套接字(Sockets)套接字是一种通信机制，凭借这种机制，客户/服务器（即要进行通信的进程）系统的开发工作既可以在本地单机上进行，也可以跨网络进行。也就是说它可以让不在同一台计算机但通过网络连接计算机上的进程进行通信。 Socket是应用层和传输层之间的桥梁 套接字是支持TCP/IP的网络通信的基本操作单元，可以看做是不同主机之间的进程进行双向通信的端点，简单的说就是通信的两方的一种约定，用套接字中的相关函数来完成通信过程。 参考进程间通信IPC (InterProcess Communication) 操作系统之进程通讯IPC","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"进程通信","slug":"进程通信","permalink":"https://xmmarlowe.github.io/tags/%E8%BF%9B%E7%A8%8B%E9%80%9A%E4%BF%A1/"}],"author":"Marlowe"},{"title":"死锁产生的原因以及产生的必要条件","slug":"操作系统/死锁产生的原因以及产生的必要条件","date":"2021-04-09T05:41:29.000Z","updated":"2021-04-23T14:26:41.304Z","comments":true,"path":"2021/04/09/操作系统/死锁产生的原因以及产生的必要条件/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/09/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%AD%BB%E9%94%81%E4%BA%A7%E7%94%9F%E7%9A%84%E5%8E%9F%E5%9B%A0%E4%BB%A5%E5%8F%8A%E4%BA%A7%E7%94%9F%E7%9A%84%E5%BF%85%E8%A6%81%E6%9D%A1%E4%BB%B6/","excerpt":"","text":"死锁产生的原因 系统资源不足。 进程运行推进的顺序不合适。 资源分配不当。 死锁产生的四个必要条件 互斥条件：一个资源每次只能被一个进程使用。 请求与保持条件：一个进程因请求资源而阻塞时，对已经获得的资源保持不放。 不剥夺条件：进程已获得的资源，在未使用完之前，不能强行剥夺。 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。 如何避免死锁？1. 避免嵌套锁如果您已经持有一个资源，请避免锁定另一个资源。如果只使用一个对象锁，则几乎不可能出现死锁情况。 2. 只锁需要的部分只获对需要的资源加锁，例如在程序中，我们锁定了完整的对象资源，但是如果我们只需要其中一个字段，那么我们应该只锁定那个特定的字段而不是完整的对象。 3. 避免无限期等待如果两个线程使用 thread join 无限期互相等待也会造成死锁，我们可以设定等待的最大时间来避免这种情况。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"死锁","slug":"死锁","permalink":"https://xmmarlowe.github.io/tags/%E6%AD%BB%E9%94%81/"}],"author":"Marlowe"},{"title":"研发高频算法题","slug":"题解/研发高频算法题","date":"2021-04-01T09:08:20.000Z","updated":"2021-08-26T10:09:02.889Z","comments":true,"path":"2021/04/01/题解/研发高频算法题/","link":"","permalink":"https://xmmarlowe.github.io/2021/04/01/%E9%A2%98%E8%A7%A3/%E7%A0%94%E5%8F%91%E9%AB%98%E9%A2%91%E7%AE%97%E6%B3%95%E9%A2%98/","excerpt":"牛客研发最爱考、剑指offer经典题目","text":"牛客研发最爱考、剑指offer经典题目 字符串进制转换题目链接 代码如下： 1234567891011121314151617181920212223242526272829303132333435import java.util.*;public class Solution &#123; /** * 进制转换 * @param M int整型 给定整数 * @param N int整型 转换到的进制 * @return string字符串 */ public String solve (int M, int N) &#123; if(M == 0)&#123; return &quot;0&quot;; &#125; // 存储各进制转换后的字符 String s = &quot;0123456789ABCDEF&quot;; // 判断是否为负数，如果为负数，则需要添加负号 boolean f = false; if(M &lt; 0)&#123; f = true; M = -M; &#125; StringBuilder sb = new StringBuilder(); // 得到进制转换逆序的结果 while(M != 0)&#123; sb.append(s.charAt(M%N)); M /= N; &#125; // 如果是负数，则添加负号 if(f)&#123; sb.append(&quot;-&quot;); &#125; return sb.reverse().toString(); &#125;&#125; 字符串解码题目链接 代码如下： 123456789101112131415161718192021222324252627282930class Solution &#123; public String decodeString(String s) &#123; char[] chars = s.toCharArray(); Deque&lt;Integer&gt; stackNum = new ArrayDeque&lt;&gt;(); Deque&lt;String&gt; stack = new ArrayDeque&lt;&gt;(); StringBuilder sb = new StringBuilder(); int num = 0; for (int i = 0; i &lt; chars.length ; i++) &#123; if (chars[i] &gt;= &#x27;0&#x27; &amp;&amp; chars[i] &lt;= &#x27;9&#x27;) &#123; num = num * 10 + Integer.parseInt(chars[i] + &quot;&quot;); //两位数以上的数字计算 &#125; else if (chars[i] == &#x27;[&#x27;) &#123; stackNum.addLast(num); //不能在上一步加入数字队列，可能会有两位数以上的数字 stack.addLast(sb.toString()); //第一个待处理的字符串 sb = new StringBuilder(); //清空结果 num = 0; &#125; else if (chars[i] == &#x27;]&#x27;) &#123; StringBuilder temp = new StringBuilder(); int n = stackNum.removeLast(); //上一步的结果，从栈弹出 while (n &gt; 0) &#123; temp.append(sb); n--; &#125; sb = new StringBuilder(stack.removeLast() + temp); //更新原来的字符串结果 &#125; else &#123; sb.append(chars[i]); &#125; &#125; return sb.toString(); &#125;&#125; 数组数组中未出现的最小正整数题目链接 代码如下： 12345678910111213141516171819202122232425262728293031public class Solution &#123; /** * return the min number * @param arr int整型一维数组 the array * @return int整型 */ public int minNumberdisappered (int[] arr) &#123; if(arr == null)&#123; return 1; &#125; // 找到连续区间的最小值和最大值 int min = arr[0]; int max = arr[0]; // 求出当前数组和 int sum1 = arr[0]; for(int i = 1; i &lt; arr.length; i++)&#123; min = Math.min(min,arr[i]); max = Math.max(max,arr[i]); sum1 += arr[i]; &#125; // 求出连续区间的和 int sum2 = 0; for(int i = min; i &lt;= max; i++)&#123; sum2 += i; &#125; // 判断两次的值是否相等，相等返回最大值+1，否则计算差值 int num = sum2 - sum1 == 0? max + 1 : sum2 - sum1; // 返回最小正整数 return Math.max(1,num); &#125;&#125; 顺时针旋转矩阵题目链接 代码如下： 12345678910111213141516public class Solution &#123; public int[][] rotateMatrix(int[][] mat, int n) &#123; // 原地旋转，注意循环结束条件 for(int i = 0; i &lt; n / 2; i++)&#123; for(int j = 0; j &lt; (n+1) / 2; j++)&#123; // 依次交换四个位置的值 int tmp = mat[i][j]; mat[i][j] = mat[n - j - 1][i]; mat[n - j - 1][i] = mat[n - i - 1][n - j - 1]; mat[n - i - 1][n - j - 1] = mat[j][n - i - 1]; mat[j][n - i - 1] = tmp; &#125; &#125; return mat; &#125;&#125; 重排数组元素，奇数放在奇数位，偶数放在偶数位题目链接 思路： 找到不符合条件的奇数和偶数进行交换。 代码如下： 123456789101112131415public static void sort(int[] arr)&#123; int i = 0; int j = 1; int len = arr.length; while(i &lt; len &amp;&amp; j &lt; len)&#123; // 找到不符合条件的偶数 while(i &lt; len &amp;&amp; (arr[i] &amp; 1) == 0) i += 2; // 找到不符合条件的奇数 while(j &lt; len &amp;&amp; (arr[j] &amp; 1) == 1) j += 2; // 交换不符合条件的奇数和偶数 if(i &lt; len &amp;&amp; j &lt; len) &#123; swap(arr[i], arr[j]); &#125; &#125; &#125; 连续的子数组和题目链接 思路： 前缀和+HashMap 代码如下： 1234567891011121314151617181920212223242526class Solution &#123; public boolean checkSubarraySum(int[] nums, int k) &#123; int n = nums.length; if(n &lt; 2)&#123; return false; &#125; // 记忆前缀和为i的下标j Map&lt;Integer,Integer&gt; map = new HashMap(); map.put(0,-1); int res = 0; for(int i = 0; i &lt; n; i++)&#123; // 计算当前前缀和 res = (res + nums[i]) % k; // 如果当前位置前缀和和之前一样，则看长度是否大于等于2 if(map.containsKey(res))&#123; if(i - map.get(res) &gt;= 2)&#123; return true; &#125; &#125;else&#123; // 将当前前缀和信息记录到map中 map.put(res,i); &#125; &#125; return false; &#125;&#125; 连续数组题目链接 思路： 前缀和+HashMap 代码如下： 1234567891011121314151617181920212223242526272829class Solution &#123; public int findMaxLength(int[] nums) &#123; int maxLen = 0; // 记忆化之前的值 Map&lt;Integer,Integer&gt; map = new HashMap(); int cnt = 0; // 初始化map map.put(0,-1); for(int i = 0; i &lt; nums.length; i++)&#123; int num = nums[i]; // 统计0，1的值 if(num == 1)&#123; cnt++; &#125;else&#123; cnt--; &#125; // 如果之前出现过，更新当前最大值 if(map.containsKey(cnt))&#123; int pre = map.get(cnt); maxLen = Math.max(maxLen,i-pre); &#125;else&#123; // 将当前值放入map中记忆 map.put(cnt,i); &#125; &#125; return maxLen; &#125;&#125; 长度最小的子数组题目链接 代码如下: 12345678910111213141516171819202122232425class Solution &#123; public int minSubArrayLen(int target, int[] nums) &#123; int n = nums.length; if(n == 0)&#123; return 0; &#125; int start = 0; int end = 0; int sum = 0; int res = Integer.MAX_VALUE; while(end &lt; n)&#123; // 求当前窗口总和 sum += nums[end]; // 如果当前总和比target大，左指针可以向右移 while(sum &gt;= target)&#123; res = Math.min(res,end - start + 1); sum -= nums[start]; start++; &#125; end++; &#125; // 判断是否有结果 return res == Integer.MAX_VALUE ? 0 : res; &#125;&#125; 二分查找在转动过的有序数组中寻找目标值题目链接 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import java.util.*;public class Solution &#123; /** * * @param A int整型一维数组 * @param target int整型 * @return int整型 */ public int search (int[] A, int target) &#123; // 如果没有翻转，则直接二分查找 if(A[0] &lt; A[A.length - 1])&#123; return bsearch(A,target,0,A.length - 1); &#125; int left = 0; int right = A.length - 1; int mid; // 找到翻转点 while(left &lt;= right)&#123; mid = (left + right) / 2; if(A[mid] &gt;= A[0])&#123; left = mid + 1; &#125;else&#123; right = mid - 1; &#125; &#125; // 如果要查找的值比最左边的大，则在翻转点左边二分查找 if(target &gt;= A[0])&#123; return bsearch(A,target,0,right); &#125;else&#123;// 反之 return bsearch(A,target,left,A.length - 1); &#125; &#125; // 二分查找标准代码 public int bsearch(int[] A,int target,int left, int right)&#123; int mid; while(left &lt;= right)&#123; mid = (left + right) / 2; if(target == A[mid])&#123; return mid; &#125;else if(target &gt; A[mid])&#123; left = mid + 1; &#125;else&#123; right = mid - 1; &#125; &#125; return -1; &#125;&#125; 搜索旋转排序数组题目链接 代码如下： 1234567891011121314151617181920212223242526272829303132333435class Solution &#123; public int search(int[] nums, int target) &#123; int n = nums.length; if(nums == null || n == 0)&#123; return -1; &#125; if(n == 1)&#123; return nums[0] == target ? 0 : -1; &#125; int left = 0; int right = n - 1; while(left &lt;= right)&#123; int mid = left + (right - left) / 2; if(nums[mid] == target)&#123; return mid; &#125; // 左边是有序的 if(nums[0] &lt;= nums[mid])&#123; if(nums[mid] &gt; target &amp;&amp; target &gt;= nums[0])&#123; right = mid - 1; &#125;else&#123; left = mid + 1; &#125; &#125;else&#123; // 右边是有序的 if(nums[mid] &lt; target &amp;&amp; target &lt;= nums[n-1])&#123; left = mid + 1; &#125;else&#123; right = mid - 1; &#125; &#125; &#125; return -1; &#125;&#125; 搜索旋转排序数组 II题目链接 代码如下： 12345678910111213141516171819202122232425262728293031323334353637class Solution &#123; public boolean search(int[] nums, int target) &#123; if(nums == null || nums.length == 0)&#123; return false; &#125; int start = 0; int end = nums.length - 1; while(start &lt;= end)&#123; int mid = start + (end - start) / 2; if(nums[mid] == target)&#123; return true; &#125; // 去掉重复的 if(nums[start] == nums[mid])&#123; start++; continue; &#125; // 左边部分有序 if(nums[start] &lt; nums[mid])&#123; // target在左边 if(nums[mid] &gt; target &amp;&amp; nums[start] &lt;= target)&#123; end = mid - 1; &#125;else&#123; start = mid + 1; &#125; &#125;else&#123; // target在右边 if(nums[mid] &lt; target &amp;&amp; target &lt;= nums[end])&#123; start = mid + 1; &#125;else&#123; end = mid - 1; &#125; &#125; &#125; return false; &#125;&#125; 寻找旋转排序数组中的最小值 II题目链接 代码如下： 12345678910111213141516171819class Solution &#123; public int findMin(int[] nums) &#123; int left= 0; int right = nums.length - 1; while(left &lt;= right)&#123; int mid = left + (right - left) / 2; if(nums[mid] == nums[right])&#123; right--; &#125;else if(nums[mid] &lt; nums[right])&#123; // 中间值比最右边的值小，而中间这个值可能是最小值，所以右指针为mid，不能为mid - 1 right = mid; &#125;else&#123; // 中间值比最右边的值大，因此左指针为mid + 1 left = mid + 1; &#125; &#125; return nums[left]; &#125;&#125; 二分查找-II题目链接 代码如下： 12345678910111213141516171819202122public int search (int[] nums, int target) &#123; // write code here int low = 0; int high = nums.length - 1; int mid = 0; while(low &lt;= high)&#123; mid = low + (high - low) / 2; if(nums[mid] == target)&#123; // 去重，找到最左边的解 while(mid != 0 &amp;&amp; nums[mid] == nums[mid - 1])&#123; mid--; &#125; return mid; &#125; else if(nums[mid] &gt; target)&#123; high = mid - 1; &#125;else&#123; low = mid + 1; &#125; &#125; return -1; &#125; 双指针删除排序数组中的重复项题目链接 代码如下： 1234567891011public int removeDuplicates(int[] nums) &#123; if (nums.length == 0) return 0; int i = 0; for (int j = 1; j &lt; nums.length; j++) &#123; if (nums[j] != nums[i]) &#123; i++; nums[i] = nums[j]; &#125; &#125; return i + 1;&#125; 回文数字题解 代码如下： 123456789101112131415161718192021222324public class Solution &#123; /** * * @param x int整型 * @return bool布尔型 */ public boolean isPalindrome (int x) &#123; // write code here if(x == 0)&#123; return true; &#125; // 如果是负数，或者能被10整除，就不是回文 if(x &lt; 0 || x % 10 == 0)&#123; return false; &#125; int reverse = 0; while(x &gt; reverse)&#123; reverse = reverse * 10 + x % 10; x /= 10; &#125; // 判断的关键点 return (reverse == x) || (reverse / 10 == x); &#125;&#125; 无重复字符的最长子串3. 无重复字符的最长子串 代码如下： 12345678910111213141516171819202122class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; int len = s.length(); if(len == 0)&#123; return 0; &#125; int left = 0; int max = 0; Map&lt;Character,Integer&gt; map = new HashMap(); for(int i = 0; i &lt; len; i++)&#123; char ch = s.charAt(i); // 如果当前窗口包含字符，更新左边界 if(map.containsKey(ch))&#123; left = Math.max(left,map.get(ch) + 1); &#125; // 将当前字符放到窗口中 map.put(ch,i); max = Math.max(max,i - left + 1); &#125; return max; &#125;&#125; 链表两个链表生成相加链表 代码如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import java.util.*;/* * public class ListNode &#123; * int val; * ListNode next = null; * &#125; */public class Solution &#123; /** * * @param head1 ListNode类 * @param head2 ListNode类 * @return ListNode类 */ public ListNode addInList (ListNode head1, ListNode head2) &#123; // write code here // 将两个链表反转 ListNode p1 = reverse(head1); ListNode p2 = reverse(head2); // 定义结果链表 ListNode res = new ListNode(-1); ListNode curr = res; // 进位 int carry = 0; while(p1 != null || p2 != null)&#123; int sum = 0; if(p1 != null)&#123; sum += p1.val; p1 = p1.next; &#125; if(p2 != null)&#123; sum += p2.val; p2 = p2.next; &#125; // 低位+进位 sum += carry; curr.next = new ListNode(sum % 10); // 从新计算进位 carry = sum / 10; curr = curr.next; &#125; // 如果有进位，直接加到结果链表尾部 if(carry &gt; 0)&#123; curr.next = new ListNode(carry); &#125; // 将结果链表反转 return reverse(res.next); &#125; // 翻转链表 public ListNode reverse(ListNode head)&#123; ListNode pre = null; ListNode next = null; while(head != null)&#123; next = head.next; head.next = pre; pre = head; head = next; &#125; return pre; &#125;&#125; 合并k个已排序的链表题目链接 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839import java.util.*;public class Solution &#123; public ListNode mergeKLists(ArrayList&lt;ListNode&gt; lists) &#123; if(lists == null || lists.size() == 0)&#123; return null; &#125; return mergeKList(lists,0,lists.size() - 1); &#125; public ListNode mergeKList(ArrayList&lt;ListNode&gt; lists,int low, int high) &#123; // 左右相等说明不能再分 if(low &gt;= high)&#123; return lists.get(low); &#125; // 计算mid int mid = low + (high - low) / 2; ListNode l1 = mergeKList(lists,low,mid); ListNode l2 = mergeKList(lists,mid + 1,high); return merge(l1,l2); &#125; // 合并两个有序链表 public ListNode merge(ListNode node1,ListNode node2)&#123; ListNode node = new ListNode(-1); ListNode tmp = node; while(node1!=null &amp;&amp; node2!=null)&#123; if(node1.val &lt;= node2.val)&#123; tmp.next = node1; node1 = node1.next; &#125;else&#123; tmp.next = node2; node2 = node2.next; &#125; tmp = tmp.next; &#125; tmp.next = node1!=null?node1:node2; return node.next; &#125;&#125; 单链表的排序题目链接 代码如下(方法1 归并排序)： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.util.*;/* * public class ListNode &#123; * int val; * ListNode next = null; * &#125; */public class Solution &#123; /** * * @param head ListNode类 the head node * @return ListNode类 */ public ListNode sortInList (ListNode head) &#123; if(head == null || head.next == null)&#123; return head; &#125; // 使用快慢指针找到链表中间部分 ListNode slow = head; ListNode fast = head.next; while(fast != null &amp;&amp; fast.next != null)&#123; slow = slow.next; fast = fast.next.next; &#125; // 截取第二部分链表 ListNode newList = slow.next; slow.next = null; // 递归继续分割两个链表 ListNode left = sortInList(head); ListNode right = sortInList(newList); // 合并两个有序链表 ListNode dummy = new ListNode(-1); ListNode res = dummy; while(left != null &amp;&amp; right != null)&#123; if(left.val &lt; right.val)&#123; res.next = left; left = left.next; &#125;else&#123; res.next = right; right = right.next; &#125; res = res.next; &#125; res.next = left == null? right : left; return dummy.next; &#125;&#125; 代码如下(方法2 插入排序)： 123456789101112131415161718192021222324252627282930313233343536373839404142/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode insertionSortList(ListNode head) &#123; // 如果头节点为空，直接返回 if(head == null)&#123; return head; &#125; // 新建哑节点，保存头结点信息 ListNode dummy = new ListNode(0); dummy.next = head; // 排序好部分最后一个元素 ListNode lastSorted = head; // 当前节点（待排序元素） ListNode curr = head.next; while(curr != null)&#123; // 如果当前元素不用排序，将排序链表增长，也即lastSorted后移 if(lastSorted.val &lt;= curr.val)&#123; lastSorted = lastSorted.next; &#125;else&#123; // 从头结点开始找，pre保存前一个元素 ListNode pre = dummy; while(pre.next.val &lt;= curr.val)&#123; pre = pre.next; &#125; // 将curr节点插入到对应位置 lastSorted.next = curr.next; curr.next = pre.next; pre.next = curr; &#125; // 更新当前节点为排序好链表下一个节点 curr = lastSorted.next; &#125; return dummy.next; &#125;&#125; 链表内指定区间反转题目链接 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839import java.util.*;/* * public class ListNode &#123; * int val; * ListNode next = null; * &#125; */public class Solution &#123; /** * * @param head ListNode类 * @param m int整型 * @param n int整型 * @return ListNode类 */ public ListNode reverseBetween (ListNode head, int m, int n) &#123; ListNode dummy = new ListNode(-1); dummy.next = head; ListNode pre = dummy; for(int i = 0; i &lt; m-1; i++)&#123; pre = pre.next; &#125; // 先获得翻转链表pre部分的头指针 head = pre.next; ListNode next = null; for(int i = m; i &lt; n; i++)&#123; next = head.next; // head节点连接next节点之后链表部分，也就是向后移动一位 head.next = next.next; // next 每次移动到头结点位置 next.next = pre.next; // 翻转部分头结点的前驱指向当前节点 pre.next = next; &#125; return dummy.next; &#125;&#125; 删除有序链表中重复出现的元素(将重复出现的元素删完)题目链接 代码如下： 123456789101112131415161718192021222324252627282930313233public class Solution &#123; /** * * @param head ListNode类 * @return ListNode类 */ public ListNode deleteDuplicates (ListNode head) &#123; ListNode dummy = new ListNode(-1); dummy.next = head; // 前驱结点 ListNode pre = dummy; // 当前结点 ListNode curr = head; while(curr != null &amp;&amp; curr.next != null)&#123; // 如果两个结点相等，说明结点重复 if(curr.val == curr.next.val)&#123; // 找到最后一个不重复的结点 ListNode tmp = curr.next; while(tmp != null &amp;&amp; tmp.val == curr.val)&#123; tmp = tmp.next; &#125; // 将当前前驱结点接最后一个不重复的结点后面 pre.next = tmp; // 更新当前结点 curr = tmp; &#125;else&#123; pre = pre.next; curr = curr.next; &#125; &#125; return dummy.next; &#125;&#125; 删除有序链表中重复的元素(使每个元素只出现一次)题目链接 代码如下： 123456789101112131415161718192021public class Solution &#123; /** * * @param head ListNode类 * @return ListNode类 */ public ListNode deleteDuplicates (ListNode head) &#123; if(head == null)&#123; return null; &#125; ListNode tmp = head; while(tmp.next != null)&#123; if(tmp.val == tmp.next.val)&#123; tmp.next = tmp.next.next; &#125;else&#123; tmp = tmp.next; &#125; &#125; return head; &#125;&#125; 链表的奇偶重排题目链接 代码如下： 12345678910111213141516171819202122232425262728public class Solution &#123; /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * * * @param head ListNode类 * @return ListNode类 */ public ListNode oddEvenList (ListNode head) &#123; if(head == null)&#123; return head; &#125; ListNode evenHead = head.next; ListNode odd = head; ListNode even = evenHead; while(even != null &amp;&amp; even.next != null)&#123; // 获得奇数结点 odd.next = even.next; odd = odd.next; // 获得偶数结点 even.next = odd.next; even = even.next; &#125; // 将奇数结点和偶数结点链接起来 odd.next = evenHead; return head; &#125;&#125; 重排链表题目链接 代码如下： 1234567891011121314151617181920212223242526272829303132333435363738public class Solution &#123; public void reorderList(ListNode head) &#123; if(head == null || head.next == null || head.next.next == null) return; // 将链表分成两个部分 ListNode slow = head; ListNode fast = head.next; while(fast != null &amp;&amp; fast.next != null)&#123; slow = slow.next; fast = fast.next.next; &#125; ListNode newList = slow.next; slow.next = null; // 将第二部分链表反转 ListNode newHead = reverse(newList); // 合并两个链表 while(newHead != null)&#123; ListNode temp = newHead.next; newHead.next = head.next; head.next = newHead; head = newHead.next; newHead = temp; &#125; &#125; // 反转链表 public ListNode reverse(ListNode head)&#123; ListNode pre = null; while(head != null)&#123; ListNode next = head.next; head.next = pre; pre = head; head = next; &#125; return pre; &#125;&#125; K 个一组翻转链表题目链接 代码如下：解法1： 1234567891011121314151617181920212223242526272829303132333435public ListNode reverseKGroup(ListNode head, int k) &#123; if (head == null || head.next == null) &#123; return head; &#125; ListNode tail = head; for (int i = 0; i &lt; k; i++) &#123; //剩余数量小于k的话，则不需要反转。 if (tail == null) &#123; return head; &#125; tail = tail.next; &#125; // 反转前 k 个元素 ListNode newHead = reverse(head, tail); //下一轮的开始的地方就是tail head.next = reverseKGroup(tail, k); return newHead;&#125;/*左闭又开区间 */private ListNode reverse(ListNode head, ListNode tail) &#123; ListNode pre = null; ListNode next = null; while (head != tail) &#123; next = head.next; head.next = pre; pre = head; head = next; &#125; return pre;&#125; 解法2： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode() &#123;&#125; * ListNode(int val) &#123; this.val = val; &#125; * ListNode(int val, ListNode next) &#123; this.val = val; this.next = next; &#125; * &#125; */class Solution &#123; public ListNode reverseKGroup(ListNode head, int k) &#123; ListNode dummy = new ListNode(-1); dummy.next = head; ListNode pre = dummy; ListNode end = dummy; // 找到待翻转链表的尾部 while(end.next != null)&#123; for(int i = 0; i &lt; k &amp;&amp; end != null; i++)&#123; end = end.next; &#125; // 如果末尾的链表长度不足k，保持原样 if(end == null)&#123; break; &#125; ListNode start = pre.next; // 找到待翻转部分的下一节点，并断开连接 ListNode next = end.next; end.next = null; // 翻转链表 pre.next = reverse(start); // 连接链表 start.next = next; // 更新pre指针和end指针 pre = start; end = start; &#125; return dummy.next; &#125; public ListNode reverse(ListNode head)&#123; ListNode pre = null; ListNode curr = head; while(curr != null)&#123; ListNode next = curr.next; curr.next = pre; pre = curr; curr = next; &#125; return pre; &#125;&#125; 环形链表Ⅱ题目链接 代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * Definition for singly-linked list. * class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; * val = x; * next = null; * &#125; * &#125; */public class Solution &#123; public ListNode detectCycle(ListNode head) &#123; if(head == null)&#123; return null; &#125; ListNode slow = head; ListNode fast = head; /** fast指针是slow的两倍，假设链表环以前部分长度为a，环的长度为b 则 （1）f = 2s,且 fast 比 slow 多走n圈，则为nb，所以（2）f = s + nb (1) - (2) --&gt; s = nb; 所有节点从链表头部走到环开始都需要走k步，k = a + nb; 因为 由上式：s = nb,得出slow指针已经走了nb，还需要走a步，则可以将fast指针放在head 处，每次走一步，并且slow指针每次也走一步，两个指针同时走a步会进行第二次相遇。 */ while(true)&#123; if(fast == null || fast.next == null)&#123; return null; &#125; slow = slow.next; fast = fast.next.next; if(fast == slow)&#123; break; &#125; &#125; fast = head; while(fast != slow)&#123; fast = fast.next; slow = slow.next; &#125; return fast; &#125;&#125; 两两交换链表中的节点题目链接 代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode() &#123;&#125; * ListNode(int val) &#123; this.val = val; &#125; * ListNode(int val, ListNode next) &#123; this.val = val; this.next = next; &#125; * &#125; */class Solution &#123; public ListNode swapPairs(ListNode head) &#123; // 边界判断 if(head == null)&#123; return null; &#125; if(head.next == null)&#123; return head; &#125; // 定义哑节点保存头节点 ListNode dummy = new ListNode(-1); dummy.next = head; // 定义头节点的前置节点 ListNode pre = dummy; // 当前节点为pre节点的后置节点 ListNode curr = pre.next; while(curr != null &amp;&amp; curr.next != null)&#123; // 记录当前节点的后一节点 ListNode next = curr.next; // 分以下三步交换curr和next节点 curr.next = next.next; next.next = curr; pre.next = next; // 更新前置节点 pre = curr; // 更新当前节点 curr = pre.next; &#125; return dummy.next; &#125;&#125; 两数相加题目链接 代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode() &#123;&#125; * ListNode(int val) &#123; this.val = val; &#125; * ListNode(int val, ListNode next) &#123; this.val = val; this.next = next; &#125; * &#125; */class Solution &#123; public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; ListNode res = new ListNode(-1); ListNode curr = res; int carry = 0; while(l1 != null || l2 != null)&#123; // 获取两个节点的值 int n1 = l1 != null ? l1.val : 0; int n2 = l2 != null ? l2.val : 0; int num = n1 + n2 + carry; // 计算进位以及取余 carry = num / 10; num = num % 10; // 创建并连接新节点 curr.next = new ListNode(num); curr = curr.next; // 如果两个链表当前节点不为空，向后移动 if(l1 != null)&#123; l1 = l1.next; &#125; if(l2 != null)&#123; l2 = l2.next; &#125; &#125; if(carry == 1)&#123; curr.next = new ListNode(1); &#125; return res.next; &#125;&#125; 大数加法 代码如下： 123456789101112131415161718192021222324252627import java.util.*;public class Solution &#123; /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * 计算两个数之和 * @param s string字符串 表示第一个整数 * @param t string字符串 表示第二个整数 * @return string字符串 */ public String solve (String s, String t) &#123; // write code here int i = s.length() - 1; int j = t.length() - 1; int carry = 0; StringBuilder sb = new StringBuilder(); while(i &gt;= 0 || j &gt;= 0 || carry != 0)&#123; int x = i &lt; 0 ? 0 : s.charAt(i--) - &#x27;0&#x27;; int y = j &lt; 0 ? 0 : t.charAt(j--) - &#x27;0&#x27;; int sum = x + y + carry; sb.append(sum % 10); carry = sum / 10; &#125; return sb.reverse().toString(); &#125;&#125; 二叉树重建二叉树 代码如下： 123456789101112131415161718192021222324import java.util.*;public class Solution &#123; public TreeNode reConstructBinaryTree(int [] pre,int [] in) &#123; Map&lt;Integer,Integer&gt; map = new HashMap(); for(int i = 0; i &lt; in.length; i++)&#123; map.put(in[i],i); &#125; return dfs(0,pre.length - 1,0,in.length - 1,pre,in,map); &#125; public TreeNode dfs(int pl,int pr,int il,int ir,int[] pre,int[] in,Map&lt;Integer,Integer&gt; map)&#123; if(pl &gt; pr)&#123; return null; &#125; // 根据先序遍历获得根节点 int k = map.get(pre[pl]); TreeNode root = new TreeNode(pre[pl]); // 递归构造左子树 root.left = dfs(pl + 1,pl + k -il,il,k-1,pre,in,map); // 递归构造右子树 root.right = dfs(pl + k- il + 1,pr,k+1,ir,pre,in,map); return root; &#125;&#125; 在二叉树中找到两个节点的最近公共祖先 以下图片来自于牛客￥ABCDEF题解代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940import java.util.*;/* * public class TreeNode &#123; * int val = 0; * TreeNode left = null; * TreeNode right = null; * &#125; */public class Solution &#123; /** * * @param root TreeNode类 * @param o1 int整型 * @param o2 int整型 * @return int整型 */ public int lowestCommonAncestor (TreeNode root, int o1, int o2) &#123; // write code here return dfs(root,o1,o2).val; &#125; public TreeNode dfs(TreeNode root ,int o1,int o2)&#123; // 如果当前节点为空，或者当前节点等于o1或者等于o2就返回值给父亲节点 if(root == null || root.val == o1 || root.val == o2)&#123; return root; &#125; // 递归遍历左子树 TreeNode left = dfs(root.left,o1,o2); // 递归遍历右子树 TreeNode right = dfs(root.right,o1,o2); // 如果left、right都不为空，那么代表o1、o2在root的两侧，所以root为他们的公共祖先 if(left != null &amp;&amp; right != null)&#123; return root; &#125; // 如果left、right有一个为空，那么就返回不为空的那一个 return left == null? right : left; &#125;&#125; 对称的二叉树题目链接 代码如下： 123456789101112131415161718192021222324252627/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public boolean isSymmetric(TreeNode root) &#123; return root == null ? true : recur(root.left,root.right); &#125; public boolean recur(TreeNode A,TreeNode B)&#123; // 左右子树都为空，则树是对称的 if(A == null &amp;&amp; B == null)&#123; return true; &#125; // 只有一边为空，且值不相等，则不对称 if(A == null || B == null || A.val != B.val)&#123; return false; &#125; // 递归判断左右子树，如果都对称，则树对称 return recur(A.left,B.right) &amp;&amp; recur(A.right,B.left); &#125;&#125; 叶子相似的树叶子相似的树 代码如下： 12345678910111213141516171819202122232425262728class Solution &#123; List&lt;Integer&gt; res1 = new ArrayList(); List&lt;Integer&gt; res2 = new ArrayList(); public boolean leafSimilar(TreeNode root1, TreeNode root2) &#123; // 分别得到两棵树的叶值序列 dfs(root1,res1); dfs(root2,res2); // 比较两个数是否相同 return res1.equals(res2); &#125; public void dfs(TreeNode root,List&lt;Integer&gt; res)&#123; // 根据题意，将叶子结点加入结果集 if(root.left == null &amp;&amp; root.right == null)&#123; res.add(root.val); &#125;else&#123; if(root.left != null)&#123; dfs(root.left,res); &#125; if(root.right != null)&#123; dfs(root.right,res); &#125; &#125; &#125;&#125; 二叉树的右视图题目链接 代码如下： 123456789101112131415161718192021222324252627class Solution &#123; public List&lt;Integer&gt; rightSideView(TreeNode root) &#123; List&lt;Integer&gt; res = new ArrayList(); if(root == null)&#123; return res; &#125; Queue&lt;TreeNode&gt; q = new LinkedList(); q.offer(root); while(!q.isEmpty())&#123; int size = q.size(); for(int i = 0; i &lt; size; i++)&#123; TreeNode node = q.poll(); if(node.left != null)&#123; q.offer(node.left); &#125; if(node.right != null)&#123; q.offer(node.right); &#125; // 将每一层的最后一个加入结果集,即为右视图 if(i == size - 1)&#123; res.add(node.val); &#125; &#125; &#125; return res; &#125;&#125; 二叉树根节点到叶子节点和为指定值的路径题目链接 代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.util.*;/* * public class TreeNode &#123; * int val = 0; * TreeNode left = null; * TreeNode right = null; * &#125; */public class Solution &#123; /** * * @param root TreeNode类 * @param sum int整型 * @return int整型ArrayList&lt;ArrayList&lt;&gt;&gt; */ ArrayList&lt;ArrayList&lt;Integer&gt;&gt; res = new ArrayList(); ArrayList&lt;Integer&gt; tmp = new ArrayList(); public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; pathSum (TreeNode root, int sum) &#123; dfs(root,sum,0); return res; &#125; public void dfs(TreeNode root, int sum,int cnt)&#123; if(root == null)&#123; return; &#125; // 如果节点为空结束当前递归 tmp.add(root.val); // 把当前节点加入到路径和中 cnt += root.val; // 如果找到叶子节点 if(root.left == null &amp;&amp; root.right == null)&#123; // 如果找到结果，将tmp加入结果集res if(sum == cnt)&#123; res.add(new ArrayList(tmp)); &#125; &#125;else&#123; // 递归左子树 dfs(root.left,sum,cnt); // 递归右子树 dfs(root.right,sum,cnt); &#125; // 移除最后一个元素 tmp.remove(tmp.size() - 1); &#125;&#125; 二叉树根节点到叶子结点的所有路径和题目链接 代码如下： 12345678910111213141516171819202122232425262728public class Solution &#123; /** * * @param root TreeNode类 * @return int整型 */ public int sumNumbers (TreeNode root) &#123; if(root == null)&#123; return 0; &#125; return dfs(root,0); &#125; public int dfs(TreeNode root, int sum)&#123; if(root == null)&#123; return 0; &#125;else&#123; sum = sum * 10 + root.val; if(root.left == null &amp;&amp; root.right == null)&#123; return sum; &#125;else&#123; // 直接将同级的叶子结点加起来 return dfs(root.left,sum) + dfs(root.right,sum); &#125; &#125; &#125;&#125; 二叉树从根节点到叶子结点路径问题使用回溯求出所有路径，再根据具体题目要求筛选结果代码如下： 12345678910111213141516171819202122232425class Solution &#123; List&lt;List&lt;Integer&gt;&gt; ret = new LinkedList&lt;List&lt;Integer&gt;&gt;(); Deque&lt;Integer&gt; path = new LinkedList&lt;Integer&gt;(); public List&lt;List&lt;Integer&gt;&gt; pathSum(TreeNode root) &#123; dfs(root); return ret; &#125; public void dfs(TreeNode root) &#123; if (root == null) &#123; return; &#125; // 将当前结点加入路径中 path.offerLast(root.val); // 如果到达最底部，说明该路径 if (root.left == null &amp;&amp; root.right == null) &#123; ret.add(new LinkedList&lt;Integer&gt;(path)); &#125; dfs(root.left); dfs(root.right); // 回到上一状态 path.pollLast(); &#125;&#125; 二叉树中是否存在节点和为指定值的路径题目链接 代码如下： 12345678910111213141516171819public class Solution &#123; /** * * @param root TreeNode类 * @param sum int整型 * @return bool布尔型 */ public boolean hasPathSum (TreeNode root, int sum) &#123; // write code here if(root == null)&#123; return false; &#125; sum -= root.val; if(sum == 0 &amp;&amp; root.left == null &amp;&amp; root.right == null)&#123; return true; &#125; return hasPathSum(root.left,sum) || hasPathSum(root.right,sum); &#125;&#125; 二叉树的最大路径和题目链接 代码如下： 123456789101112131415161718192021222324252627282930313233public class Solution &#123; /** * * @param root TreeNode类 * @return int整型 */ private int Max = Integer.MIN_VALUE; public int maxPathSum (TreeNode root) &#123; dfs(root); return Max; &#125; public int dfs(TreeNode root)&#123; if(root == null)&#123; return 0; &#125; int left = dfs(root.left); int right = dfs(root.right); int curr = root.val; // 如果左右子树大于0，则更新当前最大值 if(left &gt; 0)&#123; curr += left; &#125; if(right &gt; 0)&#123; curr += right; &#125; // 更新当前最大值 Max = Math.max(Max,curr); // 返回最大值 return Math.max(root.val,Math.max(left,right) + root.val); &#125;&#125; 左叶子之和题目链接 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode() &#123;&#125; * TreeNode(int val) &#123; this.val = val; &#125; * TreeNode(int val, TreeNode left, TreeNode right) &#123; * this.val = val; * this.left = left; * this.right = right; * &#125; * &#125; */class Solution &#123; public int sumOfLeftLeaves(TreeNode root) &#123; if(root == null)&#123; return 0; &#125; int sum = 0; Queue&lt;TreeNode&gt; q = new LinkedList(); q.offer(root); while(!q.isEmpty())&#123; int size = q.size(); for(int i = 0; i &lt; size; i++)&#123; TreeNode node = q.poll(); // 如果左孩子不为空 if(node.left != null)&#123; // 如果当前节点为左叶子节点 if(node.left.left == null &amp;&amp; node.left.right == null)&#123; sum += node.left.val; &#125;else&#123; q.offer(node.left); &#125; &#125; if(node.right != null)&#123; q.offer(node.right); &#125; &#125; &#125; return sum; &#125;&#125; 二叉树的所有路径题目链接 代码如下: 1234567891011121314151617181920212223242526272829303132333435363738394041/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode() &#123;&#125; * TreeNode(int val) &#123; this.val = val; &#125; * TreeNode(int val, TreeNode left, TreeNode right) &#123; * this.val = val; * this.left = left; * this.right = right; * &#125; * &#125; */class Solution &#123; public List&lt;String&gt; binaryTreePaths(TreeNode root) &#123; List&lt;String&gt; paths = new ArrayList(); if(root == null)&#123; return paths; &#125; constructPath(root,&quot;&quot;,paths); return paths; &#125; public void constructPath(TreeNode root,String s,List&lt;String&gt; paths)&#123; if(root != null)&#123; StringBuilder sb = new StringBuilder(s); sb.append(root.val); // 判断是不是叶子节点，如果是，加入结果集 if(root.left == null &amp;&amp; root.right == null)&#123; paths.add(sb.toString()); &#125;else&#123; // 递归构建结果 sb.append(&quot;-&gt;&quot;); constructPath(root.left,sb.toString(),paths); constructPath(root.right,sb.toString(),paths); &#125; &#125; &#125; &#125; 动态规划子数组的最大累加和问题 以下图片来自于牛客￥ABCDEF题解 代码如下：解法1：循环 1234567891011121314151617181920212223public static int helper(int[] array) &#123; int len = array.length; if (len == 0) &#123; return 0; &#125; // 累加子数组和 int currSum = array[0]; // 最大子数组和 int maxSum = array[0]; for (int i = 1; i &lt; len; i++) &#123; // 如果当前”累加子数组和“ 大于0，与当前数累加 if (currSum &gt; 0) &#123; currSum += array[i]; &#125; else &#123; // 如果当前”累加子数组和“ 小于0，抛弃前面累加和，从当前数开始累加 currSum = array[i]; &#125; // 更新最大子数组和 maxSum = Math.max(maxSum, currSum); &#125; return maxSum; &#125; 解法2：DP 123456789101112131415161718192021222324252627import java.util.*;public class Solution &#123; /** * max sum of the subarray * @param arr int整型一维数组 the array * @return int整型 */ public int maxsumofSubarray (int[] arr) &#123; // write code here //dp[i]代表到第i位的时侯,以arr[i]结尾的连续子数组最大累加和 int[] dp = new int[arr.length]; dp[0] = arr[0]; int res = arr[0]; for(int i = 1; i&lt; arr.length; i++)&#123; // 如果前面的子数组和大于0，则更新当前位置 if(dp[i-1] &gt; 0)&#123; dp[i] = dp[i-1] + arr[i]; &#125;else&#123; // 否则，更新当前dp值为当前数组值 dp[i] = arr[i]; &#125; res = Math.max(res,dp[i]); &#125; return res; &#125;&#125; 最长递增子序列1. 题目链接 代码如下:解法一：DP（超时） 12345678910111213141516171819202122232425262728293031323334import java.util.*;public class Solution &#123; /** * retrun the longest increasing subsequence * @param arr int整型一维数组 the array * @return int整型一维数组 */ public int[] LIS (int[] arr) &#123; // write code here int[] dp = new int[arr.length]; int max = 1; Arrays.fill(dp, 1); for (int i = 1; i &lt; arr.length; i ++) &#123; for (int j = 0; j &lt; i; j ++) &#123; if (arr[i] &gt; arr[j]) &#123; dp[i] = Math.max(dp[i], dp[j] + 1); max = Math.max(max, dp[i]); &#125; &#125; &#125; int[] res = new int[max]; int index = dp.length - 1; while (index &gt;= 0) &#123; if (dp[index] == max) &#123; res[max - 1] = arr[index]; max --; &#125; index --; &#125; return res; &#125;&#125; 解法二：贪心+二分 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.util.*;public class Solution &#123; /** * retrun the longest increasing subsequence * @param arr int整型一维数组 the array * @return int整型一维数组 */ public int[] LIS (int[] arr) &#123; // write code here int[] d = new int[arr.length]; int[] index = new int[arr.length]; int len = 0; d[len] = arr[len]; index[0] = 0; for (int i = 1; i &lt; arr.length; i ++) &#123; if (arr[i] &gt; d[len]) &#123; len ++; d[len] = arr[i]; index[i] = len; &#125; else &#123; int l = 0, r = len; while (l &lt;= r) &#123; int mid = (l + r) / 2; if (d[mid] &lt; arr[i]) &#123; l = mid + 1; &#125; else &#123; r = mid - 1; &#125; &#125; d[l] = arr[i]; index[i] = l; &#125; &#125; int[] res = new int[len + 1]; int cur = arr.length - 1; while (cur &gt;= 0) &#123; if (index[cur] == len) &#123; res[len] = arr[cur]; len --; &#125; cur --; &#125; return res; &#125;&#125; 2.题目链接 代码如下： 123456789101112131415161718192021222324class Solution &#123; public int lengthOfLIS(int[] nums) &#123; int n = nums.length; // dp数组表示以nums[1]结尾的“最长上升子序列”的长度 int[] dp = new int[n]; // 初始化dp数组为1 Arrays.fill(dp,1); for(int i = 1; i &lt; n; i++)&#123; // 下标为i以前上升子序列 for(int j= 0; j &lt; i; j++)&#123; if(nums[j] &lt; nums[i])&#123; // 根据状态转换图更新最长上升子序列的长度 dp[i] = Math.max(dp[i],dp[j] + 1); &#125; &#125; &#125; // 找到最大值 int max = 0; for(int i = 0; i &lt; n; i++)&#123; max = Math.max(max,dp[i]); &#125; return max; &#125;&#125; 最长公共子序列（返回值为子序列的长度）题目链接 代码如下： 12345678910111213141516171819class Solution &#123; public int longestCommonSubsequence(String text1, String text2) &#123; int m = text1.length(); int n = text2.length(); int[][] dp = new int[m+1][n+1]; for(int i = 1; i &lt;= m; i++)&#123; char ch1 = text1.charAt(i-1); for(int j = 1; j &lt;= n; j++)&#123; char ch2 = text2.charAt(j-1); if(ch1 == ch2)&#123; dp[i][j] = dp[i-1][j-1] + 1; &#125;else&#123; dp[i][j] = Math.max(dp[i-1][j],dp[i][j-1]); &#125; &#125; &#125; return dp[m][n]; &#125;&#125; 最长公共子序列（返回值为子序列）题目链接 代码如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import java.util.*;public class Solution &#123; /** * longest common subsequence * @param s1 string字符串 the string * @param s2 string字符串 the string * @return string字符串 */ public String LCS (String s1, String s2) &#123; int len1 = s1.length(); int len2 = s2.length(); int[][] dp = new int[len1 + 1][len2 + 1]; for(int i = 1; i &lt;= len1; i++)&#123; for(int j = 1; j &lt;= len2; j++)&#123; if(s1.charAt(i-1) == s2.charAt(j-1))&#123; dp[i][j] = dp[i-1][j-1] + 1; &#125;else&#123; dp[i][j] = Math.max(dp[i-1][j],dp[i][j-1]); &#125; &#125; &#125; // 从末尾开始向前找 StringBuilder sb = new StringBuilder(); int a = len1; int b = len2; while(a != 0 &amp;&amp; b != 0)&#123; // 如果是公共子串 if(s1.charAt(a-1) == s2.charAt(b-1))&#123; // 加入结果集 sb.append(s1.charAt(a-1)); a--; b--; &#125;else&#123; // 从上面来 if(dp[a-1][b] &gt; dp[a][b-1])&#123; a--; &#125;else&#123; // 从下面来 b--; &#125; &#125; &#125; if(sb.length() == 0)&#123; return &quot;-1&quot;; &#125;else&#123; return sb.reverse().toString(); &#125; &#125;&#125; 不相交的线题目链接 1234567891011121314151617181920class Solution &#123; public int maxUncrossedLines(int[] nums1, int[] nums2) &#123; int m = nums1.length; int n = nums2.length; int[][] dp = new int[m+1][n+1]; for(int i = 1; i &lt;= m; i++)&#123; int num1 = nums1[i-1]; for(int j = 1; j &lt;= n; j++)&#123; int num2 = nums2[j-1]; if(num1 == num2)&#123; dp[i][j] = dp[i-1][j-1] + 1; &#125;else&#123; dp[i][j] = Math.max(dp[i][j-1],dp[i-1][j]); &#125; &#125; &#125; return dp[m][n]; &#125;&#125; 买卖股票的最佳时机题目链接 123456789101112131415161718192021222324import java.util.*;public class Solution &#123; /** * * @param prices int整型一维数组 * @return int整型 */ public int maxProfit (int[] prices) &#123; int min = Integer.MAX_VALUE; int maxProfit = 0; for(int i = 0; i &lt; prices.length; i++)&#123; // 记录历史最低价 if(prices[i] &lt; min)&#123; min = prices[i]; &#125;else if(prices[i] - min &gt; maxProfit)&#123; // 更新最大收益 maxProfit = prices[i] - min; &#125; &#125; return maxProfit; &#125;&#125; 买卖股票的最佳时机 II题目链接 代码如下： 1 编辑距离题目链接 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142import java.util.*;public class Solution &#123; /** * min edit cost * @param str1 string字符串 the string * @param str2 string字符串 the string * @param ic int整型 insert cost * @param dc int整型 delete cost * @param rc int整型 replace cost * @return int整型 */ public int minEditCost (String str1, String str2, int ic, int dc, int rc) &#123; int m = str1.length(); int n = str2.length(); int[][] dp = new int[m+1][n+1]; // 初始化第一行 for(int j = 1; j &lt;= n; j++)&#123; dp[0][j] = j * ic; &#125; // 初始化第一列 for(int i = 1; i &lt;= m; i++)&#123; dp[i][0] = i * dc; &#125; for(int i = 1; i &lt;= m; i++)&#123; for(int j = 1; j &lt;= n; j++)&#123; if(str1.charAt(i-1) == str2.charAt(j-1))&#123; dp[i][j] = dp[i-1][j-1]; &#125;else&#123; // 将i个字符串转变为前j-1个字符串在插入第j个字符 int insert = dp[i][j-1] + ic; // 将i-1个字符串转换为前j个字符串删除第i个字符 int delete = dp[i-1][j] + dc; int replace = dp[i-1][j-1] + rc; dp[i][j] = Math.min(Math.min(insert,delete),replace); &#125; &#125; &#125; return dp[m][n]; &#125;&#125; 零钱兑换题目链接 奇怪的打印机题目链接 代码如下： 123456789101112131415161718192021class Solution &#123; public int strangePrinter(String s) &#123; int n = s.length(); int[][] f = new int[n][n]; for (int i = n - 1; i &gt;= 0; i--) &#123; f[i][i] = 1; for (int j = i + 1; j &lt; n; j++) &#123; if (s.charAt(i) == s.charAt(j)) &#123; f[i][j] = f[i][j - 1]; &#125; else &#123; int minn = Integer.MAX_VALUE; for (int k = i; k &lt; j; k++) &#123; minn = Math.min(minn, f[i][k] + f[k + 1][j]); &#125; f[i][j] = minn; &#125; &#125; &#125; return f[0][n - 1]; &#125;&#125; 最大正方形题目链接 代码如下: 123456789101112131415161718192021222324252627class Solution &#123; public int maximalSquare(char[][] matrix) &#123; int max = 0; if(matrix == null || matrix.length == 0 || matrix[0].length == 0)&#123; return max; &#125; int m = matrix.length; int n = matrix[0].length; int[][] dp = new int[m][n]; for(int i = 0; i &lt; m; i++)&#123; for(int j = 0; j &lt; n; j++)&#123; if(matrix[i][j] == &#x27;1&#x27;)&#123; // 如果是第一行或者第一列，初始化值为1 if(i == 0 || j == 0)&#123; dp[i][j] = 1; &#125;else&#123; // 更新当前位置能够组成的最大边长 dp[i][j] = Math.min(Math.min(dp[i - 1][j], dp[i][j - 1]), dp[i - 1][j - 1]) + 1; &#125; &#125; // 更新最大边长 max = Math.max(max,dp[i][j]); &#125; &#125; return max * max; &#125;&#125; 中心扩散法最长回文子串 代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940import java.util.*;public class Solution &#123; public int getLongestPalindrome(String A, int n) &#123; int len = A.length(); if(len &lt; 2)&#123; return A.length(); &#125; int max = 0; for(int i = 0; i &lt; len - 1; i++)&#123; // 回文类型为奇数型 String s1 = helper(A,i,i); // 回文类型为偶数型 String s2 = helper(A,i,i+1); // 求出最长的回文串 String s3 = s1.length() &gt; s2.length()? s1: s2; if(max &lt; s3.length())&#123; max = s3.length(); &#125; &#125; return max; &#125; public String helper(String s, int left, int right)&#123; int len = s.length(); int i = left; int j = right; // 从中间向两边扩散 while(i &gt;= 0 &amp;&amp; j &lt; len)&#123; if(s.charAt(i) == s.charAt(j))&#123; i--; j++; &#125;else&#123; break; &#125; &#125; // 截取回文串 return s.substring(i+1,j); &#125; &#125; 单调栈直方图的水量(接雨水)题目链接 代码如下(双指针)： 12345678910111213141516171819202122232425class Solution &#123; public int trap(int[] height) &#123; int left = 0; int right = height.length - 1; int ans = 0; int leftMax = 0; int rightMax = 0; while(left &lt; right)&#123; // 更新左边的最大值 leftMax = Math.max(leftMax,height[left]); // 更新右边的最大值 rightMax = Math.max(rightMax,height[right]); if(height[left] &lt; height[right])&#123; // 用左边的最大值减去当前高度 ans += leftMax - height[left]; left++; &#125;else&#123; // 用右边的最大值减去当前高度 ans += rightMax - height[right]; right--; &#125; &#125; return ans; &#125;&#125; 代码如下(单调栈): 1234567891011121314151617181920class Solution &#123; public int trap(int[] height) &#123; int ans = 0; Deque&lt;Integer&gt; stack = new LinkedList(); for(int i = 0; i &lt; height.length; i++)&#123; while(!stack.isEmpty() &amp;&amp; height[i] &gt; height[stack.peek()] )&#123; int top = stack.pop(); if(stack.isEmpty())&#123; break; &#125; int left = stack.peek(); int width = i - left - 1; int currHeight = Math.min(height[i],height[left]) - height[top]; ans += currHeight * width; &#125; stack.push(i); &#125; return ans; &#125;&#125; 柱状图中最大的矩形题目链接 123456789101112131415161718192021222324252627282930313233343536public int largestRectangleArea(int[] heights) &#123; // 初始化最终结果为0 int res = 0; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); // 将给定的原数组左右各添加一个元素0 int[] newHeights = new int[heights.length + 2]; newHeights[0] = 0; newHeights[newHeights.length-1] = 0; for (int i = 1; i &lt; heights.length + 1; i++) &#123; newHeights[i] = heights[i - 1]; &#125; // 开始遍历 for (int i = 0; i &lt; newHeights.length; i++) &#123; // 如果栈不为空且当前考察的元素值小于栈顶元素值， // 则表示以栈顶元素值为高的矩形面积可以确定 while (!stack.isEmpty() &amp;&amp; newHeights[i] &lt; newHeights[stack.peek()]) &#123; // 弹出栈顶元素 int cur = stack.pop(); // 获取栈顶元素对应的高 int curHeight = newHeights[cur]; // 栈顶元素弹出后，新的栈顶元素就是其左侧边界 int leftIndex = stack.peek(); // 右侧边界是当前考察的索引 int rightIndex = i; // 计算矩形宽度 int curWidth = rightIndex - leftIndex - 1; // 计算面积 res = Math.max(res, curWidth * curHeight); &#125; // 当前考察索引入栈 stack.push(i); &#125; return res; &#125; 去除重复字母题目链接 123456789101112131415161718192021222324252627282930313233343536373839class Solution &#123; public String removeDuplicateLetters(String s) &#123; int len = s.length(); char[] ss = s.toCharArray(); // 记录当前字符是否在栈中出现 boolean[] visited = new boolean[26]; // 记录每个元素最后出现的位置，用来判断当前元素是否可能在后面的元素中出现，如果出现则丢弃当前元素 int[] lastIndex = new int[26]; for(int i = 0; i &lt; len; i++)&#123; lastIndex[ss[i] - &#x27;a&#x27;] = i; &#125; Deque&lt;Character&gt; stack = new ArrayDeque(); for(int i = 0; i &lt; len; i++)&#123; // 判断当前元素是否在栈中出现，如果出现则跳过 if(visited[ss[i] - &#x27;a&#x27;])&#123; continue; &#125; // 如果栈不为空且不满足递增且栈顶元素会在后面出现 while(!stack.isEmpty() &amp;&amp; stack.peekLast() &gt; ss[i] &amp;&amp; lastIndex[stack.peekLast() - &#x27;a&#x27;] &gt; i)&#123; char ch = stack.pollLast(); // 栈中不存在该元素 visited[ch - &#x27;a&#x27;] = false; &#125; stack.offerLast(ss[i]); // 栈中存在该元素 visited[ss[i] - &#x27;a&#x27;] = true; &#125; StringBuilder sb = new StringBuilder(); for(char c : stack)&#123; sb.append(c); &#125; return sb.toString(); &#125;&#125; 拼接最大数拼接最大数 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990class Solution &#123; public int[] maxNumber(int[] nums1, int[] nums2, int k) &#123; int len1 = nums1.length; int len2 = nums2.length; int[] preMax = new int[k]; for(int i = 0; i &lt;= k; i++)&#123; if(i &gt; len1 || k - i &gt; len2)&#123; continue; &#125; // 用单调栈求出最优子序列 int[] max1 = minStack(nums1,i); // 用单调栈求出最优子序列 int[] max2 = minStack(nums2,k-i); // 合并两个数组 int[] tmp = mergeArray(max1,max2); // 更新最优解 if(compare(tmp,0,preMax,0) &gt; 0)&#123; System.arraycopy(tmp, 0, preMax, 0, k); &#125; &#125; return preMax; &#125; // 用单调栈求出最优子序列（核心部分） public static int[] minStack(int[] nums, int k) &#123; int n = nums.length; int[] res = new int[k]; Deque&lt;Integer&gt; stack = new ArrayDeque(); int index = 0; while (index &lt; n) &#123; // 如果栈为空或者栈顶元素大于当前元素 while (index &lt; n &amp;&amp; (stack.isEmpty() || stack.peekLast() &gt;= nums[index])) &#123; // 将当前元素入栈 stack.offerLast(nums[index++]); &#125; // 如果元素已经使用完，跳出循环 if (index == n) &#123; break; &#125; // 当栈不为空且栈顶元素小于当前元素并且栈内元素加上未使用的元素大于等于k while (!stack.isEmpty() &amp;&amp; stack.peekLast() &lt; nums[index] &amp;&amp; stack.size() + n - index - 1 &gt;= k) &#123; // 弹出栈顶元素 stack.pollLast(); &#125; // 将当前元素加入栈 stack.offerLast(nums[index++]); &#125; // 获得结果集 for (int i = 0; i &lt; k; i++) &#123; res[i] = stack.pollFirst(); &#125; return res; &#125; // 合并两个数组 public int[] mergeArray(int[] subsequence1, int[] subsequence2) &#123; int x = subsequence1.length, y = subsequence2.length; if (x == 0) &#123; return subsequence2; &#125; if (y == 0) &#123; return subsequence1; &#125; int mergeLength = x + y; int[] merged = new int[mergeLength]; int index1 = 0, index2 = 0; for (int i = 0; i &lt; mergeLength; i++) &#123; if (compare(subsequence1, index1, subsequence2, index2) &gt; 0) &#123; merged[i] = subsequence1[index1++]; &#125; else &#123; merged[i] = subsequence2[index2++]; &#125; &#125; return merged; &#125; // 比较方法 public int compare(int[] subsequence1, int index1, int[] subsequence2, int index2) &#123; int x = subsequence1.length, y = subsequence2.length; while (index1 &lt; x &amp;&amp; index2 &lt; y) &#123; int difference = subsequence1[index1] - subsequence2[index2]; if (difference != 0) &#123; return difference; &#125; index1++; index2++; &#125; return (x - index1) - (y - index2); &#125;&#125; 滑动窗口最大值(单调队列)题目链接 代码如下： 123456789101112131415161718192021222324class Solution &#123; public int[] maxSlidingWindow(int[] nums, int k) &#123; Deque&lt;Integer&gt; queue = new ArrayDeque(); int[] res = new int[nums.length - k + 1]; int index = 0; for(int i = 0; i &lt; nums.length; i++)&#123; // 保证队列单调递减 while(!queue.isEmpty() &amp;&amp; queue.peekLast() &lt; nums[i])&#123; queue.pollLast(); &#125; // 满足单调递减，加入元素 queue.addLast(nums[i]); if(i &gt;= k - 1)&#123; // 获得最大值，即队首元素 res[index++] = queue.peekFirst(); if(i &gt;= k-1 &amp;&amp; nums[i - k + 1] == queue.peekFirst())&#123; // 移除窗口最左边的值 queue.removeFirst(); &#125; &#125; &#125; return res; &#125;&#125; 栈设计getMin功能的栈题目链接 代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import java.util.*;public class Solution &#123; public Deque&lt;Integer&gt; s = new LinkedList(); public Deque&lt;Integer&gt; min_s = new LinkedList(); /** * return a array which include all ans for op3 * @param op int整型二维数组 operator * @return int整型一维数组 */ public int[] getMinStack (int[][] op) &#123; List&lt;Integer&gt; res = new ArrayList(); for(int i = 0; i &lt; op.length; i++)&#123; if(op[i][0] == 1)&#123; Push(op[i][1]); &#125;else if(op[i][0] == 2)&#123; Pop(); &#125;else&#123; res.add(getMin()); &#125; &#125; int[] ans = new int[res.size()]; for(int i = 0; i &lt; ans.length; i++)&#123; ans[i] = res.get(i); &#125; return ans; &#125; // 如果最小栈为空，或者栈顶元素大于x，则加入最小栈 public void Push(int x)&#123; s.push(x); if(min_s.isEmpty() || min_s.peek() &gt; x)&#123; min_s.push(x); &#125; &#125; // 如果最小栈栈顶元素和栈s中要出栈的元素相等，那么也需要出栈 public void Pop()&#123; if(!s.isEmpty())&#123; if(s.peek().equals(min_s.peek()))&#123; min_s.pop(); &#125; s.pop(); &#125; &#125; // 获得最小栈栈顶元素 public int getMin()&#123; return min_s.peek(); &#125;&#125; 用两个栈实现队列题目链接 12345678910111213141516171819202122232425262728class CQueue &#123; Deque&lt;Integer&gt; stack1; Deque&lt;Integer&gt; stack2; public CQueue() &#123; stack1 = new LinkedList&lt;Integer&gt;(); stack2 = new LinkedList&lt;Integer&gt;(); &#125; public void appendTail(int value) &#123; stack1.push(value); &#125; public int deleteHead() &#123; // 如果第二个栈为空 if (stack2.isEmpty()) &#123; while (!stack1.isEmpty()) &#123; stack2.push(stack1.pop()); &#125; &#125; if (stack2.isEmpty()) &#123; return -1; &#125; else &#123; int deleteItem = stack2.pop(); return deleteItem; &#125; &#125;&#125; 回溯字符串的排列题目链接 代码入下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import java.util.*;public class Solution &#123; public ArrayList&lt;String&gt; Permutation(String str) &#123; int len = str.length(); char[] strs = str.toCharArray(); // 对字符数组排序 Arrays.sort(strs); ArrayList&lt;String&gt; res = new ArrayList(); // 存放路径 Deque&lt;Character&gt; path = new ArrayDeque(); // 保存该字符是否用过 boolean[] used = new boolean[len]; // 深度有限遍历求得所有结果集 dfs(strs,len,0,used,res,path); return res; &#125; public void dfs(char[] strs,int len,int depth,boolean[] used,ArrayList&lt;String&gt; res,Deque&lt;Character&gt; path)&#123; // 如果到达最深的一层 if(len == depth)&#123; // 封装结果 StringBuilder sb = new StringBuilder(); for(char ch : path)&#123; sb.append(ch); &#125; res.add(new String(sb)); return; &#125; for(int i = 0; i &lt; len; i++)&#123; // 判断当前字符是否用过 if(used[i])&#123; continue; &#125; // 因为有重复元素，所以在下一层碰到相同元素将会使结果重复，相对于全排列，进一步剪枝 if(i &gt; 0 &amp;&amp; strs[i] == strs[i-1] &amp;&amp; !used[i-1])&#123; continue; &#125; // 回溯算法经典步骤 // 先将当前字符加入栈，并将使用过的元素标记为true path.addLast(strs[i]); used[i] = true; dfs(strs,len,depth + 1,used,res,path); // 回到之前的状态 path.removeLast(); used[i] = false; &#125; &#125;&#125; 全排列 II题目链接 代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; permuteUnique(int[] nums) &#123; // 数组长度 int len = nums.length; Arrays.sort(nums); // 结果集 List&lt;List&lt;Integer&gt;&gt; res = new ArrayList(); // 双端队列，保存临时路径 Deque&lt;Integer&gt; path = new ArrayDeque(); // 布尔数组，保存改数字是否使用过 boolean[] used = new boolean[len]; // 深度优先遍历求所有结果集 dfs(nums,len,0,used,path,res); return res; &#125; public void dfs(int[] nums,int len,int depth,boolean[] used,Deque&lt;Integer&gt; path,List&lt;List&lt;Integer&gt;&gt; res)&#123; // 如果到达最深的一层 if(depth == len)&#123; // 将当前路径加入结果集 res.add(new ArrayList(path)); return; &#125; for(int i = 0 ; i &lt; len; i++)&#123; // 判断当前数字是否用过 if(used[i])&#123; continue; &#125; // 因为有重复元素，所以在下一层碰到相同元素将会使结果重复，相对于全排列，进一步剪枝 if (i &gt; 0 &amp;&amp; nums[i] == nums[i - 1] &amp;&amp; !used[i - 1]) &#123; continue; &#125; // 回溯算法经典步骤 // 先将当前数字加入栈，并将使用过的元素标记为true path.addLast(nums[i]); used[i] = true; dfs(nums,len,depth + 1,used,path,res); // 回到之前的状态 path.removeLast(); used[i] = false; &#125; &#125;&#125; 括号生成题目链接 代码如下： 1234567891011121314151617181920212223242526public class Solution &#123; /** * * @param n int整型 * @return string字符串ArrayList */ public ArrayList&lt;String&gt; generateParenthesis (int n) &#123; ArrayList&lt;String&gt; res = new ArrayList(); backtrack(&quot;&quot;,0,0,n,res); return res; &#125; public void backtrack(String s,int open,int close,int n, List&lt;String&gt; res)&#123; // 如果长度够了，加入结果集 if(s.length() == n &lt;&lt; 1)&#123; res.add(s); return; &#125; if(open &lt; n)&#123; backtrack(s + &quot;(&quot;,open + 1,close,n,res); &#125; if(close &lt; open)&#123; backtrack(s + &quot;)&quot;,open,close + 1, n ,res); &#125; &#125;&#125; DFS剑指 Offer 12. 矩阵中的路径题目链接 代码如下： 1234567891011121314151617181920212223242526class Solution &#123; public boolean exist(char[][] board, String word) &#123; char[] words = word.toCharArray(); for(int i = 0; i &lt; board.length; i++)&#123; for(int j = 0; j &lt; board[i].length; j++)&#123; if(dfs(board,words,i,j,0))&#123; return true; &#125; &#125; &#125; return false; &#125; public boolean dfs(char[][] board,char[] words,int i, int j, int k)&#123; if(i &gt;= board.length || i &lt; 0 || j &gt;= board[0].length || j &lt; 0 || board[i][j] != words[k])&#123; return false; &#125; if( k == words.length - 1)&#123; return true; &#125; board[i][j] = &#x27;\\0&#x27;; boolean res = dfs(board,words,i+1,j,k+1) || dfs(board,words,i-1,j,k+1) || dfs(board,words,i,j+1,k+1) ||dfs(board,words,i,j-1,k+1); board[i][j] = words[k]; return res; &#125;&#125; 合并区间题目链接 代码入下： 123456789101112131415161718192021222324252627282930313233343536/** * Definition for an interval. * public class Interval &#123; * int start; * int end; * Interval() &#123; start = 0; end = 0; &#125; * Interval(int s, int e) &#123; start = s; end = e; &#125; * &#125; */import java.util.*;public class Solution &#123; public ArrayList&lt;Interval&gt; merge(ArrayList&lt;Interval&gt; intervals) &#123; ArrayList&lt;Interval&gt; res = new ArrayList(); // 根据节点左边从小到大排序 Collections.sort(intervals,(a,b)-&gt;&#123; return a.start - b.start; &#125;); int i = 0; int n = intervals.size(); while(i &lt; n)&#123; // 获得当前节点的左右端点 int left = intervals.get(i).start; int right = intervals.get(i).end; // 如果下一节点的左端点小于等于当前节点右端 while(i &lt; n - 1 &amp;&amp; intervals.get(i+1).start &lt;= right)&#123; // 更新右端点的值 right = Math.max(right,intervals.get(i+1).end); i++; &#125; // 将当前区间加入结果集 res.add(new Interval(left,right)); i++; &#125; return res; &#125;&#125;","categories":[{"name":"题解","slug":"题解","permalink":"https://xmmarlowe.github.io/categories/%E9%A2%98%E8%A7%A3/"}],"tags":[{"name":"数组","slug":"数组","permalink":"https://xmmarlowe.github.io/tags/%E6%95%B0%E7%BB%84/"},{"name":"二叉树","slug":"二叉树","permalink":"https://xmmarlowe.github.io/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"name":"链表","slug":"链表","permalink":"https://xmmarlowe.github.io/tags/%E9%93%BE%E8%A1%A8/"},{"name":"dp","slug":"dp","permalink":"https://xmmarlowe.github.io/tags/dp/"}],"author":"Marlowe"},{"title":"Leetcode-剑指offer 40.最小的K个数","slug":"题解/Leetcode-剑指offer-40-最小的K个数","date":"2021-03-27T11:51:16.000Z","updated":"2021-03-31T14:38:17.442Z","comments":true,"path":"2021/03/27/题解/Leetcode-剑指offer-40-最小的K个数/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/27/%E9%A2%98%E8%A7%A3/Leetcode-%E5%89%91%E6%8C%87offer-40-%E6%9C%80%E5%B0%8F%E7%9A%84K%E4%B8%AA%E6%95%B0/","excerpt":"","text":"输入整数数组 arr ，找出其中最小的 k 个数。例如，输入4、5、1、6、2、7、3、8这8个数字，则最小的4个数字是1、2、3、4。 示例 1： 输入：arr = [3,2,1], k = 2输出：[1,2] 或者 [2,1]示例 2： 输入：arr = [0,1,2,1], k = 1输出：[0] 限制： 0 &lt;= k &lt;= arr.length &lt;= 100000 &lt;= arr[i] &lt;= 10000 思路本题是求前 K 小，因此用一个容量为 K 的大根堆，每次 poll 出最大的数，那堆中保留的就是前 K 小。 若目前堆的大小小于K，将当前数字放入堆中。 否则判断当前数字与大根堆堆顶元素的大小关系，如果当前数字比大根堆堆顶还大，这个数就直接跳过；反之如果当前数字比大根堆堆顶小，先poll掉堆顶，再将该数字放入堆中。 代码12345678910111213141516171819202122232425class Solution &#123; public int[] getLeastNumbers(int[] arr, int k) &#123; if (k == 0 || arr.length == 0) &#123; return new int[0]; &#125; // 默认是小根堆，实现大根堆需要重写一下比较器。 Queue&lt;Integer&gt; pq = new PriorityQueue&lt;&gt;((v1, v2) -&gt; v2 - v1); for (int num: arr) &#123; if (pq.size() &lt; k) &#123; pq.offer(num); &#125; else if (num &lt; pq.peek()) &#123; pq.poll(); pq.offer(num); &#125; &#125; // 返回堆中的元素 int[] res = new int[pq.size()]; int idx = 0; for(int num: pq) &#123; res[idx++] = num; &#125; return res; &#125;&#125; 参考4种解法秒杀TopK（快排/堆/二叉搜索树/计数排序）❤️","categories":[{"name":"题解","slug":"题解","permalink":"https://xmmarlowe.github.io/categories/%E9%A2%98%E8%A7%A3/"}],"tags":[{"name":"TopK","slug":"TopK","permalink":"https://xmmarlowe.github.io/tags/TopK/"}],"author":"Marlowe"},{"title":"富途笔试-找到搜索二叉树中两个错误的节点","slug":"题解/富途笔试-找到搜索二叉树中两个错误的节点","date":"2021-03-27T08:48:17.000Z","updated":"2021-03-31T14:38:17.445Z","comments":true,"path":"2021/03/27/题解/富途笔试-找到搜索二叉树中两个错误的节点/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/27/%E9%A2%98%E8%A7%A3/%E5%AF%8C%E9%80%94%E7%AC%94%E8%AF%95-%E6%89%BE%E5%88%B0%E6%90%9C%E7%B4%A2%E4%BA%8C%E5%8F%89%E6%A0%91%E4%B8%AD%E4%B8%A4%E4%B8%AA%E9%94%99%E8%AF%AF%E7%9A%84%E8%8A%82%E7%82%B9/","excerpt":"","text":"题目描述一棵二叉树原本是搜索二叉树，但是其中有两个节点调换了位置，使得这棵二叉树不再是搜索二叉树，请按升序输出这两个错误节点的值。(每个节点的值各不相同)示例1输入{1,2,3}返回值[1,2] 思路中序遍历可以得到搜索二叉树的升序遍历结果，题目描述其中两个节点交换了位置，因此只需在中序遍历中找到异常数据即可。 中序遍历二叉树 从前面往后找，发现当前数比后一个数大，则是异常数据，放在结果集下标为1的位置 从后面往前找，发现当前数比前一个数小，则是异常数据，放在结果集下标为0的位置 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.util.*;/* * public class TreeNode &#123; * int val = 0; * TreeNode left = null; * TreeNode right = null; * &#125; */public class Solution &#123; /** * * @param root TreeNode类 the root * @return int整型一维数组 */ List&lt;Integer&gt; res = new ArrayList(); public int[] findError (TreeNode root) &#123; // write code here int[] r = new int[2]; dfs(root); for(int i = 0; i &lt; res.size(); i++)&#123; if(res.get(i) &gt; res.get(i+1))&#123; r[1] = res.get(i); break; &#125; &#125; for(int j = res.size() - 1; j &gt;= 0; j--)&#123; if(res.get(j) &lt; res.get(j-1))&#123; r[0] = res.get(j); break; &#125; &#125; return r; &#125; public void dfs(TreeNode root)&#123; if(root != null)&#123; dfs(root.left); res.add(root.val); dfs(root.right); &#125; &#125;&#125;","categories":[{"name":"题解","slug":"题解","permalink":"https://xmmarlowe.github.io/categories/%E9%A2%98%E8%A7%A3/"}],"tags":[{"name":"二叉树","slug":"二叉树","permalink":"https://xmmarlowe.github.io/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"author":"Marlowe"},{"title":"初识CAS与ABA问题","slug":"并发/初识CAS与ABA问题","date":"2021-03-25T12:34:19.000Z","updated":"2021-05-14T06:26:09.857Z","comments":true,"path":"2021/03/25/并发/初识CAS与ABA问题/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/25/%E5%B9%B6%E5%8F%91/%E5%88%9D%E8%AF%86CAS%E4%B8%8EABA%E9%97%AE%E9%A2%98/","excerpt":"","text":"什么是CAS？CAS是英文单词CompareAndSwap的缩写，中文意思是：比较并替换。CAS需要有3个操作数：内存地址V，旧的预期值A，即将要更新的目标值B。 CAS指令执行时，当且仅当内存地址V的值与预期值A相等时，将内存地址V的值修改为B，否则就什么都不做。整个比较并替换的操作是一个原子操作。它体现的一种乐观锁的思想，比如多个线程要对一个共享的整型变量执行 +1 操作 获取共享变量时，为了保证该变量的可见性，需要使用 volatile 修饰。结合 CAS 和 volatile 可以实现无锁并发，适用于竞争不激烈、多核 CPU 的场景下。 因为没有使用 synchronized，所以线程不会陷入阻塞，这是效率提升的因素之一 但如果竞争激烈，可以想到重试必然频繁发生，反而效率会受影响 CAS 底层依赖于一个 Unsafe 类来直接调用操作系统底层的 CAS 指令 伪代码： 123456789101112// 需要不断尝试while(true) &#123; int 旧值 = 共享变量;//比如拿到了当前值 0 int 结果 = 旧值 + 1;//在旧值 0 的基础上增加 1 ，正确结果是 1 //这时候如果别的线程把共享变量改成了 5，本线程的正确结果 1 就作 //废了，这时候 compareAndSwap 返回 false，重新尝试，直到： compareAndSwap 返回 //true，表示我本线程做修改的同时，别的线程没有干扰 if( compareAndSwap ( 旧值, 结果 )) &#123; // 成功，退出循环 &#125;&#125; 代码示例： 1234567891011121314public class CASDemo &#123; public static void main(String[] args) &#123; AtomicInteger atomicInteger = new AtomicInteger(2020); // public final boolean compareAndSet(int expect, int update) // 如果我的期望值达到了，就更新，否则不更新 CAS是CPU的并发原语！ System.out.println(atomicInteger.compareAndSet(2020, 2021)); System.out.println(atomicInteger.get()); System.out.println(atomicInteger.compareAndSet(2020, 2021)); System.out.println(atomicInteger.get()); &#125;&#125; 结果： 1234true2021false2021 CAS 缺点CAS虽然很高效的解决了原子操作问题，但是CAS仍然存在三大问题。 ABA问题在多线程场景下CAS会出现ABA问题，关于ABA问题这里简单科普下，例如有2个线程同时对同一个值(初始值为A)进行CAS操作，这三个线程如下 线程1，期望值为A，欲更新的值为B 线程2，期望值为A，欲更新的值为B 线程1抢先获得CPU时间片，而线程2因为其他原因阻塞了，线程1取值与期望的A值比较，发现相等然后将值更新为B，然后这个时候出现了线程3，期望值为B，欲更新的值为A，线程3取值与期望的值B比较，发现相等则将值更新为A，此时线程2从阻塞中恢复，并且获得了CPU时间片，这时候线程2取值与期望的值A比较，发现相等则将值更新为B，虽然线程2也完成了操作，但是线程2并不知道值已经经过了A-&gt;B-&gt;A的变化过程。 ABA问题带来的危害例1：小明在提款机，提取了50元，因为提款机问题，有两个线程，同时把余额从100变为50线程1（提款机）：获取当前值100，期望更新为50，线程2（提款机）：获取当前值100，期望更新为50，线程1成功执行，线程2某种原因block了，这时，某人给小明汇款50线程3（默认）：获取当前值50，期望更新为100，这时候线程3成功执行，余额变为100，线程2从Block中恢复，获取到的也是100，compare之后，继续更新余额为50！！！此时可以看到，实际余额应该为100（100-50+50），但是实际上变为了50（100-50+50-50）这就是ABA问题带来的成功提交。 例2： 假如你的老板很有洁癖，现在桌子上有一杯水，在你非常渴的情况下发现这个盛满水的杯子，你一饮而尽。之后再给杯子里重新倒满水。然后你离开，当老板回来时看到杯子还是盛满水，他当然不知道是否被人喝完重新倒满。 解决方法： 在变量前面加上版本号，每次变量更新的时候变量的版本号都+1，即A-&gt;B-&gt;A就变成了1A-&gt;2B-&gt;3A。 循环会耗时如果CAS操作失败，就需要循环进行CAS操作(循环同时将期望值更新为最新的)，如果长时间都不成功的话，那么会造成CPU极大的开销。解决办法： 限制自旋次数，防止进入死循环。 只能保证一个共享变量的原子操作CAS的原子操作只能针对一个共享变量。 解决方法： 如果需要对多个共享变量进行操作，可以使用加锁方式(悲观锁)保证原子性，或者可以把多个共享变量合并成一个共享变量进行CAS操作。 CAS:ABA问题什么是ABA问题？ABA问题通俗一点的说，就是一个从内存里面读取到了值A，正在改的时候也检查到了还是A，但是真实的值是被改成了B再改回了A的。 怎么解决ABA问题?解决ABA问题就是给操作数加上一个“版本号”，就像Mysql的乐观锁一样。而Java中提供了AtomicStampedReference类来实现这个功能。 AtomicStampedReference类可以给一个引用标记上一个标记位，来保证原子性。AtomicStampedReference可以给一个引用标记上一个整型的版本戳，来保证原子性。 代码测试： 123456789101112131415161718192021public class CASTest &#123; public static String A = &quot;A&quot;; public static String B = &quot;B&quot;; public static String C = &quot;C&quot;; public static AtomicStampedReference&lt;String&gt; atomic = new AtomicStampedReference&lt;&gt;(A, 0); public static void main(String[] args) &#123; //线程1来了，先检查是否和当前值一样,我准备把A改成C了,并且拿到线程1比较时候的stamp boolean same = atomic.attemptStamp(A, 1); int stamp = atomic.getStamp(); //线程2来了，我准备把A换成B了 atomic.compareAndSet(A, B, atomic.getStamp(), atomic.getStamp() + 1); //线程3来了，我准备把B换回A了 atomic.compareAndSet(A, B, atomic.getStamp(), atomic.getStamp() + 1); //到线程1来修改了A成C了 if (same) &#123; boolean b = atomic.compareAndSet(A, C, stamp, stamp + 1); System.out.println(b?&quot;修改成功&quot;:&quot;修改失败ABA了&quot;); &#125; &#125;&#125; CAS的应用我们知道CAS操作并不会锁住共享变量，也就是一种非阻塞的同步机制，CAS就是乐观锁的实现。 乐观锁 乐观锁总是假设最好的情况，每次去操作数据都认为不会被别的线程修改数据，所以在每次操作数据的时候都不会给数据加锁， 即在线程对数据进行操作的时候，别的线程不会阻塞仍然可以对数据进行操作，只有在需要更新数据的时候才会去判断数据是否被别的线程修改过，如果数据被修改过则会拒绝操作并且返回错误信息给用户。2. 悲观锁悲观锁总是假设最坏的情况，每次去操作数据时候都认为会被的线程修改数据，所以在每次操作数据的时候都会给数据加锁， 让别的线程无法操作这个数据，别的线程会一直阻塞直到获取到这个数据的锁。这样的话就会影响效率，比如当有个线程发生一个很耗时的操作的时候，别的线程只是想获取这个数据的值而已都要等待很久。 Java利用CAS的乐观锁、原子性的特性高效解决了多线程的安全性问题，例如JDK1.8中的集合类ConcurrentHashMap、关键字volatile、ReentrantLock等。 参考认识CAS与ABA问题 CAS原理分析及ABA问题详解","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"CAS","slug":"CAS","permalink":"https://xmmarlowe.github.io/tags/CAS/"},{"name":"ABA","slug":"ABA","permalink":"https://xmmarlowe.github.io/tags/ABA/"}],"author":"Marlowe"},{"title":"异步回调","slug":"并发/异步回调","date":"2021-03-25T08:15:43.000Z","updated":"2021-05-14T06:52:14.343Z","comments":true,"path":"2021/03/25/并发/异步回调/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/25/%E5%B9%B6%E5%8F%91/%E5%BC%82%E6%AD%A5%E5%9B%9E%E8%B0%83/","excerpt":"To be continue…","text":"To be continue… Future 设计初衷：对将来的某个事件的结果进行建模","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"异步","slug":"异步","permalink":"https://xmmarlowe.github.io/tags/%E5%BC%82%E6%AD%A5/"}],"author":"Marlowe"},{"title":"ForkJoin","slug":"并发/ForkJoin","date":"2021-03-25T05:37:54.000Z","updated":"2021-04-19T12:10:57.701Z","comments":true,"path":"2021/03/25/并发/ForkJoin/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/25/%E5%B9%B6%E5%8F%91/ForkJoin/","excerpt":"ForkJoin 在JDK1.7， 并行执行任务！ 在大数据量下提高效率。","text":"ForkJoin 在JDK1.7， 并行执行任务！ 在大数据量下提高效率。 ForkJoin特点：工作窃取 里面维护的是双端队列。 代码示例：ForkJoinDemo.java: 12345678910111213141516171819202122232425262728293031323334public class ForkJoinDemo extends RecursiveTask&lt;Long&gt; &#123; private Long start; private Long end; private Long temp = 10000L; public ForkJoinDemo(Long start, Long end) &#123; this.start = start; this.end = end; &#125; /** * The main computation performed by this task. * * @return the result of the computation */ @Override protected Long compute() &#123; if ((end - start) &lt; temp) &#123; Long sum = 0L; for (Long i = start; i &lt;= end; i++) &#123; sum += i; &#125; return sum; &#125; else &#123; long mid = (start + end) / 2; ForkJoinDemo task1 = new ForkJoinDemo(start, mid); task1.fork(); ForkJoinDemo task2 = new ForkJoinDemo(mid + 1, end); task2.fork(); return task1.join() + task2.join(); &#125; &#125;&#125; 12345678910public static void test1() throws ExecutionException, InterruptedException &#123; long start = System.currentTimeMillis(); ForkJoinPool forkJoinPool = new ForkJoinPool(); ForkJoinTask&lt;Long&gt; task = new ForkJoinDemo(0L, 10_0000_0000L); ForkJoinTask&lt;Long&gt; submit = forkJoinPool.submit(task); Long sum = submit.get(); long end = System.currentTimeMillis(); System.out.println(&quot;sum = &quot; + sum); System.out.println(&quot;耗时：&quot; + (end - start));&#125; 结果： 12sum = 500000000500000000耗时：4950 并行流 1234567public static void test2() &#123; long start = System.currentTimeMillis(); long sum = LongStream.rangeClosed(0L, 10_0000_0000L).parallel().reduce(0, Long::sum); long end = System.currentTimeMillis(); System.out.println(&quot;sum = &quot; + sum); System.out.println(&quot;耗时：&quot; + (end - start));&#125; 结果： 12sum = 500000000500000000耗时：271","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"ForkJoin","slug":"ForkJoin","permalink":"https://xmmarlowe.github.io/tags/ForkJoin/"}],"author":"Marlowe"},{"title":"Redis 相关知识点总结","slug":"NoSQL/Redis-相关知识点总结","date":"2021-03-22T17:01:53.000Z","updated":"2021-04-15T02:30:49.601Z","comments":true,"path":"2021/03/23/NoSQL/Redis-相关知识点总结/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/23/NoSQL/Redis-%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/","excerpt":"总结一些Redis常见知识点…","text":"总结一些Redis常见知识点…","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"}],"author":"Marlowe"},{"title":"B树与B+树","slug":"数据库/B树与B+树","date":"2021-03-22T16:32:18.000Z","updated":"2021-08-25T10:24:35.569Z","comments":true,"path":"2021/03/23/数据库/B树与B+树/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/23/%E6%95%B0%E6%8D%AE%E5%BA%93/B%E6%A0%91%E4%B8%8EB+%E6%A0%91/","excerpt":"","text":"B树简介 一种二叉搜索树。 除根节点外的所有非叶节点至少含有（M/2（向上取整）-1）个关键字，每个节点最多有M-1个关键字，并且以升序排列。所以M阶B树的除根节点外的所有非叶节点的关键字取值区间为[M/2-1(向上取整),M-1]。 每个节点最多有M-1个关键字。 动图演示 B+树简介 有n棵子树的非叶子结点中含有n个关键字（b树是n-1个），这些关键字不保存数据，只用来索引，所有数据都保存在叶子节点（b树是每个关键字都保存数据）。 所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接（叶子节点组成一个链表）。 所有的非叶子结点可以看成是索引部分，结点中仅含其子树中的最大（或最小）关键字。 通常在b+树上有两个头指针，一个指向根结点，一个指向关键字最小的叶子结点。 同一个数字会在不同节点中重复出现，根节点的最大元素就是b+树的最大元素。 B树与B+树的区别 B树每个节点都存储数据，所有节点组成这棵树。B+树只有叶子节点存储数据（B+数中有两个头指针：一个指向根节点，另一个指向关键字最小的叶节点），叶子节点包含了这棵树的所有数据，所有的叶子结点使用链表相连，便于区间查找和遍历，所有非叶节点起到索引作用。 B树中叶节点包含的关键字和其他节点包含的关键字是不重复的，B+树的索引项只包含对应子树的最大关键字和指向该子树的指针，不含有该关键字对应记录的存储地址。 B树中每个节点（非根节点）关键字个数的范围为m/2(向上取整)-1,m-1，并且具有n个关键字的节点包含（n+1）棵子树。B+树中每个节点（非根节点）关键字个数的范围为m/2(向上取整),m，具有n个关键字的节点包含（n）棵子树。 B+树中查找，无论查找是否成功，每次都是一条从根节点到叶节点的路径。 B树的优点B树的每一个节点都包含key和value，因此经常访问的元素可能离根节点更近，因此访问也更迅速。 B+树的优点 所有的叶子结点使用链表相连，便于区间查找和遍历。B树则需要进行每一层的递归遍历。相邻的元素可能在内存中不相邻，所以缓存命中性没有B+树好。 b+树的中间节点不保存数据，能容纳更多节点元素。B树和B+树的共同优点考虑磁盘IO的影响，它相对于内存来说是很慢的。数据库索引是存储在磁盘上的，当数据量大时，就不能把整个索引全部加载到内存了，只能逐一加载每一个磁盘页（对应索引树的节点）。所以我们要减少IO次数，对于树来说，IO次数就是树的高度，而“矮胖”就是b树的特征之一，m的大小取决于磁盘页的大小。 B+树比B树好在哪里 B+树的磁盘读写代价更低B+的内部结点并没有指向关键字具体信息的指针。因此其内部结点相对B树更小。如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多。相对来说IO读写次数也就降低了。 B+树的数据信息遍历更加方便 B+树只要遍历叶子节点就可以实现整棵树的遍历，而B树不支持这样的操作（或者说效率太低），而且 在数据库中基于范围的查询是非常频繁的，所以数据库索引基本采用B+树。 B+树的查询效率更加稳定由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。 为什么 MySQL 的索引要使用 B+ 树而不是其它树形结构？因为 B 树不管叶子节点还是非叶子节点，都会保存数据， 这样导致在非叶子节点中能保存的指针数量变少（有些资料也称为扇出），指针少的情况下要保存大量数据，只能增加树的高度，导致 IO 操作变多，查询性能变低。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"B+Tree","slug":"B-Tree","permalink":"https://xmmarlowe.github.io/tags/B-Tree/"},{"name":"BTree","slug":"BTree","permalink":"https://xmmarlowe.github.io/tags/BTree/"}],"author":"Marlowe"},{"title":"进程和线程相关知识点","slug":"操作系统/进程和线程相关知识点","date":"2021-03-22T13:42:24.000Z","updated":"2021-04-22T07:48:30.836Z","comments":true,"path":"2021/03/22/操作系统/进程和线程相关知识点/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/22/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E7%82%B9/","excerpt":"","text":"何为进程？进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。 在 Java 中，当我们启动 main 函数时其实就是启动了一个 JVM 的进程，而 main 函数所在的线程就是这个进程中的一个线程，也称主线程。 何为线程？线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享进程的堆和方法区资源，但每个线程有自己的程序计数器、虚拟机栈和本地方法栈，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。 何为协程？协程是计算机程序的一类组件，推广了协作式多任务的子程序，允许执行被挂起与被恢复。相对子例程而言，协程更为一般和灵活，但在实践中使用没有子例程那样广泛。协程更适合于用来实现彼此熟悉的程序组件，如协作式多任务、异常处理、事件循环、迭代器、无限列表和管道。 线程和进程的区别是什么？总结: 线程是进程划分成的更小的运行单位,一个进程在其执行的过程中可以产生多个线程。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。线程执行开销小，但不利于资源的管理和保护;而进程正相反。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"线程","slug":"线程","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B/"},{"name":"进程","slug":"进程","permalink":"https://xmmarlowe.github.io/tags/%E8%BF%9B%E7%A8%8B/"}],"author":"Marlowe"},{"title":"线程之间同步的机制","slug":"操作系统/线程之间同步的机制","date":"2021-03-22T11:41:19.000Z","updated":"2021-04-08T06:45:36.950Z","comments":true,"path":"2021/03/22/操作系统/线程之间同步的机制/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/22/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E7%BA%BF%E7%A8%8B%E4%B9%8B%E9%97%B4%E5%90%8C%E6%AD%A5%E7%9A%84%E6%9C%BA%E5%88%B6/","excerpt":"待完善…","text":"待完善… 临界区：不可以跨进程，忘记解锁会无限等待，要么存在要么没有，多线程访问独占性共享资源 互斥量：可以跨进程，忘记解锁会自动释放，要么存在要么没有 事件：又叫线程触发器，不可以跨进程，要么存在要么没有，一个线程来唤醒另一个线程（包括自动和人工两种方式） 信号量：可以跨进程，始终代表可用资源数量，当资源数为0时，线程阻塞，允许多个线程同时访问一个共享资源","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"线程","slug":"线程","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B/"}],"author":"Marlowe"},{"title":"常用排序算法Java实现","slug":"算法与数据结构/常用排序算法Java实现","date":"2021-03-20T07:14:27.000Z","updated":"2021-04-15T05:40:35.265Z","comments":true,"path":"2021/03/20/算法与数据结构/常用排序算法Java实现/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/20/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%B8%B8%E7%94%A8%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95Java%E5%AE%9E%E7%8E%B0/","excerpt":"排序算法可以分为内部排序和外部排序，内部排序是数据记录在内存中进行排序，而外部排序是因排序的数据很大，一次不能容纳全部的排序记录，在排序过程中需要访问外存。常见的内部排序算法有：插入排序、希尔排序、选择排序、冒泡排序、归并排序、快速排序、堆排序、基数排序等。","text":"排序算法可以分为内部排序和外部排序，内部排序是数据记录在内存中进行排序，而外部排序是因排序的数据很大，一次不能容纳全部的排序记录，在排序过程中需要访问外存。常见的内部排序算法有：插入排序、希尔排序、选择排序、冒泡排序、归并排序、快速排序、堆排序、基数排序等。 算法概览 排序算法1 冒泡排序算法思想从左到右不断交换相邻逆序的元素，在一轮的循环之后，可以让未排序的最大元素上浮到右侧。在一轮循环中，如果没有发生交换，那么说明数组已经是有序的，此时可以直接退出。动图演示最好情况当输入的数据已经是正序最差情况当输入的数据是反序代码实现 1234567891011121314151617181920/** * 冒泡排序 * * @param nums */public static void bubbleSort(int[] nums) &#123; int n = nums.length; boolean flag = false; for (int i = 0; i &lt; n - 1 &amp;&amp; !flag; i++) &#123; flag = true; for (int j = 0; j &lt; n - 1 - i; j++) &#123; // 如果全都排好，则flag = true,跳出循环 if (nums[j] &gt; nums[j + 1]) &#123; flag = false; swap(nums, j, j + 1); &#125; &#125; &#125; System.out.println(Arrays.toString(nums));&#125; 2 选择排序算法思想每一次从未排序的集合中选出最小的数，依次放在第1、2、3…位置处动图演示 最好情况当输入的数据已经是正序最差情况当输入的数据是反序 代码实现 1234567891011121314151617181920212223/** * 选择排序 * * @param nums */public static void selectSort(int[] nums) &#123; int n = nums.length; // 比较n - 1 轮 for (int i = 0; i &lt; n - 1; i++) &#123; int min = i; // 每一轮找到最小值的下标 for (int j = i + 1; j &lt; n; j++) &#123; if (nums[j] &lt; nums[min]) &#123; min = j; &#125; &#125; // 找到最小值与当前值交换 if (min != i) &#123; swap(nums, i, min); &#125; &#125; System.out.println(Arrays.toString(nums));&#125; 3 插入排序算法思想将第一待排序序列第一个元素看做一个有序序列，把第二个元素到最后一个元素当成是未排序序列。 从头到尾依次扫描未排序序列，将扫描到的每个元素插入有序序列的适当位置。（如果待插入的元素与有序序列中的某个元素相等，则将待插入元素插入到相等元素的后面。） 动图演示 最好情况如果序列是完全有序的，插入排序只要比较n次，无需移动，时间复杂度为O(N)最差情况如果序列是逆序的，插入排序要比较O（N2）和移动O(N2)代码实现 1234567891011121314/** * 插入排序 * @param nums */public static void insertSort(int[] nums) &#123; int n = nums.length; for (int i = 1; i &lt; n; i++) &#123; // 从后往前找，如果当前元素比最后一个元素都大，则当前轮次排序结束 for (int j = i; j &gt; 0 &amp;&amp; nums[j] &lt; nums[j - 1]; j--) &#123; swap(nums, j, j - 1); &#125; &#125; System.out.println(Arrays.toString(nums));&#125; 4 希尔排序算法思想希尔排序是将待排序的数组元素 按下标的一定增量分组 ，分成多个子序列，然后对各个子序列进行直接插入排序算法排序；然后依次缩减增量再进行排序，直到增量为1时，进行最后一次直接插入排序，排序结束。动图演示最好情况序列是正序排列，在这种情况下，需要进行的比较操作需（n-1）次。后移赋值操作为0次。即O(n)最差情况O(nlog2n)代码实现 123456789101112131415161718192021/** * 希尔排序 * * @param nums */public static void shellSort(int[] nums) &#123; int n = nums.length; // gap： 增量，每次减半 for (int gap = n / 2; gap &gt; 0; gap /= 2) &#123; // i:代表即将插入的元素角标，作为每一组比较数据的最后一个元素角标 for (int i = gap; i &lt; n; i++) &#123; // j:代表与i同一组的数组元素角标 for (int j = i - gap; j &gt;= 0; j -= gap) &#123; if (nums[j] &gt; nums[j + gap]) &#123; swap(nums, j, j + gap); &#125; &#125; &#125; &#125; System.out.println(Arrays.toString(nums));&#125; 5 归并排序算法思想归并排序（Merge sort）是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。算法步骤： 申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列； 设定两个指针，最初位置分别为两个已经排序序列的起始位置； 比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置； 重复步骤 3 直到某一指针达到序列尾； 将另一序列剩下的所有元素直接复制到合并序列尾。 动图演示最好情况O(nlogn)最差情况O(nlogn)代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 合并两个有序子数组 * @param nums * @param low * @param mid * @param high * @param tmp */public static void merge(int[] nums, int low, int mid, int high, int[] tmp) &#123; int i = 0; int j = low; int k = mid + 1; while (j &lt;= mid &amp;&amp; k &lt;= high) &#123; if (nums[j] &lt; nums[k]) &#123; tmp[i++] = nums[j++]; &#125; else &#123; tmp[i++] = nums[k++]; &#125; &#125; while (j &lt;= mid) &#123; tmp[i++] = nums[j++]; &#125; while (k &lt;= high) &#123; tmp[i++] = nums[k++]; &#125; for (int l = 0; l &lt; i; l++) &#123; nums[low + l] = tmp[l]; &#125;&#125;/** * 左右子数组分别递归分 * @param nums * @param low * @param high * @param tmp */public static void mergeSort(int[] nums, int low, int high, int[] tmp) &#123; if (low &lt; high) &#123; int mid = (low + high) &gt;&gt; 1; mergeSort(nums, low, mid, tmp); mergeSort(nums, mid + 1, high, tmp); merge(nums, low, mid, high, tmp); &#125;&#125; 6 快速排序算法思想快速排序通过一个切分元素将数组分为两个子数组，左子数组小于等于切分元素，右子数组大于等于切分元素，将这两个子数组排序也就将整个数组排序了。算法步骤： 从数列中挑出一个元素，称为 “基准”（pivot）; 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序； 动图演示最好情况最好情况，递归树的深度为log2n，其空间复杂度也就为O(logn)最差情况最坏情况，需要进行n‐1递归调用，其空间复杂度为O(n^2)代码实现 1234567891011121314151617181920212223242526272829303132333435363738/** * 快速排序 * * @param nums * @param low * @param high */public static void quickSort(int[] nums, int low, int high) &#123; int i, j, tmp; if (low &gt; high) &#123; return; &#125; i = low; j = high; //tmp就是基准位 tmp = nums[low]; while (i &lt; j) &#123; //先看右边，依次往左递减 while (tmp &lt;= nums[j] &amp;&amp; i &lt; j) &#123; j--; &#125; //再看左边，依次往右递增 while (tmp &gt;= nums[i] &amp;&amp; i &lt; j) &#123; i++; &#125; //如果满足条件则交换 if (i &lt; j) &#123; swap(nums, i, j); &#125; &#125; //最后将基准为与i和j相等位置的数字交换 nums[low] = nums[i]; nums[i] = tmp; //递归调用左半数组 quickSort(nums, low, j - 1); //递归调用右半数组 quickSort(nums, j + 1, high);&#125; 7 堆排序算法思想堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。堆排序可以说是一种利用堆的概念来排序的选择排序。分为两种方法： 大顶堆：每个节点的值都大于或等于其子节点的值，在堆排序算法中用于升序排列； 小顶堆：每个节点的值都小于或等于其子节点的值，在堆排序算法中用于降序排列； 堆排序的平均时间复杂度为 Ο(nlogn)。 动图演示最好情况O(nlogn)最差情况O(nlogn)代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class HeapSort &#123; public static void main(String[] args) &#123; int arr[] = &#123;88, 11, 22, 3, 5, 1, 19&#125;; sort(arr); System.out.println(Arrays.toString(arr)); &#125; public static void sort(int[] arr) &#123; int len = arr.length; buildHeap(arr, len); for (int i = len - 1; i &gt; 0; i--) &#123; //首尾交换 swap(arr, 0, i); //重新维护堆性质 heapify(arr, 0, --len); &#125; &#125; private static void buildHeap(int[] arr, int len) &#123; for (int i = 0; i &lt; len / 2; i++) &#123; heapify(arr, i, len); &#125; &#125; private static void heapify(int[] arr, int index, int len) &#123; int left = 2 * index + 1; int right = 2 * index + 2; int max = index; if (left &lt; len &amp;&amp; arr[left] &gt; arr[max]) &#123; max = left; &#125; if (right &lt; len &amp;&amp; arr[right] &gt; arr[max]) &#123; max = right; &#125; if (max != index) &#123; swap(arr, max, index); heapify(arr, max, len); &#125; &#125; /** * 交换 * * @param arr 数组 * @param self 自身 * @param other 另一个 */ private static void swap(int[] arr, int self, int other) &#123; int tmp = arr[self]; arr[self] = arr[other]; arr[other] = tmp; &#125;&#125; 8 计数排序算法思想计数排序的核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 算法的步骤如下： 找出待排序的数组中最大和最小的元素 统计数组中每个值为i的元素出现的次数，存入数组C的第i项 对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加） 反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1 动图演示 代码实现 1234567891011121314151617181920212223242526/** * 计数排序 * * @param nums */public static void countSort(int[] nums) &#123; int max = Integer.MIN_VALUE; // 找到最大值 for (int num : nums) &#123; if (num &gt; max) &#123; max = num; &#125; &#125; int[] bucket = new int[max + 1]; // 统计每个元素的个数 for (int num : nums) &#123; bucket[num]++; &#125; int index = 0; for (int i = 0; i &lt; bucket.length; i++) &#123; while (bucket[i] &gt; 0) &#123; nums[index++] = i; bucket[i]--; &#125; &#125;&#125; 9 桶排序算法思想桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。为了使桶排序更加高效，我们需要做到这两点： 在额外空间充足的情况下，尽量增大桶的数量 使用的映射函数能够将输入的 N 个数据均匀的分配到 K 个桶中 动图演示元素分布在桶中：然后，元素在每个桶中排序： 代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class BucketSort &#123; public static void main(String[] args) &#123; int arr[] = &#123;5, 11, 7, 9, 2, 3, 12, 8, 6, 1, 4, 10&#125;; sort(arr, 5); System.out.println(Arrays.toString(arr)); &#125; private static void sort(int[] arr, int bucketSize) &#123; if (arr.length == 0) &#123; return; &#125; int minValue = arr[0]; int maxValue = arr[0]; for (int value : arr) &#123; if (value &lt; minValue) &#123; minValue = value; &#125; else if (value &gt; maxValue) &#123; maxValue = value; &#125; &#125; int bucketCount = (maxValue - minValue) / bucketSize + 1; int[][] buckets = new int[bucketCount][0]; // 利用映射函数将数据分配到各个桶中 for (int item : arr) &#123; int index = (item - minValue) / bucketSize; buckets[index] = arrAppend(buckets[index], item); &#125; int arrIndex = 0; for (int[] bucket : buckets) &#123; if (bucket.length &lt;= 0) &#123; continue; &#125; // 对每个桶进行排序，这里使用了归并排序 MergeSort.sort(bucket); for (int value : bucket) &#123; arr[arrIndex++] = value; &#125; &#125; &#125; /** * 自动扩容，并保存数据 */ private static int[] arrAppend(int[] arr, int value) &#123; arr = Arrays.copyOf(arr, arr.length + 1); arr[arr.length - 1] = value; return arr; &#125;&#125; 10 基数排序算法思想基数排序是一种非比较型整数排序算法，其原理是将整数按位数切割成不同的数字，然后按每个位数分别比较。由于整数也可以表达字符串（比如名字或日期）和特定格式的浮点数，所以基数排序也不是只能使用于整数。 动图演示 代码实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/** * 基数排序 * 考虑负数的情况还可以参考： https://code.i-harness.com/zh-CN/q/e98fa9 */public class RadixSort &#123; public static void main(String[] args) &#123; int arr[] = &#123;5, 11, 7, 9, 2, 3, 12, 8, 6, 1, 4, 10&#125;; sort(arr); System.out.println(Arrays.toString(arr)); &#125; public static int[] sort(int[] arr) &#123; int maxDigit = getMaxDigit(arr); return radixSort(arr, maxDigit); &#125; /** * 获取最高位数 */ private static int getMaxDigit(int[] arr) &#123; int maxValue = getMaxValue(arr); return getNumLength(maxValue); &#125; private static int getMaxValue(int[] arr) &#123; int maxValue = arr[0]; for (int value : arr) &#123; if (maxValue &lt; value) &#123; maxValue = value; &#125; &#125; return maxValue; &#125; protected static int getNumLength(long num) &#123; if (num == 0) &#123; return 1; &#125; int lenght = 0; for (long temp = num; temp != 0; temp /= 10) &#123; lenght++; &#125; return lenght; &#125; private static int[] radixSort(int[] arr, int maxDigit) &#123; int mod = 10; int dev = 1; for (int i = 0; i &lt; maxDigit; i++, dev *= 10, mod *= 10) &#123; // 考虑负数的情况，这里扩展一倍队列数，其中 [0-9]对应负数，[10-19]对应正数 (bucket + 10) int[][] counter = new int[mod * 2][0]; for (int j = 0; j &lt; arr.length; j++) &#123; int bucket = ((arr[j] % mod) / dev) + mod; counter[bucket] = arrayAppend(counter[bucket], arr[j]); &#125; int pos = 0; for (int[] bucket : counter) &#123; for (int value : bucket) &#123; arr[pos++] = value; &#125; &#125; &#125; return arr; &#125; /** * 自动扩容，并保存数据 * * @param arr * @param value */ private static int[] arrayAppend(int[] arr, int value) &#123; arr = Arrays.copyOf(arr, arr.length + 1); arr[arr.length - 1] = value; return arr; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://xmmarlowe.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"https://xmmarlowe.github.io/tags/%E6%8E%92%E5%BA%8F/"}],"author":"Marlowe"},{"title":"JUC 核心之AQS介绍","slug":"并发/JUC-核心之AQS介绍","date":"2021-03-19T14:19:40.000Z","updated":"2021-05-23T04:41:59.946Z","comments":true,"path":"2021/03/19/并发/JUC-核心之AQS介绍/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/19/%E5%B9%B6%E5%8F%91/JUC-%E6%A0%B8%E5%BF%83%E4%B9%8BAQS%E4%BB%8B%E7%BB%8D/","excerpt":"AQS 的全称为（AbstractQueuedSynchronizer），这个类在java.util.concurrent.locks包下面。","text":"AQS 的全称为（AbstractQueuedSynchronizer），这个类在java.util.concurrent.locks包下面。 简介AQS 是一个用来构建锁和同步器的框架，使用 AQS 能简单且高效地构造出应用广泛的大量的同步器，比如我们提到的 ReentrantLock，Semaphore，其他的诸如 ReentrantReadWriteLock，SynchronousQueue，FutureTask 等等皆是基于 AQS 的。当然，我们自己也能利用 AQS 非常轻松容易地构造出符合我们自己需求的同步器。 AQS原理分析AQS原理概览AQS 核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制 AQS 是用 CLH 队列锁实现的，即将暂时获取不到锁的线程加入到队列中。 CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS 是将每条请求共享资源的线程封装成一个 CLH 锁队列的一个结点（Node）来实现锁的分配。 看个 AQS(AbstractQueuedSynchronizer)原理图： AQS 使用一个 int 成员变量来表示同步状态，通过内置的 FIFO 队列来完成获取资源线程的排队工作。AQS 使用 CAS 对该同步状态进行原子操作实现对其值的修改。 1private volatile int state;//共享变量，使用volatile修饰保证线程可见性 状态信息通过 protected 类型的 getState，setState，compareAndSetState 进行操作 12345678910111213//返回同步状态的当前值protected final int getState() &#123; return state;&#125; // 设置同步状态的值protected final void setState(int newState) &#123; state = newState;&#125;//原子地（CAS操作）将同步状态值设置为给定值update如果当前同步状态的值等于expect（期望值）protected final boolean compareAndSetState(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, stateOffset, expect, update);&#125; AQS 对资源的共享方式AQS 定义两种资源共享方式 Exclusive（独占）：只有一个线程能执行，如 ReentrantLock。又可分为公平锁和非公平锁： 公平锁：按照线程在队列中的排队顺序，先到者先拿到锁 非公平锁：当线程要获取锁时，无视队列顺序直接去抢锁，谁抢到就是谁的 Share（共享）：多个线程可同时执行，如 CountDownLatch、Semaphore、CountDownLatch、 CyclicBarrier、ReadWriteLock 我们都会在后面讲到。ReentrantReadWriteLock 可以看成是组合式，因为 ReentrantReadWriteLock 也就是读写锁允许多个线程同时对某一资源进行读。 不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源 state 的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS 已经在顶层实现好了。 AQS 底层使用了模板方法模式AQS 使用了模板方法模式，自定义同步器时需要重写下面几个 AQS 提供的模板方法： 12345isHeldExclusively()//该线程是否正在独占资源。只有用到condition才需要去实现它。tryAcquire(int)//独占方式。尝试获取资源，成功则返回true，失败则返回false。tryRelease(int)//独占方式。尝试释放资源，成功则返回true，失败则返回false。tryAcquireShared(int)//共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。tryReleaseShared(int)//共享方式。尝试释放资源，成功则返回true，失败则返回false。 默认情况下，每个方法都抛出 UnsupportedOperationException。 这些方法的实现必须是内部线程安全的，并且通常应该简短而不是阻塞。AQS 类中的其他方法都是 final ，所以无法被其他类使用，只有这几个方法可以被其他类使用。 以 ReentrantLock 为例，state 初始化为 0，表示未锁定状态。A 线程 lock()时，会调用 tryAcquire()独占该锁并将 state+1。此后，其他线程再 tryAcquire()时就会失败，直到 A 线程 unlock()到 state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A 线程自己是可以重复获取此锁的（state 会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证 state 是能回到零态的。 再以 CountDownLatch 以例，任务分为 N 个子线程去执行，state 也初始化为 N（注意 N 要与线程个数一致）。这 N 个子线程是并行执行的，每个子线程执行完后 countDown() 一次，state 会 CAS(Compare and Swap)减 1。等到所有子线程都执行完后(即 state=0)，会 unpark()主调用线程，然后主调用线程就会从 await() 函数返回，继续后余动作。 一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。但 AQS 也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。 如何表明锁状态?无锁还是有锁?state变量即可。假如咱们让这个变量类型为boolean, true表明有锁、false表明无锁。 ReentrantLock, 由于RL的设计叫做:可重入锁，而boolean只能表示两种状态，这时需要别的类型，int即可。0表明无锁，&gt;0表明重入了几次，也即获取了多少次锁。 AQS 组件总结 Semaphore(信号量)-允许多个线程同时访问： synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，Semaphore(信号量)可以指定多个线程同时访问某个资源。 CountDownLatch （倒计时器）： CountDownLatch 是一个同步工具类，用来协调多个线程之间的同步。这个工具通常用来控制线程等待，它可以让某一个线程等待直到倒计时结束，再开始执行。 CyclicBarrier (循环栅栏)： CyclicBarrier 和 CountDownLatch 非常类似，它也可以实现线程间的技术等待，但是它的功能比 CountDownLatch 更加复杂和强大。主要应用场景和 CountDownLatch 类似。CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。CyclicBarrier 默认的构造方法是 CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用 await() 方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞。 一些问题谈谈你对AQS的理解，AQS如何实现可重入锁？ AQS是一个Java线程同步框架。是JDK中很多锁工具的核心实现框架。 在AQS中，维护了一个信号量state和一个线程组成的双向链表队列。其中，这个线程队列，就是用来给线程排队的，而state就像是一个红绿灯，用来控制线程排队或者放行的。在不同的场景下，有不同的意义。 在可重入锁这个场景下，state就用来表示加锁的次数。0表示无锁，每加一次锁，state就加1。释放锁state就减1。 总结AQS底层是通过一个状态量State记录当前同步器的状态，这个状态量的更改是通过CAS方式更新，同时维护了一个等待队列，如果新的任务进来的时候发现AQS是独占模式并且状态为0，则表示可以直接执行，如果状态为1则加入等待队列（双向链表），当调用unlock的时候唤醒等待队列中没有被取消的线程。 如果为非公平模式，当AQS已经被使用完成，从等待队列中唤醒一个任务的时候同时有一个任务也正加入进来，则两个任务直接竞争。如果是公平模式则直接将新加的任务放入队尾。 而AQS中还有Condition，也就是一个锁可以有多个条件来保证并发 参考AQS JUC：AQS详解 好文推荐：【深入AQS原理】我画了35张图就是为了让你深入 AQS","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"AQS","slug":"AQS","permalink":"https://xmmarlowe.github.io/tags/AQS/"},{"name":"JUC","slug":"JUC","permalink":"https://xmmarlowe.github.io/tags/JUC/"}],"author":"Marlowe"},{"title":"Atomic 原子类","slug":"并发/Atomic-原子类","date":"2021-03-19T13:03:15.000Z","updated":"2021-03-19T14:31:01.462Z","comments":true,"path":"2021/03/19/并发/Atomic-原子类/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/19/%E5%B9%B6%E5%8F%91/Atomic-%E5%8E%9F%E5%AD%90%E7%B1%BB/","excerpt":"Atomic 翻译成中文是原子的意思。在化学上，我们知道原子是构成一般物质的最小单位，在化学反应中是不可分割的。在我们这里 Atomic 是指一个操作是不可中断的。即使是在多个线程一起执行的时候，一个操作一旦开始，就不会被其他线程干扰。","text":"Atomic 翻译成中文是原子的意思。在化学上，我们知道原子是构成一般物质的最小单位，在化学反应中是不可分割的。在我们这里 Atomic 是指一个操作是不可中断的。即使是在多个线程一起执行的时候，一个操作一旦开始，就不会被其他线程干扰。 1、简介原子类说简单点就是具有原子/原子操作特征的类。 2、JUC 包中的原子类是哪 4 类?基本类型 使用原子的方式更新基本类型 AtomicInteger：整形原子类 AtomicLong：长整型原子类 AtomicBoolean：布尔型原子类 数组类型 使用原子的方式更新数组里的某个元素 AtomicIntegerArray：整形数组原子类 AtomicLongArray：长整形数组原子类 AtomicReferenceArray：引用类型数组原子类 引用类型 AtomicReference：引用类型原子类 AtomicStampedReference：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于解决原子的更新数据和数据的版本号，可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 AtomicMarkableReference ：原子更新带有标记位的引用类型 对象的属性修改类型 AtomicIntegerFieldUpdater：原子更新整形字段的更新器 AtomicLongFieldUpdater：原子更新长整形字段的更新器 AtomicReferenceFieldUpdater：原子更新引用类型字段的更新器 3、讲讲 AtomicInteger 的使用AtomicInteger 类常用方法 1234567public final int get() //获取当前的值public final int getAndSet(int newValue)//获取当前的值，并设置新的值public final int getAndIncrement()//获取当前的值，并自增public final int getAndDecrement() //获取当前的值，并自减public final int getAndAdd(int delta) //获取当前的值，并加上预期的值boolean compareAndSet(int expect, int update) //如果输入的数值等于预期值，则以原子方式将该值设置为输入值（update）public final void lazySet(int newValue)//最终设置为newValue,使用 lazySet 设置之后可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 AtomicInteger 类的使用示例使用 AtomicInteger 之后，不用对 increment() 方法加锁也可以保证线程安全。 1234567891011class AtomicIntegerTest &#123; private AtomicInteger count = new AtomicInteger(); //使用AtomicInteger之后，不需要对该方法加锁，也可以实现线程安全。 public void increment() &#123; count.incrementAndGet(); &#125; public int getCount() &#123; return count.get(); &#125;&#125; 4、 AtomicInteger 类的原理AtomicInteger 类的部分源码： 123456789101112// setup to use Unsafe.compareAndSwapInt for updates（更新操作时提供“比较并替换”的作用）private static final Unsafe unsafe = Unsafe.getUnsafe();private static final long valueOffset;static &#123; try &#123; valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(&quot;value&quot;)); &#125; catch (Exception ex) &#123; throw new Error(ex); &#125;&#125;private volatile int value; AtomicInteger 类主要利用 CAS (compare and swap) + volatile 和 native 方法来保证原子操作，从而避免 synchronized 的高开销，执行效率大为提升。 CAS 的原理是拿期望的值和原本的一个值作比较，如果相同则更新成新的值。UnSafe 类的 objectFieldOffset() 方法是一个本地方法，这个方法是用来拿到“原来的值”的内存地址，返回值是 valueOffset。另外 value 是一个 volatile 变量，在内存中可见，因此 JVM 可以保证任何时刻任何线程总能拿到该变量的最新值。 参考Atomic 原子类","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"原子类","slug":"原子类","permalink":"https://xmmarlowe.github.io/tags/%E5%8E%9F%E5%AD%90%E7%B1%BB/"}],"author":"Marlowe"},{"title":"线程池原理分析","slug":"并发/线程池原理分析","date":"2021-03-19T12:38:30.000Z","updated":"2021-03-19T12:43:24.306Z","comments":true,"path":"2021/03/19/并发/线程池原理分析/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/19/%E5%B9%B6%E5%8F%91/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/","excerpt":"","text":"execute方法源码： 12345678910111213141516171819202122232425262728293031323334353637383940// 存放线程池的运行状态 (runState) 和线程池内有效线程的数量 (workerCount)private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); private static int workerCountOf(int c) &#123; return c &amp; CAPACITY; &#125; private final BlockingQueue&lt;Runnable&gt; workQueue; public void execute(Runnable command) &#123; // 如果任务为null，则抛出异常。 if (command == null) throw new NullPointerException(); // ctl 中保存的线程池当前的一些状态信息 int c = ctl.get(); // 下面会涉及到 3 步 操作 // 1.首先判断当前线程池中执行的任务数量是否小于 corePoolSize // 如果小于的话，通过addWorker(command, true)新建一个线程，并将任务(command)添加到该线程中；然后，启动该线程从而执行任务。 if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; // 2.如果当前执行的任务数量大于等于 corePoolSize 的时候就会走到这里 // 通过 isRunning 方法判断线程池状态，线程池处于 RUNNING 状态才会被并且队列可以加入任务，该任务才会被加入进去 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); // 再次获取线程池状态，如果线程池状态不是 RUNNING 状态就需要从任务队列中移除任务，并尝试判断线程是否全部执行完毕。同时执行拒绝策略。 if (!isRunning(recheck) &amp;&amp; remove(command)) reject(command); // 如果当前线程池为空就新创建一个线程并执行。 else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; //3. 通过addWorker(command, false)新建一个线程，并将任务(command)添加到该线程中；然后，启动该线程从而执行任务。 //如果addWorker(command, false)执行失败，则通过reject()执行相应的拒绝策略的内容。 else if (!addWorker(command, false)) reject(command); &#125; 具体流程见图解：","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"线程池","slug":"线程池","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B%E6%B1%A0/"}],"author":"Marlowe"},{"title":"线程池","slug":"并发/线程池","date":"2021-03-18T08:11:53.000Z","updated":"2021-05-18T06:52:00.495Z","comments":true,"path":"2021/03/18/并发/线程池/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/18/%E5%B9%B6%E5%8F%91/%E7%BA%BF%E7%A8%8B%E6%B1%A0/","excerpt":"池化技术的思想主要是为了减少每次获取资源的消耗，提高对资源的利用率。三大创建方法、七大参数、四种拒绝策略…","text":"池化技术的思想主要是为了减少每次获取资源的消耗，提高对资源的利用率。三大创建方法、七大参数、四种拒绝策略… 1、为什么要用线程池？ 降低资源消耗： 通过池化技术重复利用已创建的线程，降低线程创建和销毁造成的损耗。 提高响应速度： 任务到达时，无需等待线程创建即可立即执行。 提高线程的可管理性： 线程是稀缺资源，如果无限制创建，不仅会消耗系统资源，还会因为线程的不合理分布导致资源调度失衡，降低系统的稳定性。使用线程池可以进行统一的分配、调优和监控。 提供更多更强大的功能： 线程池具备可拓展性，允许开发人员向其中增加更多的功能。比如延时定时线程池ScheduledThreadPoolExecutor，就允许任务延期执行或定期执行。 线程池解决的问题是什么线程池解决的核心问题就是资源管理问题。在并发环境下，系统不能够确定在任意时刻中，有多少任务需要执行，有多少资源需要投入。这种不确定性将带来以下若干问题： 频繁申请/销毁资源和调度资源，将带来额外的消耗，可能会非常巨大。 对资源无限申请缺少抑制手段，易引发系统资源耗尽的风险。 系统无法合理管理内部的资源分布，会降低系统的稳定性。 为解决资源分配这个问题，线程池采用了“池化”（Pooling）思想。池化，顾名思义，是为了最大化收益并最小化风险，而将资源统一在一起管理的一种思想。 Pooling is the grouping together of resources (assets, equipment, personnel, effort, etc.) for the purposes of maximizing advantage or minimizing risk to the users. The term is used in finance, computing and equipment management.——wikipedia “池化”思想不仅仅能应用在计算机领域，在金融、设备、人员管理、工作管理等领域也有相关的应用。 在计算机领域中的表现为：统一管理IT资源，包括服务器、存储、和网络资源等等。通过共享资源，使用户在低投入中获益。除去线程池，还有其他比较典型的几种使用策略包括： 内存池(Memory Pooling)：预先申请内存，提升申请内存速度，减少内存碎片。 连接池(Connection Pooling)：预先申请数据库连接，提升申请连接的速度，降低系统的开销。 实例池(Object Pooling)：循环使用对象，减少资源在初始化和释放时的昂贵损耗。 在了解完“是什么”和“为什么”之后，下面我们来一起深入一下线程池的内部实现原理。 2、线程池的三大创建方法通过 Executor 框架的工具类 Executors 来实现 我们可以创建三种类型的 ThreadPoolExecutor： FixedThreadPool： 该方法返回一个固定线程数量的线程池。 该线程池中的线程数量始终不变。当有一个新的任务提交时，线程池中若有空闲线程，则立即执行。若没有，则新的任务会被暂存在一个任务队列中，待有线程空闲时，便处理在任务队列中的任务。 SingleThreadExecutor： 方法返回一个只有一个线程的线程池。 若多余一个任务被提交到该线程池，任务会被保存在一个任务队列中，待线程空闲，按先入先出的顺序执行队列中的任务。 CachedThreadPool： 该方法返回一个可根据实际情况调整线程数量的线程池。 线程池的线程数量不确定，但若有空闲线程可以复用，则会优先使用可复用的线程。若所有线程均在工作，又有新的任务提交，则会创建新的线程处理任务。所有线程在当前任务执行完毕后，将返回线程池进行复用。 3、实现 Runnable 接口和 Callable 接口的区别Runnable自 Java 1.0 以来一直存在，但Callable仅在 Java 1.5 中引入,目的就是为了来处理Runnable不支持的用例。Runnable 接口不会返回结果或抛出检查异常，但是Callable 接口可以。所以，如果任务不需要返回结果或抛出异常推荐使用 Runnable 接口， 这样代码看起来会更加简洁。 工具类 Executors 可以实现 Runnable 对象和 Callable 对象之间的相互转换。（Executors.callable（Runnable task）或 Executors.callable（Runnable task，Object resule））。 Runnable.java 1234567@FunctionalInterfacepublic interface Runnable &#123; /** * 被线程执行，没有返回值也无法抛出异常 */ public abstract void run();&#125; Callable.java 123456789@FunctionalInterfacepublic interface Callable&lt;V&gt; &#123; /** * 计算结果，或在无法这样做时抛出异常。 * @return 计算得出的结果 * @throws 如果无法计算结果，则抛出异常 */ V call() throws Exception;&#125; 4、执行 execute()方法和 submit()方法的区别是什么呢？ execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功与否； submit()方法用于提交需要返回值的任务。线程池会返回一个 Future 类型的对象，通过这个 Future 对象可以判断任务是否执行成功， 并且可以通过 Future 的 get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用 get（long timeout，TimeUnit unit）方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。 5、ThreadPoolExecutor 类分析ThreadPoolExecutor 类中提供的四个构造方法。我们来看最长的那个，其余三个都是在这个构造方法的基础上产生（其他几个构造方法说白点都是给定某些默认参数的构造方法比如默认制定拒绝策略是什么）。 123456789101112131415161718192021222324/** * 用给定的初始参数创建一个新的ThreadPoolExecutor。 */public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 5.1 ThreadPoolExecutor构造函数重要参数分析ThreadPoolExecutor 3 个最重要的参数： corePoolSize : 核心线程数线程数定义了最小可以同时运行的线程数量。 maximumPoolSize : 当队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。 workQueue: 当新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中。 ThreadPoolExecutor其他常见参数: keepAliveTime:当线程池中的线程数量大于 corePoolSize 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 keepAliveTime才会被回收销毁； unit : keepAliveTime 参数的时间单位。 threadFactory :executor 创建新线程的时候会用到。 handler :饱和策略。关于饱和策略下面单独介绍一下。 5.2 ThreadPoolExecutor 饱和策略(4种拒绝策略)ThreadPoolExecutor 饱和策略定义: 如果当前同时运行的线程数量达到最大线程数量并且队列也已经被放满了任务时，ThreadPoolTaskExecutor 定义一些策略: ThreadPoolExecutor.AbortPolicy：抛出 RejectedExecutionException来拒绝新任务的处理。 ThreadPoolExecutor.CallerRunsPolicy：调用执行自己的线程运行任务，也就是直接在调用execute方法的线程中运行(run)被拒绝的任务，如果执行程序已关闭，则会丢弃该任务。因此这种策略会降低对于新任务提交速度，影响程序的整体性能。如果您的应用程序可以承受此延迟并且你要求任何一个任务请求都要被执行的话，你可以选择这个策略。 ThreadPoolExecutor.DiscardPolicy： 不处理新任务，直接丢弃掉。 ThreadPoolExecutor.DiscardOldestPolicy： 此策略将丢弃最早的未处理的任务请求。 举个例子： Spring 通过 ThreadPoolTaskExecutor 或者我们直接通过 ThreadPoolExecutor 的构造函数创建线程池的时候，当我们不指定 RejectedExecutionHandler 饱和策略的话来配置线程池的时候默认使用的是 ThreadPoolExecutor.AbortPolicy。在默认情况下，ThreadPoolExecutor 将抛出 RejectedExecutionException 来拒绝新来的任务 ，这代表你将丢失对这个任务的处理。 对于可伸缩的应用程序，建议使用 ThreadPoolExecutor.CallerRunsPolicy。当最大池被填满时，此策略为我们提供可伸缩队列。 编写测试程序，我们这里以阿里巴巴推荐的使用 ThreadPoolExecutor 构造函数自定义参数的方式来创建线程池。 ThreadPoolExecutorDemo.java 1234567891011121314151617181920212223242526272829303132333435import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class ThreadPoolExecutorDemo &#123; private static final int CORE_POOL_SIZE = 5; private static final int MAX_POOL_SIZE = 10; private static final int QUEUE_CAPACITY = 100; private static final Long KEEP_ALIVE_TIME = 1L; public static void main(String[] args) &#123; //使用阿里巴巴推荐的创建线程池的方式 //通过ThreadPoolExecutor构造函数自定义参数创建 ThreadPoolExecutor executor = new ThreadPoolExecutor( CORE_POOL_SIZE, MAX_POOL_SIZE, KEEP_ALIVE_TIME, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(QUEUE_CAPACITY), new ThreadPoolExecutor.CallerRunsPolicy()); for (int i = 0; i &lt; 10; i++) &#123; //创建WorkerThread对象（WorkerThread类实现了Runnable 接口） Runnable worker = new MyRunnable(&quot;&quot; + i); //执行Runnable executor.execute(worker); &#125; //终止线程池 executor.shutdown(); while (!executor.isTerminated()) &#123; &#125; System.out.println(&quot;Finished all threads&quot;); &#125;&#125; 可以看到我们上面的代码指定了： corePoolSize: 核心线程数为 5。 maximumPoolSize ：最大线程数 10 keepAliveTime : 等待时间为 1L。 unit: 等待时间的单位为 TimeUnit.SECONDS。 workQueue：任务队列为 ArrayBlockingQueue，并且容量为 100; handler:饱和策略为 CallerRunsPolicy。 6、线程池原理分析为了搞懂线程池的原理，我们需要首先分析一下 execute方法。看看它的源码： 12345678910111213141516171819202122232425262728293031323334353637383940// 存放线程池的运行状态 (runState) 和线程池内有效线程的数量 (workerCount)private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); private static int workerCountOf(int c) &#123; return c &amp; CAPACITY; &#125; private final BlockingQueue&lt;Runnable&gt; workQueue; public void execute(Runnable command) &#123; // 如果任务为null，则抛出异常。 if (command == null) throw new NullPointerException(); // ctl 中保存的线程池当前的一些状态信息 int c = ctl.get(); // 下面会涉及到 3 步 操作 // 1.首先判断当前线程池中执行的任务数量是否小于 corePoolSize // 如果小于的话，通过addWorker(command, true)新建一个线程，并将任务(command)添加到该线程中；然后，启动该线程从而执行任务。 if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; // 2.如果当前执行的任务数量大于等于 corePoolSize 的时候就会走到这里 // 通过 isRunning 方法判断线程池状态，线程池处于 RUNNING 状态才会被并且队列可以加入任务，该任务才会被加入进去 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); // 再次获取线程池状态，如果线程池状态不是 RUNNING 状态就需要从任务队列中移除任务，并尝试判断线程是否全部执行完毕。同时执行拒绝策略。 if (!isRunning(recheck) &amp;&amp; remove(command)) reject(command); // 如果当前线程池为空就新创建一个线程并执行。 else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; //3. 通过addWorker(command, false)新建一个线程，并将任务(command)添加到该线程中；然后，启动该线程从而执行任务。 //如果addWorker(command, false)执行失败，则通过reject()执行相应的拒绝策略的内容。 else if (!addWorker(command, false)) reject(command); &#125; 图解： 7、关闭线程池Java提供的对ExecutorService的关闭方式有两种，一种是调用其shutdown()方法，另一种是调用shutdownNow()方法。这两者是有区别的。 以下内容摘自源代码内的注释 1234// shutdown()Initiates an orderly shutdown in which previously submitted tasks are executed, but no new tasks will be accepted.Invocation has no additional effect if already shut down.This method does not wait for previously submitted tasks to complete execution. Use awaitTermination to do that. 1234// shutdownNow()Attempts to stop all actively executing tasks, halts the processing of waiting tasks, and returns a list of the tasks that were awaiting execution.This method does not wait for actively executing tasks to terminate. Use awaitTermination to do that.There are no guarantees beyond best-effort attempts to stop processing actively executing tasks. For example, typical implementations will cancel via interrupt, so any task that fails to respond to interrupts may never terminate. shutdown()1、调用之后不允许继续往线程池内继续添加线程;2、线程池的状态变为SHUTDOWN状态;3、所有在调用shutdown()方法之前提交到ExecutorSrvice的任务都会执行;4、一旦所有线程结束执行当前任务，ExecutorService才会真正关闭。 shutdownNow()1、该方法返回尚未执行的 task 的 List;2、线程池的状态变为STOP状态;3、阻止所有正在等待启动的任务, 并且停止当前正在执行的任务。 简单点来说，就是: shutdown()调用后，不可以再 submit 新的 task，已经 submit 的将继续执行shutdownNow()调用后，试图停止当前正在执行的 task，并返回尚未执行的 task 的 list 源码分析这里用的是JDK1.8，首先进入ThreadPoolExecutor的shutDown()方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public void shutdown() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; checkShutdownAccess(); advanceRunState(SHUTDOWN); interruptIdleWorkers(); onShutdown(); // hook for ScheduledThreadPoolExecutor &#125; finally &#123; mainLock.unlock(); &#125; tryTerminate();&#125;private void checkShutdownAccess() &#123; SecurityManager security = System.getSecurityManager(); if (security != null) &#123; security.checkPermission(shutdownPerm); final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; for (Worker w : workers) security.checkAccess(w.thread); &#125; finally &#123; mainLock.unlock(); &#125; &#125;&#125;private void advanceRunState(int targetState) &#123; for (;;) &#123; int c = ctl.get(); if (runStateAtLeast(c, targetState) || ctl.compareAndSet(c, ctlOf(targetState, workerCountOf(c)))) break; &#125;&#125;private void interruptIdleWorkers() &#123; interruptIdleWorkers(false);&#125;void onShutdown() &#123;&#125;final void tryTerminate() &#123; for (;;) &#123; int c = ctl.get(); if (isRunning(c) || runStateAtLeast(c, TIDYING) || (runStateOf(c) == SHUTDOWN &amp;&amp; ! workQueue.isEmpty())) return; if (workerCountOf(c) != 0) &#123; // Eligible to terminate interruptIdleWorkers(ONLY_ONE); return; &#125; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; if (ctl.compareAndSet(c, ctlOf(TIDYING, 0))) &#123; try &#123; terminated(); &#125; finally &#123; ctl.set(ctlOf(TERMINATED, 0)); termination.signalAll(); &#125; return; &#125; &#125; finally &#123; mainLock.unlock(); &#125; // else retry on failed CAS &#125;&#125; 进入shutDownNow()方法看看： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394public List&lt;Runnable&gt; shutdownNow() &#123; List&lt;Runnable&gt; tasks; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; checkShutdownAccess(); advanceRunState(STOP); interruptWorkers(); tasks = drainQueue(); &#125; finally &#123; mainLock.unlock(); &#125; tryTerminate(); return tasks;&#125;private void checkShutdownAccess() &#123; SecurityManager security = System.getSecurityManager(); if (security != null) &#123; security.checkPermission(shutdownPerm); final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; for (Worker w : workers) security.checkAccess(w.thread); &#125; finally &#123; mainLock.unlock(); &#125; &#125;&#125;private void advanceRunState(int targetState) &#123; for (;;) &#123; int c = ctl.get(); if (runStateAtLeast(c, targetState) || ctl.compareAndSet(c, ctlOf(targetState, workerCountOf(c)))) break; &#125;&#125;private void interruptWorkers() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; for (Worker w : workers) w.interruptIfStarted(); &#125; finally &#123; mainLock.unlock(); &#125;&#125;private List&lt;Runnable&gt; drainQueue() &#123; BlockingQueue&lt;Runnable&gt; q = workQueue; ArrayList&lt;Runnable&gt; taskList = new ArrayList&lt;Runnable&gt;(); q.drainTo(taskList); if (!q.isEmpty()) &#123; for (Runnable r : q.toArray(new Runnable[0])) &#123; if (q.remove(r)) taskList.add(r); &#125; &#125; return taskList;&#125;final void tryTerminate() &#123; for (;;) &#123; int c = ctl.get(); if (isRunning(c) || runStateAtLeast(c, TIDYING) || (runStateOf(c) == SHUTDOWN &amp;&amp; ! workQueue.isEmpty())) return; if (workerCountOf(c) != 0) &#123; // Eligible to terminate interruptIdleWorkers(ONLY_ONE); return; &#125; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; if (ctl.compareAndSet(c, ctlOf(TIDYING, 0))) &#123; try &#123; terminated(); &#125; finally &#123; ctl.set(ctlOf(TERMINATED, 0)); termination.signalAll(); &#125; return; &#125; &#125; finally &#123; mainLock.unlock(); &#125; // else retry on failed CAS &#125;&#125; 实战1、Demo1 12345678910111213141516171819202122232425262728package com.concurrent.executorService;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;/** * @author riemann * @date 2019/07/28 23:41 */public class ExecutorServiceDemo1 &#123; static Runnable run = () -&gt; &#123; try &#123; Thread.sleep(5000); System.out.println(&quot;thread finish&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;; public static void main(String[] args) &#123; ExecutorService service = Executors.newFixedThreadPool(2); service.execute(run); service.shutdown(); service.execute(run); &#125;&#125; 输出结果： 123456Exception in thread &quot;main&quot; java.util.concurrent.RejectedExecutionException: Task com.concurrent.executorService.ExecutorServiceDemo1$$Lambda$1/1854731462@312b1dae rejected from java.util.concurrent.ThreadPoolExecutor@7530d0a[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 0] at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047) at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823) at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369) at com.concurrent.executorService.ExecutorServiceDemo1.main(ExecutorServiceDemo1.java:25)thread finish 当调用shutdown()之后，将不能继续添加任务，否则会抛出异常RejectedExecutionException。并且当正在执行的任务结束之后才会真正结束线程池。 2、Demo2 123456789101112131415161718192021222324252627package com.concurrent.executorService;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;/** * @author riemann * @date 2019/07/29 0:03 */public class ExecutorServiceDemo2 &#123; static Runnable run = () -&gt; &#123; try &#123; Thread.sleep(5000); System.out.println(&quot;thread finish&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;; public static void main(String[] args) &#123; ExecutorService service = Executors.newFixedThreadPool(2); service.execute(run); service.shutdownNow(); &#125;&#125; 输出结果： 123456java.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at com.concurrent.executorService.ExecutorServiceDemo2.lambda$static$0(ExecutorServiceDemo2.java:14) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) 使用shutdownNow()，若线程中有执行sleep/wait/定时锁等，直接终止正在运行的线程并抛出 interrupt 异常。因为其内部是通过Thread.interrupt()实现的。但是这种方法有很强的局限性。因为如果线程中没有执行sleep等方法的话，其无法终止线程。 3、Demo3 1234567891011121314151617181920212223242526272829package com.concurrent.executorService;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;/** * @author riemann * @date 2019/07/29 0:05 */public class ExecutorServiceDemo3 &#123; static Runnable run = () -&gt; &#123; long num = 0; boolean flag = true; while (flag) &#123; num += 1; if (num == Long.MAX_VALUE) &#123; flag = false; &#125; &#125; &#125;; public static void main(String[] args) &#123; ExecutorService service = Executors.newFixedThreadPool(1); service.execute(run); service.shutdownNow(); &#125;&#125; 很多代码中都会有这样的情况，比方说使用循环标记flag循环执行一些耗时长的计算任务， 直到满足某个条件之后才设置循环标记为false。如 Demo3 代码所示 (循环等待的情况)，shutdownNow()无法终止线程。如果遇到这种情况，可以使用如 Demo4 中的方法。 4、Demo4 1234567891011121314151617181920212223242526package com.concurrent.executorService;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;/** * @author riemann * @date 2019/07/29 0:12 */public class ExecutorServiceDemo4 &#123; static Runnable run = () -&gt; &#123; long num = 0; while (true &amp;&amp; !Thread.currentThread().isInterrupted()) &#123; num += 1; &#125; System.out.println(num); &#125;; public static void main(String[] args) &#123; ExecutorService service = Executors.newFixedThreadPool(1); service.execute(run); service.shutdownNow(); &#125;&#125; 输出结果： 10 对于循环等待的情况，可以引入变量Thread.currentThread().isInterrupted()来作为其中的一个判断条件。isInterrupted()方法返回当前线程是否有被 interrupt。shutdownNow()的内部实现实际上就是通过 interrupt 来终止线程，所以当调用shutdownNow()时，isInterrupted()会返回true。此时就可以跳出循环等待。然而这也不是最优雅的解决方式，具体可以参见 Demo5。 5、Demo5 1234567891011121314151617181920212223242526272829303132333435363738package com.concurrent.executorService;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.TimeUnit;/** * @author riemann * @date 2019/07/29 0:17 */public class ExecutorServiceDemo5 &#123; static Runnable run = () -&gt; &#123; long num = 0; boolean flag = true; while (flag &amp;&amp; !Thread.currentThread().isInterrupted()) &#123; num += 1; if (num == Long.MAX_VALUE) &#123; flag = false; &#125; &#125; System.out.println(num); &#125;; public static void main(String[] args) &#123; ExecutorService service = Executors.newFixedThreadPool(1); service.execute(run); service.shutdown(); try &#123; if (!service.awaitTermination(2, TimeUnit.SECONDS)) &#123; service.shutdownNow(); &#125; &#125; catch (InterruptedException e) &#123; service.shutdownNow(); &#125; &#125;&#125; 输出结果： 1999032162 这里。先调用shutdown()使线程池状态改变为SHUTDOWN，线程池不允许继续添加线程，并且等待正在执行的线程返回。调用awaitTermination设置定时任务，代码内的意思为 2s 后检测线程池内的线程是否均执行完毕（就像老师告诉学生，“最后给你 2s 钟时间把作业写完”），若没有执行完毕，则调用shutdownNow()方法。 一些问题线程池被创建后里面有线程吗？如果没有的话，你知道有什么方法对线程池进行预热吗？线程池被创建后如果没有任务过来，里面是不会有线程的。如果需要预热的话可以调用下面的两个方法： 启动一个 全部启动 核心线程数会被回收吗？需要什么设置？核心线程数默认是不会被回收的，如果需要回收核心线程数，需要调用下面的方法： allowCoreThreadTimeOut 该值默认为 false。 小结线程池最大线程数到底该如何定义 CPU 密集型：电脑是几核，就是几，可以保持CPU的效率最高。12// 获取CPU核心数Runtime.getRuntime().availableProcessors() IO 密集型：判断程序中十分耗IO的线程有多少个，大于这个数（或者2倍） 参考线程池 线程池的关闭方式有几种，各自的区别是什么。 好文推荐：Java线程池实现原理及其在美团业务中的实践","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"线程池","slug":"线程池","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B%E6%B1%A0/"}],"author":"Marlowe"},{"title":"volatile 关键字","slug":"并发/volatile-关键字","date":"2021-03-18T02:30:02.000Z","updated":"2021-05-14T09:07:19.867Z","comments":true,"path":"2021/03/18/并发/volatile-关键字/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/18/%E5%B9%B6%E5%8F%91/volatile-%E5%85%B3%E9%94%AE%E5%AD%97/","excerpt":"volatile 是Java虚拟机提供的轻量级同步机制，保证可见性，不保证原子性，禁止指令重排。","text":"volatile 是Java虚拟机提供的轻量级同步机制，保证可见性，不保证原子性，禁止指令重排。 1、CPU缓存模型为什么要弄一个 CPU 高速缓存呢？类比我们开发网站后台系统使用的缓存（比如 Redis）是为了解决程序处理速度和访问常规关系型数据库速度不对等的问题。 CPU 缓存则是为了解决 CPU 处理速度和内存处理速度不对等的问题。 我们甚至可以把 内存可以看作外存的高速缓存，程序运行的时候我们把外存的数据复制到内存，由于内存的处理速度远远高于外存，这样提高了处理速度。 总结： CPU Cache 缓存的是内存数据用于解决 CPU 处理速度和内存不匹配的问题，内存缓存的是硬盘数据用于解决硬盘访问速度过慢的问题。 CPU Cache 的工作方式： 先复制一份数据到 CPU Cache 中，当 CPU 需要用到的时候就可以直接从 CPU Cache 中读取数据，当运算完成后，再将运算得到的数据写回 Main Memory 中。但是，这样存在 内存缓存不一致性的问题 ！ 比如我执行一个 i++操作的话，如果两个线程同时执行的话，假设两个线程从 CPU Cache 中读取的 i=1，两个线程做了 1++运算完之后再写回 Main Memory 之后 i=2，而正确结果应该是 i=3。 CPU 为了解决内存缓存不一致性问题可以通过制定缓存一致协议或者其他手段来解决。 2、讲一下 JMM(Java 内存模型)在 JDK1.2 之前，Java 的内存模型实现总是从主存（即共享内存）读取变量，是不需要进行特别的注意的。而在当前的 Java 内存模型下，线程可以把变量保存本地内存（比如机器的寄存器）中，而不是直接在主存中进行读写。这就可能造成一个线程在主存中修改了一个变量的值，而另外一个线程还继续使用它在寄存器中的变量值的拷贝，造成数据的不一致。 要解决这个问题，就需要把变量声明为volatile，这就指示 JVM，这个变量是共享且不稳定的，每次使用它都到主存中进行读取。 所以，volatile 关键字 除了防止 JVM 的指令重排 ，还有一个重要的作用就是保证变量的可见性。 关于JMM的一些同步约定： 线程解锁前，必须把共享变量立刻刷回主存。 线程加锁前，必须读取主存中的最新值到工作内存中。 加锁和解锁是同一把锁。 关于主内存与工作内存之间的具体交互协议，即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步到主内存之间的实现细节，Java内存模型定义了以下八种操作来完成： lock（锁定）：作用于主内存的变量，把一个变量标识为一条线程独占状态。 unlock（解锁）：作用于主内存变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。 read（读取）：作用于主内存变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用 load（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。 use（使用）：作用于工作内存的变量，把工作内存中的一个变量值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。 assign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋值给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。 store（存储）：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作。 write（写入）：作用于主内存的变量，它把store操作从工作内存中一个变量的值传送到主内存的变量中。 Java内存模型还规定了在执行上述八种基本操作时，必须满足如下规则： 如果要把一个变量从主内存中复制到工作内存，就需要按顺寻地执行read和load操作， 如果把变量从工作内存中同步回主内存中，就要按顺序地执行store和write操作。但Java内存模型只要求上述操作必须按顺序执行，而没有保证必须是连续执行。 不允许read和load、store和write操作之一单独出现 不允许一个线程丢弃它的最近assign的操作，即变量在工作内存中改变了之后必须同步到主内存中。 不允许一个线程无原因地（没有发生过任何assign操作）把数据从工作内存同步回主内存中。 一个新的变量只能在主内存中诞生，不允许在工作内存中直接使用一个未被初始化（load或assign）的变量。即就是对一个变量实施use和store操作之前，必须先执行过了assign和load操作。 一个变量在同一时刻只允许一条线程对其进行lock操作，但lock操作可以被同一条线程重复执行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会被解锁。lock和unlock必须成对出现 如果对一个变量执行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前需要重新执行load或assign操作初始化变量的值 如果一个变量事先没有被lock操作锁定，则不允许对它执行unlock操作；也不允许去unlock一个被其他线程锁定的变量。 对一个变量执行unlock操作之前，必须先把此变量同步到主内存中（执行store和write操作）。 代码示例：开启两个线程，一个主线程，一个新线程。 1234567891011121314151617181920public class Test3 &#123; private static int num = 0; public static void main(String[] args) &#123; new Thread(() -&gt; &#123; // 线程1对主内存的变化是不知道的 while (num == 0) &#123; &#125; &#125;).start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; num = 1; System.out.println(num); &#125;&#125; 结果： 1231// 程序一直执行 问题：程序不知道主内存的值已经被修改为1 3、volatile 保证可见性 代码示例： 1234567891011121314151617181920212223public class Test3 &#123; /** * 不加 volatile 程序会死循环！ * 加 volatile 可以保证变量可见性 */ private volatile static int num = 0; public static void main(String[] args) &#123; new Thread(() -&gt; &#123; while (num == 0) &#123; &#125; &#125;).start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; num = 1; System.out.println(num); &#125;&#125; 结果： 1231Process finished with exit code 0 不保证原子性 原子性：不可分割线程A在执行任务的时候，是不能被打扰的，也不能被分割，要么同时成功，要么同时失败。 代码示例： 1234567891011121314151617181920212223242526public class TestVolatile &#123; // volatile 不保证原子性 private volatile static int num = 0; public static void add() &#123; // 不是原子性操作 num++; &#125; public static void main(String[] args) &#123; // 理论上num结果应该为2w for (int i = 1; i &lt;= 20; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j &lt; 1000; j++) &#123; add(); &#125; &#125;).start(); &#125; while (Thread.activeCount() &gt; 2) &#123; Thread.yield(); &#125; System.out.println(Thread.currentThread().getName() + &quot; &quot; + num); &#125;&#125; 结果： 1main 19782 // 结果每次可能不一样，但不会变为2w 如果不加lock和synchronized,怎么样保证原子性? 使用原子类解决原子性问题 代码示例: 1234567891011121314151617181920212223242526public class TestVolatile &#123; // volatile 不保证原子性 private volatile static AtomicInteger num = new AtomicInteger(); public static void add() &#123; // AtomicInteger +1 方法 不是简单的 +1 操作，而是用的CAS num.getAndIncrement(); &#125; public static void main(String[] args) &#123; // 理论上num结果应该为2w for (int i = 1; i &lt;= 20; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j &lt; 1000; j++) &#123; add(); &#125; &#125;).start(); &#125; while (Thread.activeCount() &gt; 2) &#123; Thread.yield(); &#125; System.out.println(Thread.currentThread().getName() + &quot; &quot; + num); &#125;&#125; 结果： 1main 20000 这些类的底层都直接和操作系统挂钩！在内存中修改值！Unsafe类是一个很特殊的存在。 禁止指令重排 什么是指令重排：你写的程序，计算机并不是按照你写的那样去执行的。源代码–&gt; 编译器优化的重排–&gt; 指令并行也可能重排–&gt; 内存系统也会重排–&gt; 执行. 前提：处理器在进行指令重排的时候，会考虑数据之间的依赖性！ 123456int x = 1; // 1int y = 2; // 2x = x + 5; // 3y = x * x; // 4我们所期望的：1234 但是可能执行的时候会变成 2134 1324 volatile如何保证可见性？volatile 主要是利用了java的先行发生原则 （简单介绍先行发生原则：在计算机科学中，先行发生原则是两个事件的结果之间的关系，如果一个事件发生在另一个事件之前，结果必须反映，即使这些事件实际上是乱序执行的（通常是优化程序流程））。 volatile相关的规则： 对于一个volatile变量的写操作先行发生于后面对这个变量的读操作。 因此当线程1执行了vlt=5；写操作是必然先发生2线程读操作。即线程2从主内存读到的数据一定是线程1写过的数据那就是5。所以volatile主要利用了先行发生原则保证线程之间的可见性。 volatile如何避免指令重排？内存屏障，是一个CPU指令。 作用： 保证特定操作的执行顺序。 可以保证某些变量的内存可见性（利用这些特性，volatile实现了可见性）。 volatile 可以保证可见性，不能保证原子性，由于内存屏障，可以保证避免指令重排的现象产生！ 由于编译器和处理器都能执行指令重排优化，如果在指令之间插入一条内存屏障则会告诉编译器和cup不管在任何情况下，无论任何指令都不能和这条内存屏障进行指令重排，也就是说通过插入内存屏障禁止在内存屏障前后的指令执行重排序优化。内存屏障的另外一个作用就是强制刷出各种CPU的缓存数据，因此在任何CPU上的线程都能读取到这些数据的最新值。 4、并发编程的三个重要特性 原子性 : 一个的操作或者多次操作，要么所有的操作全部都得到执行并且不会收到任何因素的干扰而中断，要么所有的操作都执行，要么都不执行。synchronized 可以保证代码片段的原子性。 可见性 ： 当一个变量对共享变量进行了修改，那么另外的线程都是立即可以看到修改后的最新值。volatile 关键字可以保证共享变量的可见性。 有序性 ： 代码在执行的过程中的先后顺序，Java 在编译器以及运行期间的优化，代码的执行顺序未必就是编写代码时候的顺序。volatile 关键字可以禁止指令进行重排序优化。 5、说说 synchronized 关键字和 volatile 关键字的区别synchronized 关键字和 volatile 关键字是两个互补的存在，而不是对立的存在！ volatile 关键字是线程同步的轻量级实现，所以volatile 性能肯定比synchronized关键字要好。但是volatile 关键字只能用于变量而 synchronized 关键字可以修饰方法以及代码块。 volatile 关键字能保证数据的可见性，但不能保证数据的原子性。 synchronized 关键字两者都能保证。 volatile 关键字主要用于解决变量在多个线程之间的可见性，而 synchronized 关键字解决的是多个线程之间访问资源的同步性。 5、参考volatile 关键字","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"volatile","slug":"volatile","permalink":"https://xmmarlowe.github.io/tags/volatile/"}],"author":"Marlowe"},{"title":"synchronized相关知识点","slug":"并发/synchronized相关知识点","date":"2021-03-17T14:39:35.000Z","updated":"2021-04-28T06:10:10.625Z","comments":true,"path":"2021/03/17/并发/synchronized相关知识点/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/17/%E5%B9%B6%E5%8F%91/synchronized%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E7%82%B9/","excerpt":"","text":"1、synchronized 关键字 1.1 synchronized 关键字简介synchronized 关键字解决的是多个线程之间访问资源的同步性，synchronized关键字可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。 另外，在 Java 早期版本中，synchronized 属于 重量级锁，效率低下。 为什么呢？因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高。 庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对 synchronized 较大优化，所以现在的 synchronized 锁效率也优化得很不错了。JDK1.6 对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。 所以，你会发现目前的话，不论是各种开源框架还是 JDK 源码都大量使用了 synchronized 关键字。 2、synchronized 关键字使用方式synchronized 关键字最主要的三种使用方式： 修饰实例方法: 作用于当前对象实例加锁，进入同步代码前要获得 当前对象实例的锁123synchronized void method() &#123; //业务代码&#125; 修饰静态方法: 也就是给当前类加锁，会作用于类的所有对象实例 ，进入同步代码前要获得 当前 class 的锁。因为静态成员不属于任何一个实例对象，是类成员（ _static 表明这是该类的一个静态资源，不管 new 了多少个对象，只有一份_）。所以，如果一个线程 A 调用一个实例对象的非静态 synchronized 方法，而线程 B 需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。123synchronized static void method() &#123;//业务代码&#125; 修饰代码块： 指定加锁对象，对给定对象/类加锁。synchronized(this|object) 表示进入同步代码库前要获得给定对象的锁。synchronized(类.class) 表示进入同步代码前要获得 当前 class 的锁 123synchronized(this) &#123; //业务代码&#125; 总结： synchronized 关键字加到 static 静态方法和 synchronized(class) 代码块上都是是给 Class 类上锁。 synchronized 关键字加到实例方法上是给对象实例上锁。 尽量不要使用 synchronized(String a) 因为 JVM 中，字符串常量池具有缓存功能！ 重点：面试中面试官经常会说：“单例模式了解吗？来给我手写一下！给我解释一下双重检验锁方式实现单例模式的原理呗！” 双重校验锁实现对象单例（线程安全） 1234567891011121314151617181920public class Singleton &#123; private volatile static Singleton uniqueInstance; private Singleton() &#123; &#125; public static Singleton getUniqueInstance() &#123; //先判断对象是否已经实例过，没有实例化过才进入加锁代码 if (uniqueInstance == null) &#123; //类对象加锁 synchronized (Singleton.class) &#123; if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; &#125; &#125; return uniqueInstance; &#125;&#125; 另外，需要注意 uniqueInstance 采用 volatile 关键字修饰也是很有必要。 uniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); 这段代码其实是分为三步执行： 为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1-&gt;3-&gt;2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。 使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。 构造方法不能使用synchronized 关键词修饰。因为构造方法本身就属于线程安全的，不存在同步的构造方法一说。 2.1 synchronized 关键字的底层原理2.1.1 synchronized 同步语句块的情况1234567public class SynchronizedDemo &#123; public void method() &#123; synchronized (this) &#123; System.out.println(&quot;synchronized 代码块&quot;); &#125; &#125;&#125; 从上面我们可以看出： synchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。 当执行 monitorenter 指令时，线程试图获取锁也就是获取 对象监视器 monitor 的持有权。 在执行monitorenter时，会尝试获取对象的锁，如果锁的计数器为 0 则表示锁可以被获取，获取后将锁计数器设为 1 也就是加 1。 在执行 monitorexit 指令后，将锁计数器设为 0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。 2.1.2 synchronized 修饰方法的的情况12345public class SynchronizedDemo2 &#123; public synchronized void method() &#123; System.out.println(&quot;synchronized 方法&quot;); &#125;&#125; synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法。JVM 通过该 ACC_SYNCHRONIZED 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。 2.1.3 总结synchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。 synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法。 不过两者的本质都是对对象监视器 monitor 的获取。 2.2 说说 JDK1.6 之后的 synchronized 关键字底层做了哪些优化，可以详细介绍一下这些优化吗？JDK1.6 对锁的实现引入了大量的优化，如偏向锁、轻量级锁、自旋锁、适应性自旋锁、锁消除、锁粗化等技术来减少锁操作的开销。 锁主要存在四种状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。注意锁可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率。 2.3 谈谈 synchronized 和 ReentrantLock 的区别2.3.1 两者都是可重入锁“可重入锁” 指的是自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果不可锁重入的话，就会造成死锁。同一个线程每次获取锁，锁的计数器都自增 1，所以要等到锁的计数器下降为 0 时才能释放锁。 2.3.2 synchronized 依赖于 JVM 而 ReentrantLock 依赖于 APIsynchronized 是依赖于 JVM 实现的，前面我们也讲到了 虚拟机团队在 JDK1.6 为 synchronized 关键字进行了很多优化，但是这些优化都是在虚拟机层面实现的，并没有直接暴露给我们。ReentrantLock 是 JDK 层面实现的（也就是 API 层面，需要 lock() 和 unlock() 方法配合 try/finally 语句块来完成），所以我们可以通过查看它的源代码，来看它是如何实现的。 2.3.3 ReentrantLock 比 synchronized 增加了一些高级功能相比synchronized，ReentrantLock增加了一些高级功能。主要来说主要有三点： 等待可中断 : ReentrantLock提供了一种能够中断等待锁的线程的机制，通过 lock.lockInterruptibly() 来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。 可实现公平锁 : ReentrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。ReentrantLock默认情况是非公平的，可以通过 ReentrantLock类的ReentrantLock(boolean fair)构造方法来制定是否是公平的。 可实现选择性通知（锁可以绑定多个条件）: synchronized关键字与wait()和notify()/notifyAll()方法相结合可以实现等待/通知机制。ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition()方法。 Condition是 JDK1.5 之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用notify()/notifyAll()方法进行通知时，被通知的线程是由 JVM 选择的，用ReentrantLock类结合Condition实例可以实现“选择性通知” ，这个功能非常重要，而且是 Condition 接口默认提供的。而synchronized关键字就相当于整个 Lock 对象中只有一个Condition实例，所有的线程都注册在它一个身上。如果执行notifyAll()方法的话就会通知所有处于等待状态的线程这样会造成很大的效率问题，而Condition实例的signalAll()方法 只会唤醒注册在该Condition实例中的所有等待线程。 3、参考synchronized 关键字","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"synchronized","slug":"synchronized","permalink":"https://xmmarlowe.github.io/tags/synchronized/"}],"author":"Marlowe"},{"title":"JDK动态代理和CGLIB动态代理","slug":"设计模式/JDK动态代理和CGLIB动态代理","date":"2021-03-15T06:21:31.000Z","updated":"2021-08-24T13:47:06.580Z","comments":true,"path":"2021/03/15/设计模式/JDK动态代理和CGLIB动态代理/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/15/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/JDK%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E5%92%8CCGLIB%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/","excerpt":"","text":"1、代理模式简介我们使用代理对象来代替对真实对象(real object)的访问，这样就可以在不修改原目标对象的前提下，提供额外的功能操作，扩展目标对象的功能。 作用代理模式的主要作用是扩展目标对象的功能，比如说在目标对象的某个方法执行前后你可以增加一些自定义的操作。 2、静态代理静态代理中，我们对目标对象的每个方法的增强都是手动完成的（后面会具体演示代码_），非常不灵活（_比如接口一旦新增加方法，目标对象和代理对象都要进行修改_）且麻烦(_需要对每个目标类都单独写一个代理类)。 实际应用场景非常非常少，日常开发几乎看不到使用静态代理的场景。 上面我们是从实现和应用角度来说的静态代理，从 JVM 层面来说， 静态代理在编译时就将接口、实现类、代理类这些都变成了一个个实际的 class 文件。 静态代理实现步骤: 定义一个接口及其实现类； 创建一个代理类同样实现这个接口 将目标对象注入进代理类，然后在代理类的对应方法调用目标类中的对应方法。这样的话，我们就可以通过代理类屏蔽对目标对象的访问，并且可以在目标方法执行前后做一些自己想做的事情。 见下面代码展示：1.定义发送短信的接口 123public interface SmsService &#123; String send(String message);&#125; 2.实现发送短信的接口 123456public class SmsServiceImpl implements SmsService &#123; public String send(String message) &#123; System.out.println(&quot;send message:&quot; + message); return message; &#125;&#125; 3.创建代理类并同样实现发送短信的接口 123456789101112131415161718public class SmsProxy implements SmsService &#123; private final SmsService smsService; public SmsProxy(SmsService smsService) &#123; this.smsService = smsService; &#125; @Override public String send(String message) &#123; //调用方法之前，我们可以添加自己的操作 System.out.println(&quot;before method send()&quot;); smsService.send(message); //调用方法之后，我们同样可以添加自己的操作 System.out.println(&quot;after method send()&quot;); return null; &#125;&#125; 4.实际使用 123456789101112131415161718public class SmsProxy implements SmsService &#123; private final SmsService smsService; public SmsProxy(SmsService smsService) &#123; this.smsService = smsService; &#125; @Override public String send(String message) &#123; //调用方法之前，我们可以添加自己的操作 System.out.println(&quot;before method send()&quot;); smsService.send(message); //调用方法之后，我们同样可以添加自己的操作 System.out.println(&quot;after method send()&quot;); return null; &#125;&#125; 运行上述代码之后，控制台打印出： 123before method send()send message:javaafter method send() 通过输出结果看出，我们已经增加了SmsServiceImpl 的send()方法。 3、动态代理3.1、JDK动态代理机制3.1.1 介绍：在 Java 动态代理机制中 InvocationHandler 接口和 Proxy 类是核心。Proxy 类中使用频率最高的方法是：newProxyInstance() ，这个方法主要用来生成一个代理对象。这个方法一共有 3 个参数： loader :类加载器，用于加载代理对象。 interfaces : 被代理类实现的一些接口。 h : 实现了 InvocationHandler 接口的对象。 要实现动态代理的话，还必须需要实现InvocationHandler 来自定义处理逻辑。 当我们的动态代理对象调用一个方法时候，这个方法的调用就会被转发到实现InvocationHandler 接口类的 invoke 方法来调用。 也就是说：你通过Proxy 类的 newProxyInstance() 创建的代理对象在调用方法的时候，实际会调用到实现InvocationHandler 接口的类的 invoke()方法。 你可以在 invoke() 方法中自定义处理逻辑，比如在方法执行前后做什么事情。 3.1.2 JDK 动态代理类使用步骤 定义一个接口及其实现类； 自定义 InvocationHandler 并重写invoke方法，在 invoke 方法中我们会调用原生方法（被代理类的方法）并自定义一些处理逻辑； 通过 Proxy.newProxyInstance(ClassLoader loader,Class&lt;?&gt;[] interfaces,InvocationHandler h) 方法创建代理对象； 3.1.3 代码示例1.定义发送短信的接口 123public interface SmsService &#123; String send(String message);&#125; 2.实现发送短信的接口 123456public class SmsServiceImpl implements SmsService &#123; public String send(String message) &#123; System.out.println(&quot;send message:&quot; + message); return message; &#125;&#125; 3.定义一个 JDK 动态代理类 12345678910111213141516171819202122232425import java.lang.reflect.InvocationHandler;import java.lang.reflect.InvocationTargetException;import java.lang.reflect.Method;public class DebugInvocationHandler implements InvocationHandler &#123; /** * 代理类中的真实对象 */ private final Object target; public DebugInvocationHandler(Object target) &#123; this.target = target; &#125; public Object invoke(Object proxy, Method method, Object[] args) throws InvocationTargetException, IllegalAccessException &#123; //调用方法之前，我们可以添加自己的操作 System.out.println(&quot;before method &quot; + method.getName()); Object result = method.invoke(target, args); //调用方法之后，我们同样可以添加自己的操作 System.out.println(&quot;after method &quot; + method.getName()); return result; &#125;&#125; invoke() 方法: 当我们的动态代理对象调用原生方法的时候，最终实际上调用到的是 invoke() 方法，然后 invoke() 方法代替我们去调用了被代理对象的原生方法。4.获取代理对象的工厂类 123456789public class JdkProxyFactory &#123; public static Object getProxy(Object target) &#123; return Proxy.newProxyInstance( target.getClass().getClassLoader(), // 目标类的类加载 target.getClass().getInterfaces(), // 代理需要实现的接口，可指定多个 new DebugInvocationHandler(target) // 代理对象对应的自定义 InvocationHandler ); &#125;&#125; getProxy() ：主要通过Proxy.newProxyInstance（）方法获取某个类的代理对象5.实际使用 12SmsService smsService = (SmsService) JdkProxyFactory.getProxy(new SmsServiceImpl());smsService.send(&quot;java&quot;); 运行上述代码之后，控制台打印出： 123before method sendsend message:javaafter method send 3.2、CGLIB 动态代理机制3.2.1 介绍JDK 动态代理有一个最致命的问题是其只能代理实现了接口的类。 为了解决这个问题，我们可以用 CGLIB 动态代理机制来避免。 CGLIB(Code Generation Library)是一个基于ASM的字节码生成库，它允许我们在运行时对字节码进行修改和动态生成。CGLIB 通过继承方式实现代理。很多知名的开源框架都使用到了CGLIB， 例如 Spring 中的 AOP 模块中：如果目标对象实现了接口，则默认采用 JDK 动态代理，否则采用 CGLIB 动态代理。 在 CGLIB 动态代理机制中 MethodInterceptor 接口和 Enhancer 类是核心。 你需要自定义 MethodInterceptor 并重写 intercept 方法，intercept 用于拦截增强被代理类的方法。 123456public interface MethodInterceptorextends Callback&#123; // 拦截被代理类中的方法 public Object intercept(Object obj, java.lang.reflect.Method method, Object[] args, MethodProxy proxy) throws Throwable;&#125; obj :被代理的对象（需要增强的对象） method :被拦截的方法（需要增强的方法） args :方法入参 methodProxy :用于调用原始方法 你可以通过 Enhancer类来动态获取被代理类，当代理类调用方法的时候，实际调用的是 MethodInterceptor 中的 intercept 方法。 3.2.2 CGLIB 动态代理类使用步骤 定义一个类； 自定义 MethodInterceptor 并重写 intercept 方法，intercept 用于拦截增强被代理类的方法，和 JDK 动态代理中的 invoke 方法类似； 通过 Enhancer 类的 create()创建代理类 3.2.3 代码示例不同于 JDK 动态代理不需要额外的依赖。CGLIB(Code Generation Library) 实际是属于一个开源项目，如果你要使用它的话，需要手动添加相关依赖。 12345&lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt;&lt;/dependency&gt; 1.实现一个使用阿里云发送短信的类 12345678package github.javaguide.dynamicProxy.cglibDynamicProxy;public class AliSmsService &#123; public String send(String message) &#123; System.out.println(&quot;send message:&quot; + message); return message; &#125;&#125; 2.自定义 MethodInterceptor（方法拦截器） 12345678910111213141516171819202122232425262728import net.sf.cglib.proxy.MethodInterceptor;import net.sf.cglib.proxy.MethodProxy;import java.lang.reflect.Method;/** * 自定义MethodInterceptor */public class DebugMethodInterceptor implements MethodInterceptor &#123; /** * @param o 被代理的对象（需要增强的对象） * @param method 被拦截的方法（需要增强的方法） * @param args 方法入参 * @param methodProxy 用于调用原始方法 */ @Override public Object intercept(Object o, Method method, Object[] args, MethodProxy methodProxy) throws Throwable &#123; //调用方法之前，我们可以添加自己的操作 System.out.println(&quot;before method &quot; + method.getName()); Object object = methodProxy.invokeSuper(o, args); //调用方法之后，我们同样可以添加自己的操作 System.out.println(&quot;after method &quot; + method.getName()); return object; &#125;&#125; 3.获取代理类import net.sf.cglib.proxy.Enhancer; public class CglibProxyFactory { public static Object getProxy(Class&lt;?&gt; clazz) &#123; // 创建动态代理增强类 Enhancer enhancer = new Enhancer(); // 设置类加载器 enhancer.setClassLoader(clazz.getClassLoader()); // 设置被代理类 enhancer.setSuperclass(clazz); // 设置方法拦截器 enhancer.setCallback(new DebugMethodInterceptor()); // 创建代理类 return enhancer.create(); &#125; } 12345```**4.实际使用**```javaAliSmsService aliSmsService = (AliSmsService) CglibProxyFactory.getProxy(AliSmsService.class);aliSmsService.send(&quot;java&quot;); 运行上述代码之后，控制台打印出： 123before method sendsend message:javaafter method send 3.3 JDK 动态代理和 CGLIB 动态代理对比 JDK 动态代理只能只能代理实现了接口的类或者直接代理接口，而 CGLIB 可以代理未实现任何接口的类。 另外， CGLIB 动态代理是通过生成一个被代理类的子类来拦截被代理类的方法调用，因此不能代理声明为 final 类型的类和方法。 就二者的效率来说，大部分情况都是 JDK 动态代理更优秀，随着 JDK 版本的升级，这个优势更加明显。 静态代理和动态代理的对比 灵活性 ：动态代理更加灵活，不需要必须实现接口，可以直接代理实现类，并且可以不需要针对每个目标类都创建一个代理类。另外，静态代理中，接口一旦新增加方法，目标对象和代理对象都要进行修改，这是非常麻烦的！ JVM 层面 ：静态代理在编译时就将接口、实现类、代理类这些都变成了一个个实际的 class 文件。而动态代理是在运行时动态生成类字节码，并加载到 JVM 中的。 参考代理模式详解","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"JDK","slug":"JDK","permalink":"https://xmmarlowe.github.io/tags/JDK/"},{"name":"动态代理","slug":"动态代理","permalink":"https://xmmarlowe.github.io/tags/%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/"}],"author":"Marlowe"},{"title":"代理模式之静态代理和动态代理","slug":"春招面试/代理模式之静态代理和动态代理","date":"2021-03-15T06:20:34.000Z","updated":"2021-03-16T01:57:29.483Z","comments":true,"path":"2021/03/15/春招面试/代理模式之静态代理和动态代理/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/15/%E6%98%A5%E6%8B%9B%E9%9D%A2%E8%AF%95/%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%E4%B9%8B%E9%9D%99%E6%80%81%E4%BB%A3%E7%90%86%E5%92%8C%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/","excerpt":"","text":"","categories":[],"tags":[],"author":"Marlowe"},{"title":"Spring是什么？","slug":"Spring/Spring是什么？","date":"2021-03-11T07:42:30.000Z","updated":"2021-04-15T09:09:24.575Z","comments":true,"path":"2021/03/11/Spring/Spring是什么？/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/11/Spring/Spring%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/","excerpt":"轻量级的开源的J2EE框架。它是一个容器框架，用来装JavaBean，也是一个中间层框架，可以起连接作用。","text":"轻量级的开源的J2EE框架。它是一个容器框架，用来装JavaBean，也是一个中间层框架，可以起连接作用。 Spring是什么？Spring是一个轻量级的控制反转(IOC)和面向切面(AOP)的容器框架。 1、Spring的核心是一个轻量级（Lightweight）的容器（Container）。2、Spring是实现IoC（Inversion of Control）容器和非入侵性（No intrusive）的框架。3、Spring提供AOP（Aspect-oriented programming）概念的实现方式。4、Spring提供对持久层（Persistence）、事物（Transcation）的支持。5、Spring供MVC Web框架的实现，并对一些常用的企业服务API（Application Interface）提供一致的模型封装。6、Spring提供了对现存的各种框架（Structs、JSF、Hibernate、Ibatis、Webwork等）相整合的方案。总之，Spring是一个全方位的应用程序框架。 对AOP的理解AOP:将程序中的交叉业务逻辑(比如安全，日志，事务等)，封装成一个切面,然后注入到目标对象(具体业务逻辑)中去。AOP可以对某个对象或某些对象的功能进行增强，比如对象中的方法进行增强，可以在执行某个方法之前额外的做一些事情，在某个方法执行之后额外的做一些事情。 对IoC的理解IOC:控制反转也叫依赖注入，IOC利用java反射机制，所谓控制反转是指，本来被调用者的实例是由调用者来创建的，这样的缺点是耦合性太强，IOC则是统一交给spring来管理创建，将对象交给容器管理，你只需要在spring配置文件中配置相应的bean，以及设置相关的属性，让spring容器来生成类的实例对象以及管理对象。在spring容器启动的时候，spring会把你在配置文件中配置的bean都初始化好，然后在你需要调用的时候，就把它已经初始化好的那些bean分配给你需要调用这些bean的类。 控制反转:没有引入IOC容器之前，对象A依赖于对象B,那么对象A在初始化或者运行到某一点的时候， 自己必须主动去创建对象B或者使用已经创建的对象B。无论是创建还是使用对象B,控制权都在自己手上。 引入IOC容器之后,对象A与对象B之间失去了直接联系,当对象A运行到需要对象B的时候，IOC容器 会主动创建一个对象B注入到对象A需要的地方。 通过前后的对比，不难看出来:对象A获得依赖对象B的过程,由主动行为变为了被动行为,控制权颠倒过来，这就是”控制反转”这个名称的由来。 全部对象的控制权全部上缴给”第三方”IOC容器,所以，IOC容器成了整个系统的关键核心，它起到了一种类似“粘合剂”的作用，把系统中的所有对象粘合在一起发挥作用，如果没有这个”粘合剂”，对象与对象之间会彼此失去联系,这就是有人把IOC容器比喻成”粘合剂”的由来。 依赖注入:“获得依赖对象的过程被反转了”。控制被反转之后，获得依赖对象的过程由自身管理变为了由IOC容器主动注入。依赖注入是实现IOC的方法,就是由IOC容器在运行期间，动态地将某种依赖关系注入到对象之中。","categories":[{"name":"春招面试","slug":"春招面试","permalink":"https://xmmarlowe.github.io/categories/%E6%98%A5%E6%8B%9B%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/tags/Spring/"}],"author":"Marlowe"},{"title":"线程池中阻塞队列的作用?为什么是先添加列队而不是先创建最大线程?线程池中线程复用原理","slug":"并发/线程池中阻塞队列的作用-为什么是先添加列队而不是先创建最大线程-线程池中线程复用原理","date":"2021-03-11T06:35:10.000Z","updated":"2021-04-16T07:00:55.029Z","comments":true,"path":"2021/03/11/并发/线程池中阻塞队列的作用-为什么是先添加列队而不是先创建最大线程-线程池中线程复用原理/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/11/%E5%B9%B6%E5%8F%91/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%B8%AD%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97%E7%9A%84%E4%BD%9C%E7%94%A8-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%98%AF%E5%85%88%E6%B7%BB%E5%8A%A0%E5%88%97%E9%98%9F%E8%80%8C%E4%B8%8D%E6%98%AF%E5%85%88%E5%88%9B%E5%BB%BA%E6%9C%80%E5%A4%A7%E7%BA%BF%E7%A8%8B-%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%B8%AD%E7%BA%BF%E7%A8%8B%E5%A4%8D%E7%94%A8%E5%8E%9F%E7%90%86/","excerpt":"","text":"线程池中阻塞队列的作用?一般的队列只能保证作为一个有限长度的缓冲区,如果超出了缓冲长度,就无法保留当前的任务了,阻塞队列通过阻塞可以保留住当前想要继续入队的任务。阻塞队列可以保证任务队列中没有任务时阻塞获取任务的线程,使得线程进入wait状态,释放cpu资源。阻塞队列自带阻塞和唤醒的功能,不需要额外处理,无任务执行时,线程池利用阻塞队列的take方法挂起,从而维持核心线程的存活、不至于一直占用cpu资源 为什么是先添加列队而不是先创建最大线程?在创建新线程的时候,是要获取全局锁的,这个时候其它的就得阻塞,影响了整体效率。 就好比一个饭店里面有10个(core)正式工的名额,最多招10个正式工,要是任务超过正式工人数(task&gt;core)的情况下,工厂领导(线程池)不是首先扩招工人,还是这10人,但是任务可以稍微积压一下,即先放到队列去(代价低) 。10个正式工慢慢干,迟早会千完的,要是任务还在继续增加,超过正式工的加班忍耐极限了(队列满了) ,就的招外包帮忙了(注意是临时工)要是正式工加上外包还是不能完成任务,那新来的任务就会被领导拒绝了(线程池的拒绝策略) 线程池中线程复用原理线程池将线程和任务进行解耦,线程是线程,任务是任务,摆脱了之前通过Thread创建线程时的一个线程必须对应一个任务的限制。 在线程池中,同一个线程可以从阻塞队列中不断获取新任务来执行,其核心原理在于线程池对Thread进行了封装,并不是每次执行任务都会调用Thread.start（)来创建新线程, 而是让每个线程去执行一个”循环任务”,在这个”循环任务”中不停检查是否有任务需要被执行,如果有则直接执行,也就是调用任务中的run方法,将run方法当成一个普通的方法执行,通过这种方式只使用固定的线程就将所有任务的run方法串联起来。 参考线程池中阻塞队列的作用?为什么是先添加列队而不是先创建最大线程?线程池中线程复用原理","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"线程","slug":"线程","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B/"}],"author":"Marlowe"},{"title":"为什么用线程池？线程池参数解释","slug":"并发/为什么用线程池？线程池参数解释","date":"2021-03-11T02:26:10.000Z","updated":"2021-05-21T01:41:17.239Z","comments":true,"path":"2021/03/11/并发/为什么用线程池？线程池参数解释/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/11/%E5%B9%B6%E5%8F%91/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8%E7%BA%BF%E7%A8%8B%E6%B1%A0%EF%BC%9F%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%8F%82%E6%95%B0%E8%A7%A3%E9%87%8A/","excerpt":"","text":"为什么使用线程池?1、降低资源消耗;提高线程利用率,降低创建和销毁线程的消耗。 2、提高响应速度;任务来了,直接有线程可用可执行,而不是先创建线程,再执行。 3、提高线程的可管理性;线程是稀缺资源,使用线程池可以统一分配调优监控。 corePoolsize代表核心线程数,也就是正常情况下创建工作的线程数,这些线程创建后并不会消除,而是一种常驻线程 maxinumPoolsize代表的是最大线程数,它与核心线程数相对应,表示最大允许被创建的线程数,比如当前任务较多,将核心线程数都用完了,还无法满足需求时,此时就会创建新的线程,但是线程池内线程总数不会超过最大线程数 keepAliverime, unit表示超出核心线程数之外的线程的空闲存活时间,也就是核心线程不会消除,但是超出核心线程数的部分线程如果空闲一定的时间则会被消除,我们可以通过setKeepAliveTime来设置空闲时间 workQueue用来存放待执行的任务,假设我们现在核心线程都已被使用,还有任务进来则全部放入队列,直到整个队列被放满但任务还再持续进入则会开始创建新的线程 ThreadFactory实际上是一个线程工厂,用来生产线程执行任务。我们可以选择使用默认的创建工厂,产生的线程都在同一个组内,拥有相同的优先级,且都不是守护线程。当然我们也可以选择自定义线程工厂,一般我们会根据业务来制定不同的线程工厂 Handler任务拒绝策略,有两种情况,第一种是当我们调用shutdown等方法关闭线程池后,这时候即使线程池内部还有没执行完的任务正在执行,但是由于线程池已经关闭,我们再继续想线程池提交任务就会遭到拒绝。另一种情况就是当达到最大线程数,线程池已经没有能力继续处理新提交的任务时,这时也就拒绝 执行线程池的流程线程池任务开始执行时，会先判断线程池是否已满，如果没有满则创建核心线程执行，如果核心线程已满那么就判断任务队列是否已满，未满则将任务放入到队列中，如果已满则判断最大线程数是否打到，未达到则创建临时线程执行，临时吸线程如果空闲时我们可以设置超时时间也就是KeepAliveTime，当达到超时时间临时线程则被回收。如果全部线程空间都满了那么我们可设置拒绝策略来处理。 参考文档为什么要用线程池，线程池的参数解释","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"线程","slug":"线程","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B/"}],"author":"Marlowe"},{"title":"并发的三大特性","slug":"并发/并发的三大特性","date":"2021-03-11T00:31:23.000Z","updated":"2021-04-14T07:33:57.274Z","comments":true,"path":"2021/03/11/并发/并发的三大特性/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/11/%E5%B9%B6%E5%8F%91/%E5%B9%B6%E5%8F%91%E7%9A%84%E4%B8%89%E5%A4%A7%E7%89%B9%E6%80%A7/","excerpt":"","text":"原子性定义：原子性是指在一个操作中cpu不可以在中途暂停然后再调度,即不被中断操作，要不全部执行完成，要不都不执行。就好比转账，从账户A向账户B转1000元，那么必然包括2个操作:从账户A减去1000元,往账户B加上1000元。2个操作必须全部完成。 关键字：synchronized 1234private long count = 0;public void calc()&#123; count++;&#125; 在上述代码中，将执行以下步骤： 将count从主存读取到工作内存中的副本 +1运算 将结果写入工作内存 将工作内存中的值刷回主存(什么时候刷入由操作系统决定，不确定的) 可见性定义：当一个线程修改了共享变量的值，其他线程会马上知道这个修改。当其他线程要读取这个变量的时候，最终会去内存中读取，而不是从缓存中读取。 关键字：volatile、synchronized、final 有序性定义：虚拟机在进行代码编译时，对于那些改变顺序之后不会对最终结果造成影响的代码，虚拟机不一定会按照我们写的代码的顺序来执行，有可能将他们重排序。实际上，对于有些代码进行重排序之后，虽然对变量的值没有造成影响，但有可能会出现线程安全问题。 1234567891011121314int a = 0;boolean flag = false;public void write()&#123; a = 2; //1 //1 flag = true; //2 //4&#125; public void multiply()&#123; if(flag)&#123; //3 //2 int res = a * a; //4 //3 &#125; &#125; 1234567如果按照1234执行，结果为：a = 2;res = 4;如果按照1423执行，结果为：a = 2;res = 0; 关键字：volatile、synchronized volatile本身就包含了禁止指令重排序的语义，而synchronized关键字是由“一个变量在同一时刻只允许一条线程对其进行lock操作”这条规则明确的。 小结 synchronized关键字同时满足以上三种特性，但是volatile关键字不满足原子性。 在某些情况下，volatile的同步机制的性能确实要优于锁(使用synchronized关键字或java.util.concurrent包里面的锁)，因为volatile的总开销要比锁低。 我们判断使用volatile还是加锁的唯一依据就是volatile的语义能否满足使用的场景(原子性) 参考文档高并发的三大特性—原子性、有序性、可见性","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/tags/%E5%B9%B6%E5%8F%91/"}],"author":"Marlowe"},{"title":"ThreadLocal原理和使用场景","slug":"并发/ThreadLocal原理和使用场景","date":"2021-03-10T07:31:01.000Z","updated":"2021-04-28T13:17:27.035Z","comments":true,"path":"2021/03/10/并发/ThreadLocal原理和使用场景/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/10/%E5%B9%B6%E5%8F%91/ThreadLocal%E5%8E%9F%E7%90%86%E5%92%8C%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF/","excerpt":"每一个Thread对象均含有一个ThreadLocalMap类型的成员变量threadLocals,它存储本线程中所有ThreadLocal对象及其对应的值。","text":"每一个Thread对象均含有一个ThreadLocalMap类型的成员变量threadLocals,它存储本线程中所有ThreadLocal对象及其对应的值。 简介ThreadLocal保存当前线程的变量，当前线程内，可以任意获取，但每个线程往ThreadLocal中读写数据是线程隔离，互不影响。 如果你创建了一个ThreadLocal变量，那么访问这个变量的每个线程都会有这个变量的本地副本，这也是ThreadLocal变量名的由来。他们可以使用 get（） 和 set（） 方法来获取默认值或将其值更改为当前线程所存的副本的值，从而避免了线程安全问题。 1234567891011ThreadLocalMap源码：static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125; &#125; ThreadLocalMap由一个个Entry对象构成Entry继承自WeakReference&lt;ThreadLoca1&lt;?&gt;&gt;,一个Entry由ThreadLocal对象和Object构成。由此可见，Entry 的key是ThreadLocal对象,并且是一个弱引用。 当没指向key的强引用后, 该key就会被垃圾收集器回收。 注意 ThreadLocal存在内存泄露强引用(StrongReference)： 使用最普遍的引用(new),一个对象具有强引用，不会被GC回收。当JVM的内存空间不足时，宁愿抛出OutOfMemoryError使得程序异常终止也不愿意回收具有强引用的存活着的对象。如果想取消强引用和某个对象之间的关联，可以显式的将引用赋值为null，这样可以是JVM在合适的时候回收该对象。 弱引⽤(WeakReference)： 在GC的时候，不管内存空间足不足都会回收这个对象。可以在缓存中使用弱引用。 当我们了解完，ThreadLocalMap 中使⽤的 key是以弱引用指向ThreadLocal，这时候垃圾回收器线程运行，发现弱引用就回收，key被回收。ThreadLocalMap里对应的Entry的key会变成null。这时候尴尬出现了，ThreadLocalMap里对应的Entry的value则无法被访问到，value作为一个强引用垃圾回收不到也不能被访问，即造成了内存溢出。 ThreadLocal正确的使用方法(如何解决内存泄漏) 在使用完ThreadLocal后，主动调用remove方法进行清理。 将ThreadLocal变量定义成private static, 这样就一 直存在ThreadLocal的强引用，也就能保证任何时候都能通过ThreadLocal的弱引用访问到Entry的value值， 进而清除掉。 123456789ThreadLocal set()方法public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); &#125; 当执行set方法时，ThreadLocal首先会获取当前线程对象，然后获取当前线程的ThreadLocalMap对象。再以当前ThreadLocal对象为key,将值存储进ThreadLocalMap对象中。 1234567891011121314ThreadLocal get()方法public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings(&quot;unchecked&quot;) T result = (T)e.value; return result; &#125; &#125; return setInitialValue(); &#125; get方法执行过程类似。ThreadLocal首先会获取当前线程对象,然后获取当前线程的ThreadLocalMap对象。再以当前ThreadLocal对象为key,获取对应的value。 由于每一条线程均含有各自私有的ThreadLocalMap容器，这些容器相互独立互不影响，因此不会存在线程安全性问题，从而也无需使用同步机制来保证多条线程访问容器的互斥性。 使用场景：1、在进行对象跨层传递的时候，使用ThreadLocal可以避免多次传递，打破层次间的约束。2、线程间数据隔离。3、进行事务操作，用于存储线程事务信息。4、数据库连接，Session会话管理。","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"线程","slug":"线程","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B/"}],"author":"Marlowe"},{"title":"浅谈对守护线程的理解","slug":"并发/浅谈对守护线程的理解","date":"2021-03-10T07:13:51.000Z","updated":"2021-05-14T06:37:50.479Z","comments":true,"path":"2021/03/10/并发/浅谈对守护线程的理解/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/10/%E5%B9%B6%E5%8F%91/%E6%B5%85%E8%B0%88%E5%AF%B9%E5%AE%88%E6%8A%A4%E7%BA%BF%E7%A8%8B%E7%9A%84%E7%90%86%E8%A7%A3/","excerpt":"为所有非守护线程提供服务的线程，也称后台线程，任何一个守护线程都是整个JVM中所有非守护线程的保姆。","text":"为所有非守护线程提供服务的线程，也称后台线程，任何一个守护线程都是整个JVM中所有非守护线程的保姆。 守护线程的作用举例，GC垃圾回收线程:就是一个经典的守护线程, 当我们的程序中不再有任何运行的Thread,程序就不会再产生垃圾，垃圾回收器也就无事可做,所以当垃圾回收线程是JVM.上仅剩的线程时,垃圾回收线程会自动离开。它始终在低级别的状态中运行，用于实时监控和管理系统中的可回收资源。 （守护线程必须在线程开启前设置！）thread.setDaemon(true)必须在thread.start()之前设置,否则会抛出一个llegalThreadStateException异常。 你不能把正在运行的常规线程设置为守护线程。 在守护(deamon)线程中产生的新线程也是守护线程","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"线程","slug":"线程","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B/"}],"author":"Marlowe"},{"title":"浅谈对线程安全的理解","slug":"并发/浅谈对线程安全的理解","date":"2021-03-10T06:46:58.000Z","updated":"2021-05-14T06:46:31.513Z","comments":true,"path":"2021/03/10/并发/浅谈对线程安全的理解/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/10/%E5%B9%B6%E5%8F%91/%E6%B5%85%E8%B0%88%E5%AF%B9%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E7%90%86%E8%A7%A3/","excerpt":"当多个线程访问一个对象时，如果不进行额外的同步控制或其他的协调操作，调用这个对象的行为都可以获得正确的结果，我们就说这个对象是线程安全的。","text":"当多个线程访问一个对象时，如果不进行额外的同步控制或其他的协调操作，调用这个对象的行为都可以获得正确的结果，我们就说这个对象是线程安全的。 简单来说，多线程情况下和单线程执行结果一样，就是线程安全的。 堆是进程和线程共有的空间，分全局堆和局部堆。全局堆就是所有没有分配的空间，局部堆就是用户分配的空间。堆在操作系统对进程初始化的时候分配，运行过程中也可以向系统要额外的堆,但是用完了要还给操作系统，要不然就是内存泄漏。 在Java中，堆是Java虚拟机所管理的内存中最大的一块,是所有线程共享的一块内存区域，在虚拟机启动时创建。堆所存在的内存区域的唯一目的就是存放对象实例， 几乎所有的对象实例以及数组都在这里分配内存。 栈是每个线程独有的，保存其运行状态和局部自动变量的。栈在线程开始的时候初始化，每个线程的栈互相独立,因此，栈是线程安全的。 操作系统在切换线程的时候会自动切换栈。栈空间不需要在高级语言里面显式的分配和释放。","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"线程","slug":"线程","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B/"}],"author":"Marlowe"},{"title":"sleep(),wait(),join(),yield()的区别","slug":"并发/sleep-wait-join-yield-的区别","date":"2021-03-09T09:12:15.000Z","updated":"2021-04-20T14:24:45.528Z","comments":true,"path":"2021/03/09/并发/sleep-wait-join-yield-的区别/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/09/%E5%B9%B6%E5%8F%91/sleep-wait-join-yield-%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"sleepwaitjoinyield 锁池所有需要竞争同步锁的线程都会放在锁池当中，比如当前对象的锁已经被其中一个线程得到，则其他线程需要在这个锁池等待，当前面的线程释放同步锁后锁池中的线程去竞争同步锁，当某个线程得到后会进入就绪队列进行等待cpu资源分配。 等待池当我们调用wait()方法后，线程会放到等待池当中，等待池的线程是不会去竞争同步锁。只有调用了notify()或notifyAll()后等待的线程才会开始去竞争锁，notify()是随机从等待池中选出一个线程放到锁池，而notifyAll()是将等待池的所有线程放到锁池当中。 sleep是Thread类的静态本地方法，wait是Object类的本地方法。 sleep方法不会释放lock，但wait会释放，而且会加入到等待队列中。1sleep就是把cpu的执行资格和执行权释放出去，不在运行此线程，当定时时间结束后再取回cpu资源，参与cpu的调度，获取到cpu资源后就可以继续运行了。而如果sleep时线程有所，那么sleep不会释放这个锁，而是把锁带着进入了冻结状态，也就是说其他需要这个锁的线程根本不可能获取到这个锁。也即无法执行程序。如果在睡眠期间其他线程调用了这个线程的interrupt方法，那么这个线程也会抛出interruptexception异常返回，这个点和wait是一样的。 sleep方法不依赖于同步器synchronized，但是wait需要依赖synchronized关键字。 sleep不需要被唤醒(休眠之后退出阻塞),但是wait需要(不指定时间需要被别人中断)。 sleep一般用于当前线程休眠，或者轮循暂停操作，wait则多用于多线程之间的通信。 sleep会放出cpu执行时间且强制上下文切换，而wait则不一定，wait后可能还是有机会重新竞争到锁继续执行的。 yield() 执行后线程直接进入就绪状态，马上释放了cpu的执行权，但是依然保留了cpu的执行资格，所以有可能cpu下次进行线程调度还会让这个线程获取到执行权继续执行。 join() 执行后线程进入阻塞状态，例如在线程B中调用线程A的join()，则线程B会进入到阻塞队列，直到线程A结束或中断线程。 1234567891011121314151617public static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;222222&quot;); &#125; &#125;); t1.start(); t1.join(); // 这行代码需等t1线程执行结束才会继续执行 System.out.println(&quot;11111&quot;); &#125; 123结果：22222211111","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"线程","slug":"线程","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B/"}],"author":"Marlowe"},{"title":"线程的生命周期包括哪几个阶段","slug":"并发/线程的生命周期包括哪几个阶段","date":"2021-03-09T08:49:03.000Z","updated":"2021-04-20T05:34:51.318Z","comments":true,"path":"2021/03/09/并发/线程的生命周期包括哪几个阶段/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/09/%E5%B9%B6%E5%8F%91/%E7%BA%BF%E7%A8%8B%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E5%8C%85%E6%8B%AC%E5%93%AA%E5%87%A0%E4%B8%AA%E9%98%B6%E6%AE%B5/","excerpt":"线程的生命周期包含5个阶段，包括：新建、就绪、运行、阻塞、销毁。","text":"线程的生命周期包含5个阶段，包括：新建、就绪、运行、阻塞、销毁。 新建（New）：就是刚使用new方法，new出来的线程。 就绪（Runnable）：就是调用的线程的start()方法。该状态的线程位于可运行线程池中，等待获取CPU的使用权。 运行（Running）：当就绪的线程被调度并获得CPU资源时，便进入运行状态。 阻塞（Blocked）：在运行状态的时候，可能因为某些原因导致运行状态的线程变成了阻塞状态，比如sleep()、wait()之后线程就处于了阻塞状态，这个时候需要其他机制将处于阻塞状态的线程唤醒，比如调用notify或者notifyAll()方法。唤醒的线程不会立刻执行run方法，它们要再次等待CPU分配资源进入运行状态。 销毁（Dead）：线程执行完了或者因异常退出了run方法，该线程结束生命周期。","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"线程","slug":"线程","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B/"}],"author":"Marlowe"},{"title":"什么是字节码？采用字节码的好处是什么？","slug":"春招面试/什么是字节码？采用字节码的好处是什么？","date":"2021-03-09T02:43:58.000Z","updated":"2021-03-10T14:29:28.660Z","comments":true,"path":"2021/03/09/春招面试/什么是字节码？采用字节码的好处是什么？/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/09/%E6%98%A5%E6%8B%9B%E9%9D%A2%E8%AF%95/%E4%BB%80%E4%B9%88%E6%98%AF%E5%AD%97%E8%8A%82%E7%A0%81%EF%BC%9F%E9%87%87%E7%94%A8%E5%AD%97%E8%8A%82%E7%A0%81%E7%9A%84%E5%A5%BD%E5%A4%84%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/","excerpt":"字节码：在Java中，供虚拟机理解的代码叫做字节码(也就是Java代码编译后的.class文件),他不面向任何特定的处理器，只面向虚拟机。","text":"字节码：在Java中，供虚拟机理解的代码叫做字节码(也就是Java代码编译后的.class文件),他不面向任何特定的处理器，只面向虚拟机。 Java中的编译器和解释器Java 中引入了虚拟机的概念，即在机器和编译程序之间加入了一层抽象的虚拟的机器。这台虚拟的机器在任何平台上都提供给编译程序一个的共同的接口。编译程序只需要面向虚拟机，生成虚拟机能够理解的代码，然后由解释器来将虚拟机代码转换为特定系统的机器码执行。在 Java 中，这种供虚拟机理解的代码叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。每一种平台的解释器是不同的，但是实现的虚拟机是相同的。Java 源程序经过编译器编译后变成字节码，字节码由虚拟机解释执行，虚拟机将每一条要执行的字节码送给解释器，解释器将其翻译成特定机器上的机器码，然后在特定的机器上运行。这也就是解释了 Java 的编译与解释并存的特点。 采用字节码的好处Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以 Java 程序运行时比较高效，而且，由于字节码并不专对一种特定的机器，因此，Java程序无须重新编译便可在多种不同的计算机上运行。","categories":[{"name":"春招面试","slug":"春招面试","permalink":"https://xmmarlowe.github.io/categories/%E6%98%A5%E6%8B%9B%E9%9D%A2%E8%AF%95/"}],"tags":[],"author":"Marlowe"},{"title":"JWTUtils","slug":"自定义工具类/JWTUtils","date":"2021-03-02T06:36:50.000Z","updated":"2021-03-02T06:45:38.286Z","comments":true,"path":"2021/03/02/自定义工具类/JWTUtils/","link":"","permalink":"https://xmmarlowe.github.io/2021/03/02/%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B7%A5%E5%85%B7%E7%B1%BB/JWTUtils/","excerpt":"Jwt 学习","text":"Jwt 学习 引入jwt依赖12345&lt;dependency&gt; &lt;groupId&gt;com.auth0&lt;/groupId&gt; &lt;artifactId&gt;java-jwt&lt;/artifactId&gt; &lt;version&gt;3.4.0&lt;/version&gt;&lt;/dependency&gt; 编写JWTUtils工具类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class JWTUtils &#123; private static final String SIGN = &quot;!QDJHFKSHFK:&quot;; /** * 生成token header.payload.sign * * @param map * @return */ public static String getToken(Map&lt;String, String&gt; map) &#123; Calendar instance = Calendar.getInstance(); // 默认7天过期 instance.add(Calendar.DATE, 7); // 创建jwt builder JWTCreator.Builder builder = JWT.create(); // payload map.forEach((k, v) -&gt; &#123; builder.withClaim(k, v); &#125;); // 指定令牌过期时间和签名 String token = builder.withExpiresAt(instance.getTime()) .sign(Algorithm.HMAC256(SIGN)); return token; &#125; /** * 验证token 合法性 * * @param token */ public static void verify(String token) &#123; JWT.require(Algorithm.HMAC256(SIGN)).build().verify(token); &#125; /** * 获取token信息 * * @param token * @return */ public static DecodedJWT getTokenInfo(String token) &#123; DecodedJWT verify = JWT.require(Algorithm.HMAC256(SIGN)).build().verify(token); return verify; &#125;&#125; 编写JWTinterceptor类12345678910111213141516171819202122232425262728293031public class JWTInterceptor implements HandlerInterceptor &#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; Map&lt;Object, Object&gt; map = new HashMap&lt;&gt;(); // 获取请求头令牌 String token = request.getHeader(&quot;token&quot;); try &#123; // 验证令牌 JWTUtils.verify(token); // 放行请求 return true; &#125; catch (SignatureVerificationException e) &#123; e.printStackTrace(); map.put(&quot;msg&quot;, &quot;无效签名&quot;); &#125; catch (TokenExpiredException e) &#123; e.printStackTrace(); map.put(&quot;msg&quot;, &quot;token过期&quot;); &#125; catch (AlgorithmMismatchException e) &#123; e.printStackTrace(); map.put(&quot;msg&quot;, &quot;token算法不一致&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); map.put(&quot;msg&quot;, &quot;token 无效&quot;); &#125; map.put(&quot;state&quot;, false); String json = new ObjectMapper().writeValueAsString(map); response.setContentType(&quot;application/json;charset=UTF-8&quot;); response.getWriter().println(json); return false; &#125;&#125; 编写拦截器配置类123456789@Configurationpublic class InterceptorConfig implements WebMvcConfigurer&#123; @Override public vpid addInterceptors(InterceptorRegistry registry)&#123; registry.addInterceptor(new JWTInterceptor()) .addPathPatterns(&quot;/xxx&quot;) .excludePathPatterns(&quot;/xxx&quot;); &#125;&#125;","categories":[{"name":"自定义工具类","slug":"自定义工具类","permalink":"https://xmmarlowe.github.io/categories/%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B7%A5%E5%85%B7%E7%B1%BB/"}],"tags":[{"name":"JWT","slug":"JWT","permalink":"https://xmmarlowe.github.io/tags/JWT/"}],"author":"Marlowe"},{"title":"浅谈DNS协议","slug":"计算机网络/浅谈DNS协议","date":"2021-02-22T16:40:29.000Z","updated":"2021-08-25T00:10:58.760Z","comments":true,"path":"2021/02/23/计算机网络/浅谈DNS协议/","link":"","permalink":"https://xmmarlowe.github.io/2021/02/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E6%B5%85%E8%B0%88DNS%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"简介DNS（Domain Name System，域名系统），万维网上作为域名和IP地址相互映射的一个分布式数据库，能够使用户更方便的访问互联网，而不用去记住IP。通过域名，最终得到该域名对应的IP地址的过程叫做域名解析（或主机名解析）。 DNS 可以使用 UDP 或者 TCP 进行传输，使用的端口号都为 53。大多数情况下 DNS 使用 UDP 进行传输，这就要求域名解析器和域名服务器都必须自己处理超时和重传从而保证可靠性。在两种情况下会使用 TCP 进行传输： 如果返回的响应超过的 512 字节（UDP 最大只支持 512 字节的数据）。 区域传送（区域传送是主域名服务器向辅助域名服务器传送变化的那部分数据）。 DNS查找过程DNS解析是一个递归查询的过程。 浏览器访问 www.baidu.com 查找浏览器缓存 查找dns解析器缓存host 查找本地dns服务器缓存 查找根dns服务器缓存，找到了返回对应后缀的dns服务器地址ip（比如com DNS服务器） 查找com dns服务器，返回baidu.com的dns服务器ip 查找baidu.com dns服务器ip 得到baidu.com服务器ip，写入缓存。 浏览器拿到ip进行访问。 查找www.google.com的IP地址过程 首先在本地域名服务器中查询IP地址，如果没有找到的情况下，本地域名服务器会向根域名服务器发送一个请求，如果根域名服务器也不存在该域名时，本地域名会向com顶级域名服务器发送一个请求，依次类推下去。直到最后本地域名服务器得到google的IP地址并把它缓存到本地，供下次查询使用。从上述过程中，可以看出网址的解析是一个从右向左的过程: com -&gt; google.com -&gt; www.google.com。但是你是否发现少了点什么，根域名服务器的解析过程呢？事实上，真正的网址是 www.google.com.，并不是我多打了一个.，这个.对应的就是根域名服务器，默认情况下所有的网址的最后一位都是.，既然是默认情况下，为了方便用户，通常都会省略，浏览器在请求DNS的时候会自动加上，所有网址真正的解析过程为: . -&gt; .com -&gt; google.com. -&gt; www.google.com.。 使用的协议 TCP：与服务器建立TCP连接 IP：建立TCP协议时，需要发送数据，发送数据在网络层使用IP协议 OSPF：开放最短路径优先协议,是由Internet工程任务组开发的路由选择协议 ARP：路由器在与服务器通信时，需要将ip地址转换为MAC地址，需要使用ARP协议 HTTP：在TCP建立完成后，使用HTTP协议访问网页 DNS的记录类型 其中CNAME解析就是将域名解析到另一个域名。 DNS负载均衡不知道大家有没有思考过一个问题: DNS返回的IP地址是否每次都一样？如果每次都一样是否说明你请求的资源都位于同一台机器上面，那么这台机器需要多高的性能和储存才能满足亿万请求呢？其实真实的互联网世界背后存在成千上百台服务器，大型的网站甚至更多。但是在用户的眼中，它需要的只是处理他的请求，哪台机器处理请求并不重要。 DNS可以返回一个合适的机器的IP给用户，例如可以根据每台机器的负载量，该机器离用户地理位置的距离等等，这种过程就是DNS负载均衡，又叫做DNS重定向。大家耳熟能详的CDN(Content Delivery Network)就是 利用DNS的重定向技术，DNS服务器会返回一个跟用户最接近的点的IP地址给用户，CDN节点的服务器负责响应用户的请求，提供所需的内容。 DNS劫持与污染DNS劫持DNS决定的是我们的域名将解析到哪一个IP地址的记录，是基于UDP协议的一种应用层协议。这种攻击的前提是攻击者掌控了你的本地DNS服务器 攻击者劫持了DNS服务器，通过某些手段取得某域名的解析记录控制权，进而修改此域名的解析结果，导致用户对该域名地址进行访问的时候，由原来的IP地址转入到修改后的IP地址。结果就是让正确的网址不能解析或者是被解析到另一个网址的IP，实现获取用户资料或者破坏原有网址正常服务的目的。 简单来说就是就是ip解析请求发送到了其他DNS服务器了，给你返回了一个错误的ip。 DNS污染又称域名服务器缓存投毒（DNS cache poisoning），它和DNS劫持的不同之处，在于污染针对的是DNS缓存，是在查询信息到达目标DNS服务器前，经过的节点上做手脚，而劫持是DNS服务器中记录的是错误的内容。 参考浅谈DNS协议 前端经典面试题: 从输入URL到页面加载发生了什么？","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"DNS","slug":"DNS","permalink":"https://xmmarlowe.github.io/tags/DNS/"}],"author":"Marlowe"},{"title":"TCP流量控制、拥塞控制","slug":"计算机网络/TCP流量控制、拥塞控制","date":"2021-02-22T16:24:56.000Z","updated":"2021-04-23T14:26:00.979Z","comments":true,"path":"2021/02/23/计算机网络/TCP流量控制、拥塞控制/","link":"","permalink":"https://xmmarlowe.github.io/2021/02/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E3%80%81%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6/","excerpt":"","text":"一、流量控制什么是流量控制？流量控制的目的？如果发送者发送数据过快，接收者来不及接收，那么就会有分组丢失。为了避免分组丢失，控制发送者的发送速度，使得接收者来得及接收，这就是流量控制。流量控制根本目的是防止分组丢失，它是构成TCP可靠性的一方面。 如何实现流量控制？由滑动窗口协议（连续ARQ协议）实现。滑动窗口协议既保证了分组无差错、有序接收，也实现了流量控制。主要的方式就是接收方返回的 ACK 中会包含自己的接收窗口的大小，并且利用大小来控制发送方的数据发送。 流量控制引发的死锁？怎么避免死锁的发生？当发送者收到了一个窗口为0的应答，发送者便停止发送，等待接收者的下一个应答。但是如果这个窗口不为0的应答在传输过程丢失，发送者一直等待下去，而接收者以为发送者已经收到该应答，等待接收新数据，这样双方就相互等待，从而产生死锁。为了避免流量控制引发的死锁，TCP使用了持续计时器。每当发送者收到一个零窗口的应答后就启动该计时器。时间一到便主动发送报文询问接收者的窗口大小。若接收者仍然返回零窗口，则重置该计时器继续等待；若窗口不为0，则表示应答报文丢失了，此时重置发送窗口后开始发送，这样就避免了死锁的产生。 二、拥塞控制和流量控制的区别拥塞控制： 拥塞控制是作用于网络的，它是防止过多的数据注入到网络中，避免出现网络负载过大的情况；常用的方法就是：（ 1 ）慢开始、拥塞避免（ 2 ）快重传、快恢复。 流量控制： 流量控制是作用于接收者的，它是控制发送者的发送速度从而使接收者来得及接收，防止分组丢失的。 三、拥塞控制的算法（一）慢开始算法：发送方维持一个叫做拥塞窗口cwnd（congestion window）的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。发送方让自己的发送窗口等于拥塞窗口，另外考虑到接受方的接收能力，发送窗口可能小于拥塞窗口。 慢开始算法的思路就是，不要一开始就发送大量的数据，先探测一下网络的拥塞程度，也就是说由小到大逐渐增加拥塞窗口的大小。cwnd初始值为1，每经过一个传播轮次，cwnd加倍。 这里用报文段的个数作为拥塞窗口的大小举例说明慢开始算法，实际的拥塞窗口大小是以字节为单位的。如下图： 从上图可以看到，一个传输轮次所经历的时间其实就是往返时间RTT，而且每经过一个传输轮次（transmission round），拥塞窗口cwnd就加倍。 为了防止cwnd增长过大引起网络拥塞，还需设置一个慢开始门限ssthresh状态变量。ssthresh的用法如下：当cwnd &lt; ssthresh时，使用慢开始算法。当cwnd&gt;ssthresh时，改用拥塞避免算法。当cwnd=ssthresh时，慢开始与拥塞避免算法任意 注意，这里的“慢”并不是指cwnd的增长速率慢，而是指在TCP开始发送报文段时先设置cwnd=1，然后逐渐增大，这当然比按照大的cwnd一下子把许多报文段突然注入到网络中要“慢得多”。 （二）拥塞避免算法：拥塞避免算法让拥塞窗口缓慢增长，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1，而不是加倍。这样拥塞窗口按线性规律缓慢增长。 无论是在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞（其根据就是没有按时收到确认，虽然没有收到确认可能是其他原因的分组丢失，但是因为无法判定，所以都当做拥塞来处理），就把慢开始门限ssthresh设置为出现拥塞时的发送窗口大小的一半（但不能小于2）。然后把拥塞窗口cwnd重新设置为1，执行慢开始算法。这样做的目的就是要迅速减少主机发送到网络中的分组数，使得发生拥塞的路由器有足够时间把队列中积压的分组处理完毕。 整个拥塞控制的流程如下图： （1）拥塞窗口cwnd初始化为1个报文段，慢开始门限初始值为16（2）执行慢开始算法，指数规律增长到第4轮，即cwnd=16=ssthresh，改为执行拥塞避免算法，拥塞窗口按线性规律增长（3）假定cwnd=24时，网络出现超时（拥塞），则更新后的ssthresh=12，cwnd重新设置为1，并执行慢开始算法。当cwnd=12=ssthresh时，改为执行拥塞避免算法 关于 乘法减小（Multiplicative Decrease）和加法增大（Additive Increase）： “乘法减小”指的是无论是在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞，就把慢开始门限ssthresh设置为出现拥塞时的发送窗口大小的一半，并执行慢开始算法，所以当网络频繁出现拥塞时，ssthresh下降的很快，以大大减少注入到网络中的分组数。“加法增大”是指执行拥塞避免算法后，使拥塞窗口缓慢增大，以防止过早出现拥塞。常合起来成为AIMD算法。 注意：“拥塞避免”并非完全能够避免了阻塞，而是使网络比较不容易出现拥塞。 （三）快重传算法：快重传要求接收方在收到一个失序的报文段后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方，可提高网络吞吐量约20%）而不要等到自己发送数据时捎带确认。快重传算法规定，发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传计时器时间到期。 如下图： （四）快恢复算法：快重传配合使用的还有快恢复算法，有以下两个要点： 当发送方连续收到三个重复确认时，就执行“乘法减小”算法，把ssthresh门限减半（为了预防网络发生拥塞）。但是接下去并不执行慢开始算法，考虑到如果网络出现拥塞的话就不会收到好几个重复的确认，所以发送方现在认为网络可能没有出现拥塞。所以此时不执行慢开始算法，而是将cwnd设置为ssthresh减半后的值，然后执行拥塞避免算法，使cwnd缓慢增大。如下图：TCP Reno版本是目前使用最广泛的版本。 注意：在采用快恢复算法时，慢开始算法只是在TCP连接建立时和网络出现超时时才使用。 参考TCP流量控制、拥塞控制","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"TCP","slug":"TCP","permalink":"https://xmmarlowe.github.io/tags/TCP/"}],"author":"Marlowe"},{"title":"HTTP 和 HTTPS","slug":"计算机网络/HTTP-和-HTTPS-","date":"2021-02-22T14:11:06.000Z","updated":"2021-05-08T07:44:45.166Z","comments":true,"path":"2021/02/22/计算机网络/HTTP-和-HTTPS-/","link":"","permalink":"https://xmmarlowe.github.io/2021/02/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTP-%E5%92%8C-HTTPS-/","excerpt":"","text":"什么是HTTP？超文本传输协议，是一个基于请求与响应，无状态的，应用层的协议，常基于TCP/IP协议传输数据，互联网上应用最为广泛的一种网络协议,所有的WWW文件都必须遵守这个标准。设计HTTP的初衷是为了提供一种发布和接收HTML页面的方法。 HTTP的特点 无状态： 协议对客户端没有状态存储，对事物处理没有“记忆”能力，比如访问一个网站需要反复进行登录操作。 无连接： HTTP/1.1之前，由于无状态特点，每次请求需要通过TCP三次握手四次挥手，和服务器重新建立连接。比如某个客户机在短时间多次请求同一个资源，服务器并不能区别是否已经响应过用户的请求，所以每次需要重新响应请求，需要耗费不必要的时间和流量。 基于请求和响应： 基本的特性，由客户端发起请求，服务端响应。 简单快速、灵活 通信使用明文、请求和响应不会对通信方进行确认、无法保护数据的完整性。 HTTP报文格式 请求方法: GET和POST是最常见的HTTP方法,初次以外还包括 DELETE、HEAD、OPTIONS、PUT、TRACE，不过现在大部分的浏览器只支持GET和POST。 请求对应的URL地址： 他和报文头的Host属性,组合起来是一个完整的请求URL。 协议名称和版本号。 报文头： 有若干个属性,形式为key:val,服务端据此获取客户端信息。 是报文体： 它将一个页面表单中的组件值通过param1=val1&amp;parma=2的键值对形式编码成一个格式化串,它承载多个请求参数的数据,不但报文头可以传递请求参数,URL也可以通过/chapter15/user.html? param1=value1&amp;param2=value2”的方式传递数值。 HTTP响应报文 报文协议及版本； 状态码及状态描述； 响应报文头: 也是由多个属性组成； 响应报文体: 即我们要的数据。 HTTP通信传输 客户端输入URL回车，DNS解析域名得到服务器的IP地址，服务器在80端口监听客户端请求，端口通过TCP/IP协议（可以通过Socket实现）建立连接。HTTP属于TCP/IP模型中的运用层协议，所以通信的过程其实是对应数据的入栈和出栈。 报文从运用层传送到运输层，运输层通过TCP三次握手和服务器建立连接，四次挥手释放连接。 HTTP的性能优化 通过以上图，可以从三个方面来优化HTTP的性能。 服务器衡量服务器性能的指标，主要有以下几个： 吞吐量（或TPS、RPS、QPS） 并发数 响应时间 资源利用率（CPU、内存、硬盘、网络） 提高吞吐量，吞吐量越高，服务器的性能越好！ 提高并发数，支持的并发数越大，服务器的性能越好！ 降低响应时间，响应时间越短，服务器的性能越好！ 合理利用服务器资源，过高肯定是不行，过低也有可能是存在问题的！ Linux服务器的监控工具主要有top、sar、glances等 客户端因为数据都要通过网络从服务器获取，所以它最基本的性能指标就是：延迟。 所谓的“延迟”其实就是“等待”，等待数据到达客户端时所花费的时间。 延迟的原因，有几点： 距离：由于地理距离导致的延迟，是无法客服的，比如访问数千公里外的网站。 带宽 DNS查询（如果域名在本地没有缓存的话） TCP握手（必须要经过 SYN、SYN/ACK、ACK 三个包之后才能建立连接） 对于 HTTP 性能优化，有一个专门的测试网站：WebPageTest，或使用浏览器的开发者工具 一次 HTTP“请求 - 响应”的过程中延迟的时间是非常大的，有可能会占到９０％以上 所以，客户端优化的关键，降低延迟 传输链路（客户端和服务器之间的传输链路）使用CDN等技术，总之，要增加带宽，降低延迟，优化传输速度。 具体的优化手段： 主要是优化服务端的性能。 前端： 可以利用PageSpeed等工具进行检测并根据提示进行优化。 后端： 主要有以下几方面 a.硬件、软件或服务 比如更换强劲的CPU、内存、磁盘、带宽等，比如使用CDN b.服务器选择、参数调优 选用高性能的服务器，比如Nginx，它强大的反向代理能力实现“动静分离”，动态页面交给Tomcat等，静态资源交给Nginx 另外，Nginx自身也有可以调优的参数，比如说禁用负载均衡锁、增大连接池，绑定 CPU 等 对于 HTTP 协议一定要启用长连接，因为TCP 和 SSL 建立新连接的成本非常高，可能会占到客户端总延迟的一半以上 TCP 的新特性“TCP Fast Open“，类似 TLS 的“False Start”，可以在初次握手的时候就传输数据，尽可能在操作系统和 Nginx 里开启这个特性，减少外网和内网里的握手延迟。 下面这个Nginx 配置，启用了长连接等优化参数，实现了动静分离 server { listen 80 deferred reuseport backlog=4096 fastopen=1024; keepalive_timeout 60; keepalive_requests 10000; location ~* .(png)$ { root /var/images/png/; } location ~* .(php)$ { proxy_pass http://php_back_end; }} 使用HTTP协议内置的“数据压缩”编码，可以选择标准的 gzip，也可以尝试新的压缩算法 br 不过在数据压缩的时候应当注意选择适当的压缩率，不是压缩的越厉害越好 c.缓存 网站系统内部，可以使用 Memcache、Redis等专门的缓存服务，把计算的中间结果和资源存储在内存或者硬盘里 Web 服务器首先检查缓存系统，如果有数据就立即返回给客户端 另外，CDN 的网络加速功能就是建立在缓存的基础之上的，可以这么说，如果没有缓存，那就没有 CDN。 利用好缓存功能的关键是理解它的工作原理，为每个资源都添加 ETag 和 Last-modified字段，再用 Cache-Control、Expires 设置好缓存控制属性。 其中最基本的是 max-age 有效期，标记资源可缓存的时间。对于图片、CSS 等静态资源可以设置较长的时间，比如一天或者 一个月，对于动态资源，除非是实时性非常高，也可以设置一个较短的时间，比如 1 秒或者 5 秒。这样一旦资源到达客户端，就 会被缓存起来，在有效期内都不会再向服务器发送请求。 什么是HTTPS？HTTPS是身披SSL外壳的HTTP。HTTPS是一种通过计算机网络进行安全通信的传输协议，经由HTTP进行通信，利用SSL/TLS建立全信道，加密数据包。HTTPS使用的主要目的是提供对网站服务器的身份认证，同时保护交换数据的隐私与完整性。 PS:TLS是传输层加密协议，前身是SSL协议，由网景公司1995年发布，有时候两者不区分。 HTTPS的特点 内容加密： 采用混合加密技术，中间者无法直接查看明文内容。 验证身份： 通过证书认证客户端访问的是自己的服务器。 保护数据完整性： 防止传输的内容被中间人冒充或者篡改。 HTTPS的缺点 HTTPS的握手协议比较费时，所以会影响服务的响应速度以及吞吐量。 HTTPS也并不是完全安全的。他的证书体系其实并不是完全安全的。并且HTTPS在面对DDOS这样的攻击时，几乎起不到任何作用。 证书需要费钱，并且功能越强大的证书费用越高。 HTTP和HTTPS的区别 端口 ：HTTP的URL由“http://”起始且默认使用端口80，而HTTPS的URL由“https://”起始且默认使用端口443。 安全性和资源消耗： HTTP协议运行在TCP之上，所有传输的内容都是明文，客户端和服务器端都无法验证对方的身份。HTTPS是运行在SSL/TLS之上的HTTP协议，SSL/TLS 运行在TCP之上。所有传输的内容都经过加密，加密采用对称加密，但对称加密的密钥用服务器方的证书进行了非对称加密。所以说，HTTP 安全性没有 HTTPS高，但是 HTTPS 比HTTP耗费更多服务器资源。 对称加密：密钥只有一个，加密解密为同一个密码，且加解密速度快，典型的对称加密算法有DES、AES等； 非对称加密：密钥成对出现（且根据公钥无法推知私钥，根据私钥也无法推知公钥），加密解密使用不同密钥（公钥加密需要私钥解密，私钥加密需要公钥解密），相对对称加密速度较慢，典型的非对称加密算法有RSA、DSA等。 如何保证HTTP传输安全性？目前大多数网站和app的接口都是采用http协议，但是http协议很容易就通过抓包工具监听到内容，甚至可以篡改内容，为了保证数据不被别人看到和修改，可以通过以下几个方面避免。 重要的数据，要加密： 比如用户名密码，我们需要加密，这样即使被抓包监听，他们也不知道原始数据是什么（如果简单的md5，是可以暴力破解），所以加密方法越复杂越安全，根据需要，常见的是 md5(不可逆)，aes（可逆），自由组合吧,你还可以加一些特殊字符啊，没有做不到只有想不到， 举例：username = aes(username), pwd = MD5(pwd + username);。。。。。 非重要数据，要签名： 签名的目的是为了防止篡改，比如http://www.xxx.com/getnews?id=1，获取id为1的新闻，如果不签名那么通过id=2,就可以获取2的内容等等。怎样签名呢？通常使用sign，比如原链接请求的时候加一个sign参数，sign=md5(id=1)，服务器接受到请求，验证sign是否等于md5(id=1)，如果等于说明正常请求。这会有个弊端，假如规则被发现，那么就会被伪造，所以适当复杂一些，还是能够提高安全性的。 登录态怎么做： http是无状态的，也就是服务器没法自己判断两个请求是否有联系，那么登录之后，以后的接口怎么判定是否登录呢，简单的做法，在数据库中存一个token字段（名字随意），当用户调用登陆接口成功的时候，就将该字段设一个值，（比如aes(过期时间)），同时返回给前端，以后每次前端请求带上该值，服务器首先校验是否过期，其次校验是否正确，不通过就让其登陆。（redis 做这个很方便哦，key有过期时间）。 HTTPS为什么安全？因为HTTPS保证了传输安全，防止传输过程被监听、防止数据被窃取，可以确认网站的真实性。 HTTPS的传输过程是怎样的?客户端发起HTTPS请求，服务端返回证书，客户端对证书进行验证，验证通过后本地生成用于改造对称加密算法的随机数，通过证书中的公钥对随机数进行加密传输到服务端，服务端接收后通过私钥解密得到随机数，之后的数据交互通过对称加密算法进行加解密。 客户使用https的URL访问Web服务器，要求与Web服务器建立SSL连接。 Web服务器收到客户端请求后，会将网站的证书信息（证书中包含公钥）传送一份给客户端。 客户端拿到证书，证书校验通过后，用系统内置的CA证书，进行对证书解密，拿到公钥。 客户端的浏览器与Web服务器开始协商SSL/TLS连接的安全等级，也就是信息加密的等级。 客户端的浏览器根据双方同意的安全等级，建立会话密钥，然后利用网站的公钥将会话密钥加密，并传送给网站。 Web服务器利用自己的私钥解密出会话密钥。 Web服务器利用会话密钥对数据进行对称加密，再与客户端进程通讯。 HTTP长连接、短连接在HTTP/1.0中默认使用短连接。 也就是说，客户端和服务器每进行一次HTTP操作，就建立一次连接，任务结束就中断连接。当客户端浏览器访问的某个HTML或其他类型的Web页中包含有其他的Web资源（如JavaScript文件、图像文件、CSS文件等），每遇到这样一个Web资源，浏览器就会重新建立一个HTTP会话。 而从HTTP/1.1起，默认使用长连接， 用以保持连接特性。使用长连接的HTTP协议，会在响应头加入这行代码： 1Connection:keep-alive 在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，客户端再次访问这个服务器时，会继续使用这一条已经建立的连接。Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。 实现长连接需要客户端和服务端都支持长连接。 HTTP协议的长连接和短连接，实质上是TCP协议的长连接和短连接。 CA证书的作用CA证书就是服务端自己有一份公钥私钥，把公钥给CA证书，获得一份数字证书，当客户端来请求时，就拿这个数字证书给客户端，客户端再通过CA认证算法拿到公钥，再进行后续操作。 为什么需要证书?防止”中间人“攻击，同时可以为网站提供身份证明。 验证证书安全性过程 当客户端收到这个证书之后，使用本地配置的权威机构的公钥对证书进行解密得到服务端的公钥和证书的数字签名，数字签名经过CA公钥解密得到证书信息摘要。 然后证书签名的方法计算一下当前证书的信息摘要，与收到的信息摘要作对比，如果一样，表示证书一定是服务器下发的，没有被中间人篡改过。因为中间人虽然有权威机构的公钥，能够解析证书内容并篡改，但是篡改完成之后中间人需要将证书重新加密，但是中间人没有权威机构的私钥，无法加密，强行加密只会导致客户端无法解密，如果中间人强行乱修改证书，就会导致证书内容和证书签名不匹配。 使用HTTPS会被抓包吗？会被抓包，HTTPS只防止用户在不知情的情况下通信被监听，如果用户主动授信，是可以构建“中间人”网络，代理软件可以对传输内容进行解密。 SSL/TLS协议的基本过程客户端发出请求（ClientHello） 支持的协议版本，比如TLS 1.0版。 一个客户端生成的随机数，稍后用于生成”对话密钥”。 支持的加密方法，比如RSA公钥加密。 支持的压缩方法。服务器回应（SeverHello） 确认使用的加密通信协议版本，比如TLS 1.0版本。如果浏览器与服务器支持的版本不一致，服务器关闭加密通信。 一个服务器生成的随机数，稍后用于生成”对话密钥”。 确认使用的加密方法，比如RSA公钥加密，此时带有公钥信息。 服务器证书。客户端回应 一个随机数pre-master key。该随机数用服务器公钥加密，防止被窃听。 编码改变通知，表示随后的信息都将用双方商定的加密方法和密钥发送。 客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时也是前面发送的所有内容的hash值，用来供服务器校验。 上面客户端回应中第一项的随机数，是整个握手阶段出现的第三个随机数，又称”pre-master key”。有了它以后，客户端和服务器就同时有了三个随机数，接着双方就用事先商定的加密方法，各自生成本次会话所用的同一把”会话密钥”。 SSL建立连接过程 client向server发送请求 https://baidu.com， 然后连接到server的443端口，发送的信息主要是随机值1和客户端支持的加密算法。 server接收到信息之后给予client响应握手信息，包括随机值2和匹配好的协商加密算法，这个加密算法一定是client发送给server加密算法的子集。 随即server给client发送第二个响应报文是数字证书。服务端必须要有一套数字证书，可以自己制作，也可以向组织申请。区别就是自己颁发的证书需要客户端验证通过，才可以继续访问，而使用受信任的公司申请的证书则不会弹出提示页面，这套证书其实就是一对公钥和私钥。传送证书，这个证书其实就是公钥，只是包含了很多信息，如证书的颁发机构，过期时间、服务端的公钥，第三方证书认证机构(CA)的签名，服务端的域名信息等内容。 客户端解析证书，这部分工作是由客户端的TLS来完成的，首先会验证公钥是否有效，比如颁发机构，过期时间等等，如果发现异常，则会弹出一个警告框，提示证书存在问题。如果证书没有问题，那么就生成一个随即值（预主秘钥）。 客户端认证证书通过之后，接下来是通过随机值1、随机值2和预主秘钥组装会话秘钥。然后通过证书的公钥加密会话秘钥。 传送加密信息，这部分传送的是用证书加密后的会话秘钥，目的就是让服务端使用秘钥解密得到随机值1、随机值2和预主秘钥。 服务端解密得到随机值1、随机值2和预主秘钥，然后组装会话秘钥，跟客户端会话秘钥相同。 客户端通过会话秘钥加密一条消息发送给服务端，主要验证服务端是否正常接受客户端加密的消息。 同样服务端也会通过会话秘钥加密一条消息回传给客户端，如果客户端能够正常接受的话表明SSL层连接建立完成了。 一些问题那么为什么一定要用三个随机数，来生成”会话密钥”呢？为了保证绝对随机，不相信服务器或者客户端的随机数，而是才用三个随机数再进行一定算法计算出真正的会话密钥(对称加密)。 为什么不都使用对称加密而是才有非对称与对称加密组合？因为非对称加密比较耗费性能，比对称加密慢了几倍甚至几百倍，所以才有了对称加密进行最终的数据加密。 参考HTTP与HTTPS详解 HTTP和HTTPS协议，看一篇就够了 HTTP的性能优化","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://xmmarlowe.github.io/tags/HTTP/"},{"name":"HTTPS","slug":"HTTPS","permalink":"https://xmmarlowe.github.io/tags/HTTPS/"}],"author":"Marlowe"},{"title":"TCP三次握手、四次挥手","slug":"计算机网络/TCP三次握手、四次挥手","date":"2021-02-22T13:47:45.000Z","updated":"2021-08-25T15:05:55.587Z","comments":true,"path":"2021/02/22/计算机网络/TCP三次握手、四次挥手/","link":"","permalink":"https://xmmarlowe.github.io/2021/02/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E3%80%81%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B/","excerpt":"","text":"三次握手三次握手过程客户端和服务端通信前要进行连接，“3次握手”的作用就是双方都能明确自己和对方的收、发能力是正常的。 第一次握手：客户端发送网络包，服务端收到了。这样服务端就能得出结论：客户端的发送能力、服务端的接收能力是正常的。 第二次握手：服务端发包，客户端收到了。这样客户端就能得出结论：服务端的接收、发送能力，客户端的接收、发送能力是正常的。 从客户端的视角来看，我接到了服务端发送过来的响应数据包，说明服务端接收到了我在第一次握手时发送的网络包，并且成功发送了响应数据包，这就说明，服务端的接收、发送能力正常。而另一方面，我收到了服务端的响应数据包，说明我第一次发送的网络包成功到达服务端，这样，我自己的发送和接收能力也是正常的。 第三次握手：客户端发包，服务端收到了。这样服务端就能得出结论：客户端的接收、发送能力，服务端的发送、接收能力是正常的。 第一、二次握手后，服务端并不知道客户端的接收能力以及自己的发送能力是否正常。而在第三次握手时，服务端收到了客户端对第二次握手作的回应。从服务端的角度，我在第二次握手时的响应数据发送出去了，客户端接收到了。所以，我的发送能力是正常的。而客户端的接收能力也是正常的。 三次握手的目的是建立可靠的通信信道，说到通讯，简单来说就是数据的发送与接收，而三次握手最主要的目的就是双方确认自己与对方的发送与接收是正常的。 第一次握手：Client 什么都不能确认；Server 确认了对方发送正常，自己接收正常 第二次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：对方发送正常，自己接收正常 第三次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己发送、接收正常，对方发送、接收正常 所以三次握手就能确认双发收发功能都正常，缺一不可。 为什么不两次握手？一句话，主要防止已经失效的连接请求报文突然又传送到了服务器，从而产生错误。 如果使用的是两次握手建立连接，假设有这样一种场景，客户端发送了第一个请求连接并且没有丢失，只是因为在网络结点中滞留的时间太长了，由于TCP的客户端迟迟没有收到确认报文，以为服务器没有收到，此时重新向服务器发送这条报文，此后客户端和服务器经过两次握手完成连接，传输数据，然后关闭连接。此时此前滞留的那一次请求连接，网络通畅了到达了服务器，这个报文本该是失效的，但是，两次握手的机制将会让客户端和服务器再次建立连接，这将导致不必要的错误和资源的浪费。 如果采用的是三次握手，就算是那一次失效的报文传送过来了，服务端接受到了那条失效报文并且回复了确认报文，但是客户端不会再次发出确认。由于服务器收不到确认，就知道客户端并没有请求连接。 为什么不四次握手？四次握手的过程就是把第二次握手拆分成了两次，一次服务器响应ACK，再一次发回SYN来确定客户端的接收是否正常。因为握手没有数据传输，所以可以放在一次就可以完成的没有必要用两次。 什么是半连接队列?服务器第一次收到客户端的 SYN 之后，就会处于 SYN_RCVD 状态，此时双方还没有完全建立其连接，服务器会把此种状态下请求连接放在一个队列里，我们把这种队列称之为半连接队列。 当然还有一个全连接队列，就是已经完成三次握手，建立起连接的就会放在全连接队列中。如果队列满了就有可能会出现丢包现象。补充一点关于SYN-ACK 重传次数的问题： 服务器发送完 SYN-ACK 包，如果未收到客户确认包，服务器进行首次重传，等待一段时间仍未收到客户确认包，进行第二次重传。如果重传次数超过系统规定的最大重传次数，系统则将该连接信息从半连接队列中删除。 ISN (Initial Sequence Number) 是固定的吗?当一端为建立连接而发送它的SYN时，它为连接选择一个初始序号。ISN随时间而变化，因此每个连接都将具有不同的ISN。ISN可以看作是一个32比特的计数器，每4 ms加1。这样选择序号的目的在于防止在网络中被延迟的分组在以后又被传送，而导致某个连接的一方对它做错误的解释。 三次握手的其中一个重要功能是客户端和服务端交换 ISN(Initial Sequence Number)，以便让对方知道接下来接收数据的时候如何按序列号组装数据。如果 ISN 是固定的，攻击者很容易猜出后续的确认号，因此 ISN 是动态生成的。 三次握手过程中可以携带数据吗?第三次握手的时候，是可以携带数据的。但是，第一次、第二次握手不可以携带数据 为什么这样呢？大家可以想一个问题，假如第一次握手可以携带数据的话，如果有人要恶意攻击服务器，那他每次都在第一次握手中的 SYN 报文中放入大量的数据。因为攻击者根本就不理服务器的接收、发送能力是否正常，然后疯狂着重复发 SYN 报文的话，这会让服务器花费很多时间、内存空间来接收这些报文。也就是说，第一次握手不可以放数据，其中一个简单的原因就是会让服务器更加容易受到攻击了。而对于第三次的话，此时客户端已经处于 ESTABLISHED 状态。对于客户端来说，他已经建立起连接了，并且也已经知道服务器的接收、发送能力是正常的了，所以能携带数据也没啥毛病。 SYN攻击是什么?SYN攻击就是Client在短时间内伪造大量不存在的IP 地址，并向Server不断地发送SYN包，Server则回复确认包，并等待Client确认，由于源地址不存在，因此Server需要不断重发直至超时，这些伪造的SYN包将长时间占用未连接队列，导致正常的SYN请求因为队列满而被丢弃，从而引起网络拥塞甚至系统瘫痪。SYN 攻击是一种典型的 DoS/DDoS 攻击。 检测 SYN 攻击非常的方便，当你在服务器上看到大量的半连接状态时，特别是源IP地址是随机的，基本上可以断定这是一次SYN攻击。在 Linux/Unix 上可以使用系统自带的 netstats 命令来检测 SYN 攻击。netstat -n -p TCP | grep SYN_RECV常见的防御 SYN 攻击的方法有如下几种：缩短超时（SYN Timeout）时间；增加最大半连接数；过滤网关防护；SYN cookies技术 在TCP建立连接的三次握手阶段，如果客户端发送的第三个ACK包丢了，那么客户端和服务器端分别进行什么处理?服务器收到 SYN 包后发出 SYN+ACK 数据包，服务器进入SYN_RECV状态。 而这个时候客户端发送 ACK 给服务器失败了，服务器没办法进入ESTABLISH状态，这个时候肯定不能传输数据的，不论客户端主动发送数据与否，服务器都会有定时器发送第二步SYN+ACK数据包，如果客户端再次发送ACK成功，建立连接。 如果一直不成功，服务器肯定会有超时设置，超时之后会给客户端发 RTS 报文，进入CLOSED状态，这个时候客户端应该也会关闭连接。 四次挥手四次挥手的过程 第一次:主机1发送关闭并且进入ESTABLISHED 状态变为 FIN_WAIT_1 第二次:主机2收到结束消息，立刻返回结束的响应，并且改变状态ESTABLISHED 状态变为 CLOSE_WAIT。主机1收到消息，也只是知道2收到消息的，FIN_WAIT_1 改成 FIN_WAIT_2。 第三次：主机2在收到消息后，不能立刻结束任务，socket是有缓存的，结束的消息不可能实时处理，当主机2知道应用程序通过系统函数read读到socket的EOF后（这个时候，证明服务以及知道要结束了），会发送一个FIN 的消息给主机一，表示可以真的结束了。并且状态从CLOSE_WAIT 变为 LAST_ACK。 第四次：主机1收到主机2真正结束的消息后，也会发生一个反馈告知自己收到了，就从FIN_WAIT_2 变成了 TIME_WAIT(我们这次的主角)。主机2 收到消息后，就能安心的结束自己的这个socket了。 最后:主机 1 在 TIME_WAIT 停留持续时间是固定的（Linux默认是60秒），是最长分节生命期 MSL（maximum segment lifetime）的两倍，一般称之为 2MSL。过了这个时间之后，主机 1 就进入 CLOSED 状态。 为什么需要四次挥手？任何⼀⽅都可以在数据传送结束后发出连接释放的通知，待对⽅确认后进⼊半关闭状态。当另⼀⽅也没有数据再发送的时候，则发出连接释放通知，对⽅确认后就完全关闭了TCP连接。 举个例⼦：A 和 B 打电话，通话即将结束后，A 说“我没啥要说的了”，B回答“我知道了”，但是 B 可能还会有要说的话，A 不能要求 B 跟着⾃⼰的节奏结束通话，于是 B 可能⼜巴拉巴拉说了⼀通，最后B 说“我说完了”，A 回答“知道了”，这样通话才算结束。 2MSL等待状态TIME_WAIT状态也称为2MSL等待状态。每个具体TCP实现必须选择一个报文段最大生存时间MSL（Maximum Segment Lifetime），它是任何报文段被丢弃前在网络内的最长时间。这个时间是有限的，因为TCP报文段以IP数据报在网络内传输，而IP数据报则有限制其生存时间的TTL字段。 对一个具体实现所给定的MSL值，处理的原则是：当TCP执行一个主动关闭，并发回最后一个ACK，该连接必须在TIME_WAIT状态停留的时间为2倍的MSL。这样可让TCP再次发送最后的ACK以防这个ACK丢失（另一端超时并重发最后的FIN）。这种2MSL等待的另一个结果是这个TCP连接在2MSL等待期间，定义这个连接的插口（客户的IP地址和端口号，服务器的IP地址和端口号）不能再被使用。这个连接只能在2MSL结束后才能再被使用。 为什么客户端最后还要等待2MSL？MSL（Maximum Segment Lifetime），TCP允许不同的实现可以设置不同的MSL值。 第一，保证客户端发送的最后一个ACK报文能够到达服务器，因为这个ACK报文可能丢失，站在服务器的角度看来，我已经发送了FIN+ACK报文请求断开了，客户端还没有给我回应，应该是我发送的请求断开报文它没有收到，于是服务器又会重新发送一次，而客户端就能在这个2MSL时间段内收到这个重传的报文，接着给出回应报文，并且会重启2MSL计时器。 第二，防止类似与“三次握手”中提到了的“已经失效的连接请求报文段”出现在本连接中。客户端发送完最后一个确认报文后，在这个2MSL时间中，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样新的连接中不会出现旧连接的请求报文。 TIME_WAIT相关问题大量TIME_WAIT产生的原因在高并发短连接的TCP服务器上，当服务器处理完请求后立刻按照主动正常关闭连接。这个场景下，会出现大量socket处于TIMEWAIT状态。如果客户端的并发量持续很高，此时部分客户端就会显示连接不上。 TIME_WAIT(主动关闭方) 状态产生的原因1）为实现TCP全双工连接的可靠释放 由TCP状态变迁图可知，假设发起主动关闭的一方（client）最后发送的ACK在网络中丢失，由于TCP协议的重传机制，执行被动关闭的一方（server）将会重发其FIN，在该FIN到达client之前，client必须维护这条连接状态，也就说这条TCP连接所对应的资源（client方的local_ip,local_port）不能被立即释放或重新分配，直到另一方重发的FIN达到之后，client重发ACK后，经过2MSL时间周期没有再收到另一方的FIN之后，该TCP连接才能恢复初始的CLOSED状态。如果主动关闭一方不维护这样一个TIME_WAIT状态，那么当被动关闭一方重发的FIN到达时，主动关闭一方的TCP传输层会用RST包响应对方，这会被对方认为是有错误发生，然而这事实上只是正常的关闭连接过程，并非异常。 2）为使旧的数据包在网络因过期而消失 为说明这个问题，我们先假设TCP协议中不存在TIME_WAIT状态的限制，再假设当前有一条TCP连接：(local_ip, local_port, remote_ip,remote_port)，因某些原因，我们先关闭，接着很快以相同的四元组建立一条新连接。本文前面介绍过，TCP连接由四元组唯一标识，因此，在我们假设的情况中，TCP协议栈是无法区分前后两条TCP连接的不同的，在它看来，这根本就是同一条连接，中间先释放再建立的过程对其来说是“感知”不到的。这样就可能发生这样的情况：前一条TCP连接由local peer发送的数据到达remote peer后，会被该remot peer的TCP传输层当做当前TCP连接的正常数据接收并向上传递至应用层（而事实上，在我们假设的场景下，这些旧数据到达remote peer前，旧连接已断开且一条由相同四元组构成的新TCP连接已建立，因此，这些旧数据是不应该被向上传递至应用层的），从而引起数据错乱进而导致各种无法预知的诡异现象。作为一种可靠的传输协议，TCP必须在协议层面考虑并避免这种情况的发生，这正是TIME_WAIT状态存在的第2个原因。 3）总结具体而言，local peer主动调用close后，此时的TCP连接进入TIME_WAIT状态，处于该状态下的TCP连接不能立即以同样的四元组建立新连接，即发起active close的那方占用的local port在TIME_WAIT期间不能再被重新分配。由于TIME_WAIT状态持续时间为2MSL，这样保证了旧TCP连接双工链路中的旧数据包均因过期（超过MSL）而消失，此后，就可以用相同的四元组建立一条新连接而不会发生前后两次连接数据错乱的情况。 TIME_WAIT过多的危害 网络情况不好时，如果主动方无TIME_WAIT等待，关闭前个连接后，主动方与被动方又建立起新的TCP连接，这时被动方重传或延时过来的FIN包过来后会直接影响新的TCP连接； 同样网络情况不好并且无TIME_WAIT等待，关闭连接后无新连接，当接收到被动方重传或延迟的FIN包后，会给被动方回一个RST包，可能会影响被动方其它的服务连接。 过多的话会占用内存，一个time_await占用4k大小 在高并发短连接的TCP服务器上，当服务器处理完请求后立刻按照主动正常关闭连接。这个场景下，会出现大量socket处于TIMEWAIT状态。如果客户端的并发量持续很高，此时部分客户端就会显示连接不上。 我来解释下这个场景。主动正常关闭TCP连接，都会出现TIMEWAIT。为什么我们要关注这个高并发短连接呢？有两个方面需要注意： ① 高并发可以让服务器在短时间范围内同时占用大量端口，而端口有个0~65535的范围，并不是很多，刨除系统和其他服务要用的，剩下的就更少了。 ②在这个场景中，短连接表示“业务处理+传输数据的时间 远远小于 TIMEWAIT超时的时间”的连接。这里有个相对长短的概念，比如，取一个web页面，1秒钟的http短连接处理完业务，在关闭连接之后，这个业务用过的端口会停留在TIMEWAIT状态几分钟，而这几分钟，其他HTTP请求来临的时候是无法占用此端口的。单用这个业务计算服务器的利用率会发现，服务器干正经事的时间和端口（资源）被挂着无法被使用的时间的比例是 1：几百，服务器资源严重浪费。（说个题外话，从这个意义出发来考虑服务器性能调优的话，长连接业务的服务就不需要考虑TIMEWAIT状态。同时，假如你对服务器业务场景非常熟悉，你会发现，在实际业务场景中，一般长连接对应的业务的并发量并不会很高） 综合这两个方面，持续的到达一定量的高并发短连接，会使服务器因端口资源不足而拒绝为一部分客户服务。 解决出现大量TIME_WAIT情况的方法1. 修改系统配置具体来说，需要修改本文前面介绍的tcp_max_tw_buckets、tcp_tw_recycle、tcp_tw_reuse这三个配置项。 1）将tcp_max_tw_buckets调大，从本文第一部分可知，其默认值为18w（不同内核可能有所不同，需以机器实际配置为准），根据文档，我们可以适当调大，至于上限是多少，文档没有给出说明，我也不清楚。个人认为这种方法只能对TIME_WAIT过多的问题起到缓解作用，随着访问压力的持续，该出现的问题迟早还是会出现，治标不治本。 2）开启tcp_tw_recycle选项：在shell终端输入命令”echo 1 &gt; /proc/sys/net/ipv4/tcp_tw_recycle”可以开启该配置。 对/etc/sysctl.conf文件进行修改： 12345678910111213141516171819202122#对于一个新建连接，内核要发送多少个 SYN 连接请求才决定放弃,不应该大于255，默认值是5，对应于180秒左右时间 net.ipv4.tcp_syn_retries=2 #net.ipv4.tcp_synack_retries=2 #表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为300秒 net.ipv4.tcp_keepalive_time=1200 net.ipv4.tcp_orphan_retries=3 #表示如果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2状态的时间 net.ipv4.tcp_fin_timeout=30 #表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数。 net.ipv4.tcp_max_syn_backlog = 4096 #表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭 net.ipv4.tcp_syncookies = 1 #表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭 net.ipv4.tcp_tw_reuse = 1 #表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭 net.ipv4.tcp_tw_recycle = 1 ##减少超时前的探测次数 net.ipv4.tcp_keepalive_probes=5 ##优化网络设备接收队列 net.core.netdev_max_backlog=3000 2. 修改应用程序如果有高并发请求功能，服务端是自己的服务器时，TCP 通信最好采用长连接，避免大量短连接每次建立/释放产生的各种开销；如果建立连接的目标服务器不是自己的，那就要考虑对方是否支持长连接方式。 TIME_WAIT能马上进入CLOSED吗？ 确保最后的 ACK 能让被动关闭方接收 TCP在设计的时候，有充分的容错性，如果报文出错，是需要可以重传的。如果TIME_WAIT马上进入CLOSED，这个时候，发现最后的一个ack传输失败，它就失去了当前状态的上下文，只能回复一个 RST 操作，从而导致被动关闭方出现错误。如果能够保留TIME_WAIT 这个状态，就能正确的发起重试。 连接“化身”和报文迷走 为了让旧连接的重复分节在网络中自然消失。在原连接中断后，又重新创建了一个原连接的“化身”，说是化身其实是因为这个连接和原先的连接四元组完全相同，如果迷失报文经过一段时间也到达，那么这个报文会被误认为是连接“化身”的一个 TCP 分节，这样就会对 TCP 通信产生影响。TCP 就设计出了这么一个机制，经过 2MSL 这个时间，足以让两个方向上的分组都被丢弃，使得原来连接的分组在网络中都自然消失，再出现的分组一定都是新化身所产生的。 为什么TIME_WAIT状态需要经过2MSL才能返回到CLOSE状态?理论上，四个报文都发送完毕，就可以直接进入CLOSE状态了，但是可能网络是不可靠的，有可能最后一个ACK丢失。所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。 CLOSE_WAIT相关问题出现的原因和解决方案close_wait是被动关闭连接是形成的，根据TCP状态机，服务器端收到客户端发送的FIN，TCP协议栈会自动发送ACK，链接进入close_wait状态。但如果服务器端不执行socket的close()操作（即不向客户端发送FIN），状态就不能由close_wait迁移到last_ack，则系统中会存在很多close_wait状态的连接。 可能的原因如下： 关闭socket不及时：例如I/O线程被意外阻塞，或者I/O线程执行的用户自定义Task比例过高，导致I/O操作处理不及时，链路不能被及时释放。 通常，CLOSE_WAIT 状态在服务器停留时间很短，如果你发现大量的 CLOSE_WAIT 状态，那么就意味着被动关闭的一方没有及时发出 FIN 包，一般有如下几种可能： 程序问题：如果代码层面忘记了 close 相应的 socket 连接，那么自然不会发出 FIN 包，从而导致 CLOSE_WAIT 累积；或者代码不严谨，出现死循环之类的问题，导致即便后面写了 close 也永远执行不到。 响应太慢或者超时设置过小：如果连接双方不和谐，一方不耐烦直接 timeout，另一方却还在忙于耗时逻辑，就会导致 close 被延后。响应太慢是首要问题，不过换个角度看，也可能是 timeout 设置过小。 一些问题什么是TIME-WAIT和CLOSE-WAIT?总所周知，由于 socket 是全双工的工作模式，一个 socket 的关闭，是需要四次握手来完成的： 主动关闭连接的一方，调用 close()；协议层发送 FIN 包; 被动关闭的一方收到 FIN 包后，协议层回复 ACK；然后被动关闭的一方，进入 CLOSE_WAIT 状态，主动关闭的一方等待对方关闭，则进入 FIN_WAIT_2 状态；此时，主动关闭的一方等待被动关闭一方的应用程序，调用 close 操作; 被动关闭的一方在完成所有数据发送后，调用 close() 操作；此时，协议层发送 FIN 包给主动关闭的一方，等待对方的 ACK，被动关闭的一方进入 LAST_ACK 状态； 主动关闭的一方收到 FIN 包，协议层回复 ACK；此时，主动关闭连接的一方，进入 TIME_WAIT 状态；而被动关闭的一方，进入 CLOSED 状态; 等待 2MSL 时间，主动关闭的一方，结束 TIME_WAIT，进入 CLOSED 状态; 通过上面的一次 socket 关闭操作，可以得出以下几点： 主动关闭连接的一方 – 也就是主动调用 socket 的 close 操作的一方，最终会进入 TIME_WAIT 状态; 被动关闭连接的一方，有一个中间状态，即 CLOSE_WAIT，因为协议层在等待上层的应用程序，主动调用 close 操作后才主动关闭这条连接; TIME_WAIT 会默认等待 2MSL 时间后，才最终进入 CLOSED 状态； 在一个连接没有进入 CLOSED 状态之前，这个连接是不能被重用的！ 所以说这里凭直觉看，TIME_WAIT 并不可怕，CLOSE_WAIT 才可怕，因为 CLOSE_WAIT 很多，表示说要么是你的应用程序写的有问题，没有合适的关闭 socket；要么是说，你的服务器 CPU 处理不过来（CPU 太忙）或者你的应用程序一直睡眠到其它地方(锁，或者文件 I/O 等等)，你的应用程序获得不到合适的调度时间，造成你的程序没法真正的执行 close 操作。 TCP初始序列号为什么是随机的？在TCP的三次握手中，采用随机产生的初始化序列号进行请求，这样做主要是出于网络安全的因素着想。如果不是随机产生初始序列号，黑客将会以很容易的方式获取到你与其他主机之间通信的初始化序列号，并且伪造序列号进行攻击，这已经成为一种很常见的网络攻击手段。 Closed状态。。。。CLOSE_WAIT(被动关闭方) 状态这种状态的含义其实是表示在等待关闭。怎么理解呢？当对方close一个SOCKET后发送FIN报文给自己，你系统毫无疑问地会回应一个ACK报文给对方，此时则进入到CLOSE_WAIT状态。接下来呢，实际上你真正需要考虑的事情是察看你是否还有数据发送给对方，如果没有的话，那么你也就可以 close这个SOCKET，发送FIN报文给对方，也即关闭连接。所以你在CLOSE_WAIT状态下，需要完成的事情是等待你去关闭连接。（被动方） 为什么建立连接是三次握手，关闭连接确是四次挥手呢？建立连接的时候， 服务器在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。而关闭连接时，服务器收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，而自己也未必全部数据都发送给对方了，所以己方可以立即关闭，也可以发送一些数据给对方后，再发送FIN报文给对方来表示同意现在关闭连接，因此，己方ACK和FIN一般都会分开发送，从而导致多了一次。 如果已经建立了连接，但是客户端突然出现故障了怎么办？TCP还设有一个保活计时器，显然，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75秒发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。 参考TCP的三次握手与四次挥手两张动图-彻底明白TCP的三次握手与四次挥手 TCP三次握手和四次挥手过程详细解析","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"TCP","slug":"TCP","permalink":"https://xmmarlowe.github.io/tags/TCP/"}],"author":"Marlowe"},{"title":"网络5层模型和7层模型","slug":"计算机网络/网络5层模型和7层模型","date":"2021-02-22T12:51:17.000Z","updated":"2021-04-23T14:25:25.645Z","comments":true,"path":"2021/02/22/计算机网络/网络5层模型和7层模型/","link":"","permalink":"https://xmmarlowe.github.io/2021/02/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C5%E5%B1%82%E6%A8%A1%E5%9E%8B%E5%92%8C7%E5%B1%82%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"体系结构图 七层OSI 物理层(Physical Laye)主要功能： 利用传输介质为数据链路层提供物理连接，实现比特流的透明传输。作用： 实现相邻计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。使其上面的数据链路层不必考虑网络的具体传输介质是什么。“透明传送比特流”表示经实际电路传送后的比特流没有发生变化，对传送的比特流来说，这个电路好像是看不见的。 数据链路层(Data Link Layer)主要功能： 通过各种控制协议，将有差错的物理信道变为无差错的、能可靠传输数据帧的数据链路。作用： 接收来自物理层的位流形式的数据，并封装成帧，传送到上一层；同样，也将来自上层的数据帧，拆装为位流形式的数据转发到物理层；并且，还负责处理接收端发回的确认帧的信息，以便提供可靠的数据传输。 网络层(Network Layer)主要功能： 通过路由选择算法，为报文或分组通过通信子网选择最适当的路径。该层控制数据链路层与传输层之间的信息转发，建立、维持和终止网络的连接。具体地说，数据链路层的数据在这一层被转换为数据包，然后通过路径选择、分段组合、顺序、进/出路由等控制，将信息从一个网络设备传送到另一个网络设备。作用： 解决不同子网间的通信。例如在广域网之间通信时，必然会遇到路由（即两节点间可能有多条路径）选择问题。 运输层(Transport Layer)主要功能： 负责向两台主机进程之间的通信提供通⽤的数据传输服务。作用： 传输连接管理：提供建立、维护和拆除传输连接的功能。传输层在网络层的基础上为高层提供“面向连接”和“面向无接连”的两种服务。 处理传输差错：提供可靠的“面向连接”和不太可靠的“面向无连接”的数据传输服务、差错控制和流量控制。在提供“面向连接”服务时，通过这一层传输的数据将由目标设备确认，如果在指定的时间内未收到确认信息，数据将被重发。会话层(Session Layer) *主要功能：** 向两个实体的表示层提供建立和使用连接的方法。将不同实体之间的表示层的连接称为会话。因此会话层的任务就是组织和协调两个会话进程之间的通信，并对数据交换进行管理。 *作用：** 会话管理：允许用户在两个实体设备之间建立、维持和终止会话，并支持它们之间的数据交换。例如提供单方向会话或双向同时会话，并管理会话中的发送顺序，以及会话所占用时间的长短。 会话流量控制：提供会话流量控制和交叉会话功能。 寻址：使用远程地址建立会话连接。l 出错控制：从逻辑上讲会话层主要负责数据交换的建立、保持和终止，但实际的工作却是接收来自传输层的数据，并负责纠正错误。会话控制和远程过程调用均属于这一层的功能。但应注意，此层检查的错误不是通信介质的错误，而是磁盘空间、打印机缺纸等类型的高级错误。表示层(Presentation Layer) *主要功能：** 对来自应用层的命令和数据进行解释，对各种语法赋予相应的含义，并按照一定的格式传送给会话层。其主要功能是“处理用户信息的表示问题，如编码、数据格式转换和加密解密”等。 *作用：** 数据格式处理：协商和建立数据交换的格式，解决各应用程序之间在数据格式表示上的差异。 数据的编码：处理字符集和数字的转换。例如由于用户程序中的数据类型（整型或实型、有符号或无符号等）、用户标识等都可以有不同的表示方式，因此，在设备之间需要具有在不同字符集或格式之间转换的功能。 压缩和解压缩：为了减少数据的传输量，这一层还负责数据的压缩与恢复。 数据的加密和解密：可以提高网络的安全性。应用层(Application Layer)应用层提供的协议有Telnet，SMTP，FTP等等。 *主要功能：** 通过应⽤进程间的交互来完成特定⽹络应⽤ *作用：** 用户接口：应用层是用户与网络，以及应用程序与网络间的直接接口，使得用户能够与网络进行交互式联系。 实现各种服务：该层具有的各种应用程序可以完成和实现用户请求的各种服务。 四层TCP网络接口层包括用于协作IP数据在已有网络介质上传输的协议。实际上TCP/IP标准并不定义与ISO数据链路层和物理层相对应的功能。相反，它定义像地址解析协议(Address Resolution Protocol,ARP)这样的协议，提供TCP/IP协议的数据结构和实际物理硬件之间的接口。 网际层对应于OSI七层参考模型的网络层。本层包含IP协议、RIP协议(Routing Information Protocol，路由信息协议)，负责数据的包装、寻址和路由。同时还包含网间控制报文协议(Internet Control Message Protocol,ICMP)用来提供网络诊断信息。 传输层对应于OSI七层参考模型的传输层，它提供两种端到端的通信服务。其中TCP协议(Transmission Control Protocol)提供可靠的数据流运输服务，UDP协议(Use Datagram Protocol)提供不可靠的用户数据报服务。 应用层对应于OSI七层参考模型的应用层和表达层。因特网的应用层协议包括Finger、Whois、FTP(文件传输协议)、Gopher、HTTP(超文本传输协议)、Telent(远程终端协议)、SMTP(简单邮件传送协议)、IRC(因特网中继会话)、NNTP（网络新闻传输协议）等，这也是本书将要讨论的重点。 参考OSI的七层模型","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"OSI","slug":"OSI","permalink":"https://xmmarlowe.github.io/tags/OSI/"}],"author":"Marlowe"},{"title":"Ping过程解析","slug":"计算机网络/Ping过程解析","date":"2021-02-16T12:03:07.000Z","updated":"2021-04-23T14:25:48.246Z","comments":true,"path":"2021/02/16/计算机网络/Ping过程解析/","link":"","permalink":"https://xmmarlowe.github.io/2021/02/16/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/Ping%E8%BF%87%E7%A8%8B%E8%A7%A3%E6%9E%90/","excerpt":"","text":"Ping过程解析 A电脑（192.168.2.135）发起ping请求，ping 192.168.2.179 A电脑广播发起ARP请求，查询 192.168.2.179的MAC地址。 B电脑应答ARP请求，向A电脑发起单向应答，告诉A电脑自己的MAC地址为90:A4:DE:C2:DF:FE 知道了MAC地址后，开始进行真正的ping请求，由于B电脑可以根据A电脑发送的请求知道源MAC地址，所有就可以根据源MAC地址进行响应了。 ARP协议的主要功能是什么arp协议的主要功能是将IP地址解析为物理地址。使用ARP协议可根据网络层IP数据包包头中的IP地址信息解析出目标硬件地址（MAC地址）信息，以保证通信的顺利进行。 总结我们分析了一次完整的ping请求过程，ping命令是依托于ICMP协议的，ICMP协议的存在就是为了更高效的转发IP数据报和提高交付成功的机会。ping命令除了依托于ICMP，在局域网下还要借助于ARP协议，ARP协议能根据IP地址查出计算机MAC地址。ARP是有缓存的，为了保证ARP的准确性，计算机会更新ARP缓存。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"Ping","slug":"Ping","permalink":"https://xmmarlowe.github.io/tags/Ping/"}],"author":"Marlowe"},{"title":"计算机网络分层的目的","slug":"计算机网络/计算机网络分层的目的","date":"2021-02-15T14:02:13.000Z","updated":"2021-04-23T14:25:20.015Z","comments":true,"path":"2021/02/15/计算机网络/计算机网络分层的目的/","link":"","permalink":"https://xmmarlowe.github.io/2021/02/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%88%86%E5%B1%82%E7%9A%84%E7%9B%AE%E7%9A%84/","excerpt":"简单讲讲计算机网络分层的原因…","text":"简单讲讲计算机网络分层的原因… 分层的优点 各层之间是独立的。 某一层并不需要知道它的下一层是如何实现的，而仅仅需要知道该层通过层间的接口（即界面）所提供的服务。由于每一层只实现一种相对独立的功能，因而可将一个难以处理的复杂问题分解为若干个较容易处理的更小一些的问题。这样，整个问题的复杂程度就下降了。 灵活性好。 当任何一层发生变化时（例如由于技术的变化），只要层间接口关系保持不变，则在这层以上或以下各层均不受影响。此外，对某一层提供的服务还可进行修改。 当某层提供的服务不再需要时，甚至可以将这层取消。 结构上可分割开。 各层都可以采用最合适的技术来实现。 易于实现和维护。 这种结构使得实现和调试一个庞大而又复杂的系统变得易于处理，因为整个的系统已被分解为若干个相对独立的子系统。 能促进标准化工作。 因为每一层的功能及其所提供的服务都已有了精确的说明。 分层时应注意使每一层的功能非常明确 若层数太少，就会使每一层的协议太复杂。 但层数太多又会在描述和综合各层功能的系统工程任务时遇到较多的困难。 通常各层所要完成的功能主要有以下一些（可以只包括一种，也可以包括多种）： 差错控制使相应层次对等方的通信更加可靠。 流量控制发送端的发送速率必须使接收端来得及接收，不要太快。 分段和重装发送端将要发送的数据块划分为更小的单位，在接收端将其还原。 复用和分用发送端几个高层会话复用一条低层的连接，在接收端再进行分用。 连接建立和释放交换数据前先建立一条逻辑连接，数据传送结束后释放连接。 分层的缺点没有考虑网络实际应用的复杂性，有的层的功能不明确。不能适应市场的需要。分层过多。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"网络分层","slug":"网络分层","permalink":"https://xmmarlowe.github.io/tags/%E7%BD%91%E7%BB%9C%E5%88%86%E5%B1%82/"}],"author":"Marlowe"},{"title":"TCP 协议如何保证可靠传输","slug":"计算机网络/TCP-协议如何保证可靠传输","date":"2021-02-10T10:34:42.000Z","updated":"2021-04-23T14:25:51.641Z","comments":true,"path":"2021/02/10/计算机网络/TCP-协议如何保证可靠传输/","link":"","permalink":"https://xmmarlowe.github.io/2021/02/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8F%E8%AE%AE%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E5%8F%AF%E9%9D%A0%E4%BC%A0%E8%BE%93/","excerpt":"","text":"可靠传输 应用数据被分割成 TCP 认为最适合发送的数据块。 TCP 给发送的每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层。 校验和： TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段。 TCP 的接收端会丢弃重复的数据。 流量控制： TCP 连接的每一方都有固定大小的缓冲空间，TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议。 （TCP 利用滑动窗口实现流量控制） 拥塞控制： 当网络拥塞时，减少数据的发送。 ARQ协议： 也是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。 超时重传： 当 TCP 发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。 ARQ协议自动重传请求（Automatic Repeat-reQuest，ARQ）是OSI模型中数据链路层和传输层的错误纠正协议之一。它通过使用确认和超时这两个机制，在不可靠服务的基础上实现可靠的信息传输。如果发送方在发送后一段时间之内没有收到确认帧，它通常会重新发送。ARQ包括停止等待ARQ协议和连续ARQ协议。 1.停止等待ARQ协议停止等待协议是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认（回复ACK）。如果过了一段时间（超时时间后），还是没有收到 ACK 确认，说明没有发送成功，需要重新发送，直到收到确认后再发下一个分组。 在停止等待协议中，若接收方收到重复分组，就丢弃该分组，但同时还要发送确认。 优缺点： 优点： 简单 缺点： 信道利用率低，等待时间长 1) 无差错情况: 发送方发送分组,接收方在规定时间内收到,并且回复确认.发送方再次发送。 2) 出现差错情况（超时重传）: 停止等待协议中超时重传是指只要超过一段时间仍然没有收到确认，就重传前面发送过的分组（认为刚才发送过的分组丢失了）。因此每发送完一个分组需要设置一个超时计时器，其重传时间应比数据在分组传输的平均往返时间更长一些。这种自动重传方式常称为 自动重传请求 ARQ 。另外在停止等待协议中若收到重复分组，就丢弃该分组，但同时还要发送确认。连续 ARQ 协议 可提高信道利用率。发送维持一个发送窗口，凡位于发送窗口内的分组可连续发送出去，而不需要等待对方确认。接收方一般采用累积确认，对按序到达的最后一个分组发送确认，表明到这个分组位置的所有分组都已经正确收到了。 3) 确认丢失和确认迟到 确认丢失 ： 确认消息在传输过程丢失。当A发送M1消息，B收到后，B向A发送了一个M1确认消息，但却在传输过程中丢失。而A并不知道，在超时计时过后，A重传M1消息，B再次收到该消息后采取以下两点措施：1. 丢弃这个重复的M1消息，不向上层交付。 2. 向A发送确认消息。（不会认为已经发送过了，就不再发送。A能重传，就证明B的确认消息丢失）。 确认迟到 ： 确认消息在传输过程中迟到。A发送M1消息，B收到并发送确认。在超时时间内没有收到确认消息，A重传M1消息，B仍然收到并继续发送确认消息（B收到了2份M1）。此时A收到了B第二次发送的确认消息。接着发送其他数据。过了一会，A收到了B第一次发送的对M1的确认消息（A也收到了2份确认消息）。处理如下：1. A收到重复的确认后，直接丢弃。2. B收到重复的M1后，也直接丢弃重复的M1。 2.连续ARQ协议连续 ARQ 协议可提高信道利用率。发送方维持一个发送窗口，凡位于发送窗口内的分组可以连续发送出去，而不需要等待对方确认。接收方一般采用累计确认，对按序到达的最后一个分组发送确认，表明到这个分组为止的所有分组都已经正确收到了。 优缺点： 优点： 信道利用率高，容易实现，即使确认丢失，也不必重传。 缺点： 不能向发送方反映出接收方已经正确收到的所有分组的信息。 比如：发送方发送了 5条 消息，中间第三条丢失（3号），这时接收方只能对前两个发送确认。发送方无法知道后三个分组的下落，而只好把后三个全部重传一次。这也叫 Go-Back-N（回退 N），表示需要退回来重传已经发送过的 N 个消息。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"TCP","slug":"TCP","permalink":"https://xmmarlowe.github.io/tags/TCP/"}],"author":"Marlowe"},{"title":"get和post的区别","slug":"计算机网络/get和post的区别","date":"2021-02-10T02:42:32.000Z","updated":"2021-08-25T00:28:39.652Z","comments":true,"path":"2021/02/10/计算机网络/get和post的区别/","link":"","permalink":"https://xmmarlowe.github.io/2021/02/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/get%E5%92%8Cpost%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"功能不同 get是从服务器上获取数据。 post是向服务器传送数据。 过程不同 get是把参数数据队列加到提交表单的ACTION属性所指的URL中，值和表单内各个字段一一对应，在URL中可以看到。 post是通过HTTP post机制，将表单内各个字段与其内容放置在HTML HEADER内一起传送到ACTION属性所指的URL地址。用户看不到这个过程。 获取值不同 对于get方式，服务器端用Request.QueryString获取变量的值。 对于post方式，服务器端用Request.Form获取提交的数据。 传送数据量不同 get传送的数据量较小，不能大于2KB。 post传送的数据量较大，一般被默认为不受限制。但理论上，IIS4中最大量为80KB，IIS5中为100KB。 安全性不同 get安全性非常低。 post安全性较高。 如果没有加密，他们安全级别都是一样的，随便一个监听器都可以把所有的数据监听到。 区别 GET和POST两种基本请求方法的区别GET - 从指定的资源请求数据。 POST - 向指定的资源提交要被处理的数据 最直观的区别就是GET把参数包含在URL内，POST通过request body传递参数。 HTTP的底层是TCP/IP。所以GET和POST的底层也是TCP/IP，也就是说，GET/POST都是TCP链接。GET和POST能做的事情是一样一样的。你要给GET加上request body，给POST带上url参数，技术上是完全行的通的。 那么区别到底是什么?在我大万维网世界中，TCP就像汽车，我们用TCP来运输数据，它很可靠，从来不会发生丢件少件的现象。但是如果路上跑的全是看起来一模一样的汽车，那这个世界看起来是一团混乱，送急件的汽车可能被前面满载货物的汽车拦堵在路上，整个交通系统一定会瘫痪。为了避免这种情况发生，交通规则HTTP诞生了。HTTP给汽车运输设定了好几个服务类别，有GET, POST, PUT, DELETE等等，HTTP规定，当执行GET请求的时候，要给汽车贴上GET的标签（设置method为GET），而且要求把传送的数据放在车顶上（url中）以方便记录。如果是POST请求，就要在车上贴上POST的标签，并把货物放在车厢里。当然，你也可以在GET的时候往车厢内偷偷藏点货物，但是这是很不光彩；也可以在POST的时候在车顶上也放一些数据，让人觉得傻乎乎的。HTTP只是个行为准则，而TCP才是GET和POST怎么实现的基本。 但是，我们只看到HTTP对GET和POST参数的传送渠道（url还是requrest body）提出了要求。“标准答案”里关于参数大小的限制又是从哪来的呢？在我大万维网世界中，还有另一个重要的角色：运输公司。不同的浏览器（发起http请求）和服务器（接受http请求）就是不同的运输公司。 虽然理论上，你可以在车顶上无限的堆货物（url中无限加参数）。但是运输公司可不傻，装货和卸货也是有很大成本的，他们会限制单次运输量来控制风险，数据量太大对浏览器和服务器都是很大负担。业界不成文的规定是，（大多数）浏览器通常都会限制url长度在2K个字节，而（大多数）服务器最多处理64K大小的url。超过的部分，恕不处理。如果你用GET服务，在request body偷偷藏了数据，不同服务器的处理方式也是不同的，有些服务器会帮你卸货，读出数据，有些服务器直接忽略，所以，虽然GET可以带request body，也不能保证一定能被接收到哦。 好了，现在你知道，GET和POST本质上就是TCP链接，并无差别。但是由于HTTP的规定和浏览器/服务器的限制，导致他们在应用过程中体现出一些不同。 GET和POST还有一个重大区别，简单的说：GET产生一个TCP数据包；POST产生两个TCP数据包。 长的说： 对于GET方式的请求，浏览器会把http header和data一并发送出去，服务器响应200（返回数据）； 而对于POST，浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200 ok（返回数据）。 也就是说，GET只需要汽车跑一趟就把货送到了，而POST得跑两趟，第一趟，先去和服务器打个招呼“嗨，我等下要送一批货来，你们打开门迎接我”，然后再回头把货送过去。 因为POST需要两步，时间上消耗的要多一点，看起来GET比POST更有效。因此Yahoo团队有推荐用GET替换POST来优化网站性能。但这是一个坑！跳入需谨慎。为什么？ GET与POST都有自己的语义，不能随便混用。 据研究，在网络环境好的情况下，发一次包的时间和发两次包的时间差别基本可以无视。而在网络环境差的情况下，两次包的TCP在验证数据包完整性上，有非常大的优点。 并不是所有浏览器都会在POST中发送两次包，Firefox就只发送一次。 还有注意几点get请求传参长度的误区实际上HTTP 协议从未规定 GET/POST 的请求长度限制是多少。对get请求参数的限制是来源与浏览器或web服务器，浏览器或web服务器限制了url的长度。为了明确这个概念，我们必须再次强调下面几点: HTTP 协议 未规定 GET 和POST的长度限制 GET的最大长度显示是因为 浏览器和 web服务器限制了 URI的长度 不同的浏览器和WEB服务器，限制的最大长度不一样 要支持IE，则最大长度为2083byte，若只支持Chrome，则最大长度 8182byte 补充get和post请求在缓存方面的区别补充补充一个get和post在缓存方面的区别： get请求类似于查找的过程，用户获取数据，可以不用每次都与数据库连接，所以可以使用缓存。 post不同，post做的一般是修改和删除的工作，所以必须与数据库交互，所以不能使用缓存。因此get请求适合于请求缓存。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"get","slug":"get","permalink":"https://xmmarlowe.github.io/tags/get/"},{"name":"post","slug":"post","permalink":"https://xmmarlowe.github.io/tags/post/"}],"author":"Marlowe"},{"title":"HTTP常见状态码","slug":"计算机网络/HTTP常见状态码","date":"2021-02-09T14:54:22.000Z","updated":"2021-04-23T14:25:41.104Z","comments":true,"path":"2021/02/09/计算机网络/HTTP常见状态码/","link":"","permalink":"https://xmmarlowe.github.io/2021/02/09/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTP%E5%B8%B8%E8%A7%81%E7%8A%B6%E6%80%81%E7%A0%81/","excerpt":"常见的http状态码：100,200,202,204,301,302,404,500,502,503,504…","text":"常见的http状态码：100,200,202,204,301,302,404,500,502,503,504… 123451xx:指示信息-表示请求已接收，继续处理。2xx:成功-表示请求已被成功接收、理解、接受。3xx:重定向-要完成请求必须进行更进一步 的操作。4xx:客户端错误-请求有语法错误或请求无法实现。5xx:服务器端错误-服务器未能实现合法的请求。 100： 这个状态码是告诉客户端应该继续发送请求，这个临时响应是用来通知客户端的，部分的请求服务器已经接受，但是客户端应继续发送求请求的剩余部分，如果请求已经完成，就忽略这个响应，而且服务器会在请求完成后向客户发送一个最终的结果 200： 这个是最常见的http状态码，表示服务器已经成功接受请求，并将返回客户端所请求的最终结果 202： 表示服务器已经接受了请求，但是还没有处理，而且这个请求最终会不会处理还不确定 204： 服务器成功处理了请求，但没有返回任何实体内容 ，可能会返回新的头部元信息 301: 永久性转移(Permanently Moved) 客户端请求的网页已经永久移动到新的位置，当链接发生变化时，返回301代码告诉客户端链接的变化，客户端保存新的链接，并向新的链接发出请求，以返回请求结果。 302: 暂时性转移(Temporarily Moved ) 转向可能会有URL规范化及网址劫持的问题。可能被搜索引擎判为可疑转向，甚至认为是作弊。 比如： 由于搜索引擎排名算法只是程序而不是人，在遇到302重定向的时候，并不能像人一样的去准确判定哪一个网址更适当，这就造成了网址URL劫持的可能性。也就是说，一个不道德的人在他自己的网址A做一个302重定向到你的网址B，出于某种原因， Google搜索结果所显示的仍然是网址A，但是所用的网页内容却是你的网址B上的内容，这种情况就叫做网址URL劫持。你辛辛苦苦所写的内容就这样被别人偷走了。 404： 请求失败，客户端请求的资源没有找到或者是不存在 500： 服务器遇到未知的错误，导致无法完成客户端当前的请求。 502：(Bad Gateway) 作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。 503： 服务器由于临时的服务器过载或者是维护，无法解决当前的请求，以上http状态码是服务器经常返回的状态代码，用户只能通过浏览器的状态了解服务器是否正常运行，一般除了错误的状态码，都不会看到服务器的状态码的. 504：(Gateway Time-out) 作为网关或者代理工作的服务器尝试执行请求时，未能及时从上游服务器（URI标识出的服务器，例如HTTP、FTP、LDAP）或者辅助服务器（例如DNS）收到响应。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://xmmarlowe.github.io/tags/HTTP/"}],"author":"Marlowe"},{"title":"HTTP是不保存状态的协议,如何保存用户状态?","slug":"计算机网络/HTTP是不保存状态的协议-如何保存用户状态","date":"2021-02-09T14:42:30.000Z","updated":"2021-04-23T14:25:44.357Z","comments":true,"path":"2021/02/09/计算机网络/HTTP是不保存状态的协议-如何保存用户状态/","link":"","permalink":"https://xmmarlowe.github.io/2021/02/09/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTP%E6%98%AF%E4%B8%8D%E4%BF%9D%E5%AD%98%E7%8A%B6%E6%80%81%E7%9A%84%E5%8D%8F%E8%AE%AE-%E5%A6%82%E4%BD%95%E4%BF%9D%E5%AD%98%E7%94%A8%E6%88%B7%E7%8A%B6%E6%80%81/","excerpt":"","text":"HTTP 是一种不保存状态，即无状态（stateless）协议。也就是说 HTTP 协议自身不对请求和响应之间的通信状态进行保存。那么我们保存用户状态呢？Session 机制的存在就是为了解决这个问题，Session 的主要作用就是通过服务端记录用户的状态。典型的场景是购物车，当你要添加商品到购物车的时候，系统不知道是哪个用户操作的，因为 HTTP 协议是无状态的。服务端给特定的用户创建特定的 Session 之后就可以标识这个用户并且跟踪这个用户了（一般情况下，服务器会在一定时间内保存这个 Session，过了时间限制，就会销毁这个Session）。 在服务端保存 Session 的方法很多，最常用的就是内存和数据库(比如是使用内存数据库redis保存)。既然 Session 存放在服务器端，那么我们如何实现 Session 跟踪呢？大部分情况下，我们都是通过在 Cookie 中附加一个 Session ID 来方式来跟踪。 Cookie 被禁用怎么办? 最常用的就是利用 URL 重写把 Session ID 直接附加在URL路径的后面。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://xmmarlowe.github.io/tags/HTTP/"}],"author":"Marlowe"},{"title":"HTTP 1.0和HTTP 1.1的主要区别是什么?","slug":"计算机网络/HTTP-1-0和HTTP-1-1的主要区别是什么","date":"2021-02-09T14:39:29.000Z","updated":"2021-04-23T14:25:34.592Z","comments":true,"path":"2021/02/09/计算机网络/HTTP-1-0和HTTP-1-1的主要区别是什么/","link":"","permalink":"https://xmmarlowe.github.io/2021/02/09/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTP-1-0%E5%92%8CHTTP-1-1%E7%9A%84%E4%B8%BB%E8%A6%81%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88/","excerpt":"","text":"HTTP1.0最早在网页中使用是在1996年，那个时候只是使用一些较为简单的网页上和网络请求上，而HTTP1.1则在1999年才开始广泛应用于现在的各大浏览器网络请求中，同时HTTP1.1也是当前使用最为广泛的HTTP协议。 主要区别主要体现在： 长连接 : 在HTTP/1.0中，默认使用的是短连接，也就是说每次请求都要重新建立一次连接。HTTP 是基于TCP/IP协议的,每一次建立或者断开连接都需要三次握手四次挥手的开销，如果每次请求都要这样的话，开销会比较大。因此最好能维持一个长连接，可以用个长连接来发多个请求。HTTP 1.1起，默认使用长连接,默认开启Connection： keep-alive。 HTTP/1.1的持续连接有非流水线方式和流水线方式 。流水线方式是客户在收到HTTP的响应报文之前就能接着发送新的请求报文。与之相对应的非流水线方式是客户在收到前一个响应后才能发送下一个请求。 错误状态响应码: 在HTTP1.1中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除。 缓存处理: 在HTTP1.0中主要使用header里的If-Modified-Since,Expires来做为缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略例如Entity tag，If-Unmodified-Since, If-Match, If-None-Match等更多可供选择的缓存头来控制缓存策略。 带宽优化及网络连接的使用: HTTP1.0中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1则在请求头引入了range头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://xmmarlowe.github.io/tags/HTTP/"}],"author":"Marlowe"},{"title":"Cookie和Session","slug":"计算机网络/Cookie和Session","date":"2021-02-09T14:35:26.000Z","updated":"2021-04-23T14:25:28.591Z","comments":true,"path":"2021/02/09/计算机网络/Cookie和Session/","link":"","permalink":"https://xmmarlowe.github.io/2021/02/09/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/Cookie%E5%92%8CSession/","excerpt":"Cookie的作用是什么? 和Session有什么区别？","text":"Cookie的作用是什么? 和Session有什么区别？ Cookie 和 Session都是用来跟踪浏览器用户身份的会话方式，但是两者的应用场景不太一样。 Cookie 一般用来保存用户信息 比如：①我们在 Cookie 中保存已经登录过得用户信息，下次访问网站的时候页面可以自动帮你登录的一些基本信息给填了；②一般的网站都会有保持登录也就是说下次你再访问网站的时候就不需要重新登录了，这是因为用户登录的时候我们可以存放了一个 Token 在 Cookie 中，下次登录的时候只需要根据 Token 值来查找用户即可(为了安全考虑，重新登录一般要将 Token 重写)；③登录一次网站后访问网站其他页面不需要重新登录。Session 的主要作用就是通过服务端记录用户的状态。 典型的场景是购物车，当你要添加商品到购物车的时候，系统不知道是哪个用户操作的，因为 HTTP 协议是无状态的。服务端给特定的用户创建特定的 Session 之后就可以标识这个用户并且跟踪这个用户了。 Cookie 数据保存在客户端(浏览器端)，Session 数据保存在服务器端。 Cookie 存储在客户端中，而Session存储在服务器上，相对来说 Session 安全性更高。如果要在 Cookie 中存储一些敏感信息，不要直接写入 Cookie 中，最好能将 Cookie 信息加密然后使用到的时候再去服务器端解密。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"Session","slug":"Session","permalink":"https://xmmarlowe.github.io/tags/Session/"},{"name":"Cookie","slug":"Cookie","permalink":"https://xmmarlowe.github.io/tags/Cookie/"}],"author":"Marlowe"},{"title":"TCP和UDP的区别，以及使用场景","slug":"计算机网络/TCP和UDP的区别，以及使用场景","date":"2021-02-07T14:50:57.000Z","updated":"2021-04-27T14:47:58.873Z","comments":true,"path":"2021/02/07/计算机网络/TCP和UDP的区别，以及使用场景/","link":"","permalink":"https://xmmarlowe.github.io/2021/02/07/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP%E5%92%8CUDP%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%8C%E4%BB%A5%E5%8F%8A%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF/","excerpt":"","text":"UDP 在传送数据之前不需要先建立连接，远地主机在收到 UDP 报文后，不需要给出任何确认。虽然 UDP 不提供可靠交付，但在某些情况下 UDP 确是一种最有效的工作方式（一般用于即时通信），比如： QQ 语音、 QQ 视频 、直播等等 TCP 提供面向连接的服务。在传送数据之前必须先建立连接，数据传送结束后要释放连接。 TCP 不提供广播或多播服务。由于 TCP 要提供可靠的，面向连接的传输服务（TCP的可靠体现在TCP在传递数据之前，会有三次握手来建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制，在数据传完后，还会断开连接用来节约系统资源），这一难以避免增加了许多开销，如确认，流量控制，计时器以及连接管理等。这不仅使协议数据单元的首部增大很多，还要占用许多处理机资源。TCP 一般用于文件传输、发送和接收邮件、远程登录等场景。 视频通话为什么采用UDPUDP 没有拥塞控制，一直会以恒定的速度发送数据。即使网络条件不好，也不会对发送速率进行调整。这样实现的弊端就是在网络条件不好的情况下可能会导致丢包，但是优点也很明显，在某些实时性要求高的场景（比如电话会议）就需要使用 UDP 而不是 TCP。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"TCP","slug":"TCP","permalink":"https://xmmarlowe.github.io/tags/TCP/"},{"name":"UDP","slug":"UDP","permalink":"https://xmmarlowe.github.io/tags/UDP/"}],"author":"Marlowe"},{"title":"B+树存储数据计算","slug":"数据库/B-树存储数据计算","date":"2021-01-23T13:50:52.000Z","updated":"2021-04-27T13:33:16.249Z","comments":true,"path":"2021/01/23/数据库/B-树存储数据计算/","link":"","permalink":"https://xmmarlowe.github.io/2021/01/23/%E6%95%B0%E6%8D%AE%E5%BA%93/B-%E6%A0%91%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97/","excerpt":"B+树存储数据量计算…","text":"B+树存储数据量计算… 问题引入InnoDB一棵B+树可以存放多少行数据？这个问题的简单回答是：约2千万。为什么是这么多呢？因为这是可以算出来的，要搞清楚这个问题，我们先从InnoDB索引数据结构、数据组织方式说起。我们都知道计算机在存储数据的时候，有最小存储单元，这就好比我们今天进行现金的流通最小单位是一毛。在计算机中磁盘存储数据最小单元是扇区，一个扇区的大小是512字节，而文件系统（例如XFS/EXT4）他的最小单元是块，一个块的大小是4k，而对于我们的InnoDB存储引擎也有自己的最小储存单元——页（Page），一个页的大小是16K。下面几张图可以帮你理解最小存储单元：文件系统中一个文件大小只有1个字节，但不得不占磁盘上4KB的空间。 innodb的所有数据文件（后缀为ibd的文件），他的大小始终都是16384（16k）的整数倍。 磁盘扇区、文件系统、InnoDB存储引擎都有各自的最小存储单元。 在MySQL中我们的InnoDB页的大小默认是16k，当然也可以通过参数设置： 1show variables like &#x27;innodb_page_size&#x27; 数据表中的数据都是存储在页中的，所以一个页中能存储多少行数据呢？假设一行数据的大小是1k，那么一个页可以存放16行这样的数据。 如果数据库只按这样的方式存储，那么如何查找数据就成为一个问题，因为我们不知道要查找的数据存在哪个页中，也不可能把所有的页遍历一遍，那样太慢了。所以人们想了一个办法，用B+树的方式组织这些数据。如图所示： 我们先将数据记录按主键进行排序，分别存放在不同的页中（为了便于理解我们这里一个页中只存放3条记录，实际情况可以存放很多），除了存放数据的页以外，还有存放键值+指针的页，如图中page number=3的页，该页存放键值和指向数据页的指针，这样的页由N个键值+指针组成。当然它也是排好序的。这样的数据组织形式，我们称为索引组织表。现在来看下，要查找一条数据，怎么查？ 如select * from user where id=5; 这里id是主键,我们通过这棵B+树来查找，首先找到根页，你怎么知道user表的根页在哪呢？其实每张表的根页位置在表空间文件中是固定的，即page number=3的页（这点我们下文还会进一步证明），找到根页后通过二分查找法，定位到id=5的数据应该在指针P5指向的页中，那么进一步去page number=5的页中查找，同样通过二分查询法即可找到id=5的记录： 现在我们清楚了InnoDB中主键索引B+树是如何组织数据、查询数据的，我们总结一下： InnoDB存储引擎的最小存储单元是页，页可以用于存放数据也可以用于存放键值+指针，在B+树中叶子节点存放数据，非叶子节点存放键值+指针。 索引组织表通过非叶子节点的二分查找法以及指针确定数据在哪个页中，进而在去数据页中查找到需要的数据； 那么回到我们开始的问题，通常一棵B+树可以存放多少行数据？ 这里我们先假设B+树高为2，即存在一个根节点和若干个叶子节点，那么这棵B+树的存放总记录数为：根节点指针数*单个叶子节点记录行数。 上文我们已经说明单个叶子节点（页）中的记录数=16K/1K=16。（这里假设一行记录的数据大小为1k，实际上现在很多互联网业务数据记录大小通常就是1K左右）。 那么现在我们需要计算出非叶子节点能存放多少指针，其实这也很好算，我们假设主键ID为bigint类型，长度为8字节，而指针大小在InnoDB源码中设置为6字节，这样一共14字节，我们一个页中能存放多少这样的单元，其实就代表有多少指针(根节点)，即16 * 1024 / 14=1170。那么可以算出一棵高度为2的B+树，能存放1170*16=18720条这样的数据记录。 根据同样的原理我们可以算出一个高度为3的B+树可以存放：1170117016=21902400条这样的记录。 所以在InnoDB中B+树高度一般为1-3层，它就能满足千万级的数据存储。在查找数据时一次页的查找代表一次IO，所以通过主键索引查询通常只需要1-3次IO操作即可查找到数据。 怎么得到InnoDB主键索引B+树的高度？上面我们通过推断得出B+树的高度通常是1-3，下面我们从另外一个侧面证明这个结论。在InnoDB的表空间文件中，约定page number为3的代表主键索引的根页，而在根页偏移量为64的地方存放了该B+树的page level。如果page level为1，树高为2，page level为2，则树高为3。即B+树的高度=page level+1；下面我们将从实际环境中尝试找到这个page level。 在实际操作之前，你可以通过InnoDB元数据表确认主键索引根页的page number为3，你也可以从《InnoDB存储引擎》这本书中得到确认。 1234567SELECTb.name, a.name, index_id, type, a.space, a.PAGE_NOFROMinformation_schema.INNODB_SYS_INDEXES a,information_schema.INNODB_SYS_TABLES bWHEREa.table_id = b.table_id AND a.space &lt;&gt; 0; 执行结果： 可以看出数据库dbt3下的customer表、lineitem表主键索引根页的page number均为3，而其他的二级索引page number为4。 下面我们对数据库表空间文件做想相关的解析： 因为主键索引B+树的根页在整个表空间文件中的第3个页开始，所以可以算出它在文件中的偏移量：(16 * 1024) * 3=49152（16384为页大小）。 另外根据《InnoDB存储引擎》中描述在根页的64偏移量位置前2个字节，保存了page level的值，因此我们想要的page level的值在整个文件中的偏移量为：16384*3+64=49152+64=49216，前2个字节中。 接下来我们用hexdump工具，查看表空间文件指定偏移量上的数据： linetem表的page level为2，B+树高度为page level+1=3； region表的page level为0，B+树高度为page level+1=1； customer表的page level为2，B+树高度为page level+1=3； 这三张表的数据量如下： 总结lineitem表的数据行数为600多万，B+树高度为3，customer表数据行数只有15万，B+树高度也为3。可以看出尽管数据量差异较大，这两个表树的高度都是3，换句话说这两个表通过索引查询效率并没有太大差异，因为都只需要做3次IO。那么如果有一张表行数是一千万，那么他的B+树高度依旧是3，查询效率仍然不会相差太大。 region表只有5行数据，当然他的B+树高度为1。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"B+Tree","slug":"B-Tree","permalink":"https://xmmarlowe.github.io/tags/B-Tree/"}],"author":"Marlowe"},{"title":"初识BufferPoll","slug":"数据库/初识BufferPoll","date":"2021-01-19T08:46:54.000Z","updated":"2021-04-23T14:24:26.920Z","comments":true,"path":"2021/01/19/数据库/初识BufferPoll/","link":"","permalink":"https://xmmarlowe.github.io/2021/01/19/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%88%9D%E8%AF%86BufferPoll/","excerpt":"","text":"Buffer Pool是什么? 是一块内存区域，当数据库操作数据的时候，把硬盘上的数据加载到buffer pool，不直接和硬盘打交道，操作的是buffer pool里面的数据 数据库的增删改查都是在buffer pool上进行，和undo log/redo log/redo log buffer/binlog一起使用，后续会把数据刷到硬盘上 默认大小 128M 数据页 磁盘文件被分成很多数据页，一个数据页里面有很多行数据 一个数据页默认大小 16K 更新一行数据，实际上是把行数据所在的 数据页 整个加载到buffer pool中 缓存页 buffer pool中存放的数据页我们叫缓存页，和磁盘上的数据页是一一对应的，都是16KB 缓存页的数据，是从磁盘上加载到buffer pool当中的 缓存页描述信息（描述信息块） 存的是数据页所属的表空间号，数据页编号，数据页地址等信息 放在缓存页的前面 每个描述信息块大小是缓存页的5%左右，大约是 1610240.05=800个字节 Buffer Pool初始化 数据库只要一启动，就会按照你设置的Buffer Pool大小，稍微再加大一点，去找操作系统申请一块内存区域，作为Buffer Pool的内存区域 然后当内存区域申请完毕之后，数据库就会按照默认的缓存页的16KB的大小以及对应的800个字节左右的描述数据的大小，在Buffer Pool中划分出来一个一个的缓存页和一个一个的他们对应的描述数据 free链表作用帮助我们找到空闲的缓存页 是一个双向链表，链表节点是空闲的缓存页对应的描述信息块（空的缓存页） 链表上除了描述信息块，还有一个基础节点，存储了free链有多少个描述信息块，也就是有多少个空闲的缓存页 当我们加载数据的时候，会从free链中找到空闲的缓存页，把数据页的表空间号和数据页号写入描述信息块；加载数据到缓存页后，会把缓存页对应的描述信息块从free链表中移除 怎么知道数据页是否被缓存？ 数据库中有一个 数据页缓存哈希表，用表空间号+数据页号，作为一个key，然后缓存页的地址作为value 表空间号+数据页号 = 缓存页地址 什么是脏缓存页？ 被更新过的缓存页，数据和磁盘上的数据不一致，所以是脏缓存页 脏缓存页的数据是要刷到磁盘上的 flush链表作用帮我们找到脏缓存页，也就是需要刷盘的缓存页 是一个双向链表，链表结点是被修改过的缓存页的描述信息块（更新过的缓存页） 和free链表一样，也有一个基础结点，链接首尾结点，并存储了有多少个描述信息块 最后要把flush链表上结点对应的缓存页刷盘，后台线程会在MySQL不怎么繁忙的时候，找个时间把flush链表中的缓存页都刷入磁盘中，这样被你修改过的数据，迟早都会刷入磁盘的；缓存页从flush链表中移除，加入到free链表当中 LRU链表作用用来淘汰不常被访问的缓存页 是一个双向链表，链表结点是 非空的缓存页对应的描述信息块（有数据的缓存页，包含更新过和未更新过的缓存页，范围比flush链表大，flush链表是它的子集） LRU链表分为热数据区和冷数据区，冷数据区占了总链表的37% (5:3) 冷数据区是不常访问的缓存页 热数据区是经常访问的缓存页 加载数据的时候，缓存页会放在冷数据区的头部 数据页加载到缓存页后，在1s之后，访问该缓存页，该缓存页会被移动到热数据区头部 数据页刚加载到缓存页后，在1s之内，访问该缓存页，该缓存页是不会被移动到热数据区头部的 什么时候会lru中的缓存页刷盘并清空？ 当缓存页用完的时候，把冷数据区尾部的缓存页刷盘清空，缓存页对应的信息描述块从lru链表中移除，加入到free链表当中 有一个后台线程，他会运行一个定时任务，这个定时任务每隔一段时间就会把LRU链表的冷数据区域的尾部的一些缓存页，刷入磁盘里去，清空这几个缓存页，把他们加入回free链表去；如果该缓存页也在flush链表中（该缓存页更新过），也需要把该缓存页从flush链表中移除 热数据区的前1/4的缓存页如果被访问，是不会移动到热数据区头部的；后3/4的缓存页被访问了，才会移动到热数据区头部 预读机制 所谓预读机制，说的就是当你从磁盘上加载一个数据页的时候，他可能会连带着把这个数据页相邻的其他数据页，也加载到缓存里去 什么时候会触发预读机制？ 有一个参数是innodb_read_ahead_threshold，他的默认值是56，意思就是如果顺序的访问了一个区里的多个数据页，访问的数据页的数量超过了这个阈值，此时就会触发预读机制，把下一个相邻区中的所有数据页都加载到缓存里去 如果Buffer Pool里缓存了一个区里的13个连续的数据页，而且这些数据页都是比较频繁会被访问的，此时就会直接触发预读机制，把这个区里的其他的数据页都加载到缓存里去 全表扫描的时候，select * from tableName 会把该表所有的数据页都缓存到buffer pool当中 Buffer Pool的缓存页以及几个链表的使用回顾 数据库启动时，会申请内存创建buffer pool，buffer pool分成一个个缓存页及其缓存页描述信息块，描述信息块加入到free链表中 数据加载到一个缓存页，free链表里会移除这个缓存页，然后lru链表的冷数据区域的头部会放入这个缓存页 如果查询了一个缓存页，那么此时就会把这个缓存页在lru链表中移动到热数据区域去，或者在热数据区域中也有可能会移动到头部去 如果更新了缓存页，会把该缓存页加入到flush链表中 如果缓存页不够用了，会把lru冷数据区尾部的缓存页刷盘，清空；该缓存页从lru链表和flush链表中移除，加入到free链表中 mysql后台线程也会定时把lru冷数据区尾部的缓存页刷盘，清空；定时把flush链表中的缓存页刷盘，清空，加入到free链表中 总结 一边不停的加载数据到缓存页里去，不停的查询和修改缓存数据，然后free链表中的缓存页不停的在减少，flush链表中的缓存页不停的在增加，lru链表中的缓存页不停的在增加和移动 另外一边，你的后台线程不停的在把lru链表的冷数据区域的缓存页以及flush链表的缓存页，刷入磁盘中来清空缓存页，然后flush链表和lru链表中的缓存页在减少，free链表中的缓存页在增加 参考buffer pool详解","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"BufferPoll","slug":"BufferPoll","permalink":"https://xmmarlowe.github.io/tags/BufferPoll/"}],"author":"Marlowe"},{"title":"MySQL中SQL是如何执行的？","slug":"数据库/MySQL中SQL是如何执行的？","date":"2021-01-16T13:49:12.000Z","updated":"2021-08-25T15:09:45.745Z","comments":true,"path":"2021/01/16/数据库/MySQL中SQL是如何执行的？/","link":"","permalink":"https://xmmarlowe.github.io/2021/01/16/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL%E4%B8%ADSQL%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84%EF%BC%9F/","excerpt":"简单分析sql的执行过程…","text":"简单分析sql的执行过程… 一、MySQL 基础架构分析MySQL 基本架构概览下图是 MySQL 的一个简要架构图，从下图你可以很清晰的看到用户的 SQL 语句在 MySQL 内部是如何执行的。 先简单介绍一下下图涉及的一些组件的基本作用帮助大家理解这幅图，在 1.2 节中会详细介绍到这些组件的作用。 连接器： 身份认证和权限相关(登录 MySQL 的时候)。 查询缓存: 执行查询语句的时候，会先查询缓存（MySQL 8.0 版本后移除，因为这个功能不太实用）。 分析器: 没有命中缓存的话，SQL 语句就会经过分析器，分析器说白了就是要先看你的 SQL 语句要干嘛，再检查你的 SQL 语句语法是否正确。 优化器： 按照 MySQL 认为最优的方案去执行。 执行器: 执行语句，然后从存储引擎返回数据。 简单来说 MySQL 主要分为 Server 层和存储引擎层： Server 层：主要包括连接器、查询缓存、分析器、优化器、执行器等，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图，函数等，还有一个通用的日志模块 binglog 日志模块。 存储引擎： 主要负责数据的存储和读取，采用可以替换的插件式架构，支持 InnoDB、MyISAM、Memory 等多个存储引擎，其中 InnoDB 引擎有自有的日志模块 redolog 模块。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始就被当做默认存储引擎了。 Server 层基本组件介绍1) 连接器连接器主要和身份认证和权限相关的功能相关，就好比一个级别很高的门卫一样。 主要负责用户登录数据库，进行用户的身份认证，包括校验账户密码，权限等操作，如果用户账户密码已通过，连接器会到权限表中查询该用户的所有权限，之后在这个连接里的权限逻辑判断都是会依赖此时读取到的权限数据，也就是说，后续只要这个连接不断开，即时管理员修改了该用户的权限，该用户也是不受影响的。 2) 查询缓存(MySQL 8.0 版本后移除)查询缓存主要用来缓存我们所执行的 SELECT 语句以及该语句的结果集。 连接建立后，执行查询语句的时候，会先查询缓存，MySQL 会先校验这个 sql 是否执行过，以 Key-Value 的形式缓存在内存中，Key 是查询预计，Value 是结果集。如果缓存 key 被命中，就会直接返回给客户端，如果没有命中，就会执行后续的操作，完成后也会把结果缓存起来，方便下一次调用。当然在真正执行缓存查询的时候还是会校验用户的权限，是否有该表的查询条件。 MySQL 查询不建议使用缓存，因为查询缓存失效在实际业务场景中可能会非常频繁，假如你对一个表更新的话，这个表上的所有的查询缓存都会被清空。对于不经常更新的数据来说，使用缓存还是可以的。 所以，一般在大多数情况下我们都是不推荐去使用查询缓存的。 MySQL 8.0 版本后删除了缓存的功能，官方也是认为该功能在实际的应用场景比较少，所以干脆直接删掉了。 3) 分析器MySQL 没有命中缓存，那么就会进入分析器，分析器主要是用来分析 SQL 语句是来干嘛的，分析器也会分为几步： 第一步，词法分析，一条 SQL 语句有多个字符串组成，首先要提取关键字，比如 select，提出查询的表，提出字段名，提出查询条件等等。做完这些操作后，就会进入第二步。 第二步，语法分析，主要就是判断你输入的 sql 是否正确，是否符合 MySQL 的语法。 完成这 2 步之后，MySQL 就准备开始执行了，但是如何执行，怎么执行是最好的结果呢？这个时候就需要优化器上场了。 4) 优化器优化器的作用就是它认为的最优的执行方案去执行（有时候可能也不是最优，这篇文章涉及对这部分知识的深入讲解），比如多个索引的时候该如何选择索引，多表查询的时候如何选择关联顺序等。 可以说，经过了优化器之后可以说这个语句具体该如何执行就已经定下来。 5) 执行器当选择了执行方案后，MySQL 就准备开始执行了，首先执行前会校验该用户有没有权限，如果没有权限，就会返回错误信息，如果有权限，就会去调用引擎的接口，返回接口执行的结果。 二、语句分析查询语句说了以上这么多，那么究竟一条 sql 语句是如何执行的呢？其实我们的 sql 可以分为两种，一种是查询，一种是更新（增加，更新，删除）。我们先分析下查询语句，语句如下： 1select * from tb_student A where A.age=&#x27;18&#x27; and A.name=&#x27; 张三 &#x27;; 结合上面的说明，我们分析下这个语句的执行流程： 先检查该语句是否有权限，如果没有权限，直接返回错误信息，如果有权限，在 MySQL8.0 版本以前，会先查询缓存，以这条 sql 语句为 key 在内存中查询是否有结果，如果有直接缓存，如果没有，执行下一步。 通过分析器进行词法分析，提取 sql 语句的关键元素，比如提取上面这个语句是查询 select，提取需要查询的表名为 tb_student,需要查询所有的列，查询条件是这个表的 id=’1’。然后判断这个 sql 语句是否有语法错误，比如关键词是否正确等等，如果检查没问题就执行下一步。 接下来就是优化器进行确定执行方案，上面的 sql 语句，可以有两种执行方案： 12a.先查询学生表中姓名为“张三”的学生，然后判断是否年龄是 18。b.先找出学生中年龄 18 岁的学生，然后再查询姓名为“张三”的学生。 那么优化器根据自己的优化算法进行选择执行效率最好的一个方案（优化器认为，有时候不一定最好）。那么确认了执行计划后就准备开始执行了。 进行权限校验，如果没有权限就会返回错误信息，如果有权限就会调用数据库引擎接口，返回引擎的执行结果。 更新语句以上就是一条查询 sql 的执行流程，那么接下来我们看看一条更新语句如何执行的呢？sql 语句如下： 1update tb_student A set A.age=&#x27;19&#x27; where A.name=&#x27; 张三 &#x27;; 我们来给张三修改下年龄，在实际数据库肯定不会设置年龄这个字段的，不然要被技术负责人打的。其实条语句也基本上会沿着上一个查询的流程走，只不过执行更新的时候肯定要记录日志啦，这就会引入日志模块了，MySQL 自带的日志模块式binlog（归档日志） ，所有的存储引擎都可以使用，我们常用的 InnoDB 引擎还自带了一个日志模块 redo log（重做日志），我们就以 InnoDB 模式下来探讨这个语句的执行流程。流程如下： 先查询到张三这一条数据，如果有缓存，也是会用到缓存。 然后拿到查询的语句，把 age 改为 19，然后调用引擎 API 接口，写入这一行数据，InnoDB 引擎把数据保存在内存中，同时记录 redo log，此时 redo log 进入 prepare 状态，然后告诉执行器，执行完成了，随时可以提交。 执行器收到通知后记录 binlog，然后调用引擎接口，提交 redo log 为提交状态。 更新完成。 这里肯定有同学会问，为什么要用两个日志模块，用一个日志模块不行吗? 这是因为最开始 MySQL 并没与 InnoDB 引擎( InnoDB 引擎是其他公司以插件形式插入 MySQL 的) ，MySQL 自带的引擎是 MyISAM，但是我们知道 redo log 是 InnoDB 引擎特有的，其他存储引擎都没有，这就导致会没有 crash-safe 的能力(crash-safe 的能力即使数据库发生异常重启，之前提交的记录都不会丢失)，binlog 日志只能用来归档。 并不是说只用一个日志模块不可以，只是 InnoDB 引擎就是通过 redo log 来支持事务的。那么，又会有同学问，我用两个日志模块，但是不要这么复杂行不行，为什么 redo log 要引入 prepare 预提交状态？这里我们用反证法来说明下为什么要这么做？ 先写 redo log 直接提交，然后写 binlog， 假设写完 redo log 后，机器挂了，binlog 日志没有被写入，那么机器重启后，这台机器会通过 redo log 恢复数据，但是这个时候 binlog 并没有记录该数据，后续进行机器备份的时候，就会丢失这一条数据，同时主从同步也会丢失这一条数据。 先写 binlog，然后写 redo log， 假设写完了 binlog，机器异常重启了，由于没有 redo log，本机是无法恢复这一条记录的，但是 binlog 又有记录，那么和上面同样的道理，就会产生数据不一致的情况。 如果采用 redo log 两阶段提交的方式就不一样了，写完 binlog 后，然后再提交 redo log 就会防止出现上述的问题，从而保证了数据的一致性。那么问题来了，有没有一个极端的情况呢？假设 redo log 处于预提交状态，binglog 也已经写完了，这个时候发生了异常重启会怎么样呢？ 这个就要依赖于 MySQL 的处理机制了，MySQL 的处理过程如下： 判断 redo log 是否完整，如果判断是完整的，就立即提交。 如果 redo log 只是预提交但不是 commit 状态，这个时候就会去判断 binlog 是否完整，如果完整就提交 redo log, 不完整就回滚事务。 这样就解决了数据一致性的问题。 三、总结 MySQL 主要分为 Server 层和引擎层，Server 层主要包括连接器、查询缓存、分析器、优化器、执行器，同时还有一个日志模块（binlog），这个日志模块所有执行引擎都可以共用,redolog 只有 InnoDB 有。 引擎层是插件式的，目前主要包括，MyISAM,InnoDB,Memory 等。 查询语句的执行流程如下：权限校验（如果命中缓存）—&gt;查询缓存—&gt;分析器—&gt;优化器—&gt;权限校验—&gt;执行器—&gt;引擎 更新语句执行流程如下：分析器—-&gt;权限校验—-&gt;执行器—&gt;引擎—redo log(prepare 状态—&gt;binlog—&gt;redo log(commit状态)","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://xmmarlowe.github.io/tags/MySQL/"}],"author":"Marlowe"},{"title":"MySQL什么时候适合建索引，什么时候不适合建索引？","slug":"数据库/MySQL什么时候适合建索引，什么时候不适合建索引？","date":"2021-01-16T13:41:00.000Z","updated":"2021-08-26T10:09:02.851Z","comments":true,"path":"2021/01/16/数据库/MySQL什么时候适合建索引，什么时候不适合建索引？/","link":"","permalink":"https://xmmarlowe.github.io/2021/01/16/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E9%80%82%E5%90%88%E5%BB%BA%E7%B4%A2%E5%BC%95%EF%BC%8C%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E4%B8%8D%E9%80%82%E5%90%88%E5%BB%BA%E7%B4%A2%E5%BC%95%EF%BC%9F/","excerpt":"","text":"1、什么是索引（本质：数据结构）索引是帮助MySQL高效获取数据的数据结构。 2、优势 提高数据检索的效率，降低数据库IO成本 通过索引对数据进行排序，降低数据排序的成本，降低了CPU的消耗 3、劣势 降低更新表的速度，如对表进行update 、delete、insert等操作时，MySQL不仅要保存数据，还要保存一下索引文件每次添加了索引列的字段，都会调整因为更新带来的键值变化后的索引信息。 4、适合创建索引条件 主键自动建立唯一索引 频繁作为查询条件的字段应该建立索引 查询中与其他表关联的字段，外键关系建立索引 单键/组合索引的选择问题，组合索引性价比更高 查询中排序的字段，排序字段若通过索引去访问将大大提高排序效率 查询中统计或者分组字段 5、不适合创建索引条件 表记录少的 经常增删改的表或者字段 where条件里用不到的字段不创建索引 过滤性不好的不适合建索引","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://xmmarlowe.github.io/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"https://xmmarlowe.github.io/tags/%E7%B4%A2%E5%BC%95/"}],"author":"Marlowe"},{"title":"MySQL怎么让左模糊查询走索引？","slug":"数据库/MySQL怎么让左模糊查询走索引？","date":"2021-01-16T13:37:00.000Z","updated":"2021-04-23T14:25:06.094Z","comments":true,"path":"2021/01/16/数据库/MySQL怎么让左模糊查询走索引？/","link":"","permalink":"https://xmmarlowe.github.io/2021/01/16/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL%E6%80%8E%E4%B9%88%E8%AE%A9%E5%B7%A6%E6%A8%A1%E7%B3%8A%E6%9F%A5%E8%AF%A2%E8%B5%B0%E7%B4%A2%E5%BC%95%EF%BC%9F/","excerpt":"","text":"需要做模糊匹配，又要用到索引，索引的最左匹配原则更是不能被打破，这时候可以增加一个字段，这个字段的内容等于USER_NAME字段内容的反转，同时加上这个字段的相关索引，如下： 此时如果是要模糊搜索出用户名后几位有杰这个词的所有用户信息，可以对REVERSE_USER_NAME字段做右模糊查询，效果其实就是和对USER_NAME字段做左模糊查询是一样的，因为二者的内容是相反的，结果如下： 1SELECT * from USER_INFO where REVERSE_USER_NAME like &#x27;杰%&#x27; 总结索引的最左匹配原则不能打破，那么要让左匹配也走索引的话，换个思路，让右匹配的效果和左匹配一样就好了，同时右匹配又能走索引，间接达到了左模糊查询也能走索引的目的。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://xmmarlowe.github.io/tags/MySQL/"}],"author":"Marlowe"},{"title":"left join、right join和join的区别","slug":"数据库/left-join、right-join和join的区别","date":"2021-01-15T09:02:11.000Z","updated":"2021-04-23T14:24:51.139Z","comments":true,"path":"2021/01/15/数据库/left-join、right-join和join的区别/","link":"","permalink":"https://xmmarlowe.github.io/2021/01/15/%E6%95%B0%E6%8D%AE%E5%BA%93/left-join%E3%80%81right-join%E5%92%8Cjoin%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"简要介绍三种连接查询的区别…","text":"简要介绍三种连接查询的区别… 首先，我们先来建两张表，第一张表命名为kemu，第二张表命名为score： left join顾名思义，就是“左连接”，表1左连接表2，以左为主，表示以表1为主，关联上表2的数据，查出来的结果显示左边的所有数据，然后右边显示的是和左边有交集部分的数据。如下： 12345select *from kemuleft join score on kemu.id = score.id 结果集： right join“右连接”，表1右连接表2，以右为主，表示以表2为主，关联查询表1的数据，查出表2所有数据以及表1和表2有交集的数据，如下： 12345select *from kemuright join score on kemu.id = score.id 结果集： join(inner join)join，其实就是“inner join”，为了简写才写成join，两个是表示一个的，内连接，表示以两个表的交集为主，查出来是两个表有交集的部分，其余没有关联就不额外显示出来，这个用的情况也是挺多的，如下： 12345select *from kemujoin score on kemu.id = score.id 结果集： 参考【mySQL】left join、right join和join的区别","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"join","slug":"join","permalink":"https://xmmarlowe.github.io/tags/join/"}],"author":"Marlowe"},{"title":"初识MySQL索引实现原理","slug":"数据库/初识MySQL索引实现原理","date":"2021-01-13T01:13:32.000Z","updated":"2021-08-26T10:09:02.885Z","comments":true,"path":"2021/01/13/数据库/初识MySQL索引实现原理/","link":"","permalink":"https://xmmarlowe.github.io/2021/01/13/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%88%9D%E8%AF%86MySQL%E7%B4%A2%E5%BC%95%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","excerpt":"MySQL索引的实现原理，以及什么时候会走索引…","text":"MySQL索引的实现原理，以及什么时候会走索引… 目前大部分数据库系统及文件系统都采用B-Tree(B树)或其变种B+Tree(B+树)作为索引结构。B+Tree是数据库系统实现索引的首选数据结构。在MySQL中,索引属于存储引擎级别的概念,不同存储引擎对索引的实现方式是不同的,本文主要讨论MyISAM和InnoDB两个存储引擎的索引实现方式。 在 MySQL 中,索引属于存储引擎级别的概念,不同存储引擎对索引的实现方式是不同的,本文主要讨论 MyISAM 和 InnoDB 两个存储引擎的索引实现方式。 MyISAM 索引实现1. 主键索引MyISAM 引擎使用 B+Tree 作为索引结构,叶节点的 data 域存放的是数据记录的地址。 下图是 MyISAM 索引的原理图: 这里设表一共有三列,假设我们以 Col1 为主键,则图 8 是一个 MyISAM 表的主索引(Primary key)示意。可以看出 MyISAM 的索引文件仅仅保存数据记录的地址。 2. 辅助索引在 MyISAM 中,主索引和辅助索引(Secondary key)在结构上没有任何区别,只是主索引要求 key 是唯一的,而辅助索引的 key 可以重复。如果我们在 Col2 上建立一个辅助索引,则此索引的结构如下图所示： 同样也是一颗 B+Tree,data 域保存数据记录的地址。因此,MyISAM 中索引检索的算法为首先按照 B+Tree 搜索算法搜索索引,如果指定的 Key 存在,则取出其data 域的值,然后以 data 域的值为地址,读取相应数据记录。 MyISAM 的索引方式也叫做“非聚集索引”, 之所以这么称呼是为了与 InnoDB的聚集索引区分。 InnoDB 索引实现虽然 InnoDB 也使用 B+Tree 作为索引结构,但具体实现方式却与 MyISAM 截然不同。 1. 主键索引1.第一个重大区别是 InnoDB 的数据文件本身就是索引文件。从上文知道,MyISAM 索引文件和数据文件是分离的,索引文件仅保存数据记录的地址。 而在InnoDB 中,表数据文件本身就是按 B+Tree 组织的一个索引结构,这棵树的叶点data 域保存了完整的数据记录。这个索引的 key 是数据表的主键,因此 InnoDB 表数据文件本身就是主索引。 上图是 InnoDB 主索引(同时也是数据文件)的示意图,可以看到叶节点包含了完整的数据记录。这种索引叫做聚集索引。因为 InnoDB 的数据文件本身要按主键聚集，InnoDB 要求表必须有主键(MyISAM 可以没有),如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。 同时,请尽量在 InnoDB 上采用自增字段做表的主键。因为 InnoDB 数据文件本身是一棵B+Tree,非单调的主键会造成在插入新记录时数据文件为了维持 B+Tree 的特性而频繁的分裂调整,十分低效,而使用自增字段作为主键则是一个很好的选择。如果表使用自增主键,那么每次插入新的记录,记录就会顺序添加到当前索引节点的后续位置,当一页写满,就会自动开辟一个新的页。如下图所示: 这样就会形成一个紧凑的索引结构,近似顺序填满。由于每次插入时也不需要移动已有数据,因此效率很高,也不会增加很多开销在维护索引上。 2. 辅助索引第二个与 MyISAM 索引的不同是 InnoDB 的辅助索引 data 域存储相应记录主键的值而不是地址。 换句话说,InnoDB 的所有辅助索引都引用主键作为 data 域。 例如,下图为定义在 Col3 上的一个辅助索引: 聚集索引这种实现方式使得按主键的搜索十分高效,但是辅助索引搜索需要检索两遍索引:首先检索辅助索引获得主键,然后用主键到主索引中检索获得记录。 拓展点: 这个重新根据主键id去查询行数据的行为被称为回表查询。所以知道为啥叫辅助了,辅助就是先找到大哥(主键),再根据主键去查到最终想要的数据。(不准抢人头)。 说到这里可以再点一下题，上面这样分的原因。方便大家记住，innodb下主键索引和非主键索引的主要区别就在于这里，主键索引直接可以拿到行数据,而非主键索引都需要回表,所以会增加Io次数,这就说明了为什么主键索引的速度最快。 引申:为什么不建议使用过长的字段作为主键? 因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。 再例如，用非单调的字段作为主键在InnoDB中不是个好主意，因为InnoDB数据文件本身是一颗B+Tree，非单调的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效， 而使用自增字段作为主键则是一个很好的选择。 InnoDB索引和MyISAM索引的区别 主索引的区别，InnoDB的数据文件本身就是索引文件，而MyISAM的索引和数据是分开的。 辅助索引的区别，InnoDB的辅助索引data域存储相应记录主键的值而不是地址。而MyISAM的辅助索引和主索引没有多大区别。 联合索引及最左原则联合索引存储数据结构图： 最左原则： 例如联合索引有三个索引字段（A,B,C） 查询条件： （A，，）—会使用索引 （A，B，）—会使用索引 （A，B，C）—会使用索引 （，B，C）—不会使用索引 （，，C）—不会使用索引 什么时候会走索引？一句话，当查询的数据是有序的时候，比如对一个对象数组进行排序后，他会默认从第一个字段到最后一个字段按照顺序排序，如果在上述例子中查询（，B，C），那么B这个属性不一定是有序的，B有序的前提是A有序的情况下，因此不会走索引。 注意： 但是你说我a、b换下位置可不可以,比如select * from table where a=xx and b=xx或者select * from table where b=xx and a=xx,这样是可以的,mysql没这么傻这样就认不出来了,查询优化器会帮你把位置调整过来的. mysql假设一行数据大小为1k，则一颗层高为3的b+树可以存放多少条数据？mysql页默认大小16k，如果数据行大小1k，叶子节点存放的完整数据，则叶子节点一页可以放16条数据；非叶子节点页面存放的是主键和指针，所以主要看主键是啥类型，假设是integer，则长度8字节，指针大小在innodb是6字节，一共14字节，所以非叶子节点每页可以存16384/14=1170个主键数据（1170个分叉），则三层b+树数据可以存1170117016=21902400条数据。（千万级别） 参考MySQL索引实现原理分析 MyISAM与InnoDB两类存储引擎的索引实现","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://xmmarlowe.github.io/tags/MySQL/"},{"name":"InnoDB","slug":"InnoDB","permalink":"https://xmmarlowe.github.io/tags/InnoDB/"},{"name":"MyISAM","slug":"MyISAM","permalink":"https://xmmarlowe.github.io/tags/MyISAM/"}],"author":"Marlowe"},{"title":"InnoDB引擎的特点以及调优","slug":"数据库/InnoDB引擎的特点以及调优","date":"2021-01-12T13:51:03.000Z","updated":"2021-04-23T14:24:48.255Z","comments":true,"path":"2021/01/12/数据库/InnoDB引擎的特点以及调优/","link":"","permalink":"https://xmmarlowe.github.io/2021/01/12/%E6%95%B0%E6%8D%AE%E5%BA%93/InnoDB%E5%BC%95%E6%93%8E%E7%9A%84%E7%89%B9%E7%82%B9%E4%BB%A5%E5%8F%8A%E8%B0%83%E4%BC%98/","excerpt":"谈谈InnoDB的特点，以及InnoDB解决的问题…","text":"谈谈InnoDB的特点，以及InnoDB解决的问题… 简介InnoDB引擎是MySQL数据库的另一个重要的存储引擎,正成为目前MySQL AB所发行的新版的标准,被包含在所有二进制安装包里,和其他存储引擎相比,InnoDB引擎的优点是支持兼容ACID的事务(类似于PostgreSQL),以及参数完整性(有外键)等.现在Innobase实行双认证授权.MySQL5.5.5以后默认的存储引擎都是InnoDB引擎 特点 支持事务(事务是指逻辑上的一组操作,组成这组操作的各个单元,要么全成功,要么全失败) 行级锁定(更新时一般是锁定当前行):通过索引实现,全表扫描仍然会是锁定整个表,注意间隙锁的影响. 读写阻塞与事务隔离级别相关. 具有非常高效的缓存特性,能缓存索引,也能缓存数据. 整个表和主键以Cluster方式存储,组成一颗平衡树. 所有Secondary Index 都会保存主键信息. 支持分区,表空间.类似于Oracle数据库. 支持外键约束,不支持全文索引,5.5之前支持,后面不再支持. 和MyISAM相比,InnoDB对于硬件资源要求比较高. 适用的生产场景 需要支持事务的业务(例如转账,付款) 行级锁定对于高并发有很好的适应能力但是需要保证查询是通过索引完成. 数据读写及更新都比较频繁的场景,如:BBS,SNS,微博,微信等. 数据一致性要求很高的业务.如:转账,充值等. 硬件设备内存较大,可以很好利用InnoDB较好的缓存能力来提高内存利用率,尽可能减少磁盘IO的开销. InnoDB引擎调优精要 主键尽可能小,避免给Secondary index带来过大的空间负担. 避免全表扫描,因为会使用表锁. 尽可能缓存所有的索引和数据,提高响应速度,减少磁盘IO消耗. 在大批量小插入的时候,尽量自己控制事务而不要使用autocommit自动提交,有开关参数可以控制提交. 合理设置Innodb_flush_log_at_trx_commit 参数值,不要过度追求安全性.如果Innodb_flush_log_at_trx_commit的值为0,log buffer每秒就会被刷写日志文件进入磁盘,提交事务的时候不做任何操作. 避免主键更新,因为这会带来大量的数据移动. 参考InnoDB引擎的特点及优化方法","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"InnoDB","slug":"InnoDB","permalink":"https://xmmarlowe.github.io/tags/InnoDB/"}],"author":"Marlowe"},{"title":"聚集索引与非聚集索引","slug":"数据库/聚集索引与非聚集索引","date":"2021-01-12T13:32:56.000Z","updated":"2021-08-26T10:09:02.886Z","comments":true,"path":"2021/01/12/数据库/聚集索引与非聚集索引/","link":"","permalink":"https://xmmarlowe.github.io/2021/01/12/%E6%95%B0%E6%8D%AE%E5%BA%93/%E8%81%9A%E9%9B%86%E7%B4%A2%E5%BC%95%E4%B8%8E%E9%9D%9E%E8%81%9A%E9%9B%86%E7%B4%A2%E5%BC%95/","excerpt":"聚集索引和非聚集索引是mysql数据库中两种主要的索引方式，不同的存储引擎支持索引类型是不一样，那这两种索引有什么不同呢？","text":"聚集索引和非聚集索引是mysql数据库中两种主要的索引方式，不同的存储引擎支持索引类型是不一样，那这两种索引有什么不同呢？ 聚集索引只有同时满足以下两个条件，才会创建聚集索引: 数据存储有序 key值应该是唯一的 每当您在表中应用聚集索引时，它将仅在该表中执行排序。您只能在像主键这样的表中创建一个聚集索引。聚簇索引与字典相同，字典按字母顺序排列数据。 在聚集索引中，索引包含指向数据存储的块而不是数据存储地址的指针。 为什么只能建一个聚集索引？因为聚焦索引决定了表的物理排列顺序，一个表只能有一个物理排列顺序，所以一个表只能建一个聚集索引。 非聚集索引数据存储在一个位置，索引存储在另一位置。 由于数据和非聚集索引是分开存储的，因此在一个表中可以有多个非聚集索引。 在非聚集索引中，索引包含指向数据的指针。 区别聚集索引 | 非聚集索引|:—:|:—:|聚集索引更快| 非聚集索引较慢聚集索引需要较少的内存来进行操作。| 非聚集索引需要更多的内存用于操作。在聚簇索引中，索引是主要数据。| 在非聚集索引中，索引是数据的副本。一个表只能有一个聚集索引。| 一个表可以有多个非聚集索引。聚集索引具有将数据存储在磁盘上的固有能力。| 非聚集索引不具有将数据存储在磁盘上的固有能力。聚集索引存储块指针不是数据指针| 非聚集索引存储值和指向保存数据的实际行的指针。在聚簇索引中，叶节点是实际数据本身。| 在非聚集索引中，叶节点不是实际数据本身，而是仅包含包含的列。在聚簇索引中，聚簇键定义表中数据的顺序。| 在非聚集索引中，索引键定义索引内数据的顺序。聚集索引是一种索引类型，其中表记录在物理上被重新排序以匹配该索引。| 非聚集索引是一种特殊类型的索引，其中索引的逻辑顺序与磁盘上行的物理存储顺序不匹配。 参考mysql聚集索引和非聚集索引之间的区别","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"索引","slug":"索引","permalink":"https://xmmarlowe.github.io/tags/%E7%B4%A2%E5%BC%95/"}],"author":"Marlowe"},{"title":"锁机制与 InnoDB 锁算法","slug":"数据库/锁机制与-InnoDB-锁算法","date":"2021-01-11T02:19:50.000Z","updated":"2021-05-09T03:07:19.078Z","comments":true,"path":"2021/01/11/数据库/锁机制与-InnoDB-锁算法/","link":"","permalink":"https://xmmarlowe.github.io/2021/01/11/%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%94%81%E6%9C%BA%E5%88%B6%E4%B8%8E-InnoDB-%E9%94%81%E7%AE%97%E6%B3%95/","excerpt":"","text":"MyISAM 和 InnoDB 存储引擎使用的锁 MyISAM 采用表级锁(table-level locking)。 InnoDB 支持行级锁(row-level locking)和表级锁,默认为行级锁 表级锁和行级锁对比 表级锁： MySQL 中锁定 粒度最大 的一种锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。其锁定粒度最大，触发锁冲突的概率最高，并发度最低，MyISAM 和 InnoDB 引擎都支持表级锁。 行级锁： MySQL 中锁定 粒度最小 的一种锁，只针对当前操作的行进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。 InnoBD的三种行级锁Record lock：单个行记录上的锁锁定一个记录上的索引，而不是记录本身。如果表没有设置索引，InnoDB 会自动在主键上创建隐藏的聚簇索引，因此 Record Locks依然可以使用。 Gap lock：间隙锁，锁定一个范围，不包括记录本身间隙锁，锁定一个范围，但不包括记录本身。GAP锁的目的，是为了防止同一事务的两次当前读，出现幻读的情况。 Next-key lock：record+gap 锁定一个范围，包含记录本身1、2组合，锁定一个范围，并且锁定记录本身。对于行的查询，都是采用该方法，主要目的是解决幻读的问题。Gap Lock Gap LockGap Lock，又称为间隙锁。存在的主要目的就是为了防止在可重复读的事务级别下，出现幻读问题。 在可重复读的事务级别下面，普通的select读的是快照，不存在幻读情况，但是如果加上for update的话，读取是已提交事务数据，gap锁保证for update情况下，不出现幻读。 以下都是在可重读隔离级别情况下。 test表如下： id value a 1 d 3 g 6 j 8 其中id是主键，value是非唯一索引 1234# T1select * from test where num=6 for update;# T2insert into test (id, value) VALUES (&#x27;a&#x27;, 3); T1这样的操作会锁定（3,6]，(6,8]，但是会发现插入操作依旧可以成功，因为虽然Value的区间是锁住了，但是根据id=‘a’这一条让排序在a前面去了 总的来说，锁的间隙是根据B+树排序后的叶子节点之间的区间，不但要看非索引，也会看主键。 假如是非索引列，那么将会全表间隙加上gap锁。 条件是唯一索引等值检索且记录不存在的情况，我们要考虑，gap lock是防止幻读，那么尝试思考，使用唯一索引所谓条件查找数据for update，如果对应的记录不存在的话，是无法使用行锁的。这时候，会使用gap lock来锁住区间，保证记录不会插入，防止出现幻读。总结 总结Next-Locks就是结合行锁和间隙锁进行的，主要是用于MVCC出现幻读的情况。 参考MySQL之Gap Locks与Next-key Locks 锁机制与 InnoDB 锁算法","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"锁","slug":"锁","permalink":"https://xmmarlowe.github.io/tags/%E9%94%81/"},{"name":"InnoDB","slug":"InnoDB","permalink":"https://xmmarlowe.github.io/tags/InnoDB/"}],"author":"Marlowe"},{"title":"MySQL 中 MyISAM 和 InnoDB 的区别有哪些？","slug":"数据库/Mysql-中-MyISAM-和-InnoDB-的区别有哪些？","date":"2021-01-11T01:42:45.000Z","updated":"2021-08-26T10:09:02.853Z","comments":true,"path":"2021/01/11/数据库/Mysql-中-MyISAM-和-InnoDB-的区别有哪些？/","link":"","permalink":"https://xmmarlowe.github.io/2021/01/11/%E6%95%B0%E6%8D%AE%E5%BA%93/Mysql-%E4%B8%AD-MyISAM-%E5%92%8C-InnoDB-%E7%9A%84%E5%8C%BA%E5%88%AB%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F/","excerpt":"","text":"区别 InnoDB 支持事务，MyISAM 不支持事务。这是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一； InnoDB 支持外键，而 MyISAM 不支持。对一个包含外键的 InnoDB 表转为 MYISAM 会失败； InnoDB 是聚集索引，MyISAM 是非聚集索引。聚簇索引的文件存放在主键索引的叶子节点上，因此 InnoDB 必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。而 MyISAM 是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。 InnoDB 不保存表的具体行数，执行 select count(*) from table 时需要全表扫描。而MyISAM 用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快； InnoDB 最小的锁粒度是行锁，MyISAM 最小的锁粒度是表锁。一个更新语句会锁住整张表，导致其他查询和更新都会被阻塞，因此并发访问受限。这也是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一； 如何选择 是否要支持事务，如果要请选择 InnoDB，如果不需要可以考虑 MyISAM； 如果表中绝大多数都只是读查询，可以考虑 MyISAM，如果既有读写也挺频繁，请使用InnoDB。 系统奔溃后，MyISAM恢复起来更困难，能否接受，不能接受就选 InnoDB； MySQL5.5版本开始Innodb已经成为Mysql的默认引擎(之前是MyISAM)，说明其优势是有目共睹的。如果你不知道用什么存储引擎，那就用InnoDB，至少不会差。 参考Mysql 中 MyISAM 和 InnoDB 的区别有哪些？","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"InnoDB","slug":"InnoDB","permalink":"https://xmmarlowe.github.io/tags/InnoDB/"},{"name":"MyISAM","slug":"MyISAM","permalink":"https://xmmarlowe.github.io/tags/MyISAM/"}],"author":"Marlowe"},{"title":"MySQL事务以及隔离级别","slug":"数据库/MySQL事务以及隔离级别","date":"2021-01-10T03:23:22.000Z","updated":"2021-08-26T10:09:02.670Z","comments":true,"path":"2021/01/10/数据库/MySQL事务以及隔离级别/","link":"","permalink":"https://xmmarlowe.github.io/2021/01/10/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL%E4%BA%8B%E5%8A%A1%E4%BB%A5%E5%8F%8A%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/","excerpt":"MySQL事务相关面试题…","text":"MySQL事务相关面试题… 何为事务？事务是逻辑上的一组操作，要么都执行，要么都不执行。 举一个简单的例子： 事务最经典也经常被拿出来说例子就是转账了。假如小明要给小红转账 1000 元，这个转账会涉及到两个关键操作就是： 将小明的余额减少 1000 元 将小红的余额增加 1000 元。 事务会把这两个操作就可以看成逻辑上的一个整体，这个整体包含的操作要么都成功，要么都要失败。 这样就不会出现小明余额减少而小红的余额却并没有增加的情况。 何为数据库事务？数据库事务在我们日常开发中接触的最多了。如果你的项目属于单体架构的话，你接触到的往往就是数据库事务了。 平时，我们在谈论事务的时候，如果没有特指分布式事务，往往指的就是数据库事务。 那数据库事务有什么作用呢？ 简单来说：数据库事务可以保证多个对数据库的操作（也就是 SQL 语句）构成一个逻辑上的整体。构成这个逻辑上的整体的这些数据库操作遵循：要么全部执行成功,要么全部不执行。 123456# 开启一个事务START TRANSACTION;# 多条 SQL 语句SQL1,SQL2...## 提交事务COMMIT; 另外，关系型数据库（例如：MySQL、SQL Server、Oracle 等）事务都有 ACID 特性： 何为ACID特性？原子性（Atomicity）事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性（Consistency）执行事务前后，数据保持一致，例如转账业务中，无论事务是否成功，转账者和收款人的总额应该是不变的； 隔离性（Isolation）并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性（Durabilily）一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 数据事务的实现原理呢？(InnoDB) 我们这里以 MySQL 的 InnoDB 引擎为例来简单说一下。 MySQL InnoDB 引擎使用 redo log(重做日志) 保证事务的持久性，使用 undo log(回滚日志) 来保证事务的原子性。 MySQL InnoDB 引擎通过 锁机制、MVCC 等手段来保证事务的隔离性（ 默认支持的隔离级别是 REPEATABLE-READ ）。 保证了事务的持久性、原子性、隔离性之后，一致性才能得到保障。 并发事务带来哪些问题?在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对同一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。 脏读（Dirty read）当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。 脏写脏写是指事务回滚了其他事务对数据项的已提交修改,比如下面这种情况，在事务1对数据A的回滚,导致事务2对A的已提交修改也被回滚了。 丢失修改（Lost to modify）指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。 这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务 1 读取某表中的数据 A=20，事务 2 也读取 A=20，事务 1 修改 A=A-1，事务 2 也修改 A=A-1，最终结果 A=19，事务 1 的修改被丢失。 不可重复读（Unrepeatableread）指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。 这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。 幻读（Phantom read）幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。 不可重复读和幻读区别不可重复读的重点是修改比如多次读取一条记录发现其中某些列的值被修改，幻读的重点在于新增或者删除比如多次读取一条记录发现记录增多或减少了。 事务隔离级别有哪些?SQL 标准定义了四个隔离级别： READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 隔离级别 脏读 不可重复读 幻读 读未提交(READ-UNCOMMITTED) √ √ √ 读已提交(READ-COMMITTED) × √ √ 可重复读(REPEATABLE-READ) × × √ 串行化(SERIALIZABLE) × × × MySQL 的默认隔离级别是什么?MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读）。我们可以通过SELECT @@tx_isolation;命令来查看，MySQL 8.0 该命令改为SELECT @@transaction_isolation; 123456mysql&gt; SELECT @@tx_isolation;+-----------------+| @@tx_isolation |+-----------------+| REPEATABLE-READ |+-----------------+ MySQL InnoDB 的 REPEATABLE-READ（可重读）并不保证避免幻读，需要应用使用加锁读来保证。而这个加锁读使用到的机制就是 Next-Key Locks。 因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是 READ-COMMITTED(读取提交内容) ，但是你要知道的是 InnoDB 存储引擎默认使用 REPEATABLE-READ（可重读） 并不会有任何性能损失。 InnoDB 存储引擎在 分布式事务 的情况下一般会用到 SERIALIZABLE(可串行化) 隔离级别。 事务隔离级别的实现–并发控制技术并发控制技术是实现事务隔离性的关键，实现方式有多种，并发控制策略可以分为两类： 乐观并发控制： 对于并发执行可能冲突的操作，假定其不会真的冲突，允许并发执行，直到真正发生冲突时才去解决冲突，比如让事务回滚。 悲观并发控制： 对于并发执行可能冲突的操作，假定其必定发生冲突，通过让事务等待(锁)或者中止(时间戳排序)的方式使并行的操作串行执行。 基于封锁的并发控制封锁粒度MySQL 中提供了两种封锁粒度：行级锁以及表级锁。 应该尽量只锁定需要修改的那部分数据，而不是所有的资源。锁定的数据量越少，发生锁争用的可能就越小，系统的并发程度就越高。 但是加锁需要消耗资源，锁的各种操作（包括获取锁、释放锁、以及检查锁状态）都会增加系统开销。因此封锁粒度越小，系统开销就越大。 在选择封锁粒度时，需要在锁开销和并发程度之间做一个权衡。 锁的种类1.读写锁 排它锁（Exclusive），简写为 X 锁，又称写锁。加了X锁，其他事务什么锁都不能加。 共享锁（Shared），简写为 S 锁，又称读锁。加了S锁其他事务可以加S锁，不能加X锁。 2.意向锁（Intention Locks） 使用意向锁（Intention Locks）可以更容易地支持多粒度封锁。 意向锁在原来的 X/S 锁之上引入了 IX/IS，IX/IS 都是表锁，用来表示一个事务想要在表中的某个数据行上加 X 锁或 S 锁。有以下两个规定： 一个事务在获得某个数据行对象的 S 锁之前，必须先获得表的 IS 锁或者更强的锁； 一个事务在获得某个数据行对象的 X 锁之前，必须先获得表的 IX 锁。 通过引入意向锁，事务 T 想要对表 A 加 X 锁，只需要先检测是否有其它事务对表 A 加了 X/IX/S/IS 锁，如果加了就表示有其它事务正在使用这个表或者表中某一行的锁，因此事务 T 加 X 锁失败。 各种锁的兼容关系如下： - X IX S IS X × × × × IX × √ × √ S × √ √ √ IS × √ √ √ 解释如下： 任意 IS/IX 锁之间都是兼容的，因为它们只是表示想要对表加锁，而不是真正加锁； S 锁只与 S 锁和 IS 锁兼容，也就是说事务 T 想要对数据行加 S 锁，其它事务可以已经获得对表或者表中的行的 S 锁。 三级封锁与两段锁协议三级封锁协议 三级封锁协议就是对锁使用的规定，来解决事务并发一致性问题。 a.一级封锁-解决丢失更新 事务 T 要修改数据 A 时必须加 X 锁，直到 T 结束才释放锁。 可以解决丢失更新问题，因为不能同时有两个事务对同一个数据进行修改，那么事务的修改就不会被覆盖。 b.二级封锁-解决脏读 在一级的基础上，要求读取数据 A 时必须加 S 锁，读取完马上释放 S 锁。 可以解决读脏数据问题，因为如果一个事务在对数据 A 进行修改，根据 1 级封锁协议，会加 X 锁，那么就不能再加 S 锁了，也就是不会读入数据。 c.三级封锁-解决不可重复读 在二级的基础上，要求读取数据 A 时必须加 S 锁，直到事务结束了才能释放 S 锁。 可以解决不可重复读的问题，因为读 A 时，其它事务不能对 A 加 X 锁，从而避免了在读的期间数据发生改变。 两段锁协议 加锁和解锁分为两个阶段进行。 可串行化调度是指，通过并发控制，使得并发执行的事务结果与某个串行执行的事务结果相同。 事务遵循两段锁协议是保证可串行化调度的充分条件。例如以下操作满足两段锁协议，它是可串行化调度。 1lock-x(A)...lock-s(B)...lock-s(C)...unlock(A)...unlock(C)...unlock(B) 但不是必要条件，例如以下操作不满足两段锁协议，但是它还是可串行化调度。 1lock-x(A)...lock-s(B)...lock-s(C)...unlock(A)...unlock(C)...unlock(B) MySQL隐式与显示锁定MySQL 的 InnoDB 存储引擎采用两段锁协议，会根据隔离级别在需要的时候自动加锁，并且所有的锁都是在同一时刻被释放，这被称为隐式锁定。 InnoDB 也可以使用特定的语句进行显示锁定： 12SELECT ... LOCK In SHARE MODE; #S锁SELECT ... FOR UPDATE; #X锁 基于时间戳的并发控制核心思想：对于并发可能冲突的操作，基于时间戳排序规则选定某事务继续执行,其他事务回滚。 系统会在每个事务开始时赋予其一个时间戳,这个时间戳可以是系统时钟也可以是一个不断累加的计数器值,当事务回滚时会为其赋予一个新的时间戳，先开始的事务时间戳小于后开始事务的时间戳。 每一个数据项Q有两个时间戳相关的字段: W-timestamp(Q):成功执行write(Q)的所有事务的最大时间戳R-timestamp(Q):成功执行read(Q)的所有事务的最大时间戳 具体排序方式就是： 假设事务T发出read(Q),T的时间戳为TSa. 若TS(T) &lt; W-timestamp(Q),则T需要读入的Q已被覆盖。此read操作将被拒绝,T回滚。b. 若TS(T) &gt;= W-timestamp(Q),则执行read操作,同时把R-timestamp(Q)设置为TS(T)与R-timestamp(Q)中的最大值 假设事务T发出write(Q)a.若TS(T) &lt; R-timestamp(Q),write操作被拒绝,T回滚。b.若TS(T) &lt; W-timestamp(Q),则write操作被拒绝,T回滚。c.其他情况:系统执行write操作,将W-timestamp(Q)设置为TS(T)。 基于时间戳排序和基于锁实现的本质一样: 对于可能冲突的并发操作,以串行的方式取代并发执行,因而它也是一种悲观并发控制。它们的区别主要有两点: 基于锁是让冲突的事务进行等待，而基于时间戳排序是让冲突的事务回滚。 基于锁冲突事务的执行次序是根据它们申请锁的顺序,先申请的先执行;而基于时间戳排序是根据特定的时间戳排序规则。 基于有效性检查的并发控制核心思想：事务对数据的更新首先在自己的工作空间进行，等到要写回数据库时才进行有效性检查，对不符合要求的事务进行回滚。 基于有效性检查的事务执行过程会被分为三个阶段: 读阶段： 数据项被读入并保存在事务的局部变量中。所有write操作都是对局部变量进行，并不对数据库进行真正的更新。 有效性检查阶段： 对事务进行有效性检查，判断是否可以执行write操作而不违反可串行性。如果失败，则回滚该事务。 写阶段： 事务已通过有效性检查，则将临时变量中的结果更新到数据库中。 有效性检查通常也是通过对事务的时间戳进行比较完成的，不过和基于时间戳排序的规则不一样。 该方法允许可能冲突的操作并发执行,因为每个事务操作的都是自己工作空间的局部变量,直到有效性检查阶段发现了冲突才回滚。因而这是一种乐观的并发策略。 基于多版本并发控制（MVCC）与快照隔离什么是MVCC？多版本并发控制（Multi-Version Concurrency Control, MVCC）是 MySQL 的 InnoDB 存储引擎实现隔离级别的一种具体方式，用于实现读已提交和可重复读这两种隔离级别。而读未提交隔离级别总是读取最新的数据行，无需使用 MVCC。可串行化隔离级别需要对所有读取的行都加锁，单纯使用 MVCC 无法实现。 可以认为MVCC是行级锁的一个变种，但是在很多情况下又避免了加锁，所以效率比较高。 MySQL的InnoDB的MVCC是通过在每行记录后面保存两个隐藏的列实现： 创建版本号：指示创建一个数据行的快照时的系统版本号； 删除版本号：如果该快照的删除版本号大于当前事务版本号表示该快照有效，否则表示该快照已经被删除了。 其中系统版本号：是一个递增的数字，每开始一个新的事务，系统版本号就会自动递增。事务版本号：事务开始时的系统版本号。 MVCC 使用到的快照存储在 Undo 日志中，该日志通过回滚指针把一个数据行（Record）的所有快照连接起来。 实现过程以下实现过程针对可重复读隔离级别。 当开始一个事务时，该事务的版本号肯定大于当前所有数据行快照的创建版本号，理解这一点很关键。数据行快照的创建版本号是创建数据行快照时的系统版本号，系统版本号随着创建事务而递增，因此新创建一个事务时，这个事务的系统版本号比之前的系统版本号都大，也就是比所有数据行快照的创建版本号都大。 1.SELECT ①只查找版本早于当前事务版本的数据行（行的系统版本号小于等于事务的系统版本号），这样可以保证要么数据行是之前存在的，要么就是自己这个事务自己修改的。 ②查找行的删除版本号要么大于当前事务版本号，要么未定义。这样可以保证这个数据行没有被删除的。 2.INSERT 将当前系统版本号作为数据行快照的创建版本号。 3.DELETE 将当前系统版本号作为数据行快照的删除版本号。 4.UPDATE 将当前系统版本号作为更新前的数据行快照的删除版本号，并将当前系统版本号作为更新后的数据行快照的创建版本号。可以理解为先执行 DELETE 后执行 INSERT。 快照读与当前读1.快照读 使用 MVCC 读取的是快照中的数据，这样可以减少加锁所带来的开销。 1select * from table ...; 2.当前读 读取的是最新的数据，不去读快照，需要加锁。以下第一个语句需要加 S 锁，其它都需要加 X 锁。 12345select * from table where ? lock in share mode;select * from table where ? for update;insert;update;delete; MVCC➕Next-key-Lock 防止幻读InnoDB存储引擎在 RR 级别下通过 MVCC和 Next-key Lock 来解决幻读问题： 1、执行普通 select，此时会以 MVCC 快照读的方式读取数据 在快照读的情况下，RR 隔离级别只会在事务开启后的第一次查询生成 Read View ，并使用至事务提交。所以在生成 Read View 之后其它事务所做的更新、插入记录版本对当前事务并不可见，实现了可重复读和防止快照读下的 “幻读”。 2、执行 select…for update/lock in share mode、insert、update、delete 等当前读 在当前读下，读取的都是最新的数据，如果其它事务有插入新的记录，并且刚好在当前事务查询范围内，就会产生幻读！InnoDB 使用 Next-key Lock 来防止这种情况。当执行当前读时，会锁定读取到的记录的同时，锁定它们的间隙，防止其它事务在查询范围内插入数据。只要我不让你插入，就不会发生幻读。 参考事务 数据库之事务与实现原理","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"事务","slug":"事务","permalink":"https://xmmarlowe.github.io/tags/%E4%BA%8B%E5%8A%A1/"},{"name":"MySQL","slug":"MySQL","permalink":"https://xmmarlowe.github.io/tags/MySQL/"},{"name":"隔离级别","slug":"隔离级别","permalink":"https://xmmarlowe.github.io/tags/%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/"}],"author":"Marlowe"},{"title":"MySQL中乐观锁实现(mvcc)","slug":"数据库/MySQL中乐观锁实现-mvcc","date":"2021-01-10T02:49:23.000Z","updated":"2021-04-23T14:25:09.939Z","comments":true,"path":"2021/01/10/数据库/MySQL中乐观锁实现-mvcc/","link":"","permalink":"https://xmmarlowe.github.io/2021/01/10/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL%E4%B8%AD%E4%B9%90%E8%A7%82%E9%94%81%E5%AE%9E%E7%8E%B0-mvcc/","excerpt":"MVCC即Multi-Version Concurrency Control，中文翻译过来叫多版本并发控制。","text":"MVCC即Multi-Version Concurrency Control，中文翻译过来叫多版本并发控制。 MVCC是解决了什么问题众所周知，在MYSQL中，MyISAM使用的是表锁，InnoDB使用的是行锁。而InnoDB的事务分为四个隔离级别，其中默认的隔离级别REPEATABLE READ需要两个不同的事务相互之间不能影响，而且还能支持并发，这点悲观锁是达不到的，所以REPEATABLE READ采用的就是乐观锁，而乐观锁的实现采用的就是MVCC。正是因为有了MVCC，才造就了InnoDB强大的事务处理能力。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://xmmarlowe.github.io/tags/MySQL/"},{"name":"mvcc","slug":"mvcc","permalink":"https://xmmarlowe.github.io/tags/mvcc/"}],"author":"Marlowe"},{"title":"countDownLatch、CyclicBarrier、Semaphore","slug":"并发/countDownLatch、CyclicBarrier、Semaphore","date":"2020-12-26T17:13:41.000Z","updated":"2021-04-19T12:47:15.194Z","comments":true,"path":"2020/12/27/并发/countDownLatch、CyclicBarrier、Semaphore/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/27/%E5%B9%B6%E5%8F%91/countDownLatch%E3%80%81CyclicBarrier%E3%80%81Semaphore/","excerpt":"","text":"countDownLatch简介 countDownLatch这个类使一个线程等待其他线程各自执行完毕后再执行。 是通过一个计数器来实现的，计数器的初始值是线程的数量。每当一个线程执行完毕后，计数器的值就-1，当计数器的值为0时，表示所有线程都执行完毕，然后在闭锁上等待的线程就可以恢复工作了。 减法计数器 原理countDownLatch.countDown(); // 数量-1countDownLatch.await(); // 等待计数器归零，然后向下执行每次有线程调用countDown(),数量减一，假设计数器变为0,countDownLatch.await();就会被唤醒，继续执行！ 代码示例： 1234567891011121314151617public class CountDownLatchDemo &#123; public static void main(String[] args) throws InterruptedException &#123; // 总数是6，必须要执行任务的时候，再使用！ CountDownLatch countDownLatch = new CountDownLatch(6); for (int i = 0; i &lt;= 6; i++) &#123; new Thread(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; Go out&quot;); countDownLatch.countDown(); &#125;, String.valueOf(i)).start(); &#125; // 等待计数器归零，然后再向下执行 countDownLatch.await(); System.out.println(&quot;close door&quot;); &#125;&#125; 结果： 12345678&#x2F;&#x2F; x顺序不确定，但只有6个线程结束后才向下执行x Go outx Go outx Go outx Go outx Go outx Go outClose door CyclicBarrier简介 从字面上的意思可以知道，这个类的中文意思是“循环栅栏”。大概的意思就是一个可循环利用的屏障。 加法计数器 举个例子，就像生活中我们会约朋友们到某个餐厅一起吃饭，有些朋友可能会早到，有些朋友可能会晚到，但是这个餐厅规定必须等到所有人到齐之后才会让我们进去。这里的朋友们就是各个线程，餐厅就是 CyclicBarrier。 作用CyclicBarrier的作用是让所有线程都等待完成后才会继续下一步行动。 代码示例： 1234567891011121314151617181920212223public class CyclicBarrierDemo &#123; public static void main(String[] args) &#123; // 召唤龙珠的线程 CyclicBarrier cyclicBarrier = new CyclicBarrier(7, () -&gt; &#123; System.out.println(&quot;召唤神龙成功！&quot;); &#125;); for (int i = 1; i &lt;= 7; i++) &#123; // 定义一个final 中间变量接收i final int temp = i; new Thread(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot;收集了&quot; + temp + &quot;个龙珠&quot;); try &#123; cyclicBarrier.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125;).start(); &#125; &#125;&#125; 结果 12345678Thread-1收集了2个龙珠Thread-2收集了3个龙珠Thread-0收集了1个龙珠Thread-3收集了4个龙珠Thread-4收集了5个龙珠Thread-5收集了6个龙珠Thread-6收集了7个龙珠召唤神龙成功！ Semaphore简介一般用来控制同时访问特定共享资源的线程数，它通过协调各个线程来保证使用公共资源的合理性。 作用 Semaphore的作用是控制并发访问的线程数目。 多个共享资源互斥使用，开发限流！ 原理semaphore.acquire() // 获得，假设已经满了，等待，等待被释放为止semaphore.release() // 释放，会将当前的信号量释放 + 1，然后唤醒等待线程！ 代码示例： 1234567891011121314151617181920212223public class SemaphoreDemo &#123; public static void main(String[] args) &#123; // 线程数量，停车位，限流 Semaphore semaphore = new Semaphore(3); for (int i = 1; i &lt;= 6; i++) &#123; new Thread(() -&gt; &#123; try &#123; // 得到 semaphore.acquire(); System.out.println(Thread.currentThread().getName() + &quot;抢到车位&quot;); TimeUnit.SECONDS.sleep(2); System.out.println(Thread.currentThread().getName() + &quot;离开车位&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; // 释放 semaphore.release(); &#125; &#125;, String.valueOf(i)).start(); &#125; &#125;&#125; 结果： 1234567891011121 抢到车位2 抢到车位3 抢到车位1 离开车位2 离开车位3 离开车位4 抢到车位6 抢到车位5 抢到车位4 离开车位6 离开车位5 离开车位 CountDownLatch和CyclicBarrier区别 countDownLatch是一个计数器，线程完成一个记录一个，计数器递减，只能只用一次 CyclicBarrier的计数器更像一个阀门，需要所有线程都到达，然后继续执行，计数器递增，提供reset功能，可以多次使用。","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"countDownLatch","slug":"countDownLatch","permalink":"https://xmmarlowe.github.io/tags/countDownLatch/"},{"name":"CyclicBarrier","slug":"CyclicBarrier","permalink":"https://xmmarlowe.github.io/tags/CyclicBarrier/"},{"name":"Semaphore","slug":"Semaphore","permalink":"https://xmmarlowe.github.io/tags/Semaphore/"}],"author":"Marlowe"},{"title":"Callable","slug":"并发/Callable","date":"2020-12-26T12:29:25.000Z","updated":"2021-04-19T12:10:57.554Z","comments":true,"path":"2020/12/26/并发/Callable/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/26/%E5%B9%B6%E5%8F%91/Callable/","excerpt":"","text":"Callable简介： 可以有返回值 可以抛出异常 方法不同，Runnable 是 run()， Callable 是call() 1234567891011121314151617181920212223242526public class CallableTest &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; MyThread thread = new MyThread(); // 适配类 FutureTask futureTask = new FutureTask(thread); new Thread(futureTask, &quot;A&quot;).start(); new Thread(futureTask, &quot;B&quot;).start(); // get方法可能会产生阻塞 Integer s = (Integer) futureTask.get(); System.out.println(s); &#125;&#125;class MyThread implements Callable&lt;Integer&gt; &#123; @Override public Integer call() throws Exception &#123; System.out.println(&quot;call()&quot;); // 耗时的操作 return 1024; &#125;&#125; 结果： 12// 只有一个call(),因为有缓存call() 注意： 有缓存 结果可能需要等待，会阻塞！","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"Callable","slug":"Callable","permalink":"https://xmmarlowe.github.io/tags/Callable/"}],"author":"Marlowe"},{"title":"Redis 发布与订阅","slug":"NoSQL/Redis-发布与订阅","date":"2020-12-25T03:59:32.000Z","updated":"2021-04-16T12:33:42.185Z","comments":true,"path":"2020/12/25/NoSQL/Redis-发布与订阅/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/25/NoSQL/Redis-%E5%8F%91%E5%B8%83%E4%B8%8E%E8%AE%A2%E9%98%85/","excerpt":"","text":"Redis 发布订阅(pub/sub)是一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接受消息。微博、微信、关注系统！Redis客户端可以订阅任意数量的频道。订阅/发布消息图：第一个：消息发送者，第二个：频道，第三个：消息订阅者使用场景 实时消息系统 实时聊天(频道当做聊天室，将信息回显给所有人即可) 订阅，关注系统 缺点：稍微复杂的场景就会使用消息中间件MQ。在消费者下线的情况下，生产的消息会丢失，得使用专业的消息队列如rabbitmq等。","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"}],"author":"Marlowe"},{"title":"Redis 缓存穿透与雪崩(面试高频)","slug":"NoSQL/Redis-缓存穿透与雪崩-面试高频","date":"2020-12-24T09:07:26.000Z","updated":"2021-05-04T12:31:13.704Z","comments":true,"path":"2020/12/24/NoSQL/Redis-缓存穿透与雪崩-面试高频/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/24/NoSQL/Redis-%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F%E4%B8%8E%E9%9B%AA%E5%B4%A9-%E9%9D%A2%E8%AF%95%E9%AB%98%E9%A2%91/","excerpt":"","text":"缓存穿透什么是缓存穿透？缓存穿透说简单点就是大量请求的 key 根本不存在于缓存中，导致请求直接到了数据库上，根本没有经过缓存这一层。举个例子：某个黑客故意制造我们缓存中不存在的 key 发起大量请求，导致大量请求落到数据库。 缓存穿透情况的处理流程是怎样的？如下图所示，用户的请求最终都要跑到数据库中查询一遍。 有哪些解决办法？最基本的就是首先做好参数校验，一些不合法的参数请求直接抛出异常信息返回给客户端。比如查询的数据库 id 不能小于 0、传入的邮箱格式不对的时候直接返回错误消息给客户端等等。 1）缓存无效 key如果缓存和数据库都查不到某个 key 的数据就写一个到 Redis 中去并设置过期时间，具体命令如下： SET key value EX 10086 。这种方式可以解决请求的 key 变化不频繁的情况，如果黑客恶意攻击，每次构建不同的请求 key，会导致 Redis 中缓存大量无效的 key 。很明显，这种方案并不能从根本上解决此问题。如果非要用这种方式来解决穿透问题的话，尽量将无效的 key 的过期时间设置短一点比如 1 分钟。 另外，这里多说一嘴，一般情况下我们是这样设计 key 的： 表名:列名:主键名:主键值 。 如果用 Java 代码展示的话，差不多是下面这样的： 123456789101112131415161718public Object getObjectInclNullById(Integer id) &#123; // 从缓存中获取数据 Object cacheValue = cache.get(id); // 缓存为空 if (cacheValue == null) &#123; // 从数据库中获取 Object storageValue = storage.get(key); // 缓存空对象 cache.set(key, storageValue); // 如果存储数据为空，需要设置一个过期时间(300秒) if (storageValue == null) &#123; // 必须设置过期时间，否则有被攻击的风险 cache.expire(key, 60 * 5); &#125; return storageValue; &#125; return cacheValue;&#125; 2）布隆过滤器布隆过滤器是一个非常神奇的数据结构，通过它我们可以非常方便地判断一个给定数据是否存在于海量数据中。我们需要的就是判断 key 是否合法，有没有感觉布隆过滤器就是我们想要找的那个“人”。 具体是这样做的：把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，存在的话才会走下面的流程。 加入布隆过滤器之后的缓存处理流程图如下: 但是，需要注意的是布隆过滤器可能会存在误判的情况。总结来说就是： 布隆过滤器说某个元素存在，小概率会误判。布隆过滤器说某个元素不在，那么这个元素一定不在。 缓存击穿什么是缓存击穿？在平常高并发的系统中，大量的请求同时查询一个 key 时，此时这个key正好失效了，就会导致大量的请求都打到数据库上面去。这种现象我们称为缓存击穿。 带来的问题？会造成某一时刻数据库请求量过大，压力剧增。 如何解决？1. 设置热点数据永不过期从缓存层面来看,没有设置过期时间,所以不会出现热点key过期后产生的问题。但是要注意在value中包含一个逻辑上的过期时间，然后另启一个线程，定期重建这些缓存。 2. 加互斥锁分布式锁:使用分布式锁,保证对于每个key同时只有一 个线程去查询后端服务,其他线程没有获得分布式锁的权限,因此只需要等待即可。这种方式将高并发的压力转移到了分布式锁,因此对分布式锁的考验很大。 缓存雪崩什么是缓存雪崩？我发现缓存雪崩这名字起的有点意思，哈哈。 实际上，缓存雪崩描述的就是这样一个简单的场景：缓存在同一时间大面积的失效，后面的请求都直接落到了数据库上，造成数据库短时间内承受大量请求。 这就好比雪崩一样，摧枯拉朽之势，数据库的压力可想而知，可能直接就被这么多请求弄宕机了。 举个例子： 系统的缓存模块出了问题比如宕机导致不可用。造成系统的所有访问，都要走数据库。 还有一种缓存雪崩的场景是：有一些被大量访问数据（热点缓存）在某一时刻大面积失效，导致对应的请求直接落到了数据库上。 这样的情况，有下面几种解决办法： 举个例子 ：秒杀开始 12 个小时之前，我们统一存放了一批商品到 Redis 中，设置的缓存过期时间也是 12 个小时，那么秒杀开始的时候，这些秒杀的商品的访问直接就失效了。导致的情况就是，相应的请求直接就落到了数据库上，就像雪崩一样可怕。 有哪些解决办法？针对 Redis 服务不可用的情况： 采用 Redis 集群，避免单机出现问题整个缓存服务都没办法使用。 限流，避免同时处理大量的请求。 针对热点缓存失效的情况： 设置不同的失效时间比如随机设置缓存的失效时间。 缓存永不失效。 如何保证缓存和数据库数据的一致性？如果更新数据库成功，而删除缓存这一步失败的情况的话，简单说两个解决方案： 缓存失效时间变短（不推荐，治标不治本）： 我们让缓存数据的过期时间变短，这样的话缓存就会从数据库中加载数据。另外，这种解决办法对于先操作缓存后操作数据库的场景不适用。 增加 cache 更新重试机制（常用）： 如果 cache 服务当前不可用导致缓存删除失败的话，我们就隔一段时间进行重试，重试次数可以自己定。如果多次重试还是失败的话，我们可以把当前更新失败的 key 存入队列中，等缓存服务可用之后，再将 缓存中对应的 key 删除即可。 参考缓存穿透与缓存雪崩","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"}],"author":"Marlowe"},{"title":"Redis 持久化","slug":"NoSQL/Redis-持久化","date":"2020-12-24T03:28:46.000Z","updated":"2021-05-06T07:07:27.839Z","comments":true,"path":"2020/12/24/NoSQL/Redis-持久化/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/24/NoSQL/Redis-%E6%8C%81%E4%B9%85%E5%8C%96/","excerpt":"Redis是内存数据库，如果不将内存中的数据库状态保存到磁盘，那么一旦服务器进程退出，服务器中的数据库状态也会消失，所以Redis提供了持久化功能！","text":"Redis是内存数据库，如果不将内存中的数据库状态保存到磁盘，那么一旦服务器进程退出，服务器中的数据库状态也会消失，所以Redis提供了持久化功能！ RDB(Redis DataBase)简介在指定的时间间隔内将内存中的数据集写入磁盘，也就是行话讲的Snapshot快照，它恢复时时将快照文件直接读到内存里。 Redis会单独创建（fork）一个子进程来进行持久化，会先将数据写入到一个临时文件中，待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件。整个过程中，主进程是不进行任何IO操作的。 这就确保了极高的性能。如果需要进行大规模数据的恢复，且对数据恢复的完整性不是非常敏感，那么RDB方式要比AOF方式更加的高效。RDB的缺点是最后一次持久化的数据可能丢失。我们默认的就是RDB，一般情况下不需要修改这个配置！ 在生产环境我们会将这个文件进行备份！ rdb保存的文件是dump.rdb 都是在配置文件中 快照中进行配置的！ 触发机制 save的规则满足情况下，会自动触发rdb规则 执行flushall命令，也会触发我们的rdb规则！ 退出redis，也会产生rdb文件备份就自动生成一个dump.rdb 如何恢复rdb文件？ 只需将rdb文件放在redis启动目录就可以，redis启动的时候就会自动检查dump.rdb，并恢复其中的数据。 查看需要存放的位置123127.0.0.1:6379&gt; config get dir1) &quot;dir&quot;2) &quot;D:\\\\Program Files\\\\redis-2.8.9&quot; 优点适合大规模的数据恢复！ 对数据的完整性要求不高 一旦采用该方式，那么你的整个Redis数据库将只包含一个文件，这对于文件备份而言是非常完美的。比如，你可能打算每个小时归档一次最近24小时的数据，同时还要每天归档一次最近30天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复。 对于灾难恢复而言，RDB是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。 性能最大化。对于Redis的服务进程而言，在开始持久化时，它唯一需要做的只是fork出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行IO操作了。 相比于AOF机制，如果数据集很大，RDB的启动效率会更高。 缺点需要一定的时间间隔进行操作，如果redis意外宕机了，这个最后一次修改的数据就没有了！ fork进程的时候，会占用一定的内存空间！ 如果你想保证数据的高可用性，即最大限度的避免数据丢失，那么RDB将不是一个很好的选择。因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。 由于RDB是通过fork子进程来协助完成数据持久化工作的，因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至是1秒钟。 RDB持久化流程 Redis父进程首先判断：当前是否在执行save，或bgsave/bgrewriteaof（后面会详细介绍该命令）的子进程，如果在执行则bgsave命令直接返回。bgsave/bgrewriteaof 的子进程不能同时执行，主要是基于性能方面的考虑：两个并发的子进程同时执行大量的磁盘写操作，可能引起严重的性能问题。 父进程执行fork操作创建子进程，这个过程中父进程是阻塞的，Redis不能执行来自客户端的任何命令 父进程fork后，bgsave命令返回”Background saving started”信息并不再阻塞父进程，并可以响应其他命令 子进程创建RDB文件，根据父进程内存快照生成临时快照文件，完成后对原有文件进行原子替换 子进程发送信号给父进程表示完成，父进程更新统计信息 AOF(Append Only File)简介将我们的所有命令都记录下来，history，恢复的时候就把这个文件全部再执行一遍！以日志的形式来记录每个读写操作，将Redis执行过的所有指令记录下来(读操作不记录),只许追加文件但不可以改写文件，redis启动之初会读取该文件重新构建数据,换言之，redis重启的话就根据日志文件的内容将写指令从前往后执行一次以完成数据的恢复工作 Aof保存的是appendonly.aof文件 默认是不开启的，需要手动进行配置，只需要将appendonly改为yes就可以开启aof！ 重启redis就可以生效如果这个aof文件有错，redis将无法启动，需要修复这个aof文件redis提供了一个工具 redis-check-aof --fix如果文件正常，重启就可以直接恢复了！ 重写规则说明aof默认就是文件的无限追加，文件会越来越大! 如果aof文件大于64m，太大了，会fork一个新的进程来将文件重写！ 优点每一次修改都同步，文件的完整性会更加好！ 每秒同步一次，可能会丢失一秒的数据。 从不同步，效率最高。 该机制可以带来更高的数据安全性，即数据持久性。Redis中提供了3中同步策略，即每秒同步、每修改同步和不同步。事实上，每秒同步也是异步完成的，其效率也是非常高的，所差的是一旦系统出现宕机现象，那么这一秒钟之内修改的数据将会丢失。而每修改同步，我们可以将其视为同步持久化，即每次发生的数据变化都会被立即记录到磁盘中。可以预见，这种方式在效率上是最低的。至于无同步，无需多言，我想大家都能正确的理解它。 由于该机制对日志文件的写入操作采用的是append模式，因此在写入过程中即使出现宕机现象，也不会破坏日志文件中已经存在的内容。然而如果我们本次操作只是写入了一半数据就出现了系统崩溃问题，不用担心，在Redis下一次启动之前，我们可以通过redis-check-aof工具来帮助我们解决数据一致性的问题。 如果日志过大，Redis可以自动启用rewrite机制。即Redis以append模式不断的将修改数据写入到老的磁盘文件中，同时Redis还会创建一个新的文件用于记录此期间有哪些修改命令被执行。因此在进行rewrite切换时可以更好的保证数据安全性。 AOF包含一个格式清晰、易于理解的日志文件用于记录所有的修改操作。事实上，我们也可以通过该文件完成数据的重建。 缺点相对于数据文件来说，aof远远大于rdb，修复的速度也比rdb慢。 Aof运行效率比rdb慢，所以redis默认的配置就是rdb持久化。 对于相同数量的数据集而言，AOF文件通常要大于RDB文件。RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 根据同步策略的不同，AOF在运行效率上往往会慢于RDB。总之，每秒同步策略的效率是比较高的，同步禁用策略的效率和RDB一样高效。 AOF持久化流程 客户端发出 bgrewriteaof命令。 redis主进程fork子进程。 父进程继续处理client请求，除了把写命令写入到原来的aof文件中。同时把收到的写命令缓存到 AOF重写缓冲区。这样就能保证如果子进程重写失败的话并不会出问题。 子进程根据内存快照，按照命令合并规则写入到新AOF文件中。 当子进程把内存快照写入临时文件中后，子进程发信号通知父进程。然后父进程把缓存的写命令也写入到临时文件。 现在父进程可以使用临时文件替换老的aof文件，并重命名，后面收到的写命令也开始往新的aof文件中追加。 常用配置RDB持久化配置Redis会将数据集的快照dump到dump.rdb文件中。此外，我们也可以通过配置文件来修改Redis服务器dump快照的频率，在打开6379.conf文件之后，我们搜索save，可以看到下面的配置信息： 12345save 900 1 #在900秒(15分钟)之后，如果至少有1个key发生变化，则dump内存快照。save 300 10 #在300秒(5分钟)之后，如果至少有10个key发生变化，则dump内存快照。save 60 10000 #在60秒(1分钟)之后，如果至少有10000个key发生变化，则dump内存快照。 AOF持久化配置在Redis的配置文件中存在三种同步方式，它们分别是： 12345appendfsync always #每次有数据修改发生时都会写入AOF文件。appendfsync everysec #每秒钟同步一次，该策略为AOF的缺省策略。appendfsync no #从不同步。高效但是数据不会被持久化。 如何选择二者选择的标准，就是看系统是愿意牺牲一些性能，换取更高的缓存一致性（aof），还是愿意写操作频繁的时候，不启用备份来换取更高的性能，待手动运行save的时候，再做备份（rdb）。rdb这个就更有些 eventually consistent的意思了。 命令 RDB AOF 启动优先级 低 高 体积 小 大 恢复速度 块 慢 数据安全性 丢数据 根据策略决定 轻重 重 轻 一些问题Redis 持久化 之 AOF 和 RDB 同时开启，Redis听谁的？听AOF的，RDB与AOF同时开启 默认无脑加载AOF的配置文件相同数据集，AOF文件要远大于RDB文件，恢复速度慢于RDBAOF运行效率慢于RDB,但是同步策略效率好，不同步效率和RDB相同 参考redis持久化的几种方式 如何彻底理解Redis持久化？触发机制注意的点 | AOF持久化过程。","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://xmmarlowe.github.io/tags/redis/"}],"author":"Marlowe"},{"title":"SpringBoot整合Redis","slug":"NoSQL/SpringBoot整合Redis","date":"2020-12-23T13:59:54.000Z","updated":"2021-06-26T01:06:11.524Z","comments":true,"path":"2020/12/23/NoSQL/SpringBoot整合Redis/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/23/NoSQL/SpringBoot%E6%95%B4%E5%90%88Redis/","excerpt":"","text":"说明：SpringBoot2.x之后，原来使用jedis被替换为了lettucejedis：采用的直连，多个线程操作的话，是不安全的，如果想要避免不安全，使用jedis pool连接池！ 更像BIO模式lettuce：采用netty，示例可以在多个线程中共享，不存在线程不安全的情况！可以减少线程数据了，更像NIO模式 原码分析：12345678910111213141516171819@Bean@ConditionalOnMissingBean(name = &quot;redisTemplate&quot;)// 我们可以自定义一个redisTemplate来替换这个默认的！@ConditionalOnSingleCandidate(RedisConnectionFactory.class)public RedisTemplate&lt;Object, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; // 默认的RedisTemplate没有过多的设置，redis对象都是需要序列化！ // 两个泛型都是Object，Object的类型，我们使用需要强制转换&lt;String,Object&gt; RedisTemplate&lt;Object, Object&gt; template = new RedisTemplate&lt;&gt;(); template.setConnectionFactory(redisConnectionFactory); return template;&#125;@Bean@ConditionalOnMissingBean // 由于tring是redis中最常使用的类型，所以单独提取出来了一个bean！@ConditionalOnSingleCandidate(RedisConnectionFactory.class)public StringRedisTemplate stringRedisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; StringRedisTemplate template = new StringRedisTemplate(); template.setConnectionFactory(redisConnectionFactory); return template;&#125; 整合测试 导入依赖12345&lt;!--操作redis--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 配置连接12spring.redis.host=127.0.0.1spring.redis.port=6379 测试12345678910@Testvoid contextLoads() &#123; // 获取redis的连接对象 // RedisConnection connection = redisTemplate.getConnectionFactory().getConnection(); // connection.flushDb(); // connection.flushAll(); redisTemplate.opsForValue().set(&quot;mykey&quot;, &quot;kuangshen&quot;); System.out.println(redisTemplate.opsForValue().get(&quot;mykey&quot;));&#125;","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://xmmarlowe.github.io/tags/SpringBoot/"},{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"}],"author":"Marlowe"},{"title":"Jedis","slug":"NoSQL/Jedis","date":"2020-12-23T13:19:43.000Z","updated":"2020-12-24T02:14:15.073Z","comments":true,"path":"2020/12/23/NoSQL/Jedis/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/23/NoSQL/Jedis/","excerpt":"…","text":"… 简介Jedis 是 Redis官方推荐的Java连接开发工具，使用Java操作Redis中间件。 测试 导入对应的依赖12345678910111213&lt;!--导入jedis的依赖--&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.62&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 编码测试 连接数据库 操作命令 断开连接12345678910public class TestPing &#123; public static void main(String[] args) &#123; // 1、new Jedis对象 Jedis jedis = new Jedis(&quot;127.0.0.1&quot;, 6379); System.out.println(jedis.ping()); &#125;&#125; 输出： 操作事务： 12345678910111213141516171819202122232425262728293031public class TestTx &#123; public static void main(String[] args) &#123; Jedis jedis = new Jedis(&quot;127.0.0.1&quot;, 6379); jedis.flushDB(); JSONObject jsonObject = new JSONObject(); jsonObject.put(&quot;hello&quot;, &quot;world&quot;); jsonObject.put(&quot;name&quot;, &quot;marlowe&quot;); // 开启事务 Transaction multi = jedis.multi(); String result = jsonObject.toJSONString(); try &#123; multi.set(&quot;user1&quot;, result); multi.set(&quot;user2&quot;, result); // 代码抛出异常，事务执行失败 int i = 1 / 0; // 执行事务 multi.exec(); &#125; catch (Exception e) &#123; // 放弃事务 multi.discard(); e.printStackTrace(); &#125; finally &#123; System.out.println(jedis.get(&quot;user1&quot;)); System.out.println(jedis.get(&quot;user2&quot;)); jedis.close(); &#125; &#125;&#125; 输出：","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"},{"name":"Jedis","slug":"Jedis","permalink":"https://xmmarlowe.github.io/tags/Jedis/"}],"author":"Marlowe"},{"title":"Redis 事务","slug":"NoSQL/Redis-事务","date":"2020-12-20T03:46:30.000Z","updated":"2021-04-21T06:02:08.083Z","comments":true,"path":"2020/12/20/NoSQL/Redis-事务/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/20/NoSQL/Redis-%E4%BA%8B%E5%8A%A1/","excerpt":"Redis 事务简介…","text":"Redis 事务简介… Redis 事务本质：一组命令的集合！一个事务中的所有命令都会被序列化，在事务执行过程中，会按照顺序执行！一次性，顺序性，排他性 执行一系列的命令！ 1-----队列 set set set 执行----- Redis事务没有隔离级别的概念所有的命令在事务中，并没有直接被执行！只有发起执行命令的时候才会执行！ExecRedis单条命令是保证原子性的，但是事务不保证原子性Redis的事务： 开启事务() 命令入队() 执行事务() 正常执行事务 123456789101112131415127.0.0.1:6379&gt; multi # 开启事务OK127.0.0.1:6379&gt; set k1 v1QUEUED127.0.0.1:6379&gt; set k2 v2QUEUED127.0.0.1:6379&gt; get k2QUEUED127.0.0.1:6379&gt; set k3 v3QUEUED127.0.0.1:6379&gt; exec # 执行事务1) OK2) OK3) &quot;v2&quot;4) OK 放弃事务 123456789101112127.0.0.1:6379&gt; multi # 开启事务OK127.0.0.1:6379&gt; set k1 v1QUEUED127.0.0.1:6379&gt; set k2 v2QUEUED127.0.0.1:6379&gt; set k4 v4QUEUED127.0.0.1:6379&gt; discard # 取消事务OK127.0.0.1:6379&gt; get k4 # 事务队列中的命令都不会被执行(nil) 编译型异常，事务中的所有命令都不会被执行！ 123456789101112131415161718127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set k1 v1QUEUED127.0.0.1:6379&gt; set k2 v2QUEUED127.0.0.1:6379&gt; set k3 v3QUEUED127.0.0.1:6379&gt; getset k3 # 错误的命令(error) ERR wrong number of arguments for &#x27;getset&#x27; command127.0.0.1:6379&gt; set k4 v4QUEUED127.0.0.1:6379&gt; set k5 v5QUEUED127.0.0.1:6379&gt; exec # 执行命令的时候报错(error) EXECABORT Transaction discarded because of previous errors.127.0.0.1:6379&gt; get k5 # 所有的命令都不会执行(nil) 运行时异常如果事务队列中存在与发行，那么执行命令的时候，其他命令是可以正常执行，错误命令抛出异常！ 1234567891011121314151617181920127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set k1 &quot;v1&quot;QUEUED127.0.0.1:6379&gt; incr k1QUEUED127.0.0.1:6379&gt; set k2 v2QUEUED127.0.0.1:6379&gt; set k3 v3QUEUED127.0.0.1:6379&gt; get k3QUEUED127.0.0.1:6379&gt; exec1) OK2) (error) ERR value is not an integer or out of range # 第一条命令报错，但是依旧正常执行成功了！3) OK4) OK5) &quot;v3&quot;127.0.0.1:6379&gt; get k2&quot;v2&quot; 监控 Watch 悲观锁 很悲观，认为什么时候都会出问题，无论做什么都会加锁！ 乐观锁 很乐观，认为什么时候都不会出问题，所以不会上锁！更新数据的时候去判断一下，在此期间是否有人修改过这个数据 获取version 更新的时候比较version Redis 监视测试正常执行成功！ 123456789101112131415127.0.0.1:6379&gt; set money 100OK127.0.0.1:6379&gt; set out 0OK127.0.0.1:6379&gt; watch money # 监视money对象OK127.0.0.1:6379&gt; multi # 事务正常结束，数据期间没有发生变动，这个时候就正常执行成功OK127.0.0.1:6379&gt; decrby money 20QUEUED127.0.0.1:6379&gt; incrby out 20QUEUED127.0.0.1:6379&gt; exec1) (integer) 802) (integer) 20 测试多线程修改值，使用watch可以当做redis的乐观锁操作 三特性单独的隔离操作事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中,不会被其他客户端发送来的命令请求所打断。 没有隔离级别的概念队列中的命令没有提交之前都不会实际被执行,因为事务提交前任何指令都不会被实际执行。 不保证原子性事务中如果有一条命令执行失败 ,后的命令仍然会被执行,没有回滚。","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"事务","slug":"事务","permalink":"https://xmmarlowe.github.io/tags/%E4%BA%8B%E5%8A%A1/"},{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"}],"author":"Marlowe"},{"title":"Redis 三种特殊数据类型","slug":"NoSQL/Redis-三种特殊数据类型","date":"2020-12-20T02:58:35.000Z","updated":"2021-05-06T08:38:43.512Z","comments":true,"path":"2020/12/20/NoSQL/Redis-三种特殊数据类型/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/20/NoSQL/Redis-%E4%B8%89%E7%A7%8D%E7%89%B9%E6%AE%8A%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","excerpt":"Radis 三种特殊数据类型…","text":"Radis 三种特殊数据类型… geospatial12345678geoadd # 添加位置geopos #获得当前定位 一定是一个坐标值geodist # 两人之间的距离georadius # 以给定的经纬度为中心，找出某一半径内的元素可以加参数 withdist 显示距离， withcoord 显示经纬度， count x 限制个数georadiusbymember 找出指定元素周围的其他元素geohash 返回11个字符串的geohash字符串geo底层的实现原理就是zset！我们可以通过zset命令来操作geo Hyperloglog1234567891011Redis Hyperloglog 基数统计的算法！优点:占用的内存固定，2^64不同的元素基数，只需要12KB内存！如果要从内存角度来比较的话Hyperloglog首选！网页UV（一个人访问一个网站多次，但是还是算作一个人！）传统的方式，set保存用户的id，然后就可以统计set中的元素数量作为标准判断！这个方式如果保存大量的用户id，就会比较麻烦！ 我们的目的是为了计数，而不是保存用户id；0.81误错率！ 统计UV任务，可以忽略不计的！pfcount 统计元素的基数数量pfmearge mykey3 mykey mykey2 #合并两组mykey mykey2 =&gt; mykey3 并集如果允许容错，使用Hyperloglog；如果不允许容错，就使用set或者自己的数据类型即可 Bitmap原理8bit = 1b = 0.001kb bitmap就是通过最小的单位bit来进行0或者1的设置，表示某个元素对应的值或者状态。一个bit的值，或者是0，或者是1；也就是说一个bit能存储的最多信息是2。 优势 基于最小的单位bit进行存储，所以非常省空间。 设置时候时间复杂度O(1)、读取时候时间复杂度O(n)，操作是非常快的。 二进制数据的存储，进行相关计算的时候非常快。 方便扩容 限制redis中bit映射被限制在512MB之内，所以最大是2^32位。建议每个key的位数都控制下，因为读取时候时间复杂度O(n)，越大的串读的时间花销越多。 案例12345678910111213141516171819统计用户信息，活跃 不活跃！ 登录 未登录! 打卡 未打卡！ 两个状态的，都可以使用BitmapsBitmaps 位图，数据结构！ 都是操作二进制来进行记录，就只有0和1两个状态！365天=365bit 1字节=8比特 46比特左右！使用bitmap来记录 周一到周日的打卡！127.0.0.1:6379&gt; setbit sign 0 1(integer) 0127.0.0.1:6379&gt; setbit sign 1 0(integer) 0127.0.0.1:6379&gt; setbit sign 2 0(integer) 0127.0.0.1:6379&gt; setbit sign 3 1(integer) 0127.0.0.1:6379&gt; setbit sign 4 1(integer) 0127.0.0.1:6379&gt; setbit sign 5 0(integer) 0127.0.0.1:6379&gt; setbit sign 6 0(integer) 0 查看某一天是否打卡！ 1234127.0.0.1:6379&gt; getbit sign 3(integer) 1127.0.0.1:6379&gt; getbit sign 6(integer) 0 统计操作，统计打卡的天数！ 12127.0.0.1:6379&gt; bitcount sign # 统计这周的打卡记录，可以看到是否全勤(integer) 3","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"},{"name":"数据类型","slug":"数据类型","permalink":"https://xmmarlowe.github.io/tags/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"}],"author":"Marlowe"},{"title":"Redis 五大数据类型","slug":"NoSQL/Redis-五大数据类型","date":"2020-12-19T09:10:33.000Z","updated":"2021-08-21T09:19:34.049Z","comments":true,"path":"2020/12/19/NoSQL/Redis-五大数据类型/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/19/NoSQL/Redis-%E4%BA%94%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","excerpt":"Redis 五大数据类型详述…","text":"Redis 五大数据类型详述… Redis-keyString（字符串）简介Strings 数据结构是简单的key-value类型，value其实不仅是String，也可以是数字. 常用命令set,get,decr,incr,mget 等。 String是最常用的一种数据类型，普通的key/ value 存储都可以归为此类.即可以完全实现目前 Memcached 的功能，并且效率更高。还可以享受Redis的定时持久化，操作日志及 Replication等功能。除了提供与 Memcached 一样的get、set、incr、decr 等操作外，Redis还提供了下面一些操作： set: 设置指定 key 的值 get: 获取指定 key 的值。 getrange: 获取值的部分 getset: 将给定 key 的值设为 value ，并返回 key 的旧值(old value)。 getbit: 对 key 所储存的字符串值，获取指定偏移量上的位(bit)。 setbit: 对 key 所储存的字符串值，设置或清除指定偏移量上的位(bit)。 mget: 获取多个key setnx: key不存在时才设置value setex: 将值value关联到key，并将key的过期时间设为seconds psetex: 将值value关联到key，并将key的过期时间设为毫秒 setrange: 用 value 参数覆写给定 key 所储存的字符串值，从偏移量 offset 开始。 strlen: 返回 key 所储存的字符串值的长度。 mset: 设置多对key/value的值 msetnx: 同时设置一个或多个 key-value 对，当且仅当所有给定 key 都不存在。 incr: 值增加1 incrby: 将 key 所储存的值加上给定的增量值（increment） 。 decr: 值减1 decrby: key 所储存的值减去给定的减量值（decrement） 。 append: 添加到尾部 del: 该命令用于在 key 存在时删除 key。 dump: 序列化给定 key ，并返回被序列化的值。 exists: 检查给定 key 是否存在。 expire: 为给定 key 设置过期时间，以秒计。 expireat: EXPIREAT 的作用和 EXPIRE 类似，都用于为 key 设置过期时间。 不同在于 EXPIREAT 命令接受的时间参数是 UNIX 时间戳(unix timestamp)。 pexpire: 设置 key 的过期时间以毫秒计。 pexpireat: 设置 key 过期时间的时间戳(unix timestamp) 以毫秒计 keys: 查找所有符合给定模式( pattern)的 key 。 move: 将当前数据库的 key 移动到给定的数据库 db 当中。 persist: 移除 key 的过期时间，key 将持久保持。 pttl: 以毫秒为单位返回 key 的剩余的过期时间。 ttl: 以秒为单位，返回给定 key 的剩余生存时间(TTL, time to live)。 randomkey: 从当前数据库中随机返回一个 key 。 rename: 修改 key 的名称 renamenx: 仅当 newkey 不存在时，将 key 改名为 newkey 。 type: 查询数据类型 应用场景 商品编号、订单号采用INCR命令生成 是否喜欢的文章 实现方式String在redis内部存储默认就是一个字符串，被redisObject所引用，当遇到incr,decr等操作时会转成数值型进行计算，此时redisObject的encoding字段为int。 代码示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127.0.0.1:6379&gt; set key1 v1OK127.0.0.1:6379&gt; get key1&quot;v1&quot;127.0.0.1:6379&gt; keys *1) &quot;key1&quot;127.0.0.1:6379&gt; exists key1(integer) 1127.0.0.1:6379&gt; append key1 &quot;hello&quot; # 如果当前key不存在，就相当于set key(integer) 7127.0.0.1:6379&gt; get key1&quot;v1hello&quot;127.0.0.1:6379&gt; strlen key1(integer) 7########################################### i++127.0.0.1:6379&gt; set views 0OK127.0.0.1:6379&gt; get views&quot;0&quot;127.0.0.1:6379&gt; incr views(integer) 1127.0.0.1:6379&gt; get views&quot;1&quot;127.0.0.1:6379&gt; decr views(integer) 0127.0.0.1:6379&gt; decr views(integer) -1127.0.0.1:6379&gt; get views&quot;-1&quot;127.0.0.1:6379&gt; incr views(integer) 0127.0.0.1:6379&gt; get views&quot;0&quot;127.0.0.1:6379&gt; incrby views 10(integer) 10127.0.0.1:6379&gt; decrby views 5(integer) 5########################################### 字符串范围 range127.0.0.1:6379&gt; set key1 &quot;hello,kuangshen&quot;OK127.0.0.1:6379&gt; get key1&quot;hello,kuangshen&quot;127.0.0.1:6379&gt; getrange key1 0 3&quot;hell&quot;127.0.0.1:6379&gt; getrange key1 0 -1&quot;hello,kuangshen&quot;# 替换127.0.0.1:6379&gt; set key2 abcdefgOK127.0.0.1:6379&gt; get key2&quot;abcdefg&quot;127.0.0.1:6379&gt; setrange key2 1 xx(integer) 7127.0.0.1:6379&gt; get key2&quot;axxdefg&quot;########################################### setex(set with expire) # 设置过期时间# setnx(set if not exist) # 不存在再设置(在分布式锁中常常使用！)127.0.0.1:6379&gt; setex key3 30 &quot;hello&quot;OK127.0.0.1:6379&gt; ttl key3(integer) 26127.0.0.1:6379&gt; get key3&quot;hello&quot;127.0.0.1:6379&gt; setnx mykey &quot;redis&quot;(integer) 1127.0.0.1:6379&gt; keys *1) &quot;key2&quot;2) &quot;key1&quot;3) &quot;mykey&quot;4) &quot;key3&quot;127.0.0.1:6379&gt; keys *1) &quot;key2&quot;2) &quot;key1&quot;3) &quot;mykey&quot;127.0.0.1:6379&gt; setnx mykey &quot;MongoDB&quot;(integer) 0127.0.0.1:6379&gt; get mykey&quot;redis&quot;##########################################127.0.0.1:6379&gt; keys *(empty list or set)127.0.0.1:6379&gt; mset k1 v1 k2 v2 k3 v3 # 同时设置多个值OK127.0.0.1:6379&gt; keys *1) &quot;k3&quot;2) &quot;k2&quot;3) &quot;k1&quot;127.0.0.1:6379&gt; get k1 k2 k3(error) ERR wrong number of arguments for &#x27;get&#x27; command127.0.0.1:6379&gt; mget k1 k2 k3 # 同时获得多个值1) &quot;v1&quot;2) &quot;v2&quot;3) &quot;v3&quot;127.0.0.1:6379&gt; msetnx k1 v1 k4 v4 # msetnx是一个原子性操作，要么一起成功，要么一起失败！(integer) 0127.0.0.1:6379&gt; get k4(nil)# 对象set user:1&#123;name:zhangsan,age:3&#125; # 设置一个user：1对象，值为json字符来保存一个对象# 这里的key是一个巧妙的设计： user:&#123;id&#125;:&#123;filed&#125;127.0.0.1:6379&gt; mset user:1:name zhangsan user:1:age 2OK127.0.0.1:6379&gt; mget user:11) (nil)127.0.0.1:6379&gt; mget user:1:name user:1:age1) &quot;zhangsan&quot;2) &quot;2&quot;##########################################getset # 先get再set127.0.0.1:6379&gt; getset db redis # 如果不存在值，则返回nil(nil)127.0.0.1:6379&gt; get db&quot;redis&quot;127.0.0.1:6379&gt; getset db mongodb # 如果存在值，则获取原来的值&quot;redis&quot;127.0.0.1:6379&gt; get db&quot;mongodb&quot; String类似的使用场景：value除了是字符串还可以是数字 计数器 统计多单位的数量 粉丝数 对象缓存存储！ List基本数据类型，列表所有的list命令都是以l开头 常用命令lpush,rpush,lpop,rpop,lrange等。 blpop: 移出并获取列表的第一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 brpop: 移出并获取列表的最后一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 brpoplpush: 从列表中弹出一个值，将弹出的元素插入到另外一个列表中并返回它； 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 lindex: 通过索引获取列表中的元素 linsert: 在列表的元素前或者后插入元素 llen: 获取列表长度 lpop: 移出并获取列表的第一个元素 lpush: 将一个或多个值插入到列表头部 lpushx: 将一个值插入到已存在的列表头部 lrange: 获取列表指定范围内的元素 lrem: 移除列表元素 lset: 通过索引设置列表元素的值 ltrim: 对一个列表进行修剪(trim)，就是说，让列表只保留指定区间内的元素，不在指定区间之内的元素都将被删除。 rpop: 移除列表的最后一个元素，返回值为移除的元素。 rpoplpush: 移除列表的最后一个元素，并将该元素添加到另一个列表并返回 rpush: 在列表中添加一个或多个值 rpushx: 为已存在的列表添加值 应用场景Redis list 的应用场景非常多，也是 Redis 最重要的数据结构之一，比如 twitter 的关注列表，粉丝列表等都可以用 Redis 的 list 结构来实现，还可以做消息队，列息队列不仅被用于系统内部组件之间的通信，同时也被用于系统跟其它服务之间的交互。消息队列的使用可以增加系统的可扩展性、灵活性和用户体验。非基于消息队列的系统，其运行速度取决于系统中最慢的组件的速度（注：木桶效应）。而基于消息队列可以将系统中各组件解除耦合，这样系统就不再受最慢组件的束缚，各组件可以异步运行从而得以更快的速度完成各自的工作。此外，当服务器处在高并发操作的时候，比如频繁地写入日志文件。可以利用消息队列实现异步处理。从而实现高性能的并发操作。 微信文章订阅公众号 大V作者李永乐老师和ICSDN发布了文章分别是11和22 阳哥关注了他们两个，只要他们发布了新文章，就会安装进我的List lpush likearticle:阳哥id1122 查看阳哥自己的号订阅的全部文章，类似分页，下面0~10就是一次显示10条 lrange likearticle:阳哥id 0 10 实现方式Redis list 的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销，Redis 内部的很多实现，包括发送缓冲队列等也都是用的这个数据结构。 代码示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163##########################################127.0.0.1:6379&gt; keys *(empty list or set)127.0.0.1:6379&gt; lpush list one # 将一个值或者多个值，插到列表的头部(左)(integer) 1127.0.0.1:6379&gt; lpush list two(integer) 2127.0.0.1:6379&gt; lpush list three(integer) 3127.0.0.1:6379&gt; lrange list 0 -11) &quot;three&quot;2) &quot;two&quot;3) &quot;one&quot;127.0.0.1:6379&gt; rpush list rigth # 将一个值或者多个值，插到列表的尾部(右)(integer) 4127.0.0.1:6379&gt; lrange list 0 -11) &quot;three&quot;2) &quot;two&quot;3) &quot;one&quot;4) &quot;rigth&quot;##########################################lpoprpop127.0.0.1:6379&gt; lrange list 0 -11) &quot;three&quot;2) &quot;two&quot;3) &quot;one&quot;4) &quot;rigth&quot;127.0.0.1:6379&gt; lpop list # 移除列表的第一个元素&quot;three&quot;127.0.0.1:6379&gt; lrange list 0 -11) &quot;two&quot;2) &quot;one&quot;3) &quot;rigth&quot;127.0.0.1:6379&gt; rpop list # 移除列表的最后一个元素&quot;rigth&quot;127.0.0.1:6379&gt; lrange list 0 -11) &quot;two&quot;2) &quot;one&quot;##########################################lindex127.0.0.1:6379&gt; lrange list 0 -11) &quot;two&quot;2) &quot;one&quot;127.0.0.1:6379&gt; lindex list 0 # 通过下标获得list中的某一个值&quot;two&quot;127.0.0.1:6379&gt; lindex list 1&quot;one&quot;##########################################llen127.0.0.1:6379&gt; lpush list one(integer) 1127.0.0.1:6379&gt; lpush list two(integer) 2127.0.0.1:6379&gt; lpush list three(integer) 3127.0.0.1:6379&gt; llen list(integer) 3##########################################移除指定的值！取关 uidlrem127.0.0.1:6379&gt; lrange list 0 -11) &quot;three&quot;2) &quot;three&quot;3) &quot;two&quot;4) &quot;one&quot;127.0.0.1:6379&gt; lrem list 1 one # 移除list集合中指定个数的value，精确匹配(integer) 1127.0.0.1:6379&gt; lrange list 0 -11) &quot;three&quot;2) &quot;three&quot;3) &quot;two&quot;127.0.0.1:6379&gt; lrem list 1 three(integer) 1127.0.0.1:6379&gt; lrange list 0 -11) &quot;three&quot;2) &quot;two&quot;127.0.0.1:6379&gt; lpush list three(integer) 3127.0.0.1:6379&gt; keys *1) &quot;list&quot;127.0.0.1:6379&gt; lrange list 0 -11) &quot;three&quot;2) &quot;three&quot;3) &quot;two&quot;127.0.0.1:6379&gt; lrem list 2 three(integer) 2127.0.0.1:6379&gt; lrange list 0 -11) &quot;two&quot;##########################################trim：修剪 list：截断127.0.0.1:6379&gt; rpush list &quot;hello&quot;(integer) 1127.0.0.1:6379&gt; rpush list &quot;hello1&quot;(integer) 2127.0.0.1:6379&gt; rpush list &quot;hello2&quot;(integer) 3127.0.0.1:6379&gt; rpush list &quot;hello3&quot;(integer) 4127.0.0.1:6379&gt; rpush list &quot;hello4&quot;(integer) 5127.0.0.1:6379&gt; ltrim list 0 1 # 通过下标截取指定的长度，这个list已经被改变了，截断了只剩下截取的元素！OK127.0.0.1:6379&gt; lrange list 0 -11) &quot;hello&quot;2) &quot;hello1&quot;##########################################rpoplpush # 移除列表的最后一个元素，将他移动到新的列表中127.0.0.1:6379&gt; rpush list &quot;hello&quot;(integer) 1127.0.0.1:6379&gt; rpush list &quot;hello1&quot;(integer) 2127.0.0.1:6379&gt; rpush list &quot;hello2&quot;(integer) 3127.0.0.1:6379&gt; rpush list &quot;hello3&quot;(integer) 4127.0.0.1:6379&gt; rpoplpush list list1 # 移除列表的最后一个元素，将他移动到新的列表中&quot;hello3&quot;127.0.0.1:6379&gt; lrange list 0 -11) &quot;hello&quot;2) &quot;hello1&quot;3) &quot;hello2&quot;127.0.0.1:6379&gt; lrange list1 0 -11) &quot;hello3&quot;##########################################lset # 将列表中指定下标的值替换为另外一个值，更新操作127.0.0.1:6379&gt; lset list 0 itemOK127.0.0.1:6379&gt; lrange list 0 -11) &quot;item&quot;2) &quot;hello1&quot;3) &quot;hello2&quot;127.0.0.1:6379&gt; lset list 1 item1OK127.0.0.1:6379&gt; lrange list 0 -11) &quot;item&quot;2) &quot;item1&quot;3) &quot;hello2&quot;##########################################linsert # 将某个具体的value插入到列表中某个元素的前面或者后面！127.0.0.1:6379&gt; rpush list &quot;hello&quot;(integer) 1127.0.0.1:6379&gt; rpush list &quot;world&quot;(integer) 2127.0.0.1:6379&gt; linsert list before world other(integer) 3127.0.0.1:6379&gt; lrange list 0 -11) &quot;hello&quot;2) &quot;other&quot;3) &quot;world&quot;127.0.0.1:6379&gt; linsert list after world other1(integer) 4127.0.0.1:6379&gt; lrange list 0 -11) &quot;hello&quot;2) &quot;other&quot;3) &quot;world&quot;4) &quot;other1&quot; ########################################## 小结 实际上是一个链表，before Node after， left right都可以插入值 如果key不存在，创建新的链表 如果key存在，新增内容 如果移除了所有值，空链表，也代表不存在！ 在两边插入或者改动值，效率最高！中间元素，相对来说效率会低一点~ 消息排队！ 消息队列 （lpush rpop），栈（lpush lpop） Set(集合)常用命令sadd,spop,smembers,sunion 等。 sadd: scard: sdiff: sdiffstore: sinsert: sinsertstore: sismember: smembers: smove: spop: srandomember: srem: sunion: sunionstore: sscan: sunionstore: sscan: 应用场景Redis set 对外提供的功能与 list 类似是一个列表的功能，特殊之处在于 set 是可以自动排重的，当你需要存储一个列表数据，又不希望出现重复数据时，set 是一个很好的选择，并且 set 提供了判断某个成员是否在一个 set 集合内的重要接口，这个也是 list 所不能提供的。 微信抽奖小程序 用户ID，立即参与按钮 SADD key 用户ID 显示已经有多少人参与了、上图23208人参加 SCARD key 抽奖(从set中任意选取N个中奖人) SRANDMEMBER key 2（随机抽奖2个人，元素不删除） SPOP key 3（随机抽奖3个人，元素会删除） 微信朋友圈点赞 新增点赞 sadd pub:msglD 点赞用户ID1 点赞用户ID2 取消点赞 srem pub:msglD 点赞用户ID 展现所有点赞过的用户 SMEMBERS pub:msglD 点赞用户数统计，就是常见的点赞红色数字 scard pub:msgID 判断某个朋友是否对楼主点赞过 SISMEMBER pub:msglD用户ID 微博好友关注社交关系 共同关注：我去到局座张召忠的微博，马上获得我和局座共同关注的人 sadd s1 1 2 3 4 5 sadd s2 3 4 5 6 7 SINTER s1 s2 我关注的人也关注他(大家爱好相同) QQ内推可能认识的人 sadd s1 1 2 3 4 5 sadd s2 3 4 5 6 7 SINTER s1 s2 SDIFF s1 s2 SDIFF s2 s1 实现方式set 的内部实现是一个 value 永远为 null 的 HashMap，实际就是通过计算 hash 的方式来快速排重的，这也是 set 能提供判断一个成员是否在集合内的原因。 代码示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104##########################################127.0.0.1:6379&gt; sadd myset hello(integer) 1127.0.0.1:6379&gt; sadd myset kuangshen(integer) 1127.0.0.1:6379&gt; sadd myset lovekuangshen(integer) 1127.0.0.1:6379&gt; smembers myset1) &quot;lovekuangshen&quot;2) &quot;hello&quot;3) &quot;kuangshen&quot;127.0.0.1:6379&gt; sismember myset hello(integer) 1127.0.0.1:6379&gt; sismember myset world(integer) 0##########################################127.0.0.1:6379&gt; scard myset # 获取set集合中的内容元素个数(integer) 4##########################################srem 127.0.0.1:6379&gt; scard myset(integer) 4127.0.0.1:6379&gt; srem myset hello # 移除set集合中指定的元素(integer) 1127.0.0.1:6379&gt; scard myset(integer) 3127.0.0.1:6379&gt; smembers myset1) &quot;lovekuangshen2&quot;2) &quot;lovekuangshen&quot;3) &quot;kuangshen&quot;##########################################set 无序不重复集合，抽随机！127.0.0.1:6379&gt; smembers myset1) &quot;lovekuangshen2&quot;2) &quot;lovekuangshen&quot;3) &quot;kuangshen&quot;127.0.0.1:6379&gt; srandmember myset # 随机抽选出一个元素&quot;lovekuangshen2&quot;127.0.0.1:6379&gt; srandmember myset # 随机抽选出一个元素&quot;lovekuangshen2&quot;127.0.0.1:6379&gt; srandmember myset # 随机抽选出一个元素&quot;kuangshen&quot;127.0.0.1:6379&gt; srandmember myset 2 # 随机抽选出指定个数的元素1) &quot;lovekuangshen&quot;2) &quot;kuangshen&quot;##########################################删除指定的key，随机删除key127.0.0.1:6379&gt; smembers myset1) &quot;lovekuangshen2&quot;2) &quot;lovekuangshen&quot;3) &quot;kuangshen&quot;127.0.0.1:6379&gt; spop myset #随机删除一些set中的元素&quot;kuangshen&quot;127.0.0.1:6379&gt; spop myset&quot;lovekuangshen2&quot;127.0.0.1:6379&gt; smembers myset1) &quot;lovekuangshen&quot;##########################################将一个指定的值，移动到另外一个set集合！127.0.0.1:6379&gt; sadd myset hello(integer) 1127.0.0.1:6379&gt; sadd myset world(integer) 1127.0.0.1:6379&gt; sadd myset kuangshen(integer) 1127.0.0.1:6379&gt; sadd myset2 set2(integer) 1127.0.0.1:6379&gt; smove myset myset2 kuangshen # 将一个指定的值，移动到另外一个set集合(integer) 1127.0.0.1:6379&gt; smembers(error) ERR wrong number of arguments for &#x27;smembers&#x27; command127.0.0.1:6379&gt; smembers myset1) &quot;hello&quot;2) &quot;world&quot;127.0.0.1:6379&gt; smembers myset21) &quot;set2&quot;2) &quot;kuangshen&quot;##########################################微博，b站，共同关注(交集)127.0.0.1:6379&gt; sadd key1 a(integer) 1127.0.0.1:6379&gt; sadd key1 b(integer) 1127.0.0.1:6379&gt; sadd key1 c(integer) 1127.0.0.1:6379&gt; sadd key2 c(integer) 1127.0.0.1:6379&gt; sadd key2 d(integer) 1127.0.0.1:6379&gt; sadd key2 e(integer) 1127.0.0.1:6379&gt; sdiff key1 key21) &quot;b&quot;2) &quot;a&quot;127.0.0.1:6379&gt; sinter key1 key21) &quot;c&quot;127.0.0.1:6379&gt; sunion key1 key21) &quot;c&quot;2) &quot;a&quot;3) &quot;b&quot;4) &quot;d&quot;5) &quot;e&quot;########################################## 微博，A用户将所有关注的人放在一个set集合中！ 将他的粉丝也放在一个集合中！共同关注，共同爱好，二度好友，推荐好友！(六度分割理论) Hash(哈希)Map 集合，key-map! 常用命令hget,hset,hgetall ,hincrby,hlen等。 hdel: 删除一个或多个哈希表字段 hexists: 查看哈希表 key 中，指定的字段是否存在。 hset: 将哈希表 key 中的字段 field 的值设为 value 。 hsetnx: 只有在字段 field 不存在时，设置哈希表字段的值。 hget: 获取存储在哈希表中指定字段的值。 hgetall: 获取在哈希表中指定 key 的所有字段和值 hincrby: 为哈希表 key 中的指定字段的整数值加上增量 increment 。 hkeys: 获取所有哈希表中的字段 hlen: 获取哈希表中字段的数量 hmget: 获取所有给定字段的值 hmset: 同时将多个 field-value (域-值)对设置到哈希表 key 中。 hvals: 获取哈希表中所有值。 hscan: 迭代哈希表中的键值对。 应用场景购物车早期，当前小中厂可用 新增商品 hset shopcar:uid1024 334488 1 新增商品 hset shopcar:uid1024 334477 1 增加商品数量 hincrby shopcar:uid1024 334477 1 商品总数 hlen shopcar:uid1024 全部选择 hgetall shopcar:uid1024 在Memcached中，我们经常将一些结构化的信息打包成HashMap，在客户端序列化后存储为一个字符串的值，比如用户的昵称、年龄、性别、积分等，这时候在需要修改其中某一项时，通常需要将所有值取出反序列化后，修改某一项的值，再序列化存储回去。这样不仅增大了开销，也不适用于一些可能并发操作的场合（比如两个并发的操作都需要修改积分）。而Redis的Hash结构可以使你像在数据库中Update一个属性一样只修改某一项属性值。 我们简单举个实例来描述下Hash的应用场景，比如我们要存储一个用户信息对象数据，包含以下信息：用户ID为查找的key，存储的value用户对象包含姓名，年龄，生日等信息，如果用普通的key/value结构来存储，主要有以下2种存储方式： 第一种方式将用户 ID 作为查找 key，把其他信息封装成一个对象以序列化的方式存储，这种方式的缺点是，增加了序列化/反序列化的开销，并且在需要修改其中一项信息时，需要把整个对象取回，并且修改操作需要对并发进行保护，引入CAS等复杂问题。 第二种方法是这个用户信息对象有多少成员就存成多少个 key-value 对儿，用用户 ID +对应属性的名称作为唯一标识来取得对应属性的值，虽然省去了序列化开销和并发问题，但是用户 ID 为重复存储，如果存在大量这样的数据，内存浪费还是非常可观的。 那么 Redis 提供的 Hash 很好的解决了这个问题，Redis 的 Hash 实际是内部存储的 Value 为一个 HashMap，并提供了直接存取这个 Map 成员的接口，如下图： 也就是说，Key 仍然是用户 ID，value 是一个 Map，这个 Map 的 key 是成员的属性名，value 是属性值，这样对数据的修改和存取都可以直接通过其内部 Map 的 Key（Redis 里称内部 Map 的 key 为 field），也就是通过 key（用户 ID） + field（属性标签）就可以操作对应属性数据了，既不需要重复存储数据，也不会带来序列化和并发修改控制的问题。很好的解决了问题。这里同时需要注意，Redis 提供了接口（hgetall）可以直接取到全部的属性数据，但是如果内部 Map 的成员很多，那么涉及到遍历整个内部 Map 的操作，由于 Redis 单线程模型的缘故，这个遍历操作可能会比较耗时，而另其它客户端的请求完全不响应，这点需要格外注意。 hash还可以通过hincrby,hlen做统计。 实现方式上面已经说到 Redis Hash 对应 Value 内部实际就是一个 HashMap，实际这里会有2种不同实现，这个 Hash 的成员比较少时 Redis 为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的 HashMap 结构，对应的 value redisObject 的 encoding 为 zipmap，当成员数量增大时会自动转成真正的 HashMap，此时 encoding 为 ht。 代码示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556127.0.0.1:6379&gt; hset myhash field1 kuangshen(integer) 1127.0.0.1:6379&gt; hget myhash field1&quot;kuangshen&quot;127.0.0.1:6379&gt; hmset myhash field1 hello field2 worldOK127.0.0.1:6379&gt; hmget myhash field1 field21) &quot;hello&quot;2) &quot;world&quot;127.0.0.1:6379&gt; hgetall myhash1) &quot;field&quot;2) &quot;kuangshen&quot;3) &quot;field1&quot;4) &quot;hello&quot;5) &quot;field2&quot;6) &quot;world&quot;127.0.0.1:6379&gt; hdel myhash field1(integer) 1127.0.0.1:6379&gt; hgetall myhash1) &quot;field&quot;2) &quot;kuangshen&quot;3) &quot;field2&quot;4) &quot;world&quot;##########################################hlen127.0.0.1:6379&gt; hgetall myhash1) &quot;field&quot;2) &quot;kuangshen&quot;3) &quot;field2&quot;4) &quot;world&quot;127.0.0.1:6379&gt; hlen myhash # 获取hash表的字段数量(integer) 2##########################################127.0.0.1:6379&gt; hexists myhash field1 # 判断hash中指定字段是否存在！(integer) 0127.0.0.1:6379&gt; hexists myhash field2(integer) 1127.0.0.1:6379&gt; hgetall myhash1) &quot;field&quot;2) &quot;kuangshen&quot;3) &quot;field2&quot;4) &quot;world&quot;########################################### 只获取所有的field127.0.0.1:6379&gt; hkeys myhash1) &quot;field&quot;2) &quot;field2&quot;# 只获取所有的value127.0.0.1:6379&gt; hvals myhash1) &quot;kuangshen&quot;2) &quot;world&quot;########################################## hash变更的数据 user name age，尤其是用户信息之类的，经常变动的数据！hash更适合于对象的存储，String更加适合字符串存储！ Zset(有序集合)常用命令zadd,zrange,zrem,zcard等。 使用场景Redis sorted set 的使用场景与 set 类似，区别是 set 不是自动有序的，而 sorted set 可以通过用户额外提供一个优先级（score）的参数来为成员排序，并且是插入有序的，即自动排序。当你需要一个有序的并且不重复的集合列表，那么可以选择 sorted set 数据结构，比如你需要存储3个有关联事物时候,常见的用户,消息,消息等级；还可以利用zIncrBy，zRevRange，zAdd,zRevRank,zScore等接口做排行榜。 根据商品销售对商品进行排序显示 定义商品销售排行榜（sorted set集合），key为goods:sellsort，分数为商品销售数量。 商品编号1001的销量是9，商品编号1002的销量是15 - zadd goods:sellsort 9 1001 15 1002 有一个客户又买了2件商品1001，商品编号1001销量加2 - zincrby goods:sellsort 2 1001 求商品销量前10名 - ZRANGE goods:sellsort 0 10 withscores 抖音热搜 点击视频 ZINCRBY hotvcr:20200919 1 八佰 ZINCRBY hotvcr:20200919 15 八佰 2 花木兰 展示当日排行前10条 ZREVRANGE hotvcr:20200919 0 9 withscores 实现方式Redis sorted set 的内部使用 HashMap 和跳跃表（SkipList）来保证数据的存储和有序，HashMap 里放的是成员到 score 的映射，而跳跃表里存放的是所有的成员，排序依据是 HashMap 里存的 score，使用跳跃表的结构可以获得比较高的查找效率，并且在实现上比较简单。 代码示例123456zaddzrangezrangebyscore xxx -inf +inf withscores # 显示全部的用户并且附带成绩zrevrange # 从大到小排序zrem # 移除元素zcount # 获取指定区间的成员数量 其余API，查官方文档案例思路：set 排序 存储班级成绩表，工资排序表！普通消息：1 重要消息：2 带权重进行判断！排行榜应用实现，取TOP 10 1################################################## Pub/SubPub/Sub 从字面上理解就是发布（Publish）与订阅（Subscribe），在Redis中，你可以设定对某一个key值进行消息发布及消息订阅，当一个key值上进行了消息发布后，所有订阅它的客户端都会收到相应的消息。这一功能最明显的用法就是用作实时消息系统，比如普通的即时聊天，群聊等功能。 Transactions虽然Redis的Transactions提供的并不是严格的ACID的事务（比如一串用EXEC提交执行的命令，在执行中服务器宕机，那么会有一部分命令执行了，剩下的没执行），但是这个Transactions还是提供了基本的命令打包执行的功能（在服务器不出问题的情况下，可以保证一连串的命令是顺序在一起执行的，中间有会有其它客户端命令插进来执行）。Redis还提供了一个Watch功能，你可以对一个key进行Watch，然后再执行Transactions，在这过程中，如果这个Watched的值进行了修改，那么这个Transactions会发现并拒绝执行。 Redis 有哪些好处 速度快，因为数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1) 支持丰富数据类型，支持string，list，set，sorted set，hash 支持事务，操作都是原子性，所谓的原子性就是对数据的更改要么全部执行，要么全部不执行 丰富的特性：可用于缓存，消息，按key设置过期时间，过期后将会自动删除 1）String 常用命令： set/get/decr/incr/mget等； 应用场景： String是最常用的一种数据类型，普通的key/value存储都可以归为此类； 实现方式： String在redis内部存储默认就是一个字符串，被redisObject所引用，当遇到incr、decr等操作时会转成数值型进行计算，此时redisObject的encoding字段为int。 2）Hash 常用命令： hget/hset/hgetall等 应用场景： 我们要存储一个用户信息对象数据，其中包括用户ID、用户姓名、年龄和生日，通过用户ID我们希望获取该用户的姓名或者年龄或者生日； 实现方式： Redis的Hash实际是内部存储的Value为一个HashMap，并提供了直接存取这个Map成员的接口。Key是用户ID, value是一个Map。这个Map的key是成员的属性名，value是属性值。这样对数据的修改和存取都可以直接通过其内部Map的Key(Redis里称内部Map的key为field), 也就是通过 key(用户ID) + field(属性标签) 就可以操作对应属性数据。 当前HashMap的实现有两种方式：当HashMap的成员比较少时Redis为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的HashMap结构，这时对应的value的redisObject的encoding为zipmap，当成员数量增大时会自动转成真正的HashMap,此时encoding为ht。 3）List 常用命令： lpush/rpush/lpop/rpop/lrange等； 应用场景： Redis list的应用场景非常多，也是Redis最重要的数据结构之一，比如twitter的关注列表，粉丝列表等都可以用Redis的list结构来实现； 实现方式： Redis list的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销，Redis内部的很多实现，包括发送缓冲队列等也都是用的这个数据结构。 4）Set 常用命令： sadd/spop/smembers/sunion等； 应用场景： Redis set对外提供的功能与list类似是一个列表的功能，特殊之处在于set是可以自动排重的，当你需要存储一个列表数据，又不希望出现重复数据时，set是一个很好的选择，并且set提供了判断某个成员是否在一个set集合内的重要接口，这个也是list所不能提供的； 实现方式： set 的内部实现是一个 value永远为null的HashMap，实际就是通过计算hash的方式来快速排重的，这也是set能提供判断一个成员是否在集合内的原因。 5）Sorted Set 常用命令： zadd/zrange/zrem/zcard等； 应用场景： Redis sorted set的使用场景与set类似，区别是set不是自动有序的，而sorted set可以通过用户额外提供一个优先级(score)的参数来为成员排序，并且是插入有序的，即自动排序。当你需要一个有序的并且不重复的集合列表，那么可以选择sorted set数据结构，比如twitter 的public timeline可以以发表时间作为score来存储，这样获取时就是自动按时间排好序的。 实现方式： Redis sorted set的内部使用HashMap和跳跃表(SkipList)来保证数据的存储和有序，HashMap里放的是成员到score的映射，而跳跃表里存放的是所有的成员，排序依据是HashMap里存的score,使用跳跃表的结构可以获得比较高的查找效率，并且在实现上比较简单。 参考redis各个数据类型的应用场景","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"},{"name":"数据类型","slug":"数据类型","permalink":"https://xmmarlowe.github.io/tags/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"}],"author":"Marlowe"},{"title":"Redis 基础知识","slug":"NoSQL/Redis-基础知识","date":"2020-12-19T08:19:14.000Z","updated":"2020-12-20T15:40:18.893Z","comments":true,"path":"2020/12/19/NoSQL/Redis-基础知识/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/19/NoSQL/Redis-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","excerpt":"Redis 基础知识","text":"Redis 基础知识 Redis 是单线程的！ Redis 是很快的，官方表示，Redis 是基于内存操作，CPU 不是Redis性能瓶颈，Redis 的瓶颈是根据机器的内存和网络带宽，既然可以使用单线程实现，就使用单线程了。 Redis 是C语言写的，官方提供的数据位100000+的QPS，完全不比同样的使用key-value的Memecache差。 Redis 单线程为什么还这么快？ 误区1： 高性能服务器一定是多线程的？ 误区2： 多线程(CPU上下文会切换)一定比单线程效率高？ 核心：Redis 是将所有的数据全部放在内存中的，所以说使用单线程去操作效率就是最高的，多线程(CPU上下文切换：耗时的操作！！)对于内存系统来说，如果没有上下文切换效率就是最高的！多次读写都是在一个CPU上，在内存情况下，这就是最佳的方案！","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"}],"author":"Marlowe"},{"title":"高效的一天","slug":"学习方法/高效的一天","date":"2020-12-19T01:18:48.000Z","updated":"2021-03-09T05:45:40.445Z","comments":true,"path":"2020/12/19/学习方法/高效的一天/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/19/%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/%E9%AB%98%E6%95%88%E7%9A%84%E4%B8%80%E5%A4%A9/","excerpt":"大佬高效工作的一天…","text":"大佬高效工作的一天… 半小时整理每日重点和待办事项12整理每日计划，梳理一下今天大概要做什么事情，思考每件事情的优先级软件推荐：Notion 独立的工作区 回想昨天的工作，处理邮件 提前准备会议内容1Visio or PPT 整理要说的内容 避免多任务切换1和计算机类似，在计算机里切换任务，或者发生中断，要保存各种状态，上下文和数据，完成中断之后，还要恢复之前保存的数据，保存和恢复的过程，都要花费大量的计算，想办法尽量避免中断 在家工作划分工作和生活的界限 必要的生产力工具1iPad Pro + Notability + Notion 琐碎任务集中处理","categories":[{"name":"学习方法","slug":"学习方法","permalink":"https://xmmarlowe.github.io/categories/%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"}],"tags":[{"name":"学习","slug":"学习","permalink":"https://xmmarlowe.github.io/tags/%E5%AD%A6%E4%B9%A0/"},{"name":"效率","slug":"效率","permalink":"https://xmmarlowe.github.io/tags/%E6%95%88%E7%8E%87/"},{"name":"工作方法","slug":"工作方法","permalink":"https://xmmarlowe.github.io/tags/%E5%B7%A5%E4%BD%9C%E6%96%B9%E6%B3%95/"}],"author":"Marlowe"},{"title":"ES 文档的API操作详情","slug":"中间件/ES-文档的API操作详情","date":"2020-12-08T11:16:02.000Z","updated":"2021-08-21T02:06:44.429Z","comments":true,"path":"2020/12/08/中间件/ES-文档的API操作详情/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/08/%E4%B8%AD%E9%97%B4%E4%BB%B6/ES-%E6%96%87%E6%A1%A3%E7%9A%84API%E6%93%8D%E4%BD%9C%E8%AF%A6%E6%83%85/","excerpt":"…","text":"… 编写ElasticSearchConfig 配置文件，将ES交给Spring托管 ElasticSearchConfig.java 1234567891011@Configurationpublic class ElasticSearchConfig &#123; @Bean public RestHighLevelClient restHighLevelClient() &#123; RestHighLevelClient restHighLevelClient = new RestHighLevelClient( RestClient.builder( new HttpHost(&quot;localhost&quot;, 9200, &quot;http&quot;))); return restHighLevelClient; &#125;&#125; ES部分APIKuangEsApiApplicationTests.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173@SpringBootTestclass KuangEsApiApplicationTests &#123; @Autowired @Qualifier(&quot;restHighLevelClient&quot;) private RestHighLevelClient client; /** * 测试索引的创建 Request PUT kuang_index */ @Test void testCreateIndex() throws IOException &#123; // 1.创建索引请求 CreateIndexRequest request = new CreateIndexRequest(&quot;kuang_index&quot;); // 2.客户端执行请求 IndicesClient 请求后获得相应 CreateIndexResponse createIndexResponse = client.indices().create(request, RequestOptions.DEFAULT); System.out.println(createIndexResponse); &#125; /** * 测试获取索引,判断其是否存在 */ @Test void testExistsIndex() throws IOException &#123; GetIndexRequest request = new GetIndexRequest(&quot;kuang_index&quot;); boolean exists = client.indices().exists(request, RequestOptions.DEFAULT); System.out.println(exists); &#125; /** * 测试删除索引，判断是否存在 * * @throws IOException */ @Test void testDeleteIndex() throws IOException &#123; DeleteIndexRequest request = new DeleteIndexRequest(&quot;kuang_index&quot;); AcknowledgedResponse delete = client.indices().delete(request, RequestOptions.DEFAULT); System.out.println(delete.isAcknowledged()); &#125; /** * 测试添加文档 */ @Test void testAddDocument() throws IOException &#123; // 创建对象 User user = new User(&quot;狂神说&quot;, 3); // 创建请求 IndexRequest request = new IndexRequest(&quot;kuang_index&quot;); // 规则 put /kuang_index/_doc/1 request.id(&quot;1&quot;); request.timeout(TimeValue.timeValueSeconds(1)); request.timeout(&quot;1s&quot;); // 将我们的数据放入请求 json IndexRequest source = request.source(JSON.toJSONString(user), XContentType.JSON); // 客户端发送请求,获取响应的结果 IndexResponse indexResponse = client.index(request, RequestOptions.DEFAULT); System.out.println(indexResponse.toString()); System.out.println(indexResponse.status()); &#125; /** * 获取文档，判断是否存在 */ @Test void testIsExists() throws IOException &#123; GetRequest getRequest = new GetRequest(&quot;kuang_index&quot;, &quot;1&quot;); // 不获取返回的_source的上下文了 getRequest.fetchSourceContext(new FetchSourceContext(false)); getRequest.storedFields(&quot;_none_&quot;); boolean exists = client.exists(getRequest, RequestOptions.DEFAULT); System.out.println(exists); &#125; /** * 获取文档的信息 */ @Test void testGetDocument() throws IOException &#123; GetRequest getRequest = new GetRequest(&quot;kuang_index&quot;, &quot;1&quot;); GetResponse getResponse = client.get(getRequest, RequestOptions.DEFAULT); // 打印文档的内容 System.out.println(getResponse.getSourceAsString()); // 返回的全部内容和命令是一样的 System.out.println(getResponse); &#125; /** * 更新文档的信息 */ @Test void testUpdateDocument() throws IOException &#123; UpdateRequest updateRequest = new UpdateRequest(&quot;kuang_index&quot;, &quot;1&quot;); updateRequest.timeout(&quot;1s&quot;); User user = new User(&quot;狂神说Java&quot;, 18); updateRequest.doc(JSON.toJSONString(user), XContentType.JSON); UpdateResponse updateResponse = client.update(updateRequest, RequestOptions.DEFAULT); System.out.println(updateResponse.status()); &#125; /** * 删除文档的信息 */ @Test void testDeleteDocument() throws IOException &#123; DeleteRequest deleteRequest = new DeleteRequest(&quot;kuang_index&quot;, &quot;1&quot;); deleteRequest.timeout(&quot;1s&quot;); DeleteResponse deleteResponse = client.delete(deleteRequest, RequestOptions.DEFAULT); System.out.println(deleteResponse); &#125; /** * 批量插入数据 */ @Test void testBulkRequest() throws IOException &#123; BulkRequest bulkRequest = new BulkRequest(); bulkRequest.timeout(&quot;10s&quot;); List&lt;User&gt; userList = new ArrayList&lt;&gt;(); userList.add(new User(&quot;kuangshen1&quot;, 3)); userList.add(new User(&quot;kuangshen2&quot;, 3)); userList.add(new User(&quot;kuangshen3&quot;, 3)); userList.add(new User(&quot;qinjiang1&quot;, 3)); userList.add(new User(&quot;qinjiang2&quot;, 3)); userList.add(new User(&quot;qinjiang3&quot;, 3)); // 批处理请求 for (int i = 0; i &lt; userList.size(); i++) &#123; bulkRequest.add( new IndexRequest(&quot;kuang_index&quot;) .id(&quot;&quot; + (i + 1)) .source(JSON.toJSONString(userList.get(i)), XContentType.JSON)); &#125; BulkResponse bulkResponse = client.bulk(bulkRequest, RequestOptions.DEFAULT); System.out.println(bulkResponse.hasFailures()); &#125; /** * 查询 * searchRequest 搜索请求 * SearchSourceBuilder 条件构造 * HighlightBuilder 构建高亮 * TermQueryBuilder 精确查询 */ @Test void testSearch() throws IOException &#123; SearchRequest searchRequest = new SearchRequest(ESConst.ES_INDEX); // 构建搜索条件 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); // 查询条件，我们可以使用QueryBuilder 工具来实现 TermQueryBuilder termQueryBuilder = QueryBuilders.termQuery(&quot;name&quot;, &quot;qinjiang1&quot;); sourceBuilder.query(termQueryBuilder); sourceBuilder.timeout(new TimeValue(60, TimeUnit.SECONDS)); searchRequest.source(sourceBuilder); SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT); System.out.println(JSON.toJSONString(searchResponse.getHits())); System.out.println(&quot;======================&quot;); for (SearchHit documentFields : searchResponse.getHits().getHits()) &#123; System.out.println(documentFields.getSourceAsMap()); &#125; &#125;&#125;","categories":[{"name":"中间件","slug":"中间件","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://xmmarlowe.github.io/tags/SpringBoot/"},{"name":"ES","slug":"ES","permalink":"https://xmmarlowe.github.io/tags/ES/"},{"name":"API","slug":"API","permalink":"https://xmmarlowe.github.io/tags/API/"}],"author":"Marlowe"},{"title":"ES之Rest风格操作","slug":"中间件/ES之Rest风格操作","date":"2020-12-08T04:24:51.000Z","updated":"2021-08-21T02:07:44.980Z","comments":true,"path":"2020/12/08/中间件/ES之Rest风格操作/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/08/%E4%B8%AD%E9%97%B4%E4%BB%B6/ES%E4%B9%8BRest%E9%A3%8E%E6%A0%BC%E6%93%8D%E4%BD%9C/","excerpt":"","text":"关于索引的基本操作12PUT /索引名/~类型名~/文档id&#123;请求体&#125; 完成了自动增加索引！数据也成功的添加了 指定字段的类型 12345678910111213141516PUT /test2&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;birthday&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125; &#125; &#125;&#125; 创建规则 可以通过GET请求获取具体信息 查看默认信息 如果自己的文档字段没有指定，那么es会给我们默认配置字段类型！ 扩展：通过命令elasticsearch索引情况！通过GET _cat/ 可以获得es当前的很多信息！ 更新方法 以前的方法 现在的方法 删除索引通过DELETE命令删除、根据你的请求来判断是删除索引还是删除文档记录！使用RESTFUL风格是ES推荐大家使用的！ 关于文档的基本操作(重点)基本操作 添加数据 1234567PUT /kuangshen/user/2&#123; &quot;name&quot;: &quot;张三&quot;, &quot;age&quot;: 20, &quot;desc&quot;: &quot;法外狂徒张三&quot;, &quot;tags&quot;: [&quot;旅游&quot;,&quot;温暖&quot;,&quot;渣男&quot;]&#125; 获取数据 GET 1GET /kuangshen/user/1 更新数据 PUT POST _updatePUT如果不传递值就会被覆盖，POST灵活度更高 简单的搜索 1GET /kuangshen/user/1 简单的条件查询,可以根据默认的映射规则，产生基本的查询！ 1GET /kuangshen/user/_search?q=name:狂神说java 复杂操作搜索select(排序，分页，高亮，模糊查询，精准查询！) hits：索引和文档的信息查询的结果总数然后就是查询出来的具体的文档数据中心的所有东西都可以遍历出来了分数：我们可以通过分数来判断谁更加符合搜索结果 输出结果，只需要指定的字段 之后Java操作es，所有的方法和对象就是这里面的key！ 排序 分页查询数据下标还是从0开始的，和学的所有的数据结构是一样的！/search/{current}/{pagesize}布尔值查询must（and）,所有的条件都要符合 where id = 1 and name = xxx多条件查询 should（or）,所有的条件都要符合 where id = 1 or name = xxx must_not (not) 过滤器 filter 匹配多个条件 term查询是直接通过倒排索引指定的词条进行精确的查找！ 关于分词 term，直接查询精确的 match，会使用分词器解析！(先分析文档，然后再通过分析的文档进行查询！) 两个类型 text keyword keyword 字段类型不会被分词器解析 多个值匹配的精确查询 高亮查询 自定义搜索高亮条件 这些MySQL也能做，只是MySQL效率比较低！ 匹配 按照条件匹配 精确匹配 区间范围匹配 区间字段匹配 多条件查询 高亮查询","categories":[{"name":"中间件","slug":"中间件","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"ES","slug":"ES","permalink":"https://xmmarlowe.github.io/tags/ES/"},{"name":"RESTful","slug":"RESTful","permalink":"https://xmmarlowe.github.io/tags/RESTful/"}],"author":"Marlowe"},{"title":"ElasticSearch之ik插件之究极大坑","slug":"环境配置之踩坑/ElasticSearch之ik插件之究极大坑","date":"2020-12-07T17:10:24.000Z","updated":"2020-12-07T17:25:12.629Z","comments":true,"path":"2020/12/08/环境配置之踩坑/ElasticSearch之ik插件之究极大坑/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/08/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E4%B9%8B%E8%B8%A9%E5%9D%91/ElasticSearch%E4%B9%8Bik%E6%8F%92%E4%BB%B6%E4%B9%8B%E7%A9%B6%E6%9E%81%E5%A4%A7%E5%9D%91/","excerpt":"由于最近要做搜索引擎课设，可以用ElasticSearch做，因此，开启了ES的学习之路，也开启了ES踩坑之路，入门一小时，配环境两小时！！~","text":"由于最近要做搜索引擎课设，可以用ElasticSearch做，因此，开启了ES的学习之路，也开启了ES踩坑之路，入门一小时，配环境两小时！！~ 在elasticsearch中安装ik中文分词器，使用的elasticsearch版本是7.10.0，elasticsearch-analysis-ik版本是7.10.0。 安装后，重新启动报错，报错信息为： 12[2020-11-18T17:14:56,012][WARN ][o.e.c.r.a.AllocationService] [LAPTOP-TLVIFKFC] failing shard [AccessControlException[access denied (&quot;java.io.FilePermission&quot; &quot;D:\\Program%20Files\\elasticsearch\\elasticsearch-7.10.0\\plugins\\ik\\config\\IKAnalyzer.cfg.xml&quot; &quot;read&quot;)]], markAsStale [true]]java.security.AccessControlException: access denied (&quot;java.io.FilePermission&quot; &quot;D:\\Program%20Files\\elasticsearch\\elasticsearch-7.10.0\\plugins\\ik\\config\\IKAnalyzer.cfg.xml&quot; &quot;read&quot;) 原因是：elasticsearch安装路径中有空格造成的，如安装路径为D:\\Program Files\\elasticsearch\\elasticsearch-7.10.0，其中”Program Files”两个词中间有空格 解决方法：elasticsearch选择没有空格的文件目录下安装 前前后后下载了很多版本的插件，以及找同学烤文件，都没能解决这个问题，在百度重新搜索elasticsearch ik 7.10.0 下载的时候，出现了一篇拯救我的文章，重新安装好es所需要的文件后，将整个文件移动到没有空格的文件夹，问题才得以解决！ 参考：elasticsearch-7.10.0使用elasticsearch-analysis-ik-7.10.0分词器插件后启动报错 ES学习教程:【狂神说Java】ElasticSearch7.6.x最新完整教程通俗易懂","categories":[{"name":"环境配置之踩坑","slug":"环境配置之踩坑","permalink":"https://xmmarlowe.github.io/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E4%B9%8B%E8%B8%A9%E5%9D%91/"}],"tags":[{"name":"ES","slug":"ES","permalink":"https://xmmarlowe.github.io/tags/ES/"},{"name":"踩坑","slug":"踩坑","permalink":"https://xmmarlowe.github.io/tags/%E8%B8%A9%E5%9D%91/"}],"author":"Marlowe"},{"title":"Java中如何跳出多重循环","slug":"Java/Java中如何跳出多重循环","date":"2020-12-07T00:20:29.000Z","updated":"2021-08-26T14:02:44.038Z","comments":true,"path":"2020/12/07/Java/Java中如何跳出多重循环/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/07/Java/Java%E4%B8%AD%E5%A6%82%E4%BD%95%E8%B7%B3%E5%87%BA%E5%A4%9A%E9%87%8D%E5%BE%AA%E7%8E%AF/","excerpt":"Java 基础回顾…","text":"Java 基础回顾… 在JAVA中如何跳出当前的多重嵌套循环在java中，要想跳出多重循环，可以在外面的循环语句前定义一个标号，然后在里层循环体的代码中使用带有标号的的break语句，即可跳出 1234567891011public static void main(String[] args) &#123; ok: while (true) &#123; for (int i = 0; i &lt; 10000; i++) &#123; System.out.println(i); if (i == 200) &#123; break ok; &#125; &#125; &#125; &#125; return和 break区别breakbreak语句虽然可以独立使用，但通常主要用于switch语句中，控制程序的执行流程转移。在switch语句中，其作用是强制退出switch结构，执行switch结构之后的语句。其本质就是在单层循环结构体系中，其作用是强制退出循环结构。 returnreturn语句用来明确地从一个方法返回。也就是，return 语句使程序控制返回到调用它方法。因此，将它分类为跳转语句.有两个作用，一个是返回方法指定类型的值（这个值总是确定的）;一个是结束方法的执行（仅仅一个return语句）。return 语句可以使其从当前方法中退出，返回到调用该方法的语句处，继续程序的执行 。 exit()函数 和 return 区别exit(0)：正常运行程序并退出程序；exit(1)：非正常运行导致退出程序；return()：返回函数，若在主函数中，则会退出函数并返回一值。 具体来说： return返回函数值，是关键字； exit 是一个函数。 return是语言级别的，它表示了调用堆栈的返回；而exit是系统调用级别的，它表示结束一个进程 。 return是函数的退出(返回)；exit是进程的退出。 return是C语言提供的，exit是操作系统提供的（或者函数库中给出的）。 return用于结束一个函数的执行，将函数的执行信息传出个其他调用函数使用；exit函数是退出应用程序，删除进程使用的内存空间，并将应用程序的一个状态返回给OS，这个状态标识了应用程序的一些运行信息，这个信息和机器和操作系统有关，一般是 0 为正常退出， 非0 为非正常退出。 非主函数中调用return和exit效果很明显，但是在main函数中调用return和exit的现象就很模糊，多数情况下现象都是一致的。 参考在java中如何跳出当前的多重嵌套循环？","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"Java基础","slug":"Java基础","permalink":"https://xmmarlowe.github.io/tags/Java%E5%9F%BA%E7%A1%80/"},{"name":"面经","slug":"面经","permalink":"https://xmmarlowe.github.io/tags/%E9%9D%A2%E7%BB%8F/"}],"author":"Marlowe"},{"title":"Swagger在线文档","slug":"Spring/Swagger在线文档","date":"2020-12-06T10:30:51.000Z","updated":"2020-12-18T02:35:43.038Z","comments":true,"path":"2020/12/06/Spring/Swagger在线文档/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/06/Spring/Swagger%E5%9C%A8%E7%BA%BF%E6%96%87%E6%A1%A3/","excerpt":"Swagger在线文档使用教程…","text":"Swagger在线文档使用教程… SpringBoot集成Swagger 新建一个SpringBoot项目==&gt;web 导入相关依赖1234567891011&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt; 编写HelloWorld 配置Swagger1234@Configuration@EnableSwagger2public class SwaggerConfig &#123;&#125; 测试运行 http://localhost:8080/swagger-ui.html 配置Swagger信息Swagger的bean示例Docket 123456789101112131415161718192021222324252627@Configuration@EnableSwagger2public class SwaggerConfig &#123; /** * 配置了Swagger的Docket的bean实例 * * @return */ @Bean public Docket docket() &#123; return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()); &#125; public ApiInfo apiInfo() &#123; // 作者信息 Contact contact = new Contact(&quot;Marlowe&quot;, &quot;https://xmmarlowe.github.io&quot;, &quot;marlowe246@qq.com&quot;); return new ApiInfo(&quot;Visit CQUT Swagger API Documentation&quot;, &quot;Api Documentation&quot;, &quot;v1.0&quot;, &quot;urn:tos&quot;, contact, &quot;Apache 2.0&quot;, &quot;http://www.apache.org/licenses/LICENSE-2.0&quot;, new ArrayList()); &#125;&#125; Swagger配置扫描接口Docket.select() 1234567891011121314151617181920212223/** * 配置了Swagger的Docket的bean实例 * * @return */ @Bean public Docket docket() &#123; return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) .select() /** * RequestHandlerSelectors，配置要扫描接口的方式 * basePackage:指定要扫描的包 * any():扫描全部 * none():不扫描 * withClassAnnotation:扫描类上的注解，参数是一个注解的反射对象 * withMethodAnnotation：扫描方法上的注解 */ .apis(RequestHandlerSelectors.basePackage(&quot;com.marlowe.swagger.controller&quot;)) // paths(): 过滤什么路径 .paths(PathSelectors.ant(&quot;/marlowe/**&quot;)) .build(); &#125; 配置是否启动swagger 123456789101112131415/** * 配置了Swagger的Docket的bean实例 * * @return */ @Bean public Docket docket() &#123; return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) // enable是否启动Swagger，如果为false，则swagger不能在浏览器中访问 .enable(false) .select() .apis(RequestHandlerSelectors.basePackage(&quot;com.marlowe.swagger.controller&quot;)) .build(); &#125; 我只希望我的Swagger在生产环境中使用，在发布的时候不使用？ 判断是不是生产环境 flag = false 注入enable(flag)123456789101112131415161718192021/** * 配置了Swagger的Docket的bean实例 * * @return */ @Bean public Docket docket(Environment environment) &#123; // 设置要现实的swagger环境 Profiles profiles = Profiles.of(&quot;dev&quot;, &quot;test&quot;); // 获取项目的环境： boolean flag = environment.acceptsProfiles(profiles); return new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo()) // enable是否启动Swagger，如果为false，则swagger不能在浏览器中访问 .enable(flag) .select() .apis(RequestHandlerSelectors.basePackage(&quot;com.marlowe.swagger.controller&quot;)) .build(); &#125; 配置API文档的分组1.groupName(&quot;Marlowe&quot;) 如何配置多个分组；多个Docket实例即可1234567891011121314@Beanpublic Docket docket1() &#123; return new Docket(DocumentationType.SWAGGER_2).groupName(&quot;A&quot;);&#125;@Beanpublic Docket docket2() &#123; return new Docket(DocumentationType.SWAGGER_2).groupName(&quot;B&quot;);&#125;@Beanpublic Docket docket3() &#123; return new Docket(DocumentationType.SWAGGER_2).groupName(&quot;C&quot;);&#125; 实体类配置12345678910111213141516171819package com.marlowe.swagger.pojo;import io.swagger.annotations.ApiModel;import io.swagger.annotations.ApiModelProperty;/** * @program: swagger-demo * @description: * @author: Marlowe * @create: 2020-12-06 19:39 **/@ApiModel(&quot;用户实体类&quot;)public class User &#123; @ApiModelProperty(&quot;用户名&quot;) public String username; @ApiModelProperty(&quot;密码&quot;) public String password;&#125; controller12345678910111213141516171819202122232425262728293031323334353637383940414243package com.marlowe.swagger.controller;import com.marlowe.swagger.pojo.User;import io.swagger.annotations.Api;import io.swagger.annotations.ApiOperation;import io.swagger.annotations.ApiParam;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PostMapping;import org.springframework.web.bind.annotation.RestController;/** * @program: swagger-demo * @description: * @author: Marlowe * @create: 2020-12-06 18:07 **/@Api(tags = &quot;hello控制类&quot;)@RestControllerpublic class HelloController &#123; @GetMapping(value = &quot;/hello&quot;) public String hello() &#123; return &quot;hello&quot;; &#125; @PostMapping(value = &quot;/user&quot;) public User user() &#123; return new User(); &#125; @ApiOperation(&quot;Hello 控制类&quot;) @GetMapping(value = &quot;/hello2&quot;) public String hello2(@ApiParam(&quot;用户名&quot;) String username) &#123; return &quot;hello&quot; + username; &#125; @ApiOperation(&quot;Post 控制类&quot;) @GetMapping(value = &quot;/postt&quot;) public User post(@ApiParam(&quot;用户&quot;) User user) &#123; return user; &#125;&#125; 总结： 可以通过Swagger给一些比较难理解的属性或者接口，增加注释信息 接口文档实时更新 可以在线测试 【注意点】在正式发布的时候，关闭Swagger！！！ 处于安全考虑，并且节省内存！","categories":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/categories/Spring/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://xmmarlowe.github.io/tags/SpringBoot/"},{"name":"Swagger","slug":"Swagger","permalink":"https://xmmarlowe.github.io/tags/Swagger/"},{"name":"配置","slug":"配置","permalink":"https://xmmarlowe.github.io/tags/%E9%85%8D%E7%BD%AE/"}],"author":"Marlowe"},{"title":"使用注解开发","slug":"Spring/使用注解开发","date":"2020-12-05T05:40:10.000Z","updated":"2020-12-05T06:18:50.748Z","comments":true,"path":"2020/12/05/Spring/使用注解开发/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/05/Spring/%E4%BD%BF%E7%94%A8%E6%B3%A8%E8%A7%A3%E5%BC%80%E5%8F%91/","excerpt":"使用注解开发…","text":"使用注解开发… 在Spring4之后，使用注解开发，必须要保证aop的包导入了使用注解需要导入context约束，增加注解的支持！ 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!--开启注解的支持--&gt; &lt;context:annotation-config/&gt;&lt;/beans&gt; 1.bean2.属性如何注入12345678910@Componentpublic class User &#123; public String name; // 相当于&lt;property name=&quot;name&quot; value=&quot;marlowe2&quot;&gt; @Value(&quot;marlowe2&quot;) public void setName(String name) &#123; this.name = name; &#125;&#125; 3.衍生的注解@Component有几个衍生注解，我们再web开发中，会按照mvc三层架构分层！ dao【@Repository】 service【@Service】 controller【@Service】这四个注解的功能都是一样的，都是代表将某个类注册到Spring中，装配Bean 4.自动装配1234- @Autowired:自动装配通过类型。名字 如果Autowired不能唯一自动装配上属性，则需要通过@Qualifier(value=&quot;xxx&quot;)- @Nullable： 字段标记了这个注解，说明这个字段可以为null- @Resource： 自动装配通过名字。类型 5.作用域1234567891011@Component@Scope(&quot;prototype&quot;)public class User &#123; public String name; // 相当于&lt;property name=&quot;name&quot; value=&quot;marlowe2&quot;&gt; @Value(&quot;marlowe2&quot;) public void setName(String name) &#123; this.name = name; &#125;&#125; 6.小结xml与注解： xml： 更加万能，适用于任何场合！维护简单方便 注解： 不是自己的类使用不了，维护相对复杂！ xml与注解最佳实践： xml用来管理bean 注解只负责完成属性的注入 我们在使用的过程中，只需要注意一个问题：必须让注解生效，就需要开启注解的支持 123&lt;!--指定要扫描的包，这个包下面的注解就会生效--&gt;&lt;context:component-scan base-package=&quot;com.marlowe&quot;/&gt;&lt;context:annotation-config/&gt;","categories":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/tags/Spring/"},{"name":"注解","slug":"注解","permalink":"https://xmmarlowe.github.io/tags/%E6%B3%A8%E8%A7%A3/"}],"author":"Marlowe"},{"title":"依赖注入","slug":"Spring/依赖注入","date":"2020-12-05T04:50:54.000Z","updated":"2020-12-05T04:55:07.288Z","comments":true,"path":"2020/12/05/Spring/依赖注入/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/05/Spring/%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/","excerpt":"简述依赖注入的三种方式","text":"简述依赖注入的三种方式 构造器注入见文章《IOC创建对象的方式》 Set方式注入【重点】 依赖注入：Set注入！ 依赖：bean对象的创建依赖于容器！ 注入：bean对象中的所有属性，由容器来注入！ 【环境搭建】 复杂类型123456789101112public class Address &#123; private String address; public String getAddress() &#123; return address; &#125; public void setAddress(String address) &#123; this.address = address; &#125;&#125; 真实测试对象 12345678910public class Student &#123; private String name; private Address address; private String[] books; private List&lt;String&gt; hobbies; private Map&lt;String, String&gt; card; private Set&lt;String&gt; games; private String wife; private Properties info;&#125; beans.xml 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!--第一种，普通值注入，value--&gt; &lt;bean id=&quot;student&quot; class=&quot;com.marlowe.pojo.Student&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;marlowe&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 测试类 1234567public class MyTest &#123; public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;beans.xml&quot;); Student student = (Student) context.getBean(&quot;student&quot;); System.out.println(student.getName()); &#125;&#125; 完善注入信息12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean id=&quot;address&quot; class=&quot;com.marlowe.pojo.Address&quot;&gt; &lt;property name=&quot;address&quot; value=&quot;China&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;student&quot; class=&quot;com.marlowe.pojo.Student&quot;&gt; &lt;!--第一种，普通值注入，value--&gt; &lt;property name=&quot;name&quot; value=&quot;marlowe&quot;/&gt; &lt;!--第二种，Bean注入，ref--&gt; &lt;property name=&quot;address&quot; ref=&quot;address&quot;/&gt; &lt;!--数组--&gt; &lt;property name=&quot;books&quot;&gt; &lt;array&gt; &lt;value&gt;红楼梦&lt;/value&gt; &lt;value&gt;西游记&lt;/value&gt; &lt;value&gt;三国演义&lt;/value&gt; &lt;/array&gt; &lt;/property&gt; &lt;!--List--&gt; &lt;property name=&quot;hobbies&quot;&gt; &lt;list&gt; &lt;value&gt;篮球&lt;/value&gt; &lt;value&gt;乒乓球&lt;/value&gt; &lt;value&gt;足球&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!--map--&gt; &lt;property name=&quot;card&quot;&gt; &lt;map&gt; &lt;entry key=&quot;身份证&quot; value=&quot;11111111&quot;/&gt; &lt;entry key=&quot;银行卡&quot; value=&quot;22222222&quot;/&gt; &lt;/map&gt; &lt;/property&gt; &lt;!--Set--&gt; &lt;property name=&quot;games&quot;&gt; &lt;set&gt; &lt;value&gt;LoL&lt;/value&gt; &lt;value&gt;DNF&lt;/value&gt; &lt;/set&gt; &lt;/property&gt; &lt;!--null--&gt; &lt;property name=&quot;wife&quot;&gt; &lt;null/&gt; &lt;/property&gt; &lt;!--Properties--&gt; &lt;property name=&quot;info&quot;&gt; &lt;props&gt; &lt;prop key=&quot;driver&quot;&gt;11111&lt;/prop&gt; &lt;prop key=&quot;url&quot;&gt;marlowe&lt;/prop&gt; &lt;prop key=&quot;username&quot;&gt;root&lt;/prop&gt; &lt;prop key=&quot;password&quot;&gt;123456&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 扩展方式注入我们可以使用p命名空间和c命名空间进行注入官方解释： 使用: 123456789101112131415&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:c=&quot;http://www.springframework.org/schema/c&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!--p命名空间注入，可以直接注入属性的值：property--&gt; &lt;bean id=&quot;user&quot; class=&quot;com.marlowe.pojo.User&quot; p:name=&quot;marlowe&quot; p:age=&quot;18&quot;&gt;&lt;/bean&gt; &lt;!--c命名空间注入，可以通过构造器注入：construct-args--&gt; &lt;bean id=&quot;user2&quot; class=&quot;com.marlowe.pojo.User&quot; c:name=&quot;marlowe&quot; c:age=&quot;18&quot;&gt;&lt;/bean&gt;&lt;/beans&gt; 测试： 123456@Testpublic void test2() &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;userbeans.xml&quot;); User user = (User) context.getBean(&quot;user2&quot;); System.out.println(user.toString());&#125; 注意点：p命名和c命名不能直接使用，需要导入xml约束！ 12xmlns:p=&quot;http://www.springframework.org/schema/p&quot;xmlns:c=&quot;http://www.springframework.org/schema/c&quot; bean的作用域 单例模式（spring默认机制）1&lt;bean id=&quot;user2&quot; class=&quot;com.marlowe.pojo.User&quot; c:name=&quot;marlowe&quot; c:age=&quot;18&quot; scope=&quot;singleton&quot;&gt;&lt;/bean&gt; 原型模式：每次从容器中get的时候，都会产生一个新对象！1&lt;bean id=&quot;user2&quot; class=&quot;com.marlowe.pojo.User&quot; c:name=&quot;marlowe&quot; c:age=&quot;18&quot; scope=&quot;singleton&quot;&gt;&lt;/bean&gt; 其余的request、session、application 这些只能在web开发中使用到！","categories":[],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/tags/Spring/"},{"name":"DI","slug":"DI","permalink":"https://xmmarlowe.github.io/tags/DI/"}],"author":"Marlowe"},{"title":"Bean的自动装配","slug":"Spring/Bean的自动装配","date":"2020-12-05T03:55:53.000Z","updated":"2020-12-05T05:38:33.810Z","comments":true,"path":"2020/12/05/Spring/Bean的自动装配/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/05/Spring/Bean%E7%9A%84%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D/","excerpt":"bean的三种自动装配方式…","text":"bean的三种自动装配方式… 自动装配是Spring满足bean依赖的一种方式 Spring会在上下文中自动寻找，并自动给bean装配属性 在Spring中有三种装配方式 在xml中显示的配置 在java中显示配置 隐式的自动装配bean【重要】 测试环境搭建：一个人有两个宠物 12345public class People &#123; private String name; private Cat cat; private Dog dog;&#125; byName自动装配123456&lt;!--byName:会自动在容器上下文中查找和自己对象set方法后面的值对应的bean id！--&gt;&lt;bean id=&quot;people&quot; class=&quot;com.marlowe.pojo.People&quot; autowire=&quot;byName&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;陈浩南&quot;/&gt;&lt;/bean&gt; byType自动装配123456789&lt;bean class=&quot;com.marlowe.pojo.Cat&quot;/&gt;&lt;bean class=&quot;com.marlowe.pojo.Dog&quot;/&gt;&lt;!--byName:会自动在容器上下文中查找，和自己对象set方法后面的值对应的bean id！byType:会自动在容器上下文中查找，和自己对象属性类型相同的bean！--&gt;&lt;bean id=&quot;people&quot; class=&quot;com.marlowe.pojo.People&quot; autowire=&quot;byType&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;陈浩南&quot;/&gt;&lt;/bean&gt; 小结： byName的时候，需要保证所有bean的id唯一，并且这个bean需要和自动注入的属性的set方法的值一致！（原理是将set方法后面部分转换成小写，再与id进行比对，例如：setDog ==&gt; id = “dog”、setdog1 ==&gt; id = “dog1”等可以自动注入、但是setDog ==&gt; id = “Dog”就不行） byType的时候，需要保证所有bean的class唯一，并且这个bean需要和自动注入的属性的类型一致！ 使用注解实现自动装配JDK1.5支持的注解，Spring2.5就支持注解了！要使用注解须知： 导入约束。context约束 配置注解的支持 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;context:annotation-config/&gt;&lt;/beans&gt; @Autowired 直接在属性上使用即可！也可以在set方式上使用！ 使用Autowired，我们可以不用编写set方法了，前提是你这个自动装配的属性在IOC（Spring）容器中存在，且符合名字byName！ 科普： 1@Nullable 字段标记了这个注解，说明这个字段可以为null 123 public @interface Autowired &#123; boolean required() default true;&#125; 测试代码 12345678public class People &#123; // 如果显示定义了Autowired的required属性为false，说明这个对象可以为null，否则不允许为空 @Autowired(required = false) private Cat cat; @Autowired private Dog dog; private String name;&#125;如果@Autowired自动装配的环境比较复杂，自动装配无法通过一个注解【@Autowired】完成额时候、我们可以使用@Qualifier(value = “xxx”)去配置@Autowired的使用，指定一个唯一的bean对象注入！ 1234567@Autowired@Qualifier(value = &quot;cat11&quot;)private Cat cat;@Autowired@Qualifier(value = &quot;dog11&quot;)private Dog dog; @Resource注解 1234567public class People &#123; @Resource(name = &quot;cat1&quot;) private Cat cat; @Resource private Dog dog;&#125; 小结：@Autowired和@Resource的区别： 都是用来自动装配的，都可以放在属性字段上 @Autowired 通过byType的方式实现，而且必须要求这个对象存在！【常用】 @Resource默认通过byName的方式实现，如果找不到名字，则通过byType实现！如果两个都找不到的情况下，就报错！【常用】 执行顺序不同：@Autowired 通过byType的方式实现。@Resource默认通过byName的方式实现。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/categories/Spring/"}],"tags":[{"name":"Bean","slug":"Bean","permalink":"https://xmmarlowe.github.io/tags/Bean/"},{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/tags/Spring/"}],"author":"Marlowe"},{"title":"IOC创建对象的方式","slug":"Spring/IOC创建对象的方式","date":"2020-12-04T13:02:41.000Z","updated":"2020-12-05T04:49:49.390Z","comments":true,"path":"2020/12/04/Spring/IOC创建对象的方式/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/04/Spring/IOC%E5%88%9B%E5%BB%BA%E5%AF%B9%E8%B1%A1%E7%9A%84%E6%96%B9%E5%BC%8F/","excerpt":"IOC创建对象的三种方式…","text":"IOC创建对象的三种方式… 下标 1234&lt;!--第一种方式：下标赋值--&gt;&lt;bean id=&quot;user&quot; class=&quot;com.marlowe.pojo.User&quot;&gt; &lt;constructor-arg index=&quot;0&quot; value=&quot;狂神说Java&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; 类型 1234&lt;!--第二种方式：不建议使用！通过类型创建--&gt;&lt;bean id=&quot;user&quot; class=&quot;com.marlowe.pojo.User&quot;&gt; &lt;constructor-arg type=&quot;java.lang.String&quot; value=&quot;狂神&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; 参数名 1234&lt;!--第三种方式，直接通过参数名来设置--&gt;&lt;bean id=&quot;user&quot; class=&quot;com.marlowe.pojo.User&quot;&gt; &lt;constructor-arg name=&quot;name&quot; value=&quot;狂神说Java&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; 总结：在配置文件加载的时候，容器中管理的对象就已经初始化了！","categories":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/tags/Spring/"},{"name":"IOC","slug":"IOC","permalink":"https://xmmarlowe.github.io/tags/IOC/"}],"author":"Marlowe"},{"title":"集合类不安全","slug":"并发/集合类不安全","date":"2020-12-03T13:49:32.000Z","updated":"2021-04-19T12:10:57.119Z","comments":true,"path":"2020/12/03/并发/集合类不安全/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/03/%E5%B9%B6%E5%8F%91/%E9%9B%86%E5%90%88%E7%B1%BB%E4%B8%8D%E5%AE%89%E5%85%A8/","excerpt":"111","text":"111 List不安全12345678910111213141516171819202122232425262728293031323334353637383940package com.marlowe.unsafe;import java.util.*;import java.util.concurrent.CopyOnWriteArrayList;/** * @program: juc * @description: java.util.ConcurrentModificationException 并发修改异常 * @author: Marlowe * @create: 2020-12-03 21:00 **/public class ListTest &#123; public static void main(String[] args) &#123; /** * 并发下ArrayList 不安全的 * * 解决方法： * 1、List&lt;String&gt; list = new Vector&lt;&gt;(); * 2、List&lt;String&gt; list = Collections.synchronizedList(new ArrayList&lt;&gt;()); * 3、List&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;(); */ /** * CopyOnWrite 写入时复制 COW 计算机程序设计领域的一种优化策略 * 多个线程调用的时候，list，读取的时候，固定的，写入的（覆盖） * 在写入的时候避免覆盖，造成数据问题！ * CopyOnWriteArrayList 比 Vector 好在那里 前者是lock，后者是是synchronized */ List&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; i++) &#123; new Thread(() -&gt; &#123; list.add(UUID.randomUUID().toString().substring(0, 5)); System.out.println(list); &#125;, String.valueOf(i)).start(); &#125; &#125;&#125; Set不安全123456789101112131415161718192021222324252627282930package com.marlowe.unsafe;import java.util.Collections;import java.util.HashSet;import java.util.Set;import java.util.UUID;import java.util.concurrent.CopyOnWriteArraySet;/** * @program: juc * @description: 同理可证 ：java.util.ConcurrentModificationException * 1、Set&lt;String&gt; set = Collections.synchronizedSet(new HashSet&lt;&gt;()); * 2、Set&lt;String&gt; set = new CopyOnWriteArraySet&lt;&gt;(); * @author: Marlowe * @create: 2020-12-03 21:53 **/public class SetTest &#123; public static void main(String[] args) &#123; // Set&lt;String&gt; set = new HashSet&lt;&gt;(); // Set&lt;String&gt; set = Collections.synchronizedSet(new HashSet&lt;&gt;()); Set&lt;String&gt; set = new CopyOnWriteArraySet&lt;&gt;(); for (int i = 0; i &lt; 30; i++) &#123; new Thread(() -&gt; &#123; set.add(UUID.randomUUID().toString().substring(0, 5)); System.out.println(set); &#125;, String.valueOf(i)).start(); &#125; &#125;&#125; HashSet 底层是什么？1234567891011public HashSet() &#123; map = new HashMap&lt;&gt;();&#125;// add set本质就是map key是无法重复的public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125;// PRESENTprivate static final Object PRESENT = new Object(); Map 不安全回顾Map基本操作 12345678910111213141516171819202122232425import java.util.UUID;import java.util.concurrent.ConcurrentHashMap;/** * @program: juc * @description: java.util.ConcurrentModificationException * @author: Marlowe * @create: 2020-12-03 22:11 **/public class MapTest &#123; public static void main(String[] args) &#123; // map是这样用的吗？不是，工作中不用HashMap // HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;(); Map&lt;String, String&gt; map = new ConcurrentHashMap&lt;&gt;(); for (int i = 0; i &lt; 30; i++) &#123; new Thread(() -&gt; &#123; map.put(Thread.currentThread().getName(), UUID.randomUUID().toString().substring(0, 5)); System.out.println(map); &#125;, String.valueOf(i)).start(); &#125; &#125;&#125;","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"https://xmmarlowe.github.io/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"list","slug":"list","permalink":"https://xmmarlowe.github.io/tags/list/"},{"name":"线程不安全","slug":"线程不安全","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B%E4%B8%8D%E5%AE%89%E5%85%A8/"}],"author":"Marlowe"},{"title":"8锁问题","slug":"并发/8锁问题","date":"2020-12-03T12:08:54.000Z","updated":"2021-04-19T12:10:56.200Z","comments":true,"path":"2020/12/03/并发/8锁问题/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/03/%E5%B9%B6%E5%8F%91/8%E9%94%81%E9%97%AE%E9%A2%98/","excerpt":"待完善…","text":"待完善… 小结 new this 具体的一个手机static Class 唯一的一个模板","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"多线程","slug":"多线程","permalink":"https://xmmarlowe.github.io/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"锁","slug":"锁","permalink":"https://xmmarlowe.github.io/tags/%E9%94%81/"}],"author":"Marlowe"},{"title":"synchronized和Lock区别","slug":"并发/synchronized和Lock区别","date":"2020-12-02T14:19:29.000Z","updated":"2021-04-26T14:28:37.552Z","comments":true,"path":"2020/12/02/并发/synchronized和Lock区别/","link":"","permalink":"https://xmmarlowe.github.io/2020/12/02/%E5%B9%B6%E5%8F%91/synchronized%E5%92%8CLock%E5%8C%BA%E5%88%AB/","excerpt":"简述 synchronized 和 Lock 区别…","text":"简述 synchronized 和 Lock 区别… synchronized 内置的Java关键字；Lock 是一个Java类 synchronized 无法判断获取锁的状态；Lock 可以判断是否获取了锁 synchronized 会自动释放锁；Lock 必须要手动释放锁！ 如果不释放，死锁 synchronized 线程1（获得锁，阻塞）、线程2（等待、傻傻的等）；Lock 锁就不一定会等待下去 synchronized 可重入锁，不可以中断的，非公平的；Lock ，可重入锁，可以判断锁，非公平（可以自己设置） synchronized 适合锁少量的代码同步问题；Lock 适合锁大量的！ Synchronized关键字，用来加锁。Volatile只是保持变 量的线程可见性。通常适用于一个线程写，多个线程读的场景。 Volatile能不能保证线程安全？不能。Volatile关键字只能保证线程可见性，不能保证原子性。详情见站内volatile关键字一文。","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"多线程","slug":"多线程","permalink":"https://xmmarlowe.github.io/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"锁","slug":"锁","permalink":"https://xmmarlowe.github.io/tags/%E9%94%81/"}],"author":"Marlowe"},{"title":"设计模式-策略","slug":"设计模式/设计模式-策略","date":"2020-11-29T12:38:08.000Z","updated":"2020-11-30T04:04:57.448Z","comments":true,"path":"2020/11/29/设计模式/设计模式-策略/","link":"","permalink":"https://xmmarlowe.github.io/2020/11/29/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%AD%96%E7%95%A5/","excerpt":"在策略模式中，我们创建表示各种策略的对象和一个行为随着策略对象改变而改变的 context 对象。策略对象改变 context 对象的执行算法。","text":"在策略模式中，我们创建表示各种策略的对象和一个行为随着策略对象改变而改变的 context 对象。策略对象改变 context 对象的执行算法。 介绍在策略模式（Strategy Pattern）中，一个类的行为或其算法可以在运行时更改。这种类型的设计模式属于行为型模式。 在策略模式中，我们创建表示各种策略的对象和一个行为随着策略对象改变而改变的 context 对象。策略对象改变 context 对象的执行算法。 使用场景 如果在一个系统里面有许多类，它们之间的区别仅在于它们的行为，那么使用策略模式可以动态地让一个对象在许多行为中选择一种行为。 一个系统需要动态地在几种算法中选择一种。 如果一个对象有很多的行为，如果不用恰当的模式，这些行为就只好使用多重的条件选择语句来实现。 优缺点及注意优点 算法可以自由切换。 避免使用多重条件判断。 扩展性良好。 缺点 策略类会增多。 所有策略类都需要对外暴露。 注意如果一个系统的策略多于四个，就需要考虑使用混合模式，解决策略类膨胀的问题。 实现策略角色 123456public interface Strategy &#123; /** * 算法方法 */ public void algorithmInterface();&#125; 以下三个实现类为具体的策略角色 123456public class ConcreteStrategyA implements Strategy&#123; @Override public void algorithmInterface() &#123; System.out.println(&quot;具体的策略A&quot;); &#125;&#125; 123456public class ConcreteStrategyB implements Strategy &#123; @Override public void algorithmInterface() &#123; System.out.println(&quot;具体的策略B&quot;); &#125;&#125; 1234567public class ConcreteStrategyC implements Strategy &#123; @Override public void algorithmInterface() &#123; System.out.println(&quot;具体的策略C&quot;); &#125;&#125; Context上下文 1234567891011121314public class Context &#123; private Strategy strategy; public Context(Strategy strategy) &#123; this.strategy = strategy; &#125; /** * 上下文接口,执行对应策略 */ public void executeStrategy() &#123; strategy.algorithmInterface(); &#125;&#125; 客户端 123456789101112131415public class Client &#123; public static void main(String[] args) &#123; Context context; context = new Context(new ConcreteStrategyA()); context.executeStrategy(); context = new Context(new ConcreteStrategyB()); context.executeStrategy(); context = new Context(new ConcreteStrategyC()); context.executeStrategy(); &#125;&#125; 结果： 123具体的策略A具体的策略B具体的策略C","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"行为型模式","slug":"行为型模式","permalink":"https://xmmarlowe.github.io/tags/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F/"},{"name":"策略","slug":"策略","permalink":"https://xmmarlowe.github.io/tags/%E7%AD%96%E7%95%A5/"}],"author":"Marlowe"},{"title":"JsonUtils","slug":"自定义工具类/JsonUtils","date":"2020-11-27T13:34:41.000Z","updated":"2020-12-11T02:07:56.029Z","comments":true,"path":"2020/11/27/自定义工具类/JsonUtils/","link":"","permalink":"https://xmmarlowe.github.io/2020/11/27/%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B7%A5%E5%85%B7%E7%B1%BB/JsonUtils/","excerpt":"","text":"123456789101112131415161718192021222324252627282930package com.kuang.utils;import com.fasterxml.jackson.core.JsonProcessingException;import com.fasterxml.jackson.databind.ObjectMapper;import com.fasterxml.jackson.databind.SerializationFeature;import java.text.SimpleDateFormat;public class JsonUtils &#123; public static String getJson(Object object) &#123; return getJson(object,&quot;yyyy-MM-dd HH:mm:ss&quot;); &#125; public static String getJson(Object object,String dateFormat) &#123; ObjectMapper mapper = new ObjectMapper(); //不使用时间差的方式 mapper.configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS, false); //自定义日期格式对象 SimpleDateFormat sdf = new SimpleDateFormat(dateFormat); //指定日期格式 mapper.setDateFormat(sdf); try &#123; return mapper.writeValueAsString(object); &#125; catch (JsonProcessingException e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125;","categories":[{"name":"自定义工具类","slug":"自定义工具类","permalink":"https://xmmarlowe.github.io/categories/%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B7%A5%E5%85%B7%E7%B1%BB/"}],"tags":[{"name":"Json","slug":"Json","permalink":"https://xmmarlowe.github.io/tags/Json/"},{"name":"Utils","slug":"Utils","permalink":"https://xmmarlowe.github.io/tags/Utils/"}],"author":"Marlowe"},{"title":"设计模式-代理","slug":"设计模式/设计模式-代理","date":"2020-11-27T08:36:13.000Z","updated":"2021-03-17T07:13:40.626Z","comments":true,"path":"2020/11/27/设计模式/设计模式-代理/","link":"","permalink":"https://xmmarlowe.github.io/2020/11/27/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E4%BB%A3%E7%90%86/","excerpt":"在代理模式中，我们创建具有现有对象的对象，以便向外界提供功能接口。","text":"在代理模式中，我们创建具有现有对象的对象，以便向外界提供功能接口。 介绍在代理模式（Proxy Pattern）中，一个类代表另一个类的功能。这种类型的设计模式属于结构型模式。 在代理模式中，我们创建具有现有对象的对象，以便向外界提供功能接口。 主要解决的问题在直接访问对象时带来的问题，比如说：要访问的对象在远程的机器上。在面向对象系统中，有些对象由于某些原因（比如对象创建开销很大，或者某些操作需要安全控制，或者需要进程外的访问），直接访问会给使用者或者系统结构带来很多麻烦，我们可以在访问此对象时加上一个对此对象的访问层。 优缺点及注意优点 职责清晰。 高扩展性。 智能化。缺点 由于在客户端和真实主题之间增加了代理对象，因此有些类型的代理模式可能会造成请求的处理速度变慢。 实现代理模式需要额外的工作，有些代理模式的实现非常复杂。 注意 和适配器模式的区别：适配器模式主要改变所考虑对象的接口，而代理模式不能改变所代理类的接口。 和装饰器模式的区别：装饰器模式为了增强功能，而代理模式是为了加以控制。 实现静态代理角色分析: 抽象角色：一般会使用接口或者抽象类来解决 真实角色：被代理的角色 代理角色：代理真实角色，代理真实角色后，我们一般会做一些附属操作 客户：访问代理对象的人！ 接口1234567public interface Rent &#123; /** * 出租房屋 */ public void rent();&#125; 真实角色123456public class Landlord implements Rent &#123; @Override public void rent() &#123; System.out.println(&quot;房东要出租房子！&quot;); &#125;&#125; 代理角色12345678910111213141516171819202122232425262728293031public class Proxy implements Rent &#123; private Landlord landlord; public Proxy() &#123; &#125; public Proxy(Landlord landlord) &#123; this.landlord = landlord; &#125; @Override public void rent() &#123; seeHouse(); landlord.rent(); signContract(); fee(); &#125; public void seeHouse() &#123; System.out.println(&quot;中介带你看房&quot;); &#125; public void signContract() &#123; System.out.println(&quot;签合同&quot;); &#125; public void fee() &#123; System.out.println(&quot;收中介费&quot;); &#125;&#125; 客户端访问代理角色1234567891011public class Client &#123; public static void main(String[] args) &#123; // 房东要租房子 Landlord landlord = new Landlord(); // 代理,中介帮房东租房子，但是 代理一般会有一些附属操作 Proxy proxy = new Proxy(landlord); // 你不用面对房东，直接找中介即可 proxy.rent(); &#125;&#125; 12345结果：中介带你看房房东要出租房子！签合同收中介费 Spring AOP 代理模式的好处： 可以是真实角色的操作更加纯粹！不用去关注一些公共的业务 公共也就交给代理角色！实现了业务的分工！ 公共业务发生扩展的时候，方便集中管理！ 缺点： 一个真实的角色就会产生一个代理角色；代码量会翻倍，开发效率会变低~ 动态代理 动态代理和静态代理角色一样 动态代理的代理类是动态生成的，不是我们直接写好的！ 动态代理分为两大类：基于接口的动态代理，基于类的动态代理 基于接口 — JDK动态代理 基于类：cglib java字节码实现：javasist 需要两节两个类：Proxy，InvocationHandler：调用处理程序 动态代理的好处： 可以使真实角色的操作更加纯粹！不用去关注一些公共的业务 公共也就交给代理角色！实现了业务的分工！ 公共业务发生扩展的时候，方便集中管理！ 一个动态代理类代理的是一个接口，一般就是对应的一类业务 一个动态代理类可以代理多个类，只要实现了同一接口即可。 接口123456789public interface UserService &#123; public void add(); public void delete(); public void update(); public void query();&#125; 实现类12345678910111213141516171819202122public class UserServiceImpl implements UserService&#123; @Override public void add() &#123; System.out.println(&quot;增加了一个用户&quot;); &#125; @Override public void delete() &#123; System.out.println(&quot;删除了一个用户&quot;); &#125; @Override public void update() &#123; System.out.println(&quot;修改了一个用户&quot;); &#125; @Override public void query() &#123; System.out.println(&quot;查询了一个用户&quot;); &#125;&#125; 动态代理工具类（通用方法）12345678910111213141516171819202122232425262728293031323334353637383940public class ProxyInvocationHandlerUtil implements InvocationHandler &#123; /** * 被代理的接口 */ private Object target; public void setTarget(Object target) &#123; this.target = target; &#125; /** * 生成得到代理类 * * @return */ public Object getProxy() &#123; return Proxy.newProxyInstance(this.getClass().getClassLoader(), target.getClass().getInterfaces(), this); &#125; /** * 处理代理实例，并返回结果 * * @param proxy * @param method * @param args * @return * @throws Throwable */ @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; log(method.getName()); Object result = method.invoke(target, args); return result; &#125; public void log(String msg) &#123; System.out.println(&quot;[Debug] 使用了&quot; + msg + &quot;方法&quot;); &#125;&#125; 客户端访问代理角色12345678910111213public class Client2 &#123; public static void main(String[] args) &#123; // 真实角色 UserServiceImpl userService = new UserServiceImpl(); // 代理角色，不存在 ProxyInvocationHandlerUtil pihu = new ProxyInvocationHandlerUtil(); // 设置需要代理的对象 pihu.setTarget(userService); // 动态生成代理类 UserService proxy = (UserService) pihu.getProxy(); proxy.add(); &#125;&#125; 123结果：[Debug] 使用了add方法增加了一个用户","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"代理","slug":"代理","permalink":"https://xmmarlowe.github.io/tags/%E4%BB%A3%E7%90%86/"},{"name":"结构型模式","slug":"结构型模式","permalink":"https://xmmarlowe.github.io/tags/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F/"}],"author":"Marlowe"},{"title":"设计模式-单例","slug":"设计模式/设计模式-单例","date":"2020-11-22T23:17:03.000Z","updated":"2021-06-01T13:51:28.036Z","comments":true,"path":"2020/11/23/设计模式/设计模式-单例/","link":"","permalink":"https://xmmarlowe.github.io/2020/11/23/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%95%E4%BE%8B/","excerpt":"单例模式涉及到一个单一的类，该类负责创建自己的对象，同时确保只有单个对象被创建。这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。","text":"单例模式涉及到一个单一的类，该类负责创建自己的对象，同时确保只有单个对象被创建。这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。 介绍单例模式（Singleton Pattern）是 Java 中最简单的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。 这种模式涉及到一个单一的类，该类负责创建自己的对象，同时确保只有单个对象被创建。 这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。 注意： 单例类只能有一个实例。 单例类必须自己创建自己的唯一实例。 单例类必须给所有其他对象提供这一实例。 使用场景： 要求生产唯一序列号。 WEB 中的计数器，不用每次刷新都在数据库里加一次，用单例先缓存起来。 创建的一个对象需要消耗的资源过多，比如 I/O 与数据库的连接等。优缺点及注意 优点 在内存里只有一个实例，减少了内存的开销，尤其是频繁的创建和销毁实例（比如管理学院首页页面缓存）。 避免对资源的多重占用（比如写文件操作）。缺点没有接口，不能继承，与单一职责原则冲突，一个类应该只关心内部逻辑，而不关心外面怎么样来实例化。注意getInstance() 方法中需要使用同步锁 synchronized (Singleton.class) 防止多线程同时进入造成 instance 被多次实例化。 实现饿汉式12345678910111213141516171819202122232425262728package com.marlowe.singleton;/** * @program: GoF23 * @description: 饿汉式 * @author: Marlowe * @create: 2020-11-23 15:07 **/public class Hungry &#123; /** * 可能会浪费空间 */ private byte[] data1 = new byte[1024 * 1024]; private byte[] data2 = new byte[1024 * 1024]; private byte[] data3 = new byte[1024 * 1024]; private Hungry() &#123; &#125; private final static Hungry HUNGRY = new Hungry(); public static Hungry getInstance() &#123; return HUNGRY; &#125;&#125; 3. 饿汉式是否 Lazy 初始化：否 是否多线程安全：是 实现难度：易 描述：这种方式比较常用，但容易产生垃圾对象。 优点：没有加锁，执行效率会提高。 缺点：类加载时就初始化，浪费内存。 它基于 classloader 机制避免了多线程的同步问题，不过，instance 在类装载时就实例化，虽然导致类装载的原因有很多种，在单例模式中大多数都是调用 getInstance 方法， 但是也不能确定有其他的方式（或者其他的静态方法）导致类装载，这时候初始化 instance 显然没有达到 lazy loading 的效果。 代码示例： 1234567public class Singleton &#123; private static Singleton instance = new Singleton(); private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; return instance; &#125; &#125; 1. 懒汉式，线程不安全是否 Lazy 初始化：是 是否多线程安全：否 实现难度：易 描述：这种方式是最基本的实现方式，这种实现最大的问题就是不支持多线程。因为没有加锁 synchronized，所以严格意义上它并不算单例模式。 这种方式 lazy loading 很明显，不要求线程安全，在多线程不能正常工作。 代码示例： 1234567891011public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 2. 懒汉式，线程安全是否 Lazy 初始化：是 是否多线程安全：是 实现难度：易 描述：这种方式具备很好的 lazy loading，能够在多线程中很好的工作，但是，效率很低，99% 情况下不需要同步。 优点：第一次调用才初始化，避免内存浪费。 缺点：必须加锁 synchronized 才能保证单例，但加锁会影响效率。 getInstance() 的性能对应用程序不是很关键（该方法使用不太频繁）。 代码示例： 12345678910public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 5. 登记式/静态内部类是否 Lazy 初始化：是 是否多线程安全：是 实现难度：一般 描述：这种方式能达到双检锁方式一样的功效，但实现更简单。对静态域使用延迟初始化，应使用这种方式而不是双检锁方式。这种方式只适用于静态域的情况，双检锁方式可在实例域需要延迟初始化时使用。 这种方式同样利用了 classloader 机制来保证初始化 instance 时只有一个线程，它跟第 3 种方式不同的是：第 3 种方式只要 Singleton 类被装载了，那么 instance 就会被实例化（没有达到 lazy loading 效果），而这种方式是 Singleton 类被装载了，instance 不一定被初始化。因为 SingletonHolder 类没有被主动使用，只有通过显式调用 getInstance 方法时，才会显式装载 SingletonHolder 类，从而实例化 instance。想象一下，如果实例化 instance 很消耗资源，所以想让它延迟加载，另外一方面，又不希望在 Singleton 类加载时就实例化，因为不能确保 Singleton 类还可能在其他的地方被主动使用从而被加载，那么这个时候实例化 instance 显然是不合适的。这个时候，这种方式相比第 3 种方式就显得很合理。 代码示例： 123456789public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 6. 枚举JDK 版本：JDK1.5 起 是否 Lazy 初始化：否 是否多线程安全：是 实现难度：易 描述：这种实现方式还没有被广泛采用，但这是实现单例模式的最佳方法。它更简洁，自动支持序列化机制，绝对防止多次实例化。 这种方式是 Effective Java 作者 Josh Bloch 提倡的方式，它不仅能避免多线程同步问题，而且还自动支持序列化机制，防止反序列化重新创建新的对象，绝对防止多次实例化。不过，由于 JDK1.5 之后才加入 enum 特性，用这种方式写不免让人感觉生疏，在实际工作中，也很少用。 不能通过 reflection attack 来调用私有构造方法。 代码示例： 12345public enum Singleton &#123; INSTANCE; public void whateverMethod() &#123; &#125; &#125; 经验之谈：一般情况下，不建议使用第 1 种和第 2 种懒汉方式，建议使用第 3 种饿汉方式。只有在要明确实现 lazy loading 效果时，才会使用第 5 种登记方式。如果涉及到反序列化创建对象时，可以尝试使用第 6 种枚举方式。如果有其他特殊的需求，可以考虑使用第 4 种双检锁方式。 4. DCL(Double CheckLock 双重校验锁(线程安全、效率高))懒汉式，深究！注意： 如果不使用volatile关键词修饰，可能会导致拿到的对象是未被初始化的。具体原因见代码注释部分 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081package com.marlowe.singleton;import java.lang.reflect.Constructor;import java.lang.reflect.Field;/** * @program: GoF23 * @description: 懒汉式 * @author: Marlowe * @create: 2020-11-23 15:11 **/public class LazyMan &#123; private static boolean marlowe = false; private LazyMan() &#123; synchronized (LazyMan.class) &#123; if (marlowe == false) &#123; marlowe = true; &#125; else &#123; throw new RuntimeException(&quot;不要试图使用反射破坏异常&quot;); &#125; &#125; System.out.println(Thread.currentThread().getName() + &quot;ok&quot;); &#125; private volatile static LazyMan lazyMan; /** * 双重检测所模式的 懒汉式单例 DCL懒汉式 * * @return */ public static LazyMan getInstance() &#123; if (lazyMan == null) &#123; synchronized (LazyMan.class) &#123; if (lazyMan == null) &#123; // 创建对象不是原子性操作，可能导致对象未初始化 lazyMan = new LazyMan(); /** * 分三步完成 * * 1. 分配内存空间 * 2. 执行构造方法，初始化对象 * 3. 把这个对象指向分配的内存空间 * * 预期执行顺序 1-&gt;2-&gt;3 * 由于JVM具有指令重排的特性 实际顺序可能是1-&gt;3-&gt;2 * 指令重排在单线程的环境下不会出现问题，但是在多线程环境下可能会导致一个线程获得还没有初始化的实例 * 例如：A线程执行了1,3，此时B线程调用getInstance() 后发现 lazyMan 不为空，因此直接返回 lazyMan * 但此时 lazyMan 还未被初始化。使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行 * */ &#125; &#125; &#125; return lazyMan; &#125; /** * 反射 * * @param args */ public static void main(String[] args) throws Exception &#123; Field marlowe = LazyMan.class.getDeclaredField(&quot;marlowe&quot;); marlowe.setAccessible(true); Constructor&lt;LazyMan&gt; declaredConstructor = LazyMan.class.getDeclaredConstructor(null); declaredConstructor.setAccessible(true); LazyMan instance = declaredConstructor.newInstance(); LazyMan instance2 = declaredConstructor.newInstance(); marlowe.set(instance, false); System.out.println(instance); System.out.println(instance2); &#125;&#125; DCL单例为什么要加volatile我们把上面写的DDL单例拿过来，加入不加volatile，如下： 1234567891011121314public class SingleInstance &#123; private SingleInstance() &#123;&#125; private static SingleInstance INSTANCE; public static SingleInstance getInstance() &#123; if (INSTANCE == null) &#123; synchronized (SingleInstance.class) &#123; if (INSTANCE == null) &#123; INSTANCE = new SingleInstance(); &#125; &#125; &#125; return INSTANCE; &#125;&#125; 当 INSTANCE = new SingleInstance() 创建实例对象时，并不是原子操作，它是分三步来完成的： 创建内存空间。 执行构造函数，初始化（init） 将INSTANCE引用指向分配的内存空间 上述正常步骤按照1–&gt;2–&gt;3来执行的，但是，我们知道，JVM为了优化指令，提高程序运行效率，允许指令重排序。正是有了指令重排序的存在，那么就有可能按照1–&gt;3–&gt;2步骤来执行，这时候，当线程a执行步骤3完毕，在执行步骤2之前，被切换到线程b上，这时候instance判断为非空，此时线程b直接来到return instance语句，拿走instance然后使用，接着就顺理成章地报错（对象尚未初始化）。 synchronized虽然保证了线程的原子性（即synchronized块中的语句要么全部执行，要么一条也不执行），但单条语句编译后形成的指令并不是一个原子操作（即可能该条语句的部分指令未得到执行，就被切换到另一个线程了）。 volatile关键字其中一个作用就是禁止指令重排序，所以DCL单例必须要加volatile volatile作用： 保证被修饰的变量对所有线程的可见性。 禁止指令重排序优化。 静态内部类12345678910111213141516171819202122package com.marlowe.singleton;/** * @program: GoF23 * @description: 静态内部类 * @author: Marlowe * @create: 2020-11-23 15:32 **/public class Holder &#123; private Holder() &#123; &#125; public static Holder getInstance() &#123; return InnerClass.HOLDER; &#125; public static class InnerClass &#123; private static final Holder HOLDER = new Holder(); &#125;&#125; 单例不安全，因为有反射12345678910111213141516171819202122232425262728293031package com.marlowe.singleton;import java.lang.reflect.Constructor;import java.lang.reflect.InvocationTargetException;/** * @program: GoF23 * @description: enum 是什么？ 本身也是一个class类 * @author: Marlowe * @create: 2020-11-23 15:49 **/public enum EnumSingleton &#123; INSTANCE; public EnumSingleton getInstance() &#123; return INSTANCE; &#125;&#125;class Test &#123; public static void main(String[] args) throws IllegalAccessException, InvocationTargetException, InstantiationException, NoSuchMethodException &#123; EnumSingleton instance1 = EnumSingleton.INSTANCE; Constructor&lt;EnumSingleton&gt; declaredConstructor = EnumSingleton.class.getDeclaredConstructor(String.class, int.class); declaredConstructor.setAccessible(true); EnumSingleton instance2 = declaredConstructor.newInstance(); System.out.println(instance1); System.out.println(instance2); &#125;&#125; 枚举类型的最终反编译原码里面是有参构造方法 经验之谈 单例对象 占用资源少，不需要延时加载，枚举 好于 饿汉 单例对象 占用资源多，需要延时加载，静态内部类 好于 懒汉式 参考文献blog.unclezs.com","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"单例","slug":"单例","permalink":"https://xmmarlowe.github.io/tags/%E5%8D%95%E4%BE%8B/"},{"name":"创建型模式","slug":"创建型模式","permalink":"https://xmmarlowe.github.io/tags/%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F/"}],"author":"Marlowe"},{"title":"设计模式-工厂","slug":"设计模式/设计模式-工厂","date":"2020-11-20T08:49:24.000Z","updated":"2020-12-05T04:44:01.060Z","comments":true,"path":"2020/11/20/设计模式/设计模式-工厂/","link":"","permalink":"https://xmmarlowe.github.io/2020/11/20/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%B7%A5%E5%8E%82/","excerpt":"工厂设计模式很常用，分为简单工厂，工厂方法和抽象工厂。","text":"工厂设计模式很常用，分为简单工厂，工厂方法和抽象工厂。 核心本质 实例化对象不使用new，用工厂方法代替 将选择实现类，创建对象统一管理和控制。从而将调用者跟我们实现类解耦 OOP七大原则 开闭原则：一个软件的实体应当对扩展开放，对修改关闭 依赖倒转原则：要针对接口编程，不要针对实现编程 迪米特法则：只与你直接的朋友通信，而避免与陌生人通信 应用场景 JDK中Calendar的getInstance方法 JDBC中的Connection对象的获取 Spring中IOC容器创建管理bean对象 反射中Class对象的newInstance方法 三种模式简单工厂(Simple Factory)用来生产同一等级结构中的任意产品（对于增加新的产品，需要扩展已有代码） 如下图需要扩展一类新车–Ford，需要扩展车工厂里面的代码 Car接口 123456789101112package com.marlowe.factory.simple;/** * @program: GoF23 * @description: 车接口 * @author: Marlowe * @create: 2020-11-20 17:00 **/public interface Car &#123; public void name();&#125; 以下两个实体类实现Car接口 123456789101112131415package com.marlowe.factory.simple;/** * @program: GoF23 * @description: 宝马 * @author: Marlowe * @create: 2020-11-20 17:01 **/public class Bmw implements Car &#123; @Override public void name() &#123; System.out.println(&quot;我是宝马！&quot;); &#125;&#125; 123456789101112131415package com.marlowe.factory.simple;/** * @program: GoF23 * @description: 特斯拉 * @author: Marlowe * @create: 2020-11-20 17:01 **/public class Tesla implements Car&#123; @Override public void name() &#123; System.out.println(&quot;我是特斯拉！&quot;); &#125;&#125; CarFactory工厂类 1234567891011121314151617181920212223242526272829303132333435363738394041package com.marlowe.factory.simple;/** * @program: GoF23 * @description: 汽车工厂 * 静态工厂模式 * 开闭原则 * @author: Marlowe * @create: 2020-11-20 17:02 **/public class CarFactory &#123; /** * 方法一 * * @param car * @return */ public static Car getCar(String car) &#123; if (&quot;宝马&quot;.equals(car)) &#123; return new Bmw(); &#125; else if (&quot;特斯拉&quot;.equals(car)) &#123; return new Tesla(); &#125; else &#123; return null; &#125; &#125; /** * 方法二 * * @return */ public static Car getBmw() &#123; return new Bmw(); &#125; public static Car getTesla() &#123; return new Tesla(); &#125;&#125; 主类 1234567891011121314151617181920212223package com.marlowe.factory.simple;/** * @program: GoF23 * @description: 顾客 * @author: Marlowe * @create: 2020-11-20 17:03 **/public class Comsumer &#123; public static void main(String[] args) &#123; System.out.println(&quot;通过方式1获取:&quot;); Car car1 = CarFactory.getCar(&quot;宝马&quot;); Car car2 = CarFactory.getCar(&quot;特斯拉&quot;); car1.name(); car2.name(); System.out.println(&quot;通过方式2获取:&quot;); Car bmw = CarFactory.getBmw(); Car tesla = CarFactory.getTesla(); bmw.name(); tesla.name(); &#125;&#125; 1234567结果：通过方式1获取:我是宝马！我是特斯拉！通过方式2获取:我是宝马！我是特斯拉！ 工厂方法(Factory Method)用来生产同一等级结构中的固定产品（支持增加任意产品） 如下图需要扩展一类新车–Ford，横向扩展即可 Car接口 123456789101112package com.marlowe.factory.method;/** * @program: GoF23 * @description: 车接口 * @author: Marlowe * @create: 2020-11-20 17:00 **/public interface Car &#123; public void name();&#125; 以下两个实体类实现Car接口 123456789101112131415package com.marlowe.factory.method;/** * @program: GoF23 * @description: 宝马 * @author: Marlowe * @create: 2020-11-20 17:01 **/public class Bmw implements Car &#123; @Override public void name() &#123; System.out.println(&quot;我是宝马！&quot;); &#125;&#125; 123456789101112131415package com.marlowe.factory.method;/** * @program: GoF23 * @description: 特斯拉 * @author: Marlowe * @create: 2020-11-20 17:01 **/public class Tesla implements Car &#123; @Override public void name() &#123; System.out.println(&quot;我是特斯拉！&quot;); &#125;&#125; CarFactory接口 123456789101112package com.marlowe.factory.method;/** * @program: GoF23 * @description: 工厂方法模式 * @author: Marlowe * @create: 2020-11-20 18:54 **/public interface CarFactory &#123; Car getCar();&#125; 以下两个类实现CarFactory接口 123456789101112131415package com.marlowe.factory.method;/** * @program: GoF23 * @description: * @author: Marlowe * @create: 2020-11-20 18:55 **/public class BmwFactory implements CarFactory &#123; @Override public Car getCar() &#123; return new Bmw(); &#125;&#125; 123456789101112131415package com.marlowe.factory.method;/** * @program: GoF23 * @description: * @author: Marlowe * @create: 2020-11-20 18:55 **/public class TeslaFactory implements CarFactory &#123; @Override public Car getCar() &#123; return new Tesla(); &#125;&#125; 主类 123456789101112131415161718192021package com.marlowe.factory.method;import com.marlowe.factory.simple.CarFactory;/** * @program: GoF23 * @description: 顾客 * @author: Marlowe * @create: 2020-11-20 17:03 **/public class Comsumer &#123; public static void main(String[] args) &#123; Car car1 = new TeslaFactory().getCar(); Car car2 = new BmwFactory().getCar(); Car car3 = new FordFactory().getCar(); car1.name(); car2.name(); car3.name(); &#125;&#125; 1234结果：我是特斯拉！我是宝马！我是福特！ 抽象工厂(Abstract Factory)围绕一个超级工厂创建其他工厂，该工厂又称为其他工厂的工厂 抽象工厂模式提供了一个创建一系列相关或者相互依赖对象的接口，无需指定它们具体的类 抽象产品工厂 123456789101112131415161718192021222324package com.marlowe.factory.abstract1;/** * @program: GoF23 * @description: 抽象产品工厂 * @author: Marlowe * @create: 2020-11-21 10:54 **/public interface IProductFactory &#123; /** * 生产手机 * * @return */ IPhoneProduct iPhoneProduct(); /** * 生产路由器 * * @return */ IRouterProduct iRouterProduct();&#125; 小米工厂 1234567891011121314151617181920package com.marlowe.factory.abstract1;/** * @program: GoF23 * @description: 小米工厂 * @author: Marlowe * @create: 2020-11-21 10:57 **/public class XiaomiFactory implements IProductFactory &#123; @Override public IPhoneProduct iPhoneProduct() &#123; return new XiaomiPhone(); &#125; @Override public IRouterProduct iRouterProduct() &#123; return new XiaomiRouter(); &#125;&#125; 华为工厂 1234567891011121314151617181920package com.marlowe.factory.abstract1;/** * @program: GoF23 * @description: 华为工厂 * @author: Marlowe * @create: 2020-11-21 10:57 **/public class HuaweiFactory implements IProductFactory &#123; @Override public IPhoneProduct iPhoneProduct() &#123; return new HuaweiPhone(); &#125; @Override public IRouterProduct iRouterProduct() &#123; return new HuaweiRouter(); &#125;&#125; 手机产品接口 123456789101112131415161718192021222324252627282930package com.marlowe.factory.abstract1;/** * @program: GoF23 * @description: 手机产品接口 * @author: Marlowe * @create: 2020-11-20 22:38 **/public interface IPhoneProduct &#123; /** * 开机 */ void start(); /** * 关机 */ void shutdown(); /** * 打电话 */ void call(); /** * 发信息 */ void sendMessage();&#125; 以下小米手机和华为手机分别实现手机产品接口 1234567891011121314151617181920212223242526272829303132package com.marlowe.factory.abstract1;/** * @program: GoF23 * @description: 小米手机 * @author: Marlowe * @create: 2020-11-20 22:41 **/public class XiaomiPhone implements IPhoneProduct &#123; @Override public void start() &#123; System.out.println(&quot;打开小米手机&quot;); &#125; @Override public void shutdown() &#123; System.out.println(&quot;关闭小米手机&quot;); &#125; @Override public void call() &#123; System.out.println(&quot;小米手机打电话&quot;); &#125; @Override public void sendMessage() &#123; System.out.println(&quot;小米手机发信息&quot;); &#125;&#125; 123456789101112131415161718192021222324252627282930package com.marlowe.factory.abstract1;/** * @program: GoF23 * @description: 华为手机 * @author: Marlowe * @create: 2020-11-20 22:44 **/public class HuaweiPhone implements IPhoneProduct &#123; @Override public void start() &#123; System.out.println(&quot;打开华为手机&quot;); &#125; @Override public void shutdown() &#123; System.out.println(&quot;关闭华为手机&quot;); &#125; @Override public void call() &#123; System.out.println(&quot;华为手机打电话&quot;); &#125; @Override public void sendMessage() &#123; System.out.println(&quot;华为手机发信息&quot;); &#125;&#125; 路由器产品接口 123456789101112131415161718192021222324252627282930package com.marlowe.factory.abstract1;/** * @program: GoF23 * @description: 路由器产品接口 * @author: Marlowe * @create: 2020-11-20 22:40 **/public interface IRouterProduct &#123; /** * 开机 */ void start(); /** * 关机 */ void shutdown(); /** * 打开wifi */ void openWifi(); /** * 设置 */ void setting();&#125; 以下小米路由器和华为路由器分别实现路由器产品接口 12345678910111213141516171819202122232425262728293031package com.marlowe.factory.abstract1;/** * @program: GoF23 * @description: 小米路由器 * @author: Marlowe * @create: 2020-11-20 22:46 **/public class XiaomiRouter implements IRouterProduct &#123; @Override public void start() &#123; System.out.println(&quot;打开小米路由器&quot;); &#125; @Override public void shutdown() &#123; System.out.println(&quot;关闭小米路由器&quot;); &#125; @Override public void openWifi() &#123; System.out.println(&quot;打开小米路由器wifi&quot;); &#125; @Override public void setting() &#123; System.out.println(&quot;设置小米路由器&quot;); &#125;&#125; 12345678910111213141516171819202122232425262728293031package com.marlowe.factory.abstract1;/** * @program: GoF23 * @description: 华为路由器 * @author: Marlowe * @create: 2020-11-20 22:46 **/public class HuaweiRouter implements IRouterProduct &#123; @Override public void start() &#123; System.out.println(&quot;打开华为路由器&quot;); &#125; @Override public void shutdown() &#123; System.out.println(&quot;关闭华为路由器&quot;); &#125; @Override public void openWifi() &#123; System.out.println(&quot;打开华为路由器wifi&quot;); &#125; @Override public void setting() &#123; System.out.println(&quot;设置华为路由器&quot;); &#125;&#125; 主类 12345678910111213141516171819202122232425262728293031package com.marlowe.factory.abstract1;/** * @program: GoF23 * @description: 客户端 * @author: Marlowe * @create: 2020-11-21 11:01 **/public class Client &#123; public static void main(String[] args) &#123; System.out.println(&quot;===============小米系列产品=================&quot;); XiaomiFactory xiaomiFactory = new XiaomiFactory(); IPhoneProduct iPhoneProduct = xiaomiFactory.iPhoneProduct(); iPhoneProduct.call(); iPhoneProduct.sendMessage(); IRouterProduct iRouterProduct = xiaomiFactory.iRouterProduct(); iRouterProduct.openWifi(); System.out.println(); System.out.println(&quot;===============华为系列产品=================&quot;); HuaweiFactory huaweiFactory = new HuaweiFactory(); iPhoneProduct = huaweiFactory.iPhoneProduct(); iPhoneProduct.call(); iPhoneProduct.sendMessage(); IRouterProduct iRouterProduct1 = huaweiFactory.iRouterProduct(); iRouterProduct1.openWifi(); &#125;&#125; 12345678910结果：===============小米系列产品=================小米手机打电话小米手机发信息打开小米路由器wifi===============华为系列产品=================华为手机打电话华为手机发信息打开华为路由器wifi 适用场景： 客户端（应用层）不依赖与产品类实例如何被创建、实现等细节 强调一系列相关的产品对象（属于同一产品族）一起使用创建对象需要大量重复代码 提供一个产品类的库，所有的产品以同样的接口出现，从而使得客户端不依赖于具体的实现 优点 具体产品在应用层的代码隔离，无需关心创建的细节 将一个系列的产品统一到一起管理 缺点 规定了所有可能被创建的产品集合，产品族中扩展新的产品困难 增加了系统的抽象性和理解难度 小结 简单工厂模式（静态工厂模式）虽然某种程度上不符合设计原则，但实际使用最多！ 工厂方法模式不修改已有类的前提下，通过新增新的工厂类实现扩展。 抽象工厂模式不可以增加产品，可以增加产品族！","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"创建型模式","slug":"创建型模式","permalink":"https://xmmarlowe.github.io/tags/%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F/"},{"name":"工厂","slug":"工厂","permalink":"https://xmmarlowe.github.io/tags/%E5%B7%A5%E5%8E%82/"}],"author":"Marlowe"},{"title":"设计模式-模板","slug":"设计模式/设计模式-模板","date":"2020-11-20T07:06:57.000Z","updated":"2020-12-05T04:44:01.103Z","comments":true,"path":"2020/11/20/设计模式/设计模式-模板/","link":"","permalink":"https://xmmarlowe.github.io/2020/11/20/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E6%A8%A1%E6%9D%BF/","excerpt":"一个抽象类公开定义了执行它的方法的方式/模板。它的子类可以按需要重写方法实现，但调用将以抽象类中定义的方式进行。这种类型的设计模式属于行为型模式。","text":"一个抽象类公开定义了执行它的方法的方式/模板。它的子类可以按需要重写方法实现，但调用将以抽象类中定义的方式进行。这种类型的设计模式属于行为型模式。 介绍在模板模式（Template Pattern）中，一个抽象类公开定义了执行它的方法的方式/模板。它的子类可以按需要重写方法实现，但调用将以抽象类中定义的方式进行。这种类型的设计模式属于行为型模式。 主要解决了一些方法通用，却在每一个子类都重新写了这一方法。 使用场景： 有多个子类共有的方法，且逻辑相同。 重要的、复杂的方法，可以考虑为模板方法。 优缺点及注意优点 封装不变部分，扩展可变部分。 提取公共代码，便于维护。 行为由父类控制，子类实现。 缺点每一个不同的实现都需要一个子类来实现，导致类的个数增加，使得系统更加庞大。 注意事项为防止恶意操作，一般模板方法都加上 final 关键词。 实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.marlowe;/** * @program: GoF23 * @description: 模板方法模式 * @author: Marlowe * @create: 2020-11-21 16:11 **/public class TemplateMethodPattern &#123; public static void main(String[] args) &#123; Cooking cooking = new CookingFood(); cooking.cook(); &#125;&#125;/** * 做饭抽象类 */abstract class Cooking &#123; protected abstract void step1(); protected abstract void step2(); /** * 模板方法 */ public final void cook() &#123; System.out.println(&quot;开始做饭:&quot;); step1(); step2(); System.out.println(&quot;做饭结束:&quot;); &#125;&#125;/** * 抽象类的具体实现 */class CookingFood extends Cooking &#123; @Override protected void step1() &#123; System.out.println(&quot;放鸡蛋和西红柿&quot;); &#125; @Override protected void step2() &#123; System.out.println(&quot;少放盐多放味精&quot;); &#125;&#125; 12345结果：开始做饭:放鸡蛋和西红柿少放盐多放味精做饭结束:","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"模板","slug":"模板","permalink":"https://xmmarlowe.github.io/tags/%E6%A8%A1%E6%9D%BF/"},{"name":"行为型模式","slug":"行为型模式","permalink":"https://xmmarlowe.github.io/tags/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F/"}],"author":"Marlowe"},{"title":"对链表进行插入排序","slug":"题解/对链表进行插入排序","date":"2020-11-19T16:37:34.000Z","updated":"2020-12-05T04:44:01.019Z","comments":true,"path":"2020/11/20/题解/对链表进行插入排序/","link":"","permalink":"https://xmmarlowe.github.io/2020/11/20/%E9%A2%98%E8%A7%A3/%E5%AF%B9%E9%93%BE%E8%A1%A8%E8%BF%9B%E8%A1%8C%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/","excerpt":"","text":"147. 对链表进行插入排序对链表进行插入排序。 插入排序的动画演示如上。从第一个元素开始，该链表可以被认为已经部分排序（用黑色表示）。每次迭代时，从输入数据中移除一个元素（用红色表示），并原地将其插入到已排好序的链表中。 插入排序算法： 插入排序是迭代的，每次只移动一个元素，直到所有元素可以形成一个有序的输出列表。 每次迭代中，插入排序只从输入数据中移除一个待排序的元素，找到它在序列中适当的位置，并将其插入。 重复直到所有输入数据插入完为止。 示例 1： 12输入: 4-&gt;2-&gt;1-&gt;3输出: 1-&gt;2-&gt;3-&gt;4 示例 2： 12输入: -1-&gt;5-&gt;3-&gt;4-&gt;0输出: -1-&gt;0-&gt;3-&gt;4-&gt;5 分析由gif可以看出，链表在插入排序过程中由排序好的部分和当前节点以及后面的节点组成，因此可以去排序好部分的下一个元素作为当前待排序节点，当当前节点为null时，所有节点排序结束。 具体步骤如下：12345678910111213141516171819202122232425262728293031323334353637383940414243/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode insertionSortList(ListNode head) &#123; // 如果头节点为空，直接返回 if(head == null)&#123; return head; &#125; // 新建哑节点，保存头结点信息 ListNode dummy = new ListNode(0); dummy.next = head; // 排序好部分最后一个元素 ListNode lastSorted = head; // 当前节点（待排序元素） ListNode curr = head.next; while(curr != null)&#123; // 如果当前元素不用排序，将排序链表增长，也即lastSorted后移 if(lastSorted.val &lt;= curr.val)&#123; lastSorted = lastSorted.next; &#125;else&#123; // 从头结点开始找，pre保存前一个元素 ListNode pre = dummy; while(pre.next.val &lt;= curr.val)&#123; pre = pre.next; &#125; // 将curr节点插入到对应位置 lastSorted.next = curr.next; curr.next = pre.next; pre.next = curr; &#125; // 更新当前节点为排序好链表下一个节点 curr = lastSorted.next; &#125; return dummy.next; &#125;&#125;","categories":[{"name":"LeetCode题解","slug":"LeetCode题解","permalink":"https://xmmarlowe.github.io/categories/LeetCode%E9%A2%98%E8%A7%A3/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xmmarlowe.github.io/tags/java/"},{"name":"链表","slug":"链表","permalink":"https://xmmarlowe.github.io/tags/%E9%93%BE%E8%A1%A8/"},{"name":"插入排序","slug":"插入排序","permalink":"https://xmmarlowe.github.io/tags/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/"}],"author":"Marlowe"},{"title":"设计模式-外观","slug":"设计模式/设计模式-外观","date":"2020-11-18T14:05:41.000Z","updated":"2020-12-05T04:44:01.066Z","comments":true,"path":"2020/11/18/设计模式/设计模式-外观/","link":"","permalink":"https://xmmarlowe.github.io/2020/11/18/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%A4%96%E8%A7%82/","excerpt":"要求一个子系统的外部与其内部的通信必须通过一个统一的对象进行。外观模式提供一个高层次的接口，使得子系统更易使用。","text":"要求一个子系统的外部与其内部的通信必须通过一个统一的对象进行。外观模式提供一个高层次的接口，使得子系统更易使用。 介绍外观模式（Facade Pattern）隐藏系统的复杂性，并向客户端提供了一个客户端可以访问系统的接口。这种类型的设计模式属于结构型模式，它向现有的系统添加一个接口，来隐藏系统的复杂性。 这种模式涉及到一个单一的类，该类提供了客户端请求的简化方法和对现有系统类方法的委托调用。传统模式 外观模式 优缺点及注意优点 为复杂的模块或子系统提供外界访问的模块。 子系统相对独立。 预防低水平人员带来的风险。 缺点 不符合开闭原则。所谓的开闭原则是软件工程里面一个最基本的原则：对扩展开放，对修改关闭。换句话说，你的系统可以提供新的功能模块而不必进行修改。 注意事项在层次化结构中，可以使用外观模式定义系统中每一层的入口。 实现12345678910111213141516171819202122232425262728293031323334353637383940414243package com.marlowe;/** * @program: GoF23 * @description: 外观模式 * @author: Marlowe * @create: 2020-11-21 17:04 **/public class FacadePattern &#123; public static void main(String[] args) &#123; Facade facade = new Facade(); System.out.println(facade.prove()); &#125;&#125;class SubFlow1 &#123; boolean isTrue() &#123; return true; &#125;&#125;class SubFlow2 &#123; boolean isOk() &#123; return true; &#125;&#125;class SubFlow3 &#123; boolean isGoodMan() &#123; return true; &#125;&#125;class Facade &#123; SubFlow1 subFlow1 = new SubFlow1(); SubFlow2 subFlow2 = new SubFlow2(); SubFlow3 subFlow3 = new SubFlow3(); boolean prove() &#123; return subFlow1.isTrue() &amp;&amp; subFlow2.isOk() &amp;&amp; subFlow3.isGoodMan(); &#125;&#125; 12结果：true","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"结构型模式","slug":"结构型模式","permalink":"https://xmmarlowe.github.io/tags/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F/"},{"name":"外观","slug":"外观","permalink":"https://xmmarlowe.github.io/tags/%E5%A4%96%E8%A7%82/"}],"author":"Marlowe"},{"title":"DockerFile","slug":"Docker/DockerFile","date":"2020-11-16T17:06:17.000Z","updated":"2021-07-21T13:46:54.783Z","comments":true,"path":"2020/11/17/Docker/DockerFile/","link":"","permalink":"https://xmmarlowe.github.io/2020/11/17/Docker/DockerFile/","excerpt":"","text":"dockerfile 是用来构建docker镜像的文件！","categories":[{"name":"Docker","slug":"Docker","permalink":"https://xmmarlowe.github.io/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://xmmarlowe.github.io/tags/Docker/"}],"author":"Marlowe"},{"title":"容器数据卷","slug":"Docker/容器数据卷","date":"2020-11-16T12:10:54.000Z","updated":"2021-07-21T13:46:42.222Z","comments":true,"path":"2020/11/16/Docker/容器数据卷/","link":"","permalink":"https://xmmarlowe.github.io/2020/11/16/Docker/%E5%AE%B9%E5%99%A8%E6%95%B0%E6%8D%AE%E5%8D%B7/","excerpt":"","text":"什么是容器数据卷docker的理念回顾将应用和环境打包成一个镜像！数据？如果数据都在容器中，那么容器一删除，数据就会丢失！==需求：数据可以持久化==MySQL，容器删了，删库跑路！==需求：MySQL数据可以存储在本地！==容器之间可以有一个数据共享的技术！Docker容器中产生的数据，同步到本地！这就是卷技术！目录的挂载，将我们容器内的目录，挂载到Linux上面！总结一句话：容器的持久化和同步操作！容器建也是可以数据共享的！ 使用数据卷 方式一：直接使用命令来挂载 -v 123456docker run -it -v 主机目录：容器内目录# 测试[root@hecs-x-large-2-linux-20200425095544 home]# docker run -it -v /home/ceshi:/home centos /bin/bash# 启动起来的时候我们可以通过docker inspect 容器id 测试：1、停止容器2、宿主机上修改文件3、启动容器4、容器内的数据依旧是同步的！好处：我们以后修改只需要在本地修改即可，容器内会自动同步！ 实战：安装MySQL思考：MySQL的数据持久化问题！ 123456789101112131415161718# 获取镜像[root@hecs-x-large-2-linux-20200425095544 home]# docker pull mysql:5.7# 运行容器，需要做数据挂载！ # 安装启动mysql，需要配置密码的，这是注意点# 官当测试： docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag# 启动我们的-d 后台运行-p 端口映射-v 卷挂载-e 环境配置--name 容器名字[root@hecs-x-large-2-linux-20200425095544 home]# docker run -d -p 3310:3306 -v /home/mysql/conf:/etc/mysql/conf.d -v /home/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=root --name mysql01 mysql:5.7# 启动成功之后，我们在本地使用navcat来连接测试一下# navcat-连接到服务器的3310 ----3310和容器内的3306映射，这个时候我们就可以连接上了！# 在本地测试创建一个数据库，查看一下我们的映射路径是否ok！ 加入我们将容器删除发现，我们挂载到本地的数据卷依旧没有丢失，这就实现了容器数据持久化功能！ 具名和匿名挂载12345678910111213141516171819# 匿名挂载-v 容器内路径！docker run -d -P --name nginx01 -v /etc/nginx nginx# 查看所有的 volume 的情况[root@hecs-x-large-2-linux-20200425095544 ~]# docker volume lsDRIVER VOLUME NAMElocal 7be1d9b8c43e3b6bedc76ab75894eb8b8a8423e83ef2c4e9cf8b4a22ee4d9f2b# 这里发现，这种就是匿名挂载，我们在 -v 只写了容器内路径，没有写容器外路径！# 具名挂载[root@hecs-x-large-2-linux-20200425095544 ~]# docker run -d -P --name nginx03 -v juming-nginx:/etc/nginx nginx86efd65c8724a4485ae7bb75b75ec8ed62a225cb33d0c75ed1b6b3652500f5e9[root@hecs-x-large-2-linux-20200425095544 ~]# docker volume lslocal juming-nginx# 通过 -v 卷名：容器内路径# 查看一下这个卷 所有docker容器内的卷，没有指定目录的情况下都是在/var/lib/docker/volume/xxx/_data我们通过具名挂载可以方便的找到我们的一个卷，大多数情况在使用的具名挂载 1234如何确定是具名挂载还是匿名挂载，还是指定路径挂载！-v 容器内路径 # 匿名挂载-v 卷名：容器内路径 # 具名挂载-v /宿主机路径:容器内路径 # 指定路径挂载！ 拓展： 123456789# 通过 -v 容器内路径：ro rw 改变读写权限ro readonly # 只读rw readwrite # 可读可写# 一旦这个设置了容器权限。容器对我们挂载出来的内容就限定了！docker run -d -P --name nginx03 -v juming-nginx:/etc/nginx:/etc/nginx:ro nginxdocker run -d -P --name nginx03 -v juming-nginx:/etc/nginx:/etc/nginx:rw nginx# ro 只要看到ro就说明这个路径只能通过宿主机来操作，容器内部是无法操作！ 初识DockerfileDockerfile就是用来构建docker镜像的构建文件！ 命令脚本！先体验一下！通过这个脚本可以生成镜像，镜像是一层一层的，脚本一个个的命令，每个命令都是一层！ 1234567891011# 创建一个dockerfile文件，名字可以随机 建议Dockerfile# 文件中的内容 指令(大写) 参数FROM centosVOLUME [&quot;volume01&quot;,&quot;volume02&quot;]CMD echo &quot;---end---&quot;CMD /bin/bash# 这里的每个命令，就是镜像的一层！ 1# 启动自己写的容器 这个卷和外部一定有一个同步的目录！ 这种方式我们未来使用的十分多，因为我们通常会构建自己的镜像！假设构建镜像时候没有挂载卷，要手动镜像挂载 -v 卷名:容器内路径！ 数据卷容器多个mysql同步数据！ 123# 测试：可以删除docker01，查看一下docker02和docker03是否还可以访问这个文件# 测试依旧可以访问(拷贝的概念) 多个mysql实现数据共享 12345[root@hecs-x-large-2-linux-20200425095544 home]# docker run -d -p 3310:3306 -v /etc/mysql/conf.d -v /var/lib/mysql -e MYSQL_ROOT_PASSWORD=root --name mysql01 mysql:5.7[root@hecs-x-large-2-linux-20200425095544 home]# docker run -d -p 3310:3306 -e MYSQL_ROOT_PASSWORD=root --name mysql02 --volumes-form mysql01 mysql:5.7# 这个时候，可以实现两个容器数据同步！ 结论：容器之间配置信息的传递，数据卷容器的生命周期一直持续到没有容器使用为止。但是一旦持久到了本地，这个时候，本地的数据是不会删除的！","categories":[{"name":"Docker","slug":"Docker","permalink":"https://xmmarlowe.github.io/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://xmmarlowe.github.io/tags/Docker/"}],"author":"Marlowe"},{"title":"Docker镜像讲解","slug":"Docker/Docker镜像讲解","date":"2020-11-16T11:46:45.000Z","updated":"2021-07-21T13:46:51.847Z","comments":true,"path":"2020/11/16/Docker/Docker镜像讲解/","link":"","permalink":"https://xmmarlowe.github.io/2020/11/16/Docker/Docker%E9%95%9C%E5%83%8F%E8%AE%B2%E8%A7%A3/","excerpt":"","text":"如何提交一个自己的镜像 commit镜像1234docker commit 提交容器成为一个新的副本# 命令和git原理类似docker commit -m=&quot;提交的描述信息&quot; -a=&quot;作者&quot; 容器id 目标镜像名：[TAG] 实战测试 1234567891011# 1、启动一个默认的tomcat[root@hecs-x-large-2-linux-20200425095544 ~]# docker run -it -p 8080:8080 tomcat# 2、发现这个默认的tomcat是没有webapps应用，镜像的原因，官方的镜像默认webapps下面是没有文件的！# 3、我自己拷贝进去了基本的文件root@186285ef065e:/usr/local/tomcat# cp -r webapps.dist/* webapps# 4、将我们操作过的容器通过commit提交为一个镜像！我们以后就使用我们修改过的镜像即可，这就是我们自己的一个修改的镜像[root@hecs-x-large-2-linux-20200425095544 ~]# docker commit -a=&quot;marlowe&quot; -m=&quot;add web app&quot; 186285ef065e tomcat02:1.0 1如果你想要保存当前容器的状态，就可以通过commit来提交，获得一个镜像，就好比以前学习VM的时候，快照！ 到这里才算是入门Docker！","categories":[{"name":"Docker","slug":"Docker","permalink":"https://xmmarlowe.github.io/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://xmmarlowe.github.io/tags/Docker/"}],"author":"Marlowe"},{"title":"Docker常用命令","slug":"Docker/Docker常用命令","date":"2020-11-11T08:16:31.000Z","updated":"2021-07-21T13:46:44.954Z","comments":true,"path":"2020/11/11/Docker/Docker常用命令/","link":"","permalink":"https://xmmarlowe.github.io/2020/11/11/Docker/Docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","excerpt":"","text":"帮助命令123docker version # 显示docker的版本信息docker info # 显示docker的系统信息，包括镜像和容器的数量docker 命令 --help # 帮助命令 帮助文档地址：https://docs.docker.com/reference/ 镜像命令docker images 查看所有本地的主机上的镜像 123456789[root@hecs-x-large-2-linux-20200425095544 home]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEmysql 8.0 db2b37ec6181 2 weeks ago 545MBmysql latest db2b37ec6181 2 weeks ago 545MBhello-world latest bf756fb1ae65 10 months ago 13.3kB# 可选项 -a，--all # 列出所有的镜像 -q，--quiet # 只显示镜像的id docker search 搜索镜像 12345678910[root@hecs-x-large-2-linux-20200425095544 home]# docker search mysqlNAME DESCRIPTION STARS OFFICIAL mysql MySQL is a widely used, open-source relation… 10148 [OK] # 可选项，通过收藏来过滤--filter=STARS=3000 # 搜索出来的镜像就是STARS大于3000的[root@hecs-x-large-2-linux-20200425095544 home]# docker search mysql --filter=stars=3000NAME DESCRIPTION STARS OFFICIAL AUTOMATEDmysql MySQL is a widely used, open-source relation… 10148 [OK] mariadb MariaDB is a community-developed fork of MyS… 3737 [OK] docker pull 下载镜像 12345678910111213141516171819202122# 下载镜像 docker pull 镜像名[:tag][root@hecs-x-large-2-linux-20200425095544 ~]# docker pull mysql:8.08.0: Pulling from library/mysql # 如果不写tag，默认就是latestbb79b6b2107f: Pull complete 49e22f6fb9f7: Pull complete 842b1255668c: Pull complete 9f48d1f43000: Pull complete c693f0615bce: Pull complete 8a621b9dbed2: Pull complete 0807d32aef13: Pull complete a56aca0feb17: Pull complete de9d45fd0f07: Pull complete 1d68a49161cc: Pull complete d16d318b774e: Pull complete 49e112c55976: Pull complete Digest: sha256:8c17271df53ee3b843d6e16d46cff13f22c9c04d6982eb15a9a47bd5c9ac7e2d # 签名Status: Downloaded newer image for mysql:8.0docker.io/library/mysql:8.0 # 真实地址# 等价于它docker pull mysqldocker pull docker.io/library/mysql:8.0 docker rmi 删除镜像 123[root@hecs-x-large-2-linux-20200425095544 ~]# docker rmi -f 镜像id # 删除指定的镜像[root@hecs-x-large-2-linux-20200425095544 ~]# docker rmi -f 镜像id 镜像id # 删除多个镜像[root@hecs-x-large-2-linux-20200425095544 ~]# docker rmi -f $(docker images -aq) # 删除全部的镜像 容器命令说明：我们有了镜像才可以创建容器，linux，下载一个centos镜像来测试学习 1docker pull centos 新建容器并启动 123456789101112131415161718192021222324docker run[可选参数] image# 参数说明--name=&quot;Name&quot; 容器名字 tomcat01 tomcat02 用来区分容器-d 后台方式运行-it 使用交互方式运行，进入容器查看内容-p 指定容器的端口 -p 8080:8080 -p ip:主机端口：容器端口 -p 主机端口：容器端口（常用） -p 容器端口 容器端口-P 随机指定端口# 测试，启动并进入容器[root@hecs-x-large-2-linux-20200425095544 ~]# docker run -it centos /bin/bash[root@9b4676b718b5 /]# ls # 查看容器内的centos，基础版本，很多命令都是不完善的！bin etc lib lost+found mnt proc run srv tmp vardev home lib64 media opt root sbin sys usr# 从容器中退回主机[root@9b4676b718b5 /]# exitexit[root@hecs-x-large-2-linux-20200425095544 ~]# lsinstall.sh 列出所有运行的容器 123456789101112# docker ps 命令 # 列出当前正在运行的容器-a # 列出当前正在运行的容器 + 带出历史运行过的容器-n=? # 显示最近创建的容器-q # 只显示容器的编号[root@hecs-x-large-2-linux-20200425095544 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES[root@hecs-x-large-2-linux-20200425095544 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES9b4676b718b5 centos &quot;/bin/bash&quot; 26 minutes ago Exited (0) 16 minutes ago festive_feistelc8c1137aaa4e bf756fb1ae65 &quot;/hello&quot; 5 hours ago Exited (0) 5 hours ago confident_cannon 退出容器 12345exit # 容器停止并退出Ctrl + P + Q # 容器不停止退出[root@hecs-x-large-2-linux-20200425095544 ~]# docker run -it centos /bin/bash[root@9d6ac1f17089 /]# [root@hecs-x-large-2-linux-20200425095544 ~]# 删除容器 123docker rm 容器id # 删除指定的容器，不能删除正在运行的容器，如果要强制删除 rm -f docker rm -f $(docker ps -aq) # 删除所有的容器docker ps -a -q|xargs docker rm # 删除所有的容器，使用管道 启动和容器的操作 1234docker start 容器id # 启动容器docker restart 容器id # 重启容器docker stop 容器id # 停止当前正在运行的容器docker kill 容器id # 强制停止当前容器 常用其他命令 后台启动容器 1234567# 命令 docker run -d 镜像名：[root@hecs-x-large-2-linux-20200425095544 ~]# docker run -d centos# 问题docker ps，发现 centos停止了# 常见的坑！！ docker容器使用后台运行，就唏嘘有一个前台进程，docker发现没有应用，就自动停止# nginx，容器启动后，发现自己没有提供服务，就会立刻停止，就是没有程序了 查看日志 12345678910111213 docker logs -f -t --tail 容器id ,没有日志 # 自己编写一段shell脚本 [root@hecs-x-large-2-linux-20200425095544 ~]# docker run -d centos /bin/bash -c &quot;while true;do echo kuangshen; sleep 1;done&quot;[root@hecs-x-large-2-linux-20200425095544 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESaadc743a101c centos &quot;/bin/bash -c &#x27;while…&quot; 4 seconds ago Up 3 seconds tender_moser # 显示日志 -tf # 显示日志 -tail number # 要显示日志条数 [root@hecs-x-large-2-linux-20200425095544 ~]# docker logs -tf --tail 10 284eaba4616b 查看容器中进程信息 ps 12345# 命令 docker top 容器id [root@hecs-x-large-2-linux-20200425095544 ~]# docker top 284eaba4616bUID PID PPID C STIME TTY TIME CMDroot 15211 15194 0 18:31 ? 00:00:00 /bin/bash -c while true;do echo kuangshen; sleep 1;doneroot 15918 15211 0 18:37 ? 00:00:00 /usr/bin/coreutils --coreutils-prog-shebang=sleep /usr/bin/sleep 1 查看镜像的元数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211# 命令docker inspect 容器id# 测试[root@hecs-x-large-2-linux-20200425095544 ~]# docker inspect 284eaba4616b[ &#123; &quot;Id&quot;: &quot;284eaba4616b4e748dc87a6aedf14d3b7bb508ef28d77cf215f01677a9149ae6&quot;, &quot;Created&quot;: &quot;2020-11-12T10:31:46.264703694Z&quot;, &quot;Path&quot;: &quot;/bin/bash&quot;, &quot;Args&quot;: [ &quot;-c&quot;, &quot;while true;do echo kuangshen; sleep 1;done&quot; ], &quot;State&quot;: &#123; &quot;Status&quot;: &quot;running&quot;, &quot;Running&quot;: true, &quot;Paused&quot;: false, &quot;Restarting&quot;: false, &quot;OOMKilled&quot;: false, &quot;Dead&quot;: false, &quot;Pid&quot;: 15211, &quot;ExitCode&quot;: 0, &quot;Error&quot;: &quot;&quot;, &quot;StartedAt&quot;: &quot;2020-11-12T10:31:46.559658378Z&quot;, &quot;FinishedAt&quot;: &quot;0001-01-01T00:00:00Z&quot; &#125;, &quot;Image&quot;: &quot;sha256:0d120b6ccaa8c5e149176798b3501d4dd1885f961922497cd0abef155c869566&quot;, &quot;ResolvConfPath&quot;: &quot;/var/lib/docker/containers/284eaba4616b4e748dc87a6aedf14d3b7bb508ef28d77cf215f01677a9149ae6/resolv.conf&quot;, &quot;HostnamePath&quot;: &quot;/var/lib/docker/containers/284eaba4616b4e748dc87a6aedf14d3b7bb508ef28d77cf215f01677a9149ae6/hostname&quot;, &quot;HostsPath&quot;: &quot;/var/lib/docker/containers/284eaba4616b4e748dc87a6aedf14d3b7bb508ef28d77cf215f01677a9149ae6/hosts&quot;, &quot;LogPath&quot;: &quot;/var/lib/docker/containers/284eaba4616b4e748dc87a6aedf14d3b7bb508ef28d77cf215f01677a9149ae6/284eaba4616b4e748dc87a6aedf14d3b7bb508ef28d77cf215f01677a9149ae6-json.log&quot;, &quot;Name&quot;: &quot;/dazzling_roentgen&quot;, &quot;RestartCount&quot;: 0, &quot;Driver&quot;: &quot;overlay2&quot;, &quot;Platform&quot;: &quot;linux&quot;, &quot;MountLabel&quot;: &quot;&quot;, &quot;ProcessLabel&quot;: &quot;&quot;, &quot;AppArmorProfile&quot;: &quot;&quot;, &quot;ExecIDs&quot;: null, &quot;HostConfig&quot;: &#123; &quot;Binds&quot;: null, &quot;ContainerIDFile&quot;: &quot;&quot;, &quot;LogConfig&quot;: &#123; &quot;Type&quot;: &quot;json-file&quot;, &quot;Config&quot;: &#123;&#125; &#125;, &quot;NetworkMode&quot;: &quot;default&quot;, &quot;PortBindings&quot;: &#123;&#125;, &quot;RestartPolicy&quot;: &#123; &quot;Name&quot;: &quot;no&quot;, &quot;MaximumRetryCount&quot;: 0 &#125;, &quot;AutoRemove&quot;: false, &quot;VolumeDriver&quot;: &quot;&quot;, &quot;VolumesFrom&quot;: null, &quot;CapAdd&quot;: null, &quot;CapDrop&quot;: null, &quot;Capabilities&quot;: null, &quot;Dns&quot;: [], &quot;DnsOptions&quot;: [], &quot;DnsSearch&quot;: [], &quot;ExtraHosts&quot;: null, &quot;GroupAdd&quot;: null, &quot;IpcMode&quot;: &quot;private&quot;, &quot;Cgroup&quot;: &quot;&quot;, &quot;Links&quot;: null, &quot;OomScoreAdj&quot;: 0, &quot;PidMode&quot;: &quot;&quot;, &quot;Privileged&quot;: false, &quot;PublishAllPorts&quot;: false, &quot;ReadonlyRootfs&quot;: false, &quot;SecurityOpt&quot;: null, &quot;UTSMode&quot;: &quot;&quot;, &quot;UsernsMode&quot;: &quot;&quot;, &quot;ShmSize&quot;: 67108864, &quot;Runtime&quot;: &quot;runc&quot;, &quot;ConsoleSize&quot;: [ 0, 0 ], &quot;Isolation&quot;: &quot;&quot;, &quot;CpuShares&quot;: 0, &quot;Memory&quot;: 0, &quot;NanoCpus&quot;: 0, &quot;CgroupParent&quot;: &quot;&quot;, &quot;BlkioWeight&quot;: 0, &quot;BlkioWeightDevice&quot;: [], &quot;BlkioDeviceReadBps&quot;: null, &quot;BlkioDeviceWriteBps&quot;: null, &quot;BlkioDeviceReadIOps&quot;: null, &quot;BlkioDeviceWriteIOps&quot;: null, &quot;CpuPeriod&quot;: 0, &quot;CpuQuota&quot;: 0, &quot;CpuRealtimePeriod&quot;: 0, &quot;CpuRealtimeRuntime&quot;: 0, &quot;CpusetCpus&quot;: &quot;&quot;, &quot;CpusetMems&quot;: &quot;&quot;, &quot;Devices&quot;: [], &quot;DeviceCgroupRules&quot;: null, &quot;DeviceRequests&quot;: null, &quot;KernelMemory&quot;: 0, &quot;KernelMemoryTCP&quot;: 0, &quot;MemoryReservation&quot;: 0, &quot;MemorySwap&quot;: 0, &quot;MemorySwappiness&quot;: null, &quot;OomKillDisable&quot;: false, &quot;PidsLimit&quot;: null, &quot;Ulimits&quot;: null, &quot;CpuCount&quot;: 0, &quot;CpuPercent&quot;: 0, &quot;IOMaximumIOps&quot;: 0, &quot;IOMaximumBandwidth&quot;: 0, &quot;MaskedPaths&quot;: [ &quot;/proc/asound&quot;, &quot;/proc/acpi&quot;, &quot;/proc/kcore&quot;, &quot;/proc/keys&quot;, &quot;/proc/latency_stats&quot;, &quot;/proc/timer_list&quot;, &quot;/proc/timer_stats&quot;, &quot;/proc/sched_debug&quot;, &quot;/proc/scsi&quot;, &quot;/sys/firmware&quot; ], &quot;ReadonlyPaths&quot;: [ &quot;/proc/bus&quot;, &quot;/proc/fs&quot;, &quot;/proc/irq&quot;, &quot;/proc/sys&quot;, &quot;/proc/sysrq-trigger&quot; ] &#125;, &quot;GraphDriver&quot;: &#123; &quot;Data&quot;: &#123; &quot;LowerDir&quot;: &quot;/var/lib/docker/overlay2/9f2dc5029ecc2e3355d547bc678a613bc9fcaf84e23692e9ae8d36b010c2392a-init/diff:/var/lib/docker/overlay2/ab2394ffb62a3a589a4794ed317cdec52ff1b73d6c0025a32b56cfa266fe4d97/diff&quot;, &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/9f2dc5029ecc2e3355d547bc678a613bc9fcaf84e23692e9ae8d36b010c2392a/merged&quot;, &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/9f2dc5029ecc2e3355d547bc678a613bc9fcaf84e23692e9ae8d36b010c2392a/diff&quot;, &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/9f2dc5029ecc2e3355d547bc678a613bc9fcaf84e23692e9ae8d36b010c2392a/work&quot; &#125;, &quot;Name&quot;: &quot;overlay2&quot; &#125;, &quot;Mounts&quot;: [], &quot;Config&quot;: &#123; &quot;Hostname&quot;: &quot;284eaba4616b&quot;, &quot;Domainname&quot;: &quot;&quot;, &quot;User&quot;: &quot;&quot;, &quot;AttachStdin&quot;: false, &quot;AttachStdout&quot;: false, &quot;AttachStderr&quot;: false, &quot;Tty&quot;: false, &quot;OpenStdin&quot;: false, &quot;StdinOnce&quot;: false, &quot;Env&quot;: [ &quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot; ], &quot;Cmd&quot;: [ &quot;/bin/bash&quot;, &quot;-c&quot;, &quot;while true;do echo kuangshen; sleep 1;done&quot; ], &quot;Image&quot;: &quot;centos&quot;, &quot;Volumes&quot;: null, &quot;WorkingDir&quot;: &quot;&quot;, &quot;Entrypoint&quot;: null, &quot;OnBuild&quot;: null, &quot;Labels&quot;: &#123; &quot;org.label-schema.build-date&quot;: &quot;20200809&quot;, &quot;org.label-schema.license&quot;: &quot;GPLv2&quot;, &quot;org.label-schema.name&quot;: &quot;CentOS Base Image&quot;, &quot;org.label-schema.schema-version&quot;: &quot;1.0&quot;, &quot;org.label-schema.vendor&quot;: &quot;CentOS&quot; &#125; &#125;, &quot;NetworkSettings&quot;: &#123; &quot;Bridge&quot;: &quot;&quot;, &quot;SandboxID&quot;: &quot;da70a9e57940a409d3f4827907ee892aa3a9a20aa2575fbeffd380cedfc6b03a&quot;, &quot;HairpinMode&quot;: false, &quot;LinkLocalIPv6Address&quot;: &quot;&quot;, &quot;LinkLocalIPv6PrefixLen&quot;: 0, &quot;Ports&quot;: &#123;&#125;, &quot;SandboxKey&quot;: &quot;/var/run/docker/netns/da70a9e57940&quot;, &quot;SecondaryIPAddresses&quot;: null, &quot;SecondaryIPv6Addresses&quot;: null, &quot;EndpointID&quot;: &quot;9e9c2139cd06d068313f00e7e7ec3bf9411a3344792d962fffecd4324d1ff87a&quot;, &quot;Gateway&quot;: &quot;172.17.0.1&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;IPAddress&quot;: &quot;172.17.0.2&quot;, &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:02&quot;, &quot;Networks&quot;: &#123; &quot;bridge&quot;: &#123; &quot;IPAMConfig&quot;: null, &quot;Links&quot;: null, &quot;Aliases&quot;: null, &quot;NetworkID&quot;: &quot;7a8c920abbd19ce06b9315879005e6d73adea85afc13f16ca1bd88c49bf5694b&quot;, &quot;EndpointID&quot;: &quot;9e9c2139cd06d068313f00e7e7ec3bf9411a3344792d962fffecd4324d1ff87a&quot;, &quot;Gateway&quot;: &quot;172.17.0.1&quot;, &quot;IPAddress&quot;: &quot;172.17.0.2&quot;, &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:02&quot;, &quot;DriverOpts&quot;: null &#125; &#125; &#125; &#125;] 进入当前正在运行的容器 12345678910111213141516171819202122# 我们通常容器都是使用后台方式运行的，需要进入容器，修改一些配置# 命令 docker exec -it 容器id bashShell# 测试[root@hecs-x-large-2-linux-20200425095544 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe60fd257e7cf mysql:8.0 &quot;docker-entrypoint.s…&quot; 6 hours ago Up 6 hours 33060/tcp, 0.0.0.0:3307-&gt;3306/tcp mysql[root@hecs-x-large-2-linux-20200425095544 ~]# docker exec -it e60fd257e7cf /bin/bashroot@e60fd257e7cf:/# lsbin docker-entrypoint-initdb.d home media proc sbin tmpboot entrypoint.sh lib mnt root srv usrdev etc lib64 opt run sys varroot@e60fd257e7cf:/# ps -ef# 方式2 docker attach 容器id# docker exec # 进入容器后开启一个新的终端，可以在里面操作（常用）# docker attach # 进入容器正在执行的终端，不会启动新的进程 从容器内拷贝文件到主机上 12345678910111213141516171819202122232425262728293031docker cp 容器id：容器内路径 目的的主机路径# 查看当前主机目录下[root@hecs-x-large-2-linux-20200425095544 home]# lschn hello.java hh leo Marlowe www[root@hecs-x-large-2-linux-20200425095544 home]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES37b64bd24047 centos &quot;/bin/bash&quot; About a minute ago Up About a minute funny_williamse60fd257e7cf mysql:8.0 &quot;docker-entrypoint.s…&quot; 7 hours ago Up 7 hours 33060/tcp, 0.0.0.0:3307-&gt;3306/tcp mysql# 进入docker容器内部[root@hecs-x-large-2-linux-20200425095544 home]# docker attach 37b64bd24047[root@37b64bd24047 /]# cd /home[root@37b64bd24047 home]# ls# 在容器内新建一个文件[root@37b64bd24047 home]# touch test.java[root@37b64bd24047 home]# lstest.java[root@37b64bd24047 home]# exitexit[root@hecs-x-large-2-linux-20200425095544 home]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe60fd257e7cf mysql:8.0 &quot;docker-entrypoint.s…&quot; 7 hours ago Up 7 hours 33060/tcp, 0.0.0.0:3307-&gt;3306/tcp mysql# 将这文件拷贝出来到主机上[root@hecs-x-large-2-linux-20200425095544 home]# docker cp 37b64bd24047:/home/test.java /home [root@hecs-x-large-2-linux-20200425095544 home]# lschn hello.java hh leo Marlowe test.java www# 拷贝是一个手动过程，未来我们使用 -v 卷的技术，可以实现，自动同步 /home /home Docker 安装nginx 123456789101112131415161718192021# 1.搜索镜像 search 建议大家去docker搜素，可以看帮助文档# 2.下载镜像 pull# 3.运行测试[root@hecs-x-large-2-linux-20200425095544 home]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx latest c39a868aad02 7 days ago 133MBmysql 8.0 db2b37ec6181 2 weeks ago 545MBmysql latest db2b37ec6181 2 weeks ago 545MBcentos latest 0d120b6ccaa8 3 months ago 215MB# -d后台运行# --name 给容器命名# -p 宿主机端口，容器内部端口[root@hecs-x-large-2-linux-20200425095544 home]# docker run -d --name nginx01 -p 3344:80 nginx100d4c411f6d16c5ff4e630f521f59448d065cb2b201bd0b3a1ea6840045e955[root@hecs-x-large-2-linux-20200425095544 home]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES100d4c411f6d nginx &quot;/docker-entrypoint.…&quot; 8 seconds ago Up 7 seconds 0.0.0.0:3344-&gt;80/tcp nginx01e60fd257e7cf mysql:8.0 &quot;docker-entrypoint.s…&quot; 7 hours ago Up 7 hours 33060/tcp, 0.0.0.0:3307-&gt;3306/tcp mysql[root@hecs-x-large-2-linux-20200425095544 home]# curl localhost:3344 作业练习 Docker 安装 Nginx 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 1.搜索镜像 search 建议去docker搜索，可以看到帮助文档# 2.下载镜像 pull# 3.运行测试[root@hecs-x-large-2-linux-20200425095544 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx latest c39a868aad02 9 days ago 133MBmysql 8.0 db2b37ec6181 3 weeks ago 545MBmysql latest db2b37ec6181 3 weeks ago 545MBcentos latest 0d120b6ccaa8 3 months ago 215MB# -d 后台运行# --name 容器命名# -p 宿主机端口：容器内端口[root@hecs-x-large-2-linux-20200425095544 ~]# docker run -d --name nginx01 -p 3344:80 nginx[root@hecs-x-large-2-linux-20200425095544 ~]# curl localhost:3344&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;# 进入容器[root@hecs-x-large-2-linux-20200425095544 ~]# docker exec -it nginx01 /bin/bashroot@100d4c411f6d:/# whereis nginxnginx: /usr/sbin/nginx /usr/lib/nginx /etc/nginx /usr/share/nginx 思考问题：我们每次改动nginx配置文件，都需要进入容器内部？十分麻烦，我要是可以在容器外部提供一个映射路径，达到在容器修改文件名，容器内部就可以自动修改？ -v 数据卷！ 作业2：docker装tomcat 123456789101112131415161718# 官方安装docker run -it --rm tomcat:9.0# 我们之前的启动都是后台，停止了容器之后，容器还是可以查到， docker run -it --rm 一般用来测试，用完就删除# 下载再启用docker pull tomcat# 启动运行docker run -d -p 3355:8080 --name tomcat01 tomcat# 测试访问没有问题# 进入容器[root@hecs-x-large-2-linux-20200425095544 ~]# docker exec -it tomcat01 /bin/bash# 发现问题：1.linux命令少了，2.没有webapps，阿里云镜像的原因，默认是最小的镜像，左右不必要的都删除掉。# 保证最小可运行的环境！ 思考问题：我们以后要部署项目，如果每次都要进入容器是不是十分麻烦？我要是可以在容器外提供一个映射路径，webapps，我们在外部放置项目，就自动同步到内部就好了！ 作业：部署es+kibana 123456789101112131415161718192021222324252627282930313233# es 暴露的端口很多# es 十分的耗内存# es的数据一般需要放置到安全目录！ 挂载# --net somenetwork ？ 网络配置# 下载启动docker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; elasticsearch:7.6.2# 启动后非常卡 linux卡住了，docker status 查看cpu状态# es十分耗内存；# 查看 docker stats# 测试es是否成功了[root@hecs-x-large-2-linux-20200425095544 ~]# curl localhost:9200&#123; &quot;name&quot; : &quot;6e4e7e14f10d&quot;, &quot;cluster_name&quot; : &quot;docker-cluster&quot;, &quot;cluster_uuid&quot; : &quot;C4GbFU9pQ7m0WT6ko_pkJA&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;7.6.2&quot;, &quot;build_flavor&quot; : &quot;default&quot;, &quot;build_type&quot; : &quot;docker&quot;, &quot;build_hash&quot; : &quot;ef48eb35cf30adf4db14086e8aabd07ef6fb113f&quot;, &quot;build_date&quot; : &quot;2020-03-26T06:34:37.794943Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;8.4.0&quot;, &quot;minimum_wire_compatibility_version&quot; : &quot;6.8.0&quot;, &quot;minimum_index_compatibility_version&quot; : &quot;6.0.0-beta1&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 12# 增加内存限制，修改配置文件 -e 环境配置修改docker run -d --name elasticsearch02 -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; -e ES_JAVA_OPTS=&quot;-Xms64m -Xmx512m&quot; elasticsearch:7.6.2 作业：使用kibana连接es？思考网络如何才能连接过去！ 可视化 portainer(先用这个)1docker run -d -p 8088:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer Rancher(CI/CD再用) 什么是portainer？Docker图形化界面管理工具！提供一个后台面板供我们操作！ 1docker run -d -p 8088:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer","categories":[{"name":"Docker","slug":"Docker","permalink":"https://xmmarlowe.github.io/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://xmmarlowe.github.io/tags/Docker/"}],"author":"Marlowe"},{"title":"二叉树的前中后序非递归遍历算法","slug":"题解/二叉树的前中后序非递归遍历算法","date":"2020-10-12T14:01:09.000Z","updated":"2020-12-05T04:44:01.014Z","comments":true,"path":"2020/10/12/题解/二叉树的前中后序非递归遍历算法/","link":"","permalink":"https://xmmarlowe.github.io/2020/10/12/%E9%A2%98%E8%A7%A3/%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E5%89%8D%E4%B8%AD%E5%90%8E%E5%BA%8F%E9%9D%9E%E9%80%92%E5%BD%92%E9%81%8D%E5%8E%86%E7%AE%97%E6%B3%95/","excerpt":"","text":"二叉树的前中后序非递归遍历算法学过数据结构的同学都知道二叉树的深度优先遍历算法有三种，前序，中序，后序遍历。 前序：根–&gt;左–&gt;右 中序：左–&gt;根–&gt;右 后序：左–&gt;右–&gt;根 不难发现，后序遍历和前序遍历有相似的地方，如果我们将后序遍历变成根右左的顺序，将结果集翻转后就会变成前序的根左右顺序。 前中后序非递归遍历的核心算法：前序遍历：123456789101112while(root != null || !stack.isEmpty())&#123; // 一直往左边走 while(root != null)&#123; res.add(root.val); stack.push(root); root = root.left; &#125; // 开始回退 TreeNode cur = stack.pop(); // 往右边走 root = cur.right;&#125; 后序遍历：123456789101112131415while(root != null || !stack.isEmpty())&#123; // 一直往右边走 while(root != null)&#123; res.add(root.val); stack.push(root); root = root.right; &#125; // 开始回退 TreeNode cur = stack.pop(); // 往左边走 root = cur.left; // 反转，使变成后序遍历 Collections.reverse(res);&#125; 中序遍历：12345678910111213while(root != null || !stack.isEmpty())&#123; // 碰到根节点，压栈 while(root != null)&#123; stack.push(root); // 往左边走 root = root.left; &#125; // 开始回退 root = stack.pop(); res.add(root.val); // 往右边走 root = root.right;&#125; 前中后序递归遍历的核心算法：前序遍历：1234567public void dfs(TreeNode root)&#123; while(root != null)&#123; res.add(root.val); dfs(root.left); dfs(root.right); &#125;&#125; 中序遍历：1234567public void dfs(TreeNode root)&#123; while(root != null)&#123; dfs(root.left); res.add(root.val); dfs(root.right); &#125;&#125; 后序遍历：1234567public void dfs(TreeNode root)&#123; while(root != null)&#123; dfs(root.left); dfs(root.right); res.add(root.val); &#125;&#125;","categories":[{"name":"LeetCode题解","slug":"LeetCode题解","permalink":"https://xmmarlowe.github.io/categories/LeetCode%E9%A2%98%E8%A7%A3/"}],"tags":[{"name":"二叉树","slug":"二叉树","permalink":"https://xmmarlowe.github.io/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"author":"Marlowe"},{"title":"Map集合的统计每个字符出现次数的两种方法","slug":"题解/Map集合的统计每个字符出现次数的两种方法","date":"2020-10-12T13:53:51.000Z","updated":"2021-06-03T13:38:17.947Z","comments":true,"path":"2020/10/12/题解/Map集合的统计每个字符出现次数的两种方法/","link":"","permalink":"https://xmmarlowe.github.io/2020/10/12/%E9%A2%98%E8%A7%A3/Map%E9%9B%86%E5%90%88%E7%9A%84%E7%BB%9F%E8%AE%A1%E6%AF%8F%E4%B8%AA%E5%AD%97%E7%AC%A6%E5%87%BA%E7%8E%B0%E6%AC%A1%E6%95%B0%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95/","excerpt":"","text":"Map集合的统计每个字符出现次数的两种方法一、map.containsKey()方法Map可以出现在k与v的映射中，v为null的情况。Map集合允许值对象为null，并且没有个数限制，所以当get()方法的返回值为null时，可能有两种情况，一种是在集合中没有该键对象，另一种是该键对象没有映射任何值对象，即值对象为null。因此，在Map集合中不应该利用get()方法来判断是否存在某个键，而应该利用containsKey()方法来判断。 1234567891011121314151617/** * map.containsKey()方法 * * @param nums */ public static void test1(int[] nums) &#123; HashMap&lt;Integer, Integer&gt; cnt = new HashMap&lt;&gt;(); for (int num : nums) &#123; if (!cnt.containsKey(num)) &#123; cnt.put(num, 1); &#125; else &#123; cnt.put(num, cnt.get(num) + 1); &#125; &#125; // 遍历HashMap pnt(cnt); &#125; 1234567原始数组：int[] nums &#x3D; new int[]&#123;1, 2, 2, 3, 5, 5, 5, 9, 1, 1&#125;;结果： 1出现的次数：3 2出现的次数：2 3出现的次数：1 5出现的次数：3 9出现的次数：1 二、map.getOrDefault()方法 当Map集合中有这个key时，就使用这个key值，如果没有就使用默认值defaultValue 。 12345678910111213/** * map.getOrDefault()方法 * * @param nums */ public static void test2(int[] nums) &#123; HashMap&lt;Integer, Integer&gt; cnt = new HashMap&lt;&gt;(); for (int num : nums) &#123; cnt.put(num, cnt.getOrDefault(num, 0) + 1); &#125; // 遍历HashMap pnt(cnt); &#125; 1234567原始数组：int[] nums = new int[]&#123;1, 2, 2, 3, 5, 5, 5, 9, 1, 1&#125;;结果： 1出现的次数：3 2出现的次数：2 3出现的次数：1 5出现的次数：3 9出现的次数：1 三、demo源代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import java.util.HashMap;import java.util.Map;/** * @program: leecode1 * @description: * @author: Marlowe * @create: 2020-09-07 15:28 **/public class map集合统计每个字符出现的次数 &#123; public static void main(String[] args) &#123; int[] nums = new int[]&#123;1, 2, 2, 3, 5, 5, 5, 9, 1, 1&#125;; test1(nums); test2(nums); &#125; /** * map.containsKey()方法 * * @param nums */ public static void test1(int[] nums) &#123; HashMap&lt;Integer, Integer&gt; cnt = new HashMap&lt;&gt;(); for (int num : nums) &#123; if (!cnt.containsKey(num)) &#123; cnt.put(num, 1); &#125; else &#123; cnt.put(num, cnt.get(num) + 1); &#125; &#125; pnt(cnt); &#125; /** * map.getOrDefault()方法 * * @param nums */ public static void test2(int[] nums) &#123; HashMap&lt;Integer, Integer&gt; cnt = new HashMap&lt;&gt;(); for (int num : nums) &#123; cnt.put(num, cnt.getOrDefault(num, 0) + 1); &#125; pnt(cnt); &#125; /** * 遍历HashMap */ public static void pnt(HashMap&lt;Integer, Integer&gt; map) &#123; for (Map.Entry&lt;Integer, Integer&gt; entry : map.entrySet()) &#123; int num = entry.getKey(); int count = entry.getValue(); System.out.println(num + &quot;出现的次数：&quot; + count); &#125; &#125;&#125;","categories":[{"name":"题解","slug":"题解","permalink":"https://xmmarlowe.github.io/categories/%E9%A2%98%E8%A7%A3/"}],"tags":[{"name":"HashMap","slug":"HashMap","permalink":"https://xmmarlowe.github.io/tags/HashMap/"}],"author":"Marlowe"},{"title":"Leetcode组合总和1-4题题解","slug":"题解/Leetcode组合总和1-4题题解","date":"2020-10-12T13:52:22.000Z","updated":"2021-06-03T13:38:11.404Z","comments":true,"path":"2020/10/12/题解/Leetcode组合总和1-4题题解/","link":"","permalink":"https://xmmarlowe.github.io/2020/10/12/%E9%A2%98%E8%A7%A3/Leetcode%E7%BB%84%E5%90%88%E6%80%BB%E5%92%8C1-4%E9%A2%98%E9%A2%98%E8%A7%A3/","excerpt":"","text":"Leetcode组合总和1-4题题解Leecode最近几天的每日一题都是组合总和问题，预测明天是组合总和Ⅳ，因此，提前将组合总和的所有题目刷了，前三题的思路都差不多，最后一题做法有所不同： 组合总和：candidates 中的数字可以无限制重复被选取。 组合总和Ⅱ： candidates 中的每个数字在每个组合中只能使用一次。 组合总和Ⅲ：组合中只允许有1-9的数字，并且每种组合中不存在重复的数字。 组合总和Ⅳ：找出符合要求组合的个数。 39. 组合总和给定一个无重复元素的数组 candidates 和一个目标数 target ，找出 candidates 中所有可以使数字和为 target 的组合。 candidates 中的数字可以无限制重复被选取。 说明： 所有数字（包括 target）都是正整数。 解集不能包含重复的组合。 示例 1： 123456输入：candidates &#x3D; [2,3,6,7], target &#x3D; 7,所求解集为：[ [7], [2,2,3]] 示例 2： 1234567输入：candidates &#x3D; [2,3,5], target &#x3D; 8,所求解集为：[ [2,2,2,2], [2,3,3], [3,5]] 提示： 1 &lt;= candidates.length &lt;= 30 1 &lt;= candidates[i] &lt;= 200 candidate 中的每个元素都是独一无二的。 1 &lt;= target &lt;= 500 题解（dfs，回溯算法）分析：此类问题可以画出树形图，然后就会发现此题可以用dfs+回溯算法解决，用到的数据结构为双端队列，具有栈和队列的性质，其定义方式为：Deque stack = new ArrayDeque();具体步骤见代码。 具体步骤如下：1234567891011121314151617181920212223242526272829303132333435363738class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; combinationSum(int[] candidates, int target) &#123; // 保存结果集 List&lt;List&lt;Integer&gt;&gt; res = new ArrayList(); // 获取数组的长度 int len = candidates.length; //如果数组为空，直接返回空集合 if(len == 0)&#123; return res; &#125; // 双端队列，保存临时路径 Deque&lt;Integer&gt; path = new ArrayDeque(); // 深度优先遍历求所有结果集 dfs(candidates,0,len,target,path,res); return res; &#125; public void dfs(int[] candidates,int start,int len,int target,Deque&lt;Integer&gt; path,List&lt;List&lt;Integer&gt;&gt; res)&#123; // 如果选多了，也即target &lt; 0,直接return if(target &lt; 0)&#123; return; &#125; // 找到一条路径 if(target == 0)&#123; // 将路径加入结果集 res.add(new ArrayList(path)); &#125; // 从下标为start的数开始寻找 for(int i = start; i &lt; len; i++)&#123; // 将当前元素入栈 path.addLast(candidates[i]); // 由于可以选择重复的元素，因此i不变，但是选择了东西，target对应减少 dfs(candidates,i,len,target - candidates[i],path,res); // 回到之前的状态 path.removeLast(); &#125; &#125;&#125; 40. 组合总和 II给定一个数组 candidates 和一个目标数 target ，找出 candidates 中所有可以使数字和为 target 的组合。 candidates 中的每个数字在每个组合中只能使用一次。 说明： 所有数字（包括目标数）都是正整数。 解集不能包含重复的组合。 示例 1: 12345678输入: candidates &#x3D; [10,1,2,7,6,1,5], target &#x3D; 8,所求解集为:[ [1, 7], [1, 2, 5], [2, 6], [1, 1, 6]] 示例 2: 123456输入: candidates &#x3D; [2,5,2,1,2], target &#x3D; 5,所求解集为:[ [1,2,2], [5]] 题解（dfs，回溯算法，哈希表）分析：此题和组合总和的区别在于 candidates 中的每个数字在每个组合中只能使用一次，并且解集不能包含重复的元素，因此可用哈希表对重复解集去重，具体步骤看下方代码注释。 具体步骤如下：1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; combinationSum2(int[] candidates, int target) &#123; // 将原始数组排序 Arrays.sort(candidates); // 获取数组长度 int len = candidates.length; // 结果集列表 List&lt;List&lt;Integer&gt;&gt; res = new ArrayList(); // 双端队列 Deque&lt;Integer&gt; path = new ArrayDeque(); // 深度优先遍历 + 回溯 dfs(candidates,0,len,target,path,res); // 去重，因为解集不能有重复元素 HashSet&lt;List&lt;Integer&gt;&gt; set = new HashSet(); for(List&lt;Integer&gt; list : res)&#123; set.add(list); &#125; // 将HashSet转换为List集合 return new ArrayList(set); &#125; public void dfs(int[] candidates,int start,int len,int target,Deque&lt;Integer&gt; path,List&lt;List&lt;Integer&gt;&gt; res)&#123; // 如果选多了，也即target &lt; 0,直接return if(target &lt; 0)&#123; return; &#125; // 找到一条路径 if(target == 0)&#123; // 将路径加入结果集 res.add(new ArrayList(path)); &#125; for(int i = start; i &lt; len; i++)&#123; // 将当前元素入栈 path.addLast(candidates[i]); // 由于数组中的元素只能用一次，因此i + 1,并且target减少 dfs(candidates,i+1,len,target - candidates[i],path,res); // 回到之前的状态 path.removeLast(); &#125; &#125;&#125; 216. 组合总和 III 找出所有相加之和为 n 的 k 个数的组合。组合中只允许含有 1 - 9 的正整数，并且每种组合中不存在重复的数字。 说明： 所有数字都是正整数。 解集不能包含重复的组合。 示例 1: 12输入: k &#x3D; 3, n &#x3D; 7输出: [[1,2,4]] 示例 2: 12输入: k &#x3D; 3, n &#x3D; 9输出: [[1,2,6], [1,3,5], [2,3,4]] 题解（dfs，回溯算法）分析：此题和组合总和Ⅱ的区别在于在1-9中选择数据,并且每个数据只能选一次，且只需返回长度为k的路径,因此需对结果集进行筛选，具体步骤看下方代码注释。 具体步骤如下：12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; combinationSum3(int k, int n) &#123; // 手动将1-9加入数组arr中 int[] arr = new int[]&#123;1,2,3,4,5,6,7,8,9&#125;; // 初始结果集 List&lt;List&lt;Integer&gt;&gt; res = new ArrayList(); // 最终结果集 List&lt;List&lt;Integer&gt;&gt; res1 = new ArrayList(); // 临时路径 Deque&lt;Integer&gt; path = new ArrayDeque(); // 深度优先遍历求出所有解集 dfs(arr,0,n,path,res); // 选出符合长度为k的解集 for(List&lt;Integer&gt; list : res)&#123; if(list.size() == k)&#123; res1.add(list); &#125; &#125; return res1; &#125; public void dfs(int[] arr,int start,int n ,Deque&lt;Integer&gt; path,List&lt;List&lt;Integer&gt;&gt; res)&#123; // 如果选多了，也即n &lt; 0,直接return if(n &lt; 0)&#123; return; &#125; // 找到一条路径 if(n == 0)&#123; // 将路径加入结果集 res.add(new ArrayList(path)); &#125; for(int i = start; i &lt; 9; i++)&#123; // 将当前元素入栈 path.addLast(arr[i]); // 由于数组中的元素只能用一次，因此i + 1,并且n减少 dfs(arr,i + 1,n - arr[i],path,res); // 回到之前的状态 path.removeLast(); &#125; &#125;&#125; 377. 组合总和 Ⅳ给定一个由正整数组成且不存在重复数字的数组，找出和为给定目标正整数的组合的个数。 示例: 12345678910111213141516nums &#x3D; [1, 2, 3]target &#x3D; 4所有可能的组合为：(1, 1, 1, 1)(1, 1, 2)(1, 2, 1)(1, 3)(2, 1, 1)(2, 2)(3, 1)请注意，顺序不同的序列被视作不同的组合。因此输出为 7。 进阶：如果给定的数组中含有负数会怎么样？问题会产生什么变化？我们需要在题目中添加什么限制来允许负数的出现？ 题解（1.dfs,回溯算法 2.动态规划）分析：此题和组合总和类似，区别在于求出所有解集后，还需求出解集的全排列，并返回全排列的个数。 组合总数前三题都是同样的套路，只是在结果处理以及中间过程有略微差别，但是这题不同的是要求结果集的全排列，因此，我就想用第一题的算法 + 全排列算法求出此题，代码如demo1，结果超时，代码逻辑是没有问题的，但题目所给数据过大，导致算全排列的时候使用过多时间，因此未通过。 查看题解，发现正确的解法为动态规划，根据分析可以得到状态转移方程： dp[i] = dp[i - nums[0]] + dp[i - nums[1]] + dp[i - nums[2]]...... 例如nums = [1,3,4],target = 7; dp[7] = dp[6] + dp[4] + dp[3]; 具体代码见demo2 具体步骤如下：demo1 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475class Solution &#123; public int combinationSum4(int[] nums, int target) &#123; Arrays.sort(nums); int sum = 0; // 保存结果集 List&lt;List&lt;Integer&gt;&gt; res = new ArrayList(); // 获取数组的长度 int len = nums.length; // 双端队列，保存临时路径 Deque&lt;Integer&gt; path = new ArrayDeque(); // 深度优先遍历求所有结果集 dfs(nums,0,len,target,path,res); // 求出解集中的所有情况 for(List&lt;Integer&gt; list : res)&#123; sum += isok(list); &#125; return sum; &#125; // 求出所有解集 public void dfs(int[] nums,int start,int len,int target,Deque&lt;Integer&gt; path,List&lt;List&lt;Integer&gt;&gt; res)&#123; if(target &lt; 0)&#123; return; &#125; if(target == 0)&#123; res.add(new ArrayList(path)); &#125; for(int i = start; i &lt; len; i++)&#123; path.addLast(nums[i]); // 可以重复使用，因此i不用+1 dfs(nums,i,len,target - nums[i],path,res); path.removeLast(); &#125; &#125; // 求出列表的所有组合情况 public int isok(List&lt;Integer&gt; list)&#123; int[] nums = new int[list.size()]; for(int i = 0 ; i &lt; nums.length; i++)&#123; nums[i] = list.get(i); &#125; int len = nums.length; Deque&lt;Integer&gt; path = new ArrayDeque(); List&lt;List&lt;Integer&gt;&gt; res = new ArrayList(); // 布尔数组，用于标记改数是否使用过 boolean[] used = new boolean[len]; dfs2(nums,len,0,used,path,res); return res.size(); &#125; // 求全排列 public void dfs2(int[] nums,int len,int depth,boolean[] used,Deque&lt;Integer&gt; path,List&lt;List&lt;Integer&gt;&gt; res)&#123; if(depth == len)&#123; res.add(new ArrayList(path)); return; &#125; for(int i = 0 ; i &lt; len; i++)&#123; if(used[i])&#123; continue; &#125; // 因为有重复元素，所以在下一层碰到相同元素会使结果重复，相对于全排列，进一步剪枝 if(i &gt; 0 &amp;&amp; nums[i] == nums[i - 1] &amp;&amp; !used[i - 1])&#123; continue; &#125; // 回溯算法经典步骤 // 先将当前数字加入栈，并将使用过的元素标记为true path.addLast(nums[i]); used[i] = true; dfs2(nums,len,depth + 1,used,path,res); // 回到之前的状态 path.removeLast(); used[i] = false; &#125; &#125;&#125; demo2 123456789101112131415class Solution &#123; public int combinationSum4(int[] nums, int target) &#123; int[] dp = new int[target + 1]; dp[0] = 1; for(int i = 0; i &lt;= target; i++)&#123; for(int num : nums)&#123; if(num &lt;= i)&#123; dp[i] += dp[i - num]; &#125; &#125; &#125; return dp[target]; &#125;&#125; :smile:以上题解仅限于个人理解，如有更好的方法或者更高效的解法，请移步至评论区，谢谢！","categories":[{"name":"题解","slug":"题解","permalink":"https://xmmarlowe.github.io/categories/%E9%A2%98%E8%A7%A3/"}],"tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://xmmarlowe.github.io/tags/Leetcode/"}],"author":"Marlowe"},{"title":"Leetcode全排列1-2题题解","slug":"题解/Leetcode全排列1-2题题解","date":"2020-10-12T13:49:35.000Z","updated":"2021-06-03T13:38:06.899Z","comments":true,"path":"2020/10/12/题解/Leetcode全排列1-2题题解/","link":"","permalink":"https://xmmarlowe.github.io/2020/10/12/%E9%A2%98%E8%A7%A3/Leetcode%E5%85%A8%E6%8E%92%E5%88%971-2%E9%A2%98%E9%A2%98%E8%A7%A3/","excerpt":"","text":"Leetcode全排列1-2题题解对于全排列问题，可能我们很多人从小在数学课上都做过，并且都能由一定的规律将所有排列情况写出来，但如何用编码的方式求解此类问题成了我的问题，或许也成是你们还未解决的问题，其实这类问题的套路都是 dfs + 回溯算法，然后，根据题目要求进行剪枝，我将通过下面两题来讲解这类问题具体做法。 46. 全排列给定一个 没有重复 数字的序列，返回其所有可能的全排列。 示例: 12345678910输入: [1,2,3]输出:[ [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1]] 题解（dfs，回溯算法）分析：由于是回溯算法，因此，会用到栈，通常我们所学的栈是这种用法 Stack&lt;Integer&gt; stack = new Stack&lt;Integer&gt;();,但在Stack的源码中发现了Deque&lt;Integer&gt; stack = new ArrayDeque&lt;Integer&gt;();这种用法，百度之后，知道了Deque : （double-ended queue，双端队列）是一种具有队列和栈的性质的数据结构。双端队列中的元素可以从两端弹出，相比list增加运算符重载。 具体步骤如下：1234567891011121314151617181920212223242526272829303132333435363738class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; permute(int[] nums) &#123; // 数组长度 int len = nums.length; // 结果集 List&lt;List&lt;Integer&gt;&gt; res = new ArrayList(); // 双端队列，保存临时路径 Deque&lt;Integer&gt; path = new ArrayDeque(); // 布尔数组，保存改数字是否使用过 boolean[] used = new boolean[len]; // 深度优先遍历求所有结果集 dfs(nums,len,0,used,path,res); return res; &#125; public void dfs(int[] nums,int len,int depth,boolean[] used,Deque&lt;Integer&gt; path,List&lt;List&lt;Integer&gt;&gt; res)&#123; // 如果到达最深的一层 if(depth == len)&#123; // 将当前路径加入结果集 res.add(new ArrayList(path)); return; &#125; for(int i = 0 ; i &lt; len; i++)&#123; // 判断当前数字是否用过 if(used[i])&#123; continue; &#125; // 回溯算法经典步骤 // 先将当前数字加入栈，并将使用过的元素标记为true path.addLast(nums[i]); used[i] = true; dfs(nums,len,depth + 1,used,path,res); // 回到之前的状态 path.removeLast(); used[i] = false; &#125; &#125;&#125; 47. 全排列 II给定一个可包含重复数字的序列，返回所有不重复的全排列。 示例: 1234567输入: [1,1,2]输出:[ [1,1,2], [1,2,1], [2,1,1]] 题解（dfs，回溯算法）分析：此题和全排列解法类似，唯一的差别在于可选数组nums中存在重复的数字，可能会产生重复的路径，因此，需要在判断当前数字是否用过后，再次判断上一次使用的数字和当前数字是否相同，如果相同，进行剪枝，具体差别见代码。 具体步骤如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; permuteUnique(int[] nums) &#123; // 数组长度 int len = nums.length; // 对数组排序 Arrays.sort(nums); // 结果集 List&lt;List&lt;Integer&gt;&gt; res = new ArrayList(); // 双端队列，保存临时路径 Deque&lt;Integer&gt; path = new ArrayDeque(); // 布尔数组，保存改数字是否使用过 boolean[] used = new boolean[len]; // 深度优先遍历求所有结果集 dfs(nums,len,0,used,path,res); return res; &#125; public void dfs(int[] nums,int len,int depth,boolean[] used,Deque&lt;Integer&gt; path,List&lt;List&lt;Integer&gt;&gt; res)&#123; // 如果到达最深的一层 if(depth == len)&#123; // 将当前路径加入结果集 res.add(new ArrayList(path)); return; &#125; for(int i = 0 ; i &lt; len; i++)&#123; // 判断当前数字是否用过 if(used[i])&#123; continue; &#125; // 因为有重复元素，所以在下一层碰到相同元素会使结果重复，相对于全排列，进一步剪枝 if (i &gt; 0 &amp;&amp; nums[i] == nums[i - 1] &amp;&amp; !used[i - 1]) &#123; continue; &#125; // 回溯算法经典步骤 // 先将当前数字加入栈，并将使用过的元素标记为true path.addLast(nums[i]); used[i] = true; dfs(nums,len,depth + 1,used,path,res); // 回到之前的状态 path.removeLast(); used[i] = false; &#125; &#125;&#125; :smile:以上题解仅限于个人理解，如有更好的方法或者更高效的解法，请移步至评论区，谢谢！","categories":[{"name":"题解","slug":"题解","permalink":"https://xmmarlowe.github.io/categories/%E9%A2%98%E8%A7%A3/"}],"tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://xmmarlowe.github.io/tags/Leetcode/"}],"author":"Marlowe"},{"title":"Leetcode两数-四数之和题解","slug":"题解/Leetcode两数-四数之和题解","date":"2020-10-12T13:35:23.000Z","updated":"2021-06-03T13:37:59.698Z","comments":true,"path":"2020/10/12/题解/Leetcode两数-四数之和题解/","link":"","permalink":"https://xmmarlowe.github.io/2020/10/12/%E9%A2%98%E8%A7%A3/Leetcode%E4%B8%A4%E6%95%B0-%E5%9B%9B%E6%95%B0%E4%B9%8B%E5%92%8C%E9%A2%98%E8%A7%A3/","excerpt":"","text":"Leecode两数-四数之和题解最近两天做了两数之和，四数之和，并且之前也做过三数之和，感觉这几道题解法都差不多，并且用同样的方法能求解n数之和。 1. 两数之和给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。 你可以假设每种输入只会对应一个答案。但是，数组中同一个元素不能使用两遍。 示例: 1234给定 nums &#x3D; [2, 7, 11, 15], target &#x3D; 9因为 nums[0] + nums[1] &#x3D; 2 + 7 &#x3D; 9所以返回 [0, 1] 题解（哈希表）分析：利用哈希map，key存放数字，value存放索引，遍历数组，依次取一个数，然后计算出另外一个数，如果哈希map中存在，直接取出索引，返回结果，如果不存在，向哈希map中添加当前元素和对应的下标。 具体步骤如下：1234567891011121314151617class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; // key存放数字，value存放index HashMap&lt;Integer,Integer&gt; map = new HashMap(); for(int i = 0; i &lt; nums.length;i++)&#123; int num2 = target - nums[i]; // 如果哈希map中存在当前数，直接返回i和当前数的下标 if(map.containsKey(num2))&#123; return new int[] &#123; map.get(num2), i &#125;; &#125;else&#123; // 将当前数放入哈希map map.put(nums[i],i); &#125; &#125; return null; &#125;&#125; 15. 三数之和给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？请你找出所有满足条件且不重复的三元组。 注意：答案中不可以包含重复的三元组。 示例： 1234567给定数组 nums &#x3D; [-1, 0, 1, 2, -1, -4]，满足要求的三元组集合为：[ [-1, 0, 1], [-1, -1, 2]] 题解（排序，双指针）分析：此题要求出三个数的和为0的结果集，则只需对原数组排序，然后从最小的数开始选，接着设置左右指针，如果当前三个数和为0，将这三个数加入结果集，继续寻找，如果当前三个数和大于0，右指针左移，小于0，左指针右移。 具体步骤如下：12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; threeSum(int[] nums) &#123; int len = nums.length; List&lt;List&lt;Integer&gt;&gt; ans = new ArrayList(); //如果数组为空 或者长度小于三 直接返回空 if(nums == null || len &lt;3) return ans; //对数组排序 Arrays.sort(nums); for(int i = 0 ; i &lt; len;i++)&#123; //如果当前最小的数大于0，直接结束循环 if(nums[i] &gt; 0) break; //去重 if(i &gt; 0 &amp;&amp; nums[i] == nums[i-1]) continue; //设置左右指针 int left = i + 1; int right = len - 1; while(left &lt; right)&#123; int sum = nums[i] + nums[left] + nums[right]; if( sum == 0)&#123; ans.add(Arrays.asList(nums[i],nums[left],nums[right])); //左边元素去重 while(left &lt; right &amp;&amp; nums[left] == nums[left + 1]) left++; //右边元素去重 while(left &lt; right &amp;&amp; nums[right] == nums[right - 1]) right--; //移动左右指针 left++; right--; &#125; if(sum &gt; 0) right--; if(sum &lt; 0) left++; &#125; &#125; return ans; &#125;&#125; 18. 四数之和给定一个包含 n 个整数的数组 nums 和一个目标值 target，判断 nums 中是否存在四个元素 a，b，c 和 d ，使得 a + b + c + d 的值与 target 相等？找出所有满足条件且不重复的四元组。 注意： 答案中不可以包含重复的四元组。 示例： 12345678给定数组 nums &#x3D; [1, 0, -1, 0, -2, 2]，和 target &#x3D; 0。满足要求的四元组集合为：[ [-1, 0, 0, 1], [-2, -1, 1, 2], [-2, 0, 0, 2]] 题解（排序，双指针）分析：此题要求出四个数的和为target的结果集，则只需对原数组排序，然后将四数之和降为三数之和，接着设置左右指针，如果当前四个数和为target，将这四个数加入结果集，继续寻找，如果当前四个数和大于target，右指针左移，小于0，左指针右移，具体步骤见代码注释。 具体步骤如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; fourSum(int[] nums, int target) &#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList(); // 边界条件判断 if(nums == null || nums.length &lt; 4)&#123; return res; &#125; // 对原数组排序 Arrays.sort(nums); // 获取原数组长度 int l = nums.length; for(int i = 0; i &lt; l - 3; i++)&#123; // 去重 if( i &gt; 0 &amp;&amp; nums[i] == nums[i-1])&#123; continue; &#125; // 如果当前数加上后面最小的三个数都比target大，直接跳出 if(nums[i] + nums[i + 1] + nums[i + 2] + nums[i + 3] &gt; target)&#123; break; &#125; // 如果当前数加上最大的三个数逗比target小，跳过当前数 if(nums[i] + nums[l - 3] + nums[l - 2] + nums[l - 1] &lt; target)&#123; continue; &#125; // 同上（n数之和直接重复此操作即可） for(int j = i + 1; j &lt; l - 2; j++)&#123; if(j &gt; i + 1 &amp;&amp; nums[j] == nums[j - 1])&#123; continue; &#125; if(nums[i] + nums[j] + nums[j + 1] + nums[j + 2] &gt; target)&#123; break; &#125; if(nums[i] + nums[j] + nums[l - 1] + nums[l - 2] &lt; target)&#123; continue; &#125; // 将n树之和转为两数之和 int left = j + 1; int right = l - 1; while(left &lt; right)&#123; int sum = nums[i] + nums[j] + nums[left] + nums[right]; if(sum == target)&#123; // 加入结果集 res.add(Arrays.asList(nums[i],nums[j],nums[left],nums[right])); // 去重 while(left &lt; right &amp;&amp; nums[left] == nums[left + 1])&#123; left++; &#125; left++; // 去重 while(left &lt; right &amp;&amp; nums[right] == nums[right - 1])&#123; right--; &#125; right--; &#125;else if(sum &gt; target)&#123; right--; &#125;else&#123; left++; &#125; &#125; &#125; &#125; return res; &#125;&#125; 由三数之和和四数之和可以得出n数之和的解法，思想是一样的，都是枚举，去重，再将最后两个数的和转换为双指针，降低时间复杂度","categories":[{"name":"题解","slug":"题解","permalink":"https://xmmarlowe.github.io/categories/%E9%A2%98%E8%A7%A3/"}],"tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://xmmarlowe.github.io/tags/Leetcode/"}],"author":"Marlowe"},{"title":"test","slug":"随笔/test","date":"2020-10-12T12:35:34.000Z","updated":"2020-12-02T15:04:50.067Z","comments":true,"path":"2020/10/12/随笔/test/","link":"","permalink":"https://xmmarlowe.github.io/2020/10/12/%E9%9A%8F%E7%AC%94/test/","excerpt":"","text":"testhexohello,world!","categories":[],"tags":[{"name":"test","slug":"test","permalink":"https://xmmarlowe.github.io/tags/test/"}]},{"title":"我的第一篇博客","slug":"随笔/我的第一篇博客","date":"2020-10-12T12:20:20.000Z","updated":"2021-05-05T09:08:19.969Z","comments":true,"path":"2020/10/12/随笔/我的第一篇博客/","link":"","permalink":"https://xmmarlowe.github.io/2020/10/12/%E9%9A%8F%E7%AC%94/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/","excerpt":"","text":"我的第一篇博客记录一下最近使用hexo搭建的博客 &ensp;&ensp;周六晚上开始搭建，安装node、npm等等，碰到了一系列问题，换了一个主题后，总算把博客整体框架搭建好了，接着周日本是完整的一天，但早上九点多才起床，到实验室接近10点了，继续研究配置文件，以及主题的源代码，但是没啥效果，github有时候也抽风，就问了问学长,最后重新配置了仓库，总算解决了。&ensp;&ensp;周一中午开始研究上传到github以及自动部署脚本文件，到了晚上都没解决，最后才知道博客仓库只是部署编译出来的网站静态文件，如果想要使用github进行代码托管，只有新建一个代码库，把所有文件上传上去。脚本文件如下 12345678910111213@echo offD:cd D:\\PersonalFile\\HexoBlogecho &#39;start git sync&#39;git add .git add -Agit add -ugit commit -m &quot;update...&quot;git pull HexoBlog mastergit push HexoBlog mastercall hexo ghexo d","categories":[{"name":"随笔","slug":"随笔","permalink":"https://xmmarlowe.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"test","slug":"test","permalink":"https://xmmarlowe.github.io/tags/test/"},{"name":"随笔","slug":"随笔","permalink":"https://xmmarlowe.github.io/tags/%E9%9A%8F%E7%AC%94/"}]},{"title":"public、private、protected、default的区别","slug":"Java/public、private、protected、default的区别","date":"2020-04-20T05:49:35.000Z","updated":"2021-04-23T14:22:55.683Z","comments":true,"path":"2020/04/20/Java/public、private、protected、default的区别/","link":"","permalink":"https://xmmarlowe.github.io/2020/04/20/Java/public%E3%80%81private%E3%80%81protected%E3%80%81default%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"publicJava语言中访问限制最宽的修饰符，一般称之为“公共的”。被其修饰的类、属性以及方法不仅可以跨类访问，而且允许跨包（package）访问。 privateJava语言中对访问权限限制的最窄的修饰符，一般称之为“私有的”。被其修饰的类、属性以及方法只能被该类的对象访问，其子类不能访问，更不能允许跨包访问。 protected介于public 和 private 之间的一种访问修饰符，一般称之为“保护形”。被其修饰的类、属性以及方法只能被类本身的方法及子类访问，即使子类在不同的包中也可以访问。 default即不加任何访问修饰符，通常称为“默认访问模式”。该模式下，只允许在同一个包中进行访问。","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"public","slug":"public","permalink":"https://xmmarlowe.github.io/tags/public/"},{"name":"private","slug":"private","permalink":"https://xmmarlowe.github.io/tags/private/"},{"name":"protected","slug":"protected","permalink":"https://xmmarlowe.github.io/tags/protected/"},{"name":"default","slug":"default","permalink":"https://xmmarlowe.github.io/tags/default/"}],"author":"Marlowe"},{"title":"JDK1.8新特性","slug":"Java/JDK1-8新特性","date":"2020-04-16T07:55:02.000Z","updated":"2021-04-23T14:22:41.706Z","comments":true,"path":"2020/04/16/Java/JDK1-8新特性/","link":"","permalink":"https://xmmarlowe.github.io/2020/04/16/Java/JDK1-8%E6%96%B0%E7%89%B9%E6%80%A7/","excerpt":"Java 8 (又称为 jdk 1.8) 是 Java 语言开发的一个主要版本。 Oracle 公司于 2014 年 3 月 18 日发布 Java 8 ，它支持函数式编程，新的 JavaScript 引擎，新的日期 API，新的Stream API 等。","text":"Java 8 (又称为 jdk 1.8) 是 Java 语言开发的一个主要版本。 Oracle 公司于 2014 年 3 月 18 日发布 Java 8 ，它支持函数式编程，新的 JavaScript 引擎，新的日期 API，新的Stream API 等。 Lambda 表达式Lambda 允许把函数作为一个方法的参数（函数作为参数传递到方法中）。 以下是lambda表达式的重要特征: 可选类型声明： 不需要声明参数类型，编译器可以统一识别参数值。 可选的参数圆括号： 一个参数无需定义圆括号，但多个参数需要定义圆括号。 可选的大括号： 如果主体包含了一个语句，就不需要使用大括号。 可选的返回关键字： 如果主体只有一个表达式返回值则编译器会自动返回值，大括号需要指定明表达式返回了一个数值。 变量作用域lambda 表达式只能引用标记了 final 的外层局部变量，这就是说不能在 lambda 内部修改定义在域外的局部变量，否则会编译错误。 1234567891011121314public class Java8Tester &#123; final static String salutation = &quot;Hello! &quot;; public static void main(String args[])&#123; GreetingService greetService1 = message -&gt; System.out.println(salutation + message); greetService1.sayMessage(&quot;Runoob&quot;); &#125; interface GreetingService &#123; void sayMessage(String message); &#125;&#125; 我们也可以直接在 lambda 表达式中访问外层的局部变量： 1234567891011public class Java8Tester &#123; public static void main(String args[]) &#123; final int num = 1; Converter&lt;Integer, String&gt; s = (param) -&gt; System.out.println(String.valueOf(param + num)); s.convert(2); // 输出结果为 3 &#125; public interface Converter&lt;T1, T2&gt; &#123; void convert(int i); &#125;&#125; lambda 表达式的局部变量可以不用声明为 final，但是必须不可被后面的代码修改（即隐性的具有 final 的语义） 12345int num = 1; Converter&lt;Integer, String&gt; s = (param) -&gt; System.out.println(String.valueOf(param + num));s.convert(2);num = 5; //报错信息：Local variable num defined in an enclosing scope must be final or effectively final 在 Lambda 表达式当中不允许声明一个与局部变量同名的参数或者局部变量。 12String first = &quot;&quot;; Comparator&lt;String&gt; comparator = (first, second) -&gt; Integer.compare(first.length(), second.length()); //编译会出错 方法引用方法引用提供了非常有用的语法，可以直接引用已有Java类或对象（实例）的方法或构造器。与lambda联合使用，方法引用可以使语言的构造更紧凑简洁，减少冗余代码。 方法引用通过方法的名字来指向一个方法。 方法引用可以使语言的构造更紧凑简洁，减少冗余代码。 方法引用使用一对冒号 :: 。 下面，我们在 Car 类中定义了 4 个方法作为例子来区分 Java 中 4 种不同方法的引用。 12345678910111213141516171819202122232425package com.runoob.main; @FunctionalInterfacepublic interface Supplier&lt;T&gt; &#123; T get();&#125; class Car &#123; //Supplier是jdk1.8的接口，这里和lamda一起使用了 public static Car create(final Supplier&lt;Car&gt; supplier) &#123; return supplier.get(); &#125; public static void collide(final Car car) &#123; System.out.println(&quot;Collided &quot; + car.toString()); &#125; public void follow(final Car another) &#123; System.out.println(&quot;Following the &quot; + another.toString()); &#125; public void repair() &#123; System.out.println(&quot;Repaired &quot; + this.toString()); &#125;&#125; 构造器引用： 它的语法是Class::new，或者更一般的Class&lt; T &gt;::new实例如下： 12final Car car = Car.create( Car::new );final List&lt; Car &gt; cars = Arrays.asList( car ); 静态方法引用： 它的语法是Class::static_method，实例如下： 1cars.forEach( Car::collide ); 特定类的任意对象的方法引用： 它的语法是Class::method实例如下： 1cars.forEach( Car::repair ); 特定对象的方法引用： 它的语法是instance::method实例如下： 12final Car police = Car.create( Car::new );cars.forEach( police::follow ); 默认方法简单说，默认方法就是接口可以有实现方法，而且不需要实现类去实现其方法。 我们只需在方法名前面加个 default 关键字即可实现默认方法。 为什么要有这个特性？首先，之前的接口是个双刃剑，好处是面向抽象而不是面向具体编程，缺陷是，当需要修改接口时候，需要修改全部实现该接口的类，目前的 java 8 之前的集合框架没有 foreach 方法，通常能想到的解决办法是在JDK里给相关的接口添加新的方法及实现。然而，对于已经发布的版本，是没法在给接口添加新方法的同时不影响已有的实现。所以引进的默认方法。他们的目的是为了解决接口的修改与现有的实现不兼容的问题。 语法默认方法语法格式如下： 12345public interface Vehicle &#123; default void print()&#123; System.out.println(&quot;我是一辆车!&quot;); &#125;&#125; 多个默认方法一个接口有默认方法，考虑这样的情况，一个类实现了多个接口，且这些接口有相同的默认方法，以下实例说明了这种情况的解决方法： 1234567891011public interface Vehicle &#123; default void print()&#123; System.out.println(&quot;我是一辆车!&quot;); &#125;&#125; public interface FourWheeler &#123; default void print()&#123; System.out.println(&quot;我是一辆四轮车!&quot;); &#125;&#125; 第一个解决方案是创建自己的默认方法，来覆盖重写接口的默认方法： 12345public class Car implements Vehicle, FourWheeler &#123; default void print()&#123; System.out.println(&quot;我是一辆四轮汽车!&quot;); &#125;&#125; 第二种解决方案可以使用 super 来调用指定接口的默认方法： 12345public class Car implements Vehicle, FourWheeler &#123; public void print()&#123; Vehicle.super.print(); &#125;&#125; 静态默认方法Java 8 的另一个特性是接口可以声明（并且可以提供实现）静态方法,通过类名.方法名调用。例如： 123456789public interface Vehicle &#123; default void print()&#123; System.out.println(&quot;我是一辆车!&quot;); &#125; // 静态方法 static void blowHorn()&#123; System.out.println(&quot;按喇叭!!!&quot;); &#125;&#125; 新工具新的编译工具，如：Nashorn引擎 jjs、 类依赖分析器jdeps。 Stream API新添加的Stream API（java.util.stream） 把真正的函数式编程风格引入到Java中。 Java 8 API添加了一个新的抽象称为流Stream，可以让你以一种声明的方式处理数据。 Stream 使用一种类似用 SQL 语句从数据库查询数据的直观方式来提供一种对 Java 集合运算和表达的高阶抽象。 Stream API可以极大提高Java程序员的生产力，让程序员写出高效率、干净、简洁的代码。 这种风格将要处理的元素集合看作一种流， 流在管道中传输， 并且可以在管道的节点上进行处理， 比如筛选， 排序，聚合等。 元素流在管道中经过中间操作（intermediate operation）的处理，最后由最终操作(terminal operation)得到前面处理的结果。 123+--------------------+ +------+ +------+ +---+ +-------+| stream of elements +-----&gt; |filter+-&gt; |sorted+-&gt; |map+-&gt; |collect|+--------------------+ +------+ +------+ +---+ +-------+ 以上的流程转换为 Java 代码为： 123456List&lt;Integer&gt; transactionsIds = widgets.stream() .filter(b -&gt; b.getColor() == RED) .sorted((x,y) -&gt; x.getWeight() - y.getWeight()) .mapToInt(Widget::getWeight) .sum(); 生成流在 Java 8 中, 集合接口有两个方法来生成流： stream() − 为集合创建串行流。 parallelStream() − 为集合创建并行流。 12List&lt;String&gt; strings = Arrays.asList(&quot;abc&quot;, &quot;&quot;, &quot;bc&quot;, &quot;efg&quot;, &quot;abcd&quot;,&quot;&quot;, &quot;jkl&quot;);List&lt;String&gt; filtered = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.toList()); forEachStream 提供了新的方法 ‘forEach’ 来迭代流中的每个数据。以下代码片段使用 forEach 输出了10个随机数： 12Random random = new Random();random.ints().limit(10).forEach(System.out::println); mapmap 方法用于映射每个元素到对应的结果，以下代码片段使用 map 输出了元素对应的平方数： 123List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);// 获取对应的平方数List&lt;Integer&gt; squaresList = numbers.stream().map( i -&gt; i*i).distinct().collect(Collectors.toList()); filterfilter 方法用于通过设置的条件过滤出元素。以下代码片段使用 filter 方法过滤出空字符串： 123List&lt;String&gt;strings = Arrays.asList(&quot;abc&quot;, &quot;&quot;, &quot;bc&quot;, &quot;efg&quot;, &quot;abcd&quot;,&quot;&quot;, &quot;jkl&quot;);// 获取空字符串的数量long count = strings.stream().filter(string -&gt; string.isEmpty()).count(); limitlimit 方法用于获取指定数量的流。 以下代码片段使用 limit 方法打印出 10 条数据： 12Random random = new Random();random.ints().limit(10).forEach(System.out::println); sortedsorted 方法用于对流进行排序。以下代码片段使用 sorted 方法对输出的 10 个随机数进行排序： 12Random random = new Random();random.ints().limit(10).sorted().forEach(System.out::println); 并行（parallel）程序parallelStream 是流并行处理程序的代替方法。以下实例我们使用 parallelStream 来输出空字符串的数量： 123List&lt;String&gt; strings = Arrays.asList(&quot;abc&quot;, &quot;&quot;, &quot;bc&quot;, &quot;efg&quot;, &quot;abcd&quot;,&quot;&quot;, &quot;jkl&quot;);// 获取空字符串的数量long count = strings.parallelStream().filter(string -&gt; string.isEmpty()).count(); Date Time API加强对日期与时间的处理。 在旧版的 Java 中，日期时间 API 存在诸多问题，其中有： 非线程安全 − java.util.Date 是非线程安全的，所有的日期类都是可变的，这是Java日期类最大的问题之一。 设计很差 − Java的日期/时间类的定义并不一致，在java.util和java.sql的包中都有日期类，此外用于格式化和解析的类在java.text包中定义。java.util.Date同时包含日期和时间，而java.sql.Date仅包含日期，将其纳入java.sql包并不合理。另外这两个类都有相同的名字，这本身就是一个非常糟糕的设计。 时区处理麻烦 − 日期类并不提供国际化，没有时区支持，因此Java引入了java.util.Calendar和java.util.TimeZone类，但他们同样存在上述所有的问题。 Java 8 在 java.time 包下提供了很多新的 API。以下为两个比较重要的 API： Local(本地) − 简化了日期时间的处理，没有时区的问题。 Zoned(时区) − 通过制定的时区处理日期时间。 新的java.time包涵盖了所有处理日期，时间，日期/时间，时区，时刻（instants），过程（during）与时钟（clock）的操作。 Optional 类Optional 类已经成为 Java 8 类库的一部分，用来解决空指针异常。 Optional 类是一个可以为null的容器对象。如果值存在则isPresent()方法会返回true，调用get()方法会返回该对象。 Optional 是个容器：它可以保存类型T的值，或者仅仅保存null。Optional提供很多有用的方法，这样我们就不用显式进行空值检测。 Optional 类的引入很好的解决空指针异常。 类声明以下是一个 java.util.Optional&lt;T&gt; 类 的声明： 1public final class Optional&lt;T&gt; extends Object Optional 实例我们可以通过以下实例来更好的了解 Optional 类的使用： 1234567891011121314151617181920212223242526272829303132import java.util.Optional; public class Java8Tester &#123; public static void main(String args[])&#123; Java8Tester java8Tester = new Java8Tester(); Integer value1 = null; Integer value2 = new Integer(10); // Optional.ofNullable - 允许传递为 null 参数 Optional&lt;Integer&gt; a = Optional.ofNullable(value1); // Optional.of - 如果传递的参数是 null，抛出异常 NullPointerException Optional&lt;Integer&gt; b = Optional.of(value2); System.out.println(java8Tester.sum(a,b)); &#125; public Integer sum(Optional&lt;Integer&gt; a, Optional&lt;Integer&gt; b)&#123; // Optional.isPresent - 判断值是否存在 System.out.println(&quot;第一个参数值存在: &quot; + a.isPresent()); System.out.println(&quot;第二个参数值存在: &quot; + b.isPresent()); // Optional.orElse - 如果值存在，返回它，否则返回默认值 Integer value1 = a.orElse(new Integer(0)); //Optional.get - 获取值，值需要存在 Integer value2 = b.get(); return value1 + value2; &#125;&#125; 执行以上脚本，输出结果为： 12345$ javac Java8Tester.java $ java Java8Tester第一个参数值存在: false第二个参数值存在: true10 Nashorn, JavaScript 引擎Java 8提供了一个新的Nashorn javascript引擎，它允许我们在JVM上运行特定的javascript应用。","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"JDK","slug":"JDK","permalink":"https://xmmarlowe.github.io/tags/JDK/"}],"author":"Marlowe"},{"title":"8种基本类型的包装类和常量池","slug":"Java/8种基本类型的包装类和常量池","date":"2020-04-16T05:40:01.000Z","updated":"2021-04-23T14:20:50.810Z","comments":true,"path":"2020/04/16/Java/8种基本类型的包装类和常量池/","link":"","permalink":"https://xmmarlowe.github.io/2020/04/16/Java/8%E7%A7%8D%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%8C%85%E8%A3%85%E7%B1%BB%E5%92%8C%E5%B8%B8%E9%87%8F%E6%B1%A0/","excerpt":"","text":"Java 基本类型的包装类的大部分都实现了常量池技术，即 Byte,Short,Integer,Long,Character,Boolean；前面 4 种包装类默认创建了数值[-128，127] 的相应类型的缓存数据，Character创建了数值在[0,127]范围的缓存数据，Boolean 直接返回True Or False。如果超出对应范围仍然会去创建新的对象。 为啥把缓存设置为[-128，127]区间？（参见issue/461）性能和资源之间的权衡。 123public static Boolean valueOf(boolean b) &#123; return (b ? TRUE : FALSE);&#125; 123456789private static class CharacterCache &#123; private CharacterCache()&#123;&#125; static final Character cache[] = new Character[127 + 1]; static &#123; for (int i = 0; i &lt; cache.length; i++) cache[i] = new Character((char)i); &#125; &#125; 两种浮点数类型的包装类 Float,Double 并没有实现常量池技术。 123456789Integer i1 = 33;Integer i2 = 33;System.out.println(i1 == i2);// 输出 trueInteger i11 = 333;Integer i22 = 333;System.out.println(i11 == i22);// 输出 falseDouble i3 = 1.2;Double i4 = 1.2;System.out.println(i3 == i4);// 输出 false Integer 缓存源代码： 12345678/***此方法将始终缓存-128 到 127（包括端点）范围内的值，并可以缓存此范围之外的其他值。*/ public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); &#125; 应用场景： Integer i1=40；Java 在编译的时候会直接将代码封装成 Integer i1=Integer.valueOf(40);，从而使用常量池中的对象。 Integer i1 = new Integer(40);这种情况下会创建新的对象。 123Integer i1 = 40;Integer i2 = new Integer(40);System.out.println(i1==i2);//输出 false Integer 比较更丰富的一个例子: 12345678910111213Integer i1 = 40;Integer i2 = 40;Integer i3 = 0;Integer i4 = new Integer(40);Integer i5 = new Integer(40);Integer i6 = new Integer(0);System.out.println(&quot;i1=i2 &quot; + (i1 == i2));System.out.println(&quot;i1=i2+i3 &quot; + (i1 == i2 + i3));System.out.println(&quot;i1=i4 &quot; + (i1 == i4));System.out.println(&quot;i4=i5 &quot; + (i4 == i5));System.out.println(&quot;i4=i5+i6 &quot; + (i4 == i5 + i6)); System.out.println(&quot;40=i5+i6 &quot; + (40 == i5 + i6)); 结果： 123456i1=i2 truei1=i2+i3 truei1=i4 falsei4=i5 falsei4=i5+i6 true40=i5+i6 true 解释： 语句 i4 == i5 + i6，因为+这个操作符不适用于 Integer 对象，首先 i5 和 i6 进行自动拆箱操作，进行数值相加，即 i4 == 40。然后 Integer 对象无法与数值进行直接比较，所以 i4 自动拆箱转为 int 值 40，最终这条语句转为 40 == 40 进行数值比较。 参考8 种基本类型的包装类和常量池","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"包装类","slug":"包装类","permalink":"https://xmmarlowe.github.io/tags/%E5%8C%85%E8%A3%85%E7%B1%BB/"},{"name":"常量池","slug":"常量池","permalink":"https://xmmarlowe.github.io/tags/%E5%B8%B8%E9%87%8F%E6%B1%A0/"}],"author":"Marlowe"},{"title":"String类和常量池","slug":"Java/String类和常量池","date":"2020-04-16T05:39:36.000Z","updated":"2021-04-23T14:22:59.741Z","comments":true,"path":"2020/04/16/Java/String类和常量池/","link":"","permalink":"https://xmmarlowe.github.io/2020/04/16/Java/String%E7%B1%BB%E5%92%8C%E5%B8%B8%E9%87%8F%E6%B1%A0/","excerpt":"","text":"String 对象的两种创建方式： 12345String str1 = &quot;abcd&quot;;//先检查字符串常量池中有没有&quot;abcd&quot;，如果字符串常量池中没有，则创建一个，然后 str1 指向字符串常量池中的对象，如果有，则直接将 str1 指向&quot;abcd&quot;&quot;；String str2 = new String(&quot;abcd&quot;);//堆中创建一个新的对象String str3 = new String(&quot;abcd&quot;);//堆中创建一个新的对象System.out.println(str1==str2);//falseSystem.out.println(str2==str3);//false 这两种不同的创建方法是有差别的。 第一种方式是在常量池中拿对象； 第二种方式是直接在堆内存空间创建一个新的对象。 记住一点：只要使用 new 方法，便需要创建新的对象。 String 类型的常量池比较特殊。它的主要使用方法有两种： 直接使用双引号声明出来的 String 对象会直接存储在常量池中。 如果不是用双引号声明的 String 对象，可以使用 String 提供的 intern 方法。String.intern() 是一个 Native 方法，它的作用是：如果运行时常量池中已经包含一个等于此 String 对象内容的字符串，则返回常量池中该字符串的引用；如果没有，JDK1.7之前（不包含1.7）的处理方式是在常量池中创建与此 String 内容相同的字符串，并返回常量池中创建的字符串的引用，JDK1.7以及之后的处理方式是在常量池中记录此字符串的引用，并返回该引用。 123456String s1 = new String(&quot;计算机&quot;);String s2 = s1.intern();String s3 = &quot;计算机&quot;;System.out.println(s2);//计算机System.out.println(s1 == s2);//false，因为一个是堆内存中的 String 对象一个是常量池中的 String 对象，System.out.println(s3 == s2);//true，因为两个都是常量池中的 String 对象 字符串拼接: 12345678910String str1 = &quot;str&quot;;String str2 = &quot;ing&quot;;String str3 = &quot;str&quot; + &quot;ing&quot;;//常量池中的对象String str4 = str1 + str2; //在堆上创建的新的对象 String str5 = &quot;string&quot;;//常量池中的对象System.out.println(str3 == str4);//falseSystem.out.println(str3 == str5);//trueSystem.out.println(str4 == str5);//false 尽量避免多个字符串拼接，因为这样会重新创建对象。如果需要改变字符串的话，可以使用 StringBuilder 或者 StringBuffer。 String s1 = new String(“abc”);这句话创建了几个字符串对象？将创建 1 或 2 个字符串。如果池中已存在字符串常量“abc”，则只会在堆空间创建一个字符串常量“abc”。如果池中没有字符串常量“abc”，那么它将首先在池中创建，然后在堆空间中创建，因此将创建总共 2 个字符串对象。 验证： 1234String s1 = new String(&quot;abc&quot;);// 堆内存的地址值String s2 = &quot;abc&quot;;System.out.println(s1 == s2);// 输出 false,因为一个是堆内存，一个是常量池的内存，故两者是不同的。System.out.println(s1.equals(s2));// 输出 true 结果： 12falsetrue","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"常量池","slug":"常量池","permalink":"https://xmmarlowe.github.io/tags/%E5%B8%B8%E9%87%8F%E6%B1%A0/"},{"name":"String","slug":"String","permalink":"https://xmmarlowe.github.io/tags/String/"}],"author":"Marlowe"},{"title":"final finally finalize的区别","slug":"Java/final-finally-finalize的区别","date":"2020-04-15T14:37:21.000Z","updated":"2021-04-23T14:21:28.552Z","comments":true,"path":"2020/04/15/Java/final-finally-finalize的区别/","link":"","permalink":"https://xmmarlowe.github.io/2020/04/15/Java/final-finally-finalize%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"简单概述final 、 finally 、 finalize三个关键字的区别…","text":"简单概述final 、 finally 、 finalize三个关键字的区别… final可以修饰类、变量、方法，修饰类表示该类不能被继承、修饰方法表示该方法不能被重写、修饰变量表示该变量是一个常量不能被重新赋值。 finally一般作用在try-catch代码块中，在处理异常的时候，通常我们将一定要执行的代码方法finally代码块中，表示不管是否出现异常，该代码块都会执行，一般用来存放一些关闭资源的代码。 finalize是一个方法，属于Object类的一个方法，而Object类是所有类的父类，该方法一般由垃圾回收器来调用，当我们调用System的gc()方法的时候，由垃圾回收器调用finalize(),回收垃圾。","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"finally","slug":"finally","permalink":"https://xmmarlowe.github.io/tags/finally/"},{"name":"final","slug":"final","permalink":"https://xmmarlowe.github.io/tags/final/"},{"name":"finalize","slug":"finalize","permalink":"https://xmmarlowe.github.io/tags/finalize/"}],"author":"Marlowe"},{"title":"Java高并发之锁总结","slug":"并发/Java高并发之锁总结","date":"2020-04-11T13:37:12.000Z","updated":"2021-04-29T14:29:46.990Z","comments":true,"path":"2020/04/11/并发/Java高并发之锁总结/","link":"","permalink":"https://xmmarlowe.github.io/2020/04/11/%E5%B9%B6%E5%8F%91/Java%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B9%8B%E9%94%81%E6%80%BB%E7%BB%93/","excerpt":"","text":"Java线程锁机制是怎样的？ JAVA的锁就是在对象的Markword中记录一个锁状态。无锁，偏向锁，轻量级锁，重量级锁对应不同的锁状态。 JAVA的锁机制就是根据资源竞争的激烈程度不断进行锁升级的过程。 锁的分类1. 乐观锁与悲观锁 乐观锁 对共享数据进行访问时，乐观锁总是认为不会有其他线程修改数据修改数据。 于是直接执行操作，只是在更新时检查数据是否已经被其他线程修改。 如果没有被修改，则操作执行成功；否则，添加其他补偿措施。 常见的补偿措施是不断尝试，直到成功。 Java中的非阻塞同步都是采用这种乐观的并发策略，乐观锁在Java中是通过使用无锁编程来实现，最常使用的CAS操作。 比如，线程安全的原子类的自增操作，就是通过循环的CAS操作实现的。 悲观锁 对共享数据进行访问时，悲观锁总是认为一定会有其他线程修改数据。如果不加锁，肯定会出问题。 因此，悲观锁无论是否出现共享数据的争用，在访问数据时都会先加锁。 Java中同步互斥都是采用这种悲观的并发策略，synchronized关键字和Lock接口的实现类都是悲观锁。 2. 独占锁和共享锁 独占锁 又叫排它锁，同一个锁对象，同一时刻只允许一个线程获取到锁。 如果线程T对数据A加上独占锁后，其他线程不能对该数据再加任何类型的锁（包括独占锁和共享锁），自己可以对数据进行读操作或者写操作。 独占锁允许线程对数据进行读写操作。 Java中的 synchronized关键字、Mutex、ReentrantLock、ReentrantReadWriteLock 中写锁，都是独占锁。 共享锁 同一个所对象，同一时刻允许多个线程获取到锁。 线程T对数据A加上共享锁，则其他线程只能对数据A加共享锁，不能加独占锁。 共享锁只允许对数据进行读操作。 java中ReentrantReadWriteLock中读锁是共享锁。 ReentrantReadWriteLock读写锁的获取： 同步状态不为0，如果有其他线程获取到读锁或者当前线程不是持有写锁的线程，则获取写锁失败进入阻塞状态；否则，当前线程是持有写锁的线程，直接通过setState()方法增加写状态。 同步状态为0，直接通过compareAndSetState()方法实现写状态的CAS增加，并将当前线程设置为持有写锁的线程。 如果有其他线程获取到了写锁，则获取读锁失败进入阻塞状态。 如果写锁未被获取或者该线程为持有写锁的线程，则获取读锁成功，通过compareAndSetState()方法实现读状态的CAS增加 独占锁和共享锁都是通过AQS实现的，tryAcquire()或者tryAcquireShared()方法支持独占式或者共享式的获取同步状态。 3. 公平锁和非公平锁 公平锁 当锁被释放，按照阻塞的先后顺序获取锁，即同步队列头节点中的线程将获取锁。 公平锁可以保证锁的获取按照FIFO原则，但需要进行大量的线程切换，导致吞吐率较低。 非公平锁： 当锁被释放，所有阻塞的线程都可以争抢获取锁的资格，可能导致先阻塞的线程最后获取锁。 非公平锁虽然可能造成线程饥饿，但极少进行线程的切换，保证了更大的吞吐量。 Java中ReentrantLock和ReentrantReadWriteLock支持公平和非公平访问，而synchronized关键字只支持非公平访问。 公平与非公平可以通过构造函数的fair参数进行指定，默认是false，即默认为非公平的获取锁。 公平和非公平都是依靠AQS实现的，公平使用FairSync同步器，非公平使用NoFairSync同步器。123public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 4. 可重入锁和非可重入锁 可重入锁又名递归锁，是指在同一个线程在外层方法获取锁的时候，再进入该线程的内层方法会自动获取锁（前提锁对象得是同一个对象或者class），不会因为之前已经获取过还没释放而阻塞。Java中ReentrantLock和synchronized都是可重入锁，可重入锁的一个优点是可一定程度避免死锁。下面用示例代码来进行分析： 可重入锁： 已经获取锁的线程再次获取该锁而不被锁所阻塞，需要解决线程再次获取锁和锁的最终释放两个问题。 可重入锁可以一定程度的避免死锁。 非可重入锁： 已经获取锁的线程再次获取该锁，会因为需要等待自身释放锁而被阻塞。 非可重入锁容易造成当前线程死锁，从而使整个队列中线程永久阻塞。 Java中的synchronized关键字、ReentrantLock锁和ReentrantReadWriteLock锁都支持重进入，其中ReentrantReadWriteLock的读锁是支持重进入的共享锁，写锁是支持重进入的独占锁。 5.无锁VS偏向锁VS轻量级锁VS重量级锁 synchronized关键字实现同步的基础是每个对象都是一个锁，它依靠对象头存储锁。 无锁、偏向锁、轻量级锁、重量级锁都是专门针对synchronized关键字设计的、级别从低到高的4种状态。 注意： 锁状态只能升级，不能降级。 对象头中的第一个字宽叫做Mark Word，用于存储对象的hashCode、分代年龄、锁等信息。 其中最后2 bit的标志位，用于标记锁的状态。根据标志位的不同，可以有如下几种状态： 无锁 不对资源进行锁定，所有的线程都可以访问并修改同一资源，但同一时刻只有一个线程能修改成功。 无锁的修改操作依靠循环实现： 如果没有争用，修改成功并退出循环；否则，循环尝试修改操作，直到成功。 无锁无法全面代替有锁，但在某些场景下具有非常高的性能。 无锁的经典实现： CAS操作。 偏向锁 出现的原因： 在无竞争的情况下，同一线程可能多次进入同一个同步块，即多次获取同一个锁。 如果进入和退出同步块都使用CAS操作来加锁和解锁，则会消耗一定的资源。 于是通过CAS操作将线程ID存储到Mark Word中，线程再次进入或退出同步块时，直接检查Mark Word中是否存储指向当前线程的偏向锁。 如果存储了，则直接进入或退出同步块。 偏向锁可以在无竞争的情况下，尽量减少不必要的轻量级锁执行路径。轻量级锁的加锁和解锁都需要CAS操作，而偏向锁只有将线程ID存储到Mark Word中时才执行一次CAS操作。 偏向锁的释放： 当有其他线程竞争偏向锁时，持有偏向锁的线程会释放锁偏向锁。 释放时，会根据锁对象是否处于锁定状态而恢复到不同的状态。 如果锁对象处于未锁定状态，撤销偏向后恢复到无锁的状态（0 + 01 ）；如果锁对象处于锁定状态，撤销偏向后恢复到轻量级锁的状态（00）。 偏向锁在JDK1.6及以后，默认是启用的，即-XX:+UseBiasedLocking。可以通过-XX:-UseBiasedLocking关闭偏向锁。 ​偏向锁使用了一种等待竞争出现才会释放锁的机制。所以当其他线程尝试获取偏向锁时，持有偏向锁的线程才会释放锁。但是偏向锁的撤销需要等到全局安全点(就是当前线程没有正在执行的字节码)。它会首先暂停拥有偏向锁的线程，让你后检查持有偏向锁的线程是否活着。如果线程不处于活动状态，直接将对象头设置为无锁状态。如果线程活着，JVM会遍历栈帧中的锁记录，栈帧中的锁记录和对象头要么偏向于其他线程，要么恢复到无锁状态或者标记对象不适合作为偏向锁。 轻量级锁 多个线程竞争同步资源时，没有获取到资源的线程自旋等待锁的释放。 加锁过程： 线程进入同步块时，如果同步对象处于无锁状态（0 + 01），JVM 首先在当前线程的栈帧中开辟一块叫做锁记录（Lock Record）的空间，用于存储同步对象的Mark Word的拷贝。这个拷贝加了一个前缀，叫Displaced Mark Word。 然后通过CAS操作将同步对象的Mark Word更新为指向Lock Record的指针，并将Lock Record里的owner指针指向同步对象的Mark Word。 如果这个更新动作成功，则当前线程拥有了该对象的锁，Mark Word中的标志位更新为00，表示对象处于轻量级锁定状态。 如果更新动作失败，JVM首先会检查同步对象的Mark Word是否指向当前线程的栈帧。如果是，说明当前线程已经持有了该对象的锁，可以直接进入同步块继续执行；否则，说明存在多线程竞争锁。 轻量级锁升级为重量级锁 若当前只有一个线程在等待，则通过自旋进行等待。自旋超过一定的次数，轻量级锁升级为重量级锁。 若一个线程持有锁，一个线程自旋等待锁，又有第三个线程想要获取锁，轻量级锁升级为重量级锁。 锁的释放： 通过CAS操作，将Lock Record中的Displaced Mark Word与对象中的Mark Word进行替换。 替换成功，同步状态完成；替换失败，说明有其他线程尝试获取过该锁，释放锁的同时需要唤醒被挂起的线程。 重量级锁 多线程竞争同步资源时，没有获取到资源的线程阻塞等待锁的释放。 轻量级锁升级为重量级锁，锁的标志位变成10，Mark Word中存储的是指向重量级锁的指针。 所有等待锁的线程都会进入阻塞状态。 锁升级过程锁升级的顺序为： 无锁 -&gt; 偏向锁 -&gt; 轻量级锁 -&gt; 重量级锁，且锁升级的顺序是不可逆的。 线程第一次获取锁获时锁的状态为偏向锁，如果下次还是这个线程获取锁，则锁的状态不变，否则会升级为CAS轻量级锁；如果还有线程竞争获取锁，如果线程获取到了轻量级锁没啥事了，如果没获取到会自旋，自旋期间获取到了锁没啥事，超过了10次还没获取到锁，锁就升级为重量级的锁，此时如果其他线程没获取到重量级锁，就会被阻塞等待唤起，此时效率就低了。 升级图解 什么是锁降级？一个线程执行写操作，先获取写锁，再获取读锁，完成写操作后先释放写锁，接下来的程序里可能要依赖写操作后的变量值，待程序全部执行完后再释放读锁。先释放了写锁，只剩下了读锁，称之为“锁降级”。 为什么需要锁降级？一句话：为了保证数据可见性。假设线程A修改了数据，释放了写锁，这个时候线程T获得了写锁，修改了数据，然后也释放了写锁，线程A读取数据的时候，读到的是线程T修改的，并不是线程A自己修改的，那么在使用修改后的数据时，就会忽略线程A之前的修改结果。因此通过锁降级来保证数据每次修改后的可见性。 6. 自旋锁与自适应自旋锁 自旋锁： 阻塞或唤醒一个线程都需要从用户态切换到内核态去完成，会对性能造成很大影响。 有时一个线程持有锁的时间很短，如果在很短的时间内让后续获取锁的线程都进入阻塞态，这是很不值得。 可以让后续线程持有CPU时间等待一会，这个等待需要执行忙循环（自旋） 来实现。 自旋等待的时间由自旋次数来衡量，默认为10，可以使用-XX:PreBlockSpin来进行设置。 如果在自旋等待中，持有锁的线程释放该锁，当前线程可以不必阻塞直接获取同步资源。 如果超过自旋次数仍未获取成功，则使用传统的方法将其阻塞。 自旋锁的实现原理： 循环的CAS操作 自旋锁的缺点： 自旋锁虽然避免了线程的切换开销，但是会占用CPU时间。 如果每个等待获取锁的线程总是自旋规定的次数，却又没有等到锁的释放，这样就白白浪费了CPU时间。 自旋锁在JDK1.4.2中引入，默认是关闭的；在JDK1.6中变成默认开启，并为了解决自旋锁中浪费CPU资源的问题，而引入了自适应自旋锁。 自适应自旋锁： 自适应意味着自旋的次数不再固定，而是根据上一次在同一个锁自旋的次数和锁的拥有者的状态来决定。 如果在同一个锁对象上自旋刚刚成功获取过锁，并持有锁的线程处于运行状态，则可以认为这一次自旋也很可能成功，允许它自旋更长的时间。 如果在一个锁上，自旋很少成功，则下一次可以省略自旋过程，直接阻塞线程，避免浪费处理器资源。 一些问题轻量级锁一定比重量级锁快吗？在回答这个问题之前，我们先来了解一下：什么是轻量级锁？什么是重量级锁？ 锁概念轻量级锁是 JDK 1.6 新增的概念，是相对于传统的重量级锁而已的一种状态，在 JDK 1.5 时，synchronized 是需要通过操作系统自身的互斥量（mutex lock）来实现，然而这种实现方式需要通过用户态与和核心态的切换来实现，但这个切换的过程会带来很大的性能开销，所以在 JDK 1.6 就引入了轻量级锁来避免此问题的发生。 轻量级锁执行过程再讲轻量级锁执行过程之前，要先从虚拟机的对象头开始说起，HotSpot 的对象头（Object Header）分为两部分： Mark Word 区域，用于存储对象自身的运行时数据，如哈希码（HashCode）、GC 分带年龄等； 用于存储指向方法区对象类型数据的指针（如果是数组对象的话，还有一个存储数组长度的额外信息）。 Mark Word 在 32 位系统中，有 32bit 空间，其中： 25bit 用来存储 HashCode； 4bit 用来存储对象的分带年龄； 2bit 用来存储锁标志位，01=可偏向锁、00=轻量级锁、10=重量级锁； 1bit 固定为 0。 再说会轻量级锁的执行过程，在代码进入同步块的时候，如果此对象没有被线程所占用，虚拟机会先将此线程的栈帧拷贝一份存储在当前对象的 Lock Record (锁记录) 区域中。 然后虚拟机再使用 CAS (Compare and Swap, 比较并交换) 将本线程的 Mark Word 更新为指向对象 Lock Record 区域的指针，如果更新成功，则表示这个线程拥有了该对象，轻量级锁添加成功，如果更新失败，虚拟机会先检查对象 Mark Word 是否指向了当前线程的线帧，如果是则表明此线程已经拥有了此锁，如果不是，则表明该锁已经被其他线程占用了。如果有两条以上的线程在争抢死锁，那么锁就会膨胀为重量锁，Mark Word 中存储的就是指向重量级锁的互斥量指针，后面等待锁的线程也会进入阻塞状态。 从以上的过程，我们可以看出轻量级锁可以理解为是通过 CAS 实现的，理想的情况下是整个同步周期内不存在锁竞争，那么轻量锁可以有效的提高程序的同步性能，然而，如果情况相反，轻量级锁不但要承担 CAS 的开销还要承担互斥量的开销，这种情况下轻量级锁就会比重量级锁更慢，这就是我们本文的答案。 总结轻量级锁不是在任何情况下都比重量级锁快的，要看同步块执行期间有没有多个线程抢占资源的情况，如果有，那么轻量级线程要承担 CAS + 互斥量锁的性能消耗，就会比重量锁执行的更慢。 打开偏向锁是否效率一定会提升?为什么?偏向锁（不太需要竞争的，一般一个线程）未必会提高，尤其是当你知道一定会有大量线程去竞争的时候。打开偏向锁偏向锁还有一个锁撤销的过程（把ID撕下来）。 为什么要延迟4s？偏向锁默认是在JVM启动4s后再初始化偏向锁，可用如下参数修改启动时间，设为0则表示立即启用。之所以这么设计是因为JVM启动的时候，如果立即启动偏向，有可能会因为线程竞争太激烈导致产生太多安全点挂起。-XX:BiasedLockingStartupDelay=0 参考Java高并发之锁总结、常见的面试问题Java并发编程五 同步之ReentrantLock与Condition","categories":[{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"锁","slug":"锁","permalink":"https://xmmarlowe.github.io/tags/%E9%94%81/"}],"author":"Marlowe"},{"title":"Java中的集合类及关系图","slug":"Java/Java中的集合类及关系图","date":"2020-04-11T02:07:50.000Z","updated":"2021-04-23T14:22:30.557Z","comments":true,"path":"2020/04/11/Java/Java中的集合类及关系图/","link":"","permalink":"https://xmmarlowe.github.io/2020/04/11/Java/Java%E4%B8%AD%E7%9A%84%E9%9B%86%E5%90%88%E7%B1%BB%E5%8F%8A%E5%85%B3%E7%B3%BB%E5%9B%BE/","excerpt":"","text":"List 和 Set 继承自 Collection 接口。 Set 无序不允许元素重复。HashSet 和 TreeSet 是两个主要的实现类。 List 有序且允许元素重复。ArrayList、LinkedList 和 Vector 是三个主要的实现 类。 Map 也属于集合系统，但和 Collection 接口没关系。Map 是 key 对 value 的映 射集合，其中 key 列就是一个集合。key 不能重复，但是 value 可以重复。 HashMap、TreeMap 和 Hashtable 是三个主要的实现类。 SortedSet 和 SortedMap 接口对元素按指定规则排序，SortedMap 是对 key 列 进行排序。","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"集合类","slug":"集合类","permalink":"https://xmmarlowe.github.io/tags/%E9%9B%86%E5%90%88%E7%B1%BB/"},{"name":"类图","slug":"类图","permalink":"https://xmmarlowe.github.io/tags/%E7%B1%BB%E5%9B%BE/"}],"author":"Marlowe"},{"title":"抽象类和接口的区别","slug":"Java/抽象类和接口的区别","date":"2020-04-11T01:51:44.000Z","updated":"2021-04-23T14:21:00.865Z","comments":true,"path":"2020/04/11/Java/抽象类和接口的区别/","link":"","permalink":"https://xmmarlowe.github.io/2020/04/11/Java/%E6%8A%BD%E8%B1%A1%E7%B1%BB%E5%92%8C%E6%8E%A5%E5%8F%A3%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"抽象类抽象类是用来捕捉子类的通用特性的 。它不能被实例化，只能被用作子类的超类。抽象类是被用来创建继承层级里子类的模板。 抽象类不能被实例化只能被继承； 包含抽象方法的一定是抽象类，但是抽象类不一定含有抽象方法； 抽象类中的抽象方法的修饰符只能为public或者protected，默认为public； 一个子类继承一个抽象类，则子类必须实现父类抽象方法，否则子类也必须定义为抽象类； 抽象类可以包含属性、方法、构造方法，但是构造方法不能用于实例化，主要用途是被子类调用。 接口接口是抽象方法的集合。如果一个类实现了某个接口，那么它就继承了这个接口的抽象方法。这就像契约模式，如果实现了这个接口，那么就必须确保使用这些方法。接口只是一种形式，接口自身不能做任何事情。 接口可以包含变量、方法；变量被隐士指定为public static final，方法被隐士指定为public abstract（JDK1.8之前）； 接口支持多继承，即一个接口可以extends多个接口，间接的解决了Java中类的单继承问题； 一个类可以实现多个接口； JDK1.8中对接口增加了新的特性：（1）、默认方法（default method）：JDK 1.8允许给接口添加非抽象的方法实现，但必须使用default关键字修饰；定义了default的方法可以不被实现子类所实现，但只能被实现子类的对象调用；如果子类实现了多个接口，并且这些接口包含一样的默认方法，则子类必须重写默认方法；（2）、静态方法（static method）：JDK 1.8中允许使用static关键字修饰一个方法，并提供实现，称为接口静态方法。接口静态方法只能通过接口调用（接口名.静态方法名）。 相同点 都不能被实例化 接口的实现类或抽象类的子类都只有实现了接口或抽象类中的方法后才能实例化。 不同点 接口只有定义，不能有方法的实现，java 1.8中可以定义default方法体，而抽象类可以有定义与实现，方法可在抽象类中实现。 实现接口的关键字为implements，继承抽象类的关键字为extends。一个类可以实现多个接口，但一个类只能继承一个抽象类。所以，使用接口可以间接地实现多重继承。 接口强调特定功能的实现，而抽象类强调所属关系。 接口成员变量默认为public static final，必须赋初值，不能被修改；其所有的成员方法都是public、abstract的。抽象类中成员变量默认default，可在子类中被重新定义，也可被重新赋值；抽象方法被abstract修饰，不能被private、static、synchronized和native等修饰，必须以分号结尾，不带花括号。 什么时候使用抽象类和接口 如果你拥有一些方法并且想让它们中的一些有默认实现，那么使用抽象类吧。 如果你想实现多重继承，那么你必须使用接口。由于Java不支持多继承，子类不能够继承多个类，但可以实现多个接口。因此你就可以使用接口来解决它。 如果基本功能在不断改变，那么就需要使用抽象类。如果不断改变基本功能并且使用接口，那么就需要改变所有实现了该接口的类。","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"抽象类","slug":"抽象类","permalink":"https://xmmarlowe.github.io/tags/%E6%8A%BD%E8%B1%A1%E7%B1%BB/"},{"name":"接口","slug":"接口","permalink":"https://xmmarlowe.github.io/tags/%E6%8E%A5%E5%8F%A3/"}],"author":"Marlowe"},{"title":"JVM 垃圾收集器","slug":"Java/JVM-垃圾收集器","date":"2020-04-10T14:39:51.000Z","updated":"2021-05-04T02:49:55.048Z","comments":true,"path":"2020/04/10/Java/JVM-垃圾收集器/","link":"","permalink":"https://xmmarlowe.github.io/2020/04/10/Java/JVM-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/","excerpt":"","text":"不同的垃圾收集器下图是HotSpot虚拟机1.6版Undate 22d的所有收集器： JVM 垃圾收集器 注： 如果两个收集器之间存在连线，就说明它们可以搭配使用。 七种垃圾收集器:1. Serial（串行GC）-复制Serial是一个新生代收集器，曾经是JDK1.3.1之前新生代唯一的垃圾收集器。采用复制算法。 Serial是一个单线程收集器，会使用一个CPU、一条线程去完成垃圾回收，并且在进行垃圾回收的时候必须暂停其他所有的工作线程，直到垃圾收集结束（这被称为“Stop The World”）。 Serial收集器仍然是虚拟机运行在Client模式下的默认新生代收集器。它的优点是：简单而高效（与其他收集器的单线程比）。对于限定单个CPU的环境来说，Seria收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。 Serial/Serial Old收集器运行示意图如下： 2. ParNew（并行GC）复制ParNew收集器也是一个新生代收集器，是Serial收集器的多线程版本。也采用复制算法。除了使用多条线程进行垃圾回收之外，其他行为与Serial收集器一样。 ParNew收集器在单CPU的环境中效果不会比Serial收集器更好，甚至由于存在线程交互的开销，性能可能会更差。 ParNew收集器在多CPU环境下是更高效的，它默认开启的收集线程数与CPU的数量相同。 ParNew/Serial Old收集器运行示意图如下： 3. Parallel Scavenge（并行回收GC）标记-复制Parallel Scavenge收集器也是一个新生代收集器。也是用复制算法，同时也是并行的多线程收集器。 Parallel Scavenge收集器的特点是关注点与其他收集器不同，Parallel Scavenge的关注点是“吞吐量（Throughput）”，吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即吞吐量=运行用户代码时间/(运行用户代码时间+垃圾收集时间)。其他收集器关注的是“垃圾收集时的停顿时间”。 停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户的体验； 而高吞吐量则可以最高效率地利用CPU时间，尽快地完成程序的运算任务，适合在后台运算不需要太多交互的任务。 Parallel Scavenge收集器可以通过参数控制最大垃圾收集停顿时间和吞吐量大小。注意：GC停顿时间缩短是以牺牲吞吐量和新生代空间来换取的。因为：系统把新生代空间调小一些，收集的速度就快一些，也就导致垃圾收集要更频繁（空间不够用），比如原来10秒收集一次，一次停顿100毫秒，现在5秒收集一次，每次停顿70毫秒，停顿时间的确在下降，但吞吐量也降下来了。 此外Parallel Scavenge收集器可通过参数开关控制GC自动动态调整参数来提供最合适的停顿时间或最大吞吐量，这种调节方式称为GC自适应的调节策略（GC Ergonomics）。 自适应调节策略也是Parallel Scavenge收集器与ParNew收集器的一个重要区别。 注： Parallel Scavenge收集器无法与CMS收集器配合使用。（原因是Parallel Scavenge收集器及G1收集器都没有使用传统的GC收集器代码框架，而是另外独立实现的） 4. Serial Old（MSC）（串行GC）标记-整理Serial Old是Serial收集器的老年代版本。同样是一个单线程收集器，使用“标记-整理”算法。这个收集器的主要意义是被Client模式下的虚拟机使用。在Server模式下，它主要还有两大用途：一个是在JDK1.5及以前的版本中与Parallel Scavenge收集器搭配使用，另外一个就是作为CMS收集器的后备预案，在并发收集发生Concurrent Mode Failure的时候使用。 5. CMS(Concurrent Mark Sweep)（并发GC）标记-清除CMS(Concurrent Mark Sweep)收集器是一种以获取最短回收停顿时间为目标的收集器。最符合重视服务响应速度。希望系统停顿时间最短的应用。 该收集器是基于“标记-清除”算法实现的。过程分为4个步骤： 初始标记（CMS initial mark）、并发标记（CMS concurrent mark）、重新标记（CMS remark）、并发清除（CMS concurrent sweep）. 其中初始标记、重新标记这两个步骤仍然需要“Stop The World”。初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，并发标记阶段就是进行GC Roots Tracing的过程，而重新标记阶段则是为了修正并发标记期间，因用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短。 核心思想，就是将STW打散，让-部分GC线程与用户线程并发执行。整个GC过程分为四个阶段 初始标记阶段: STW只标记出根对象直接引用的对象。 并发标记:继续标记其他对象，与应用程序是并发执行。 重新标记: STW 对并发执行阶段的对象进行重新标记。 并发清除:并行。将产生的垃圾清除。清除过程中，应用程序又会不断的产生新的垃圾，叫做浮动垃圾。这些垃圾就要留到下一次GC过程中清除。 Concurrent Mark Sweep收集器运行示意图如下： 6. Parallel Old（并行GC）标记-整理Parallel Old是Parallel Scavenge收集器的老年代版本。使用多线程和“标记-整理”算法。单线程的老年代Serial Old收集器在服务端性能比较差，即使新生代使用了Parallel Scavenge收集器也未必能在整体应用上获得吞吐量最大化的效果。 在注重吞吐量及CPU资源敏感的场合，都可以优先考虑Parallel Scavenge收集器加上Parallel Old收集器组合使用。 Parallel Scavenge/ Parallel Old收集器运行示意图如下： 7. G1(Garbage First)（JDK1.7update14才可以正式商用）G1收集器是一款面向服务端应用的收集器，它能充分利用多CPU、多核环境。因此它是一款并行与并发收集器，并且它能建立可预测的停顿时间模型。 在G1中分代概念仍然保留。虽然G1不需要和其他收集器配合，可以独立管理GC堆，但它能够采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次GC的旧对象，以获取更好的收集效果。 G1将内存分成多个大小相等的独立区域，虽然还保留着新生代和老年代的概念，但是新生代和老年代不再是物理隔离的，它们都是一部分Region(不需要连续)的集合。 GC分四个阶段 初始标记标记出GCRoot直接引用的对象。STW 标记Region,通过RSet标记出上-个阶段标记的Region引用到的Old区Region。 并发标记阶段:跟CMS的步骤是差不多的。只是遍历的范围不再是整个Old区，而只需要遍历第二步标记出来的Region。 重新标记:跟CMS中的重 新标记过程是差不多的。 垃圾清理:与CMS不同的是，G1可以采用拷贝算法，直接将整个Region中的对象拷贝到另一个Region。而这个阶段，G1只选择垃圾较多的Region来清理，并不是完全清理。 G1收集器的优点： 1）空间整合： G1从整体看是基于标记-整理算法实现的收集器，从局部看是基于复制算法。这两种算法都意味着G1运行期间不会产生大量内存空间碎片。 2）可预测的停顿： 降低停顿时间是G1和CMS共同关注的，但G1能建立可预测的停顿时间模型，让使用者明确指定在一个长度为M毫秒的时间片段内，GC的时间不得超过N毫秒。 3）有计划的垃圾回收： G1可以有计划的在Java堆中进行全区域的垃圾收集。G1跟踪各个Region里面的垃圾堆的价值大小(回收所获得的空间大小，以及回收所需要的时间)，在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的Region。这就是Garbage-First的由来。 说明： 1~3用于年轻代垃圾回收：年轻代的垃圾回收称为minor GC 4~6用于年老代垃圾回收（当然也可以用于方法区的回收）：年老代的垃圾回收称为full GC G1独立完成”分代垃圾回收” 注意： 并行与并发 并行：多条垃圾回收线程同时操作 并发：垃圾回收线程与用户线程一起操作 一些问题什么是STW？STW: Stop-The-World。是在垃圾回收算法执行过程当中，需要将JVM内存冻结的一种状态。在STW状态下，JAVA的所有线程都是停止执行的-GC线程除外，native方法可以执行， 但是，不能与JVM交互。GC各种算法优化的重点，就是减少STW,同时这也是JVM调优的重点。 什么是三色标记？CMS的核心算法就是三色标记。 三色标记:是一种逻辑上的抽象。将每个内存对象分成三种颜色。黑色: 表示自己和成员变量都已经标记完毕。灰色: 自己标记完了，但是成员变量还没有完全标记完。白色: 自己未标记完。 CMS通过增量标记increment update的方式来解决漏标的问题。 参考JVM几种垃圾回收器介绍读《深入理解java虚拟机》（三）垃圾回收器","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://xmmarlowe.github.io/tags/JVM/"},{"name":"GC","slug":"GC","permalink":"https://xmmarlowe.github.io/tags/GC/"}],"author":"Marlowe"},{"title":"finally中的代码真的一定会执行吗？","slug":"Java/finally中的代码真的一定会执行吗？","date":"2020-04-10T14:17:23.000Z","updated":"2021-04-23T14:21:36.777Z","comments":true,"path":"2020/04/10/Java/finally中的代码真的一定会执行吗？/","link":"","permalink":"https://xmmarlowe.github.io/2020/04/10/Java/finally%E4%B8%AD%E7%9A%84%E4%BB%A3%E7%A0%81%E7%9C%9F%E7%9A%84%E4%B8%80%E5%AE%9A%E4%BC%9A%E6%89%A7%E8%A1%8C%E5%90%97%EF%BC%9F/","excerpt":"finally中的代码在某些情况下不一定能执行…","text":"finally中的代码在某些情况下不一定能执行… 在执行异常处理代码之前程序已经返回 1234567891011public static boolean getTrue(boolean flag) &#123; if (flag) &#123; return flag; &#125; try &#123; flag = true; return flag; &#125; finally &#123; System.out.println(&quot;我是一定会执行的代码？&quot;); &#125; &#125; 如果上述代码传入的参数为true那finally中的代码就不会执行了。 在执行异常处理代码之前程序抛出异常 123456789public static boolean getTrue(boolean flag) &#123; int i = 1/0; try &#123; flag = true; return flag; &#125; finally &#123; System.out.println(&quot;我是一定会执行的代码？&quot;); &#125; &#125; 这里会抛出异常，finally中的代码同样不会执行。原理同1中差不多，只有与 finally 相对应的 try 语句块得到执行的情况下，finally 语句块才会执行。就算try语句执行了finally中的代码一定会执行吗，答案是no，请看下面两种情况。 finally之前执行了System.exit() 123456789public static boolean getTrue(boolean flag) &#123; try &#123; flag = true; System.exit(1); return flag; &#125; finally &#123; System.out.println(&quot;我是一定会执行的代码？&quot;); &#125; &#125; System.exit是用于结束当前正在运行中的java虚拟机，参数为0代表程序正常退出，非0代表程序非正常退出。道理也很简单整个程序都结束了，拿什么来执行finally呢。 所有后台线程终止时，后台线程会突然终止 12345678910111213141516public static void main(String[] args) &#123; Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(5); &#125; catch (Exception e) &#123; &#125;finally&#123; System.out.println(&quot;我是一定会执行的代码？&quot;); &#125; &#125; &#125;); t1.setDaemon(true);//设置t1为后台线程 t1.start(); System.out.println(&quot;我是主线程中的代码,主线程是非后台线程。&quot;); &#125; 上述代码，后台线程t1中有finally块，但在执行前，主线程终止了，导致后台线程立即终止，故finally块无法执行 总结： 与finally相对应的try语句得到执行的情况下，finally才有可能执行。 finally执行前，程序或线程终止，finally不会执行。","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"finally","slug":"finally","permalink":"https://xmmarlowe.github.io/tags/finally/"}],"author":"Marlowe"},{"title":"Java对象的创建过程","slug":"Java/Java对象的创建过程","date":"2020-04-09T07:30:21.000Z","updated":"2021-04-23T14:22:14.341Z","comments":true,"path":"2020/04/09/Java/Java对象的创建过程/","link":"","permalink":"https://xmmarlowe.github.io/2020/04/09/Java/Java%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%88%9B%E5%BB%BA%E8%BF%87%E7%A8%8B/","excerpt":"","text":"对象的创建 Step1:类加载检查虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 Step2:分配内存在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。分配方式有 “指针碰撞” 和 “空闲列表” 两种，选择哪种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 内存分配的两种方式：（补充内容，需要掌握） 选择以上两种方式中的哪一种，取决于 Java 堆内存是否规整。而 Java 堆内存是否规整，取决于 GC 收集器的算法是”标记-清除”，还是”标记-整理”（也称作”标记-压缩”），值得注意的是，复制算法内存也是规整的. 内存分配并发问题（补充内容，需要掌握） 在创建对象的时候有一个很重要的问题，就是线程安全，因为在实际开发过程中，创建对象是很频繁的事情，作为虚拟机来说，必须要保证线程是安全的，通常来讲，虚拟机采用两种方式来保证线程安全： CAS+失败重试： CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。 TLAB： 为每一个线程预先在 Eden 区分配一块儿内存，JVM 在给线程中的对象分配内存时，首先在 TLAB 分配，当对象大于 TLAB 中的剩余内存或 TLAB 的内存已用尽时，再采用上述的 CAS 进行内存分配 Step3:初始化零值内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 Step4:设置对象头初始化零值完成之后，虚拟机要对对象进行必要的设置， 例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。 Step5:执行 init 方法在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，&lt;init&gt; 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 &lt;init&gt; 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。 参考Java对象的创建过程","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"对象","slug":"对象","permalink":"https://xmmarlowe.github.io/tags/%E5%AF%B9%E8%B1%A1/"}],"author":"Marlowe"},{"title":"JVM垃圾回收算法","slug":"Java/JVM垃圾回收算法","date":"2020-04-09T07:13:36.000Z","updated":"2021-04-25T08:14:02.108Z","comments":true,"path":"2020/04/09/Java/JVM垃圾回收算法/","link":"","permalink":"https://xmmarlowe.github.io/2020/04/09/Java/JVM%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%97%E6%B3%95/","excerpt":"","text":"两个概念：新生代： 存放生命周期较短的对象的区域。老年代： 存放生命周期较长的对象的区域。 相同点： 都在Java堆上。 1. 标记-清除算法执行步骤： 标记：遍历内存区域，对需要回收的对象打上标记。 清除：再次遍历内存，对已经标记过的内存进行回收。 图解： 缺点： 效率问题；遍历了两次内存空间（第一次标记，第二次清除）。 空间问题：容易产生大量内存碎片，当再需要一块比较大的内存时，无法找到一块满足要求的，因而不得不再次出发GC。 2. 复制算法将内存划分为等大的两块，每次只使用其中的一块。当一块用完了，触发GC时，将该块中存活的对象复制到另一块区域，然后一次性清理掉这块没有用的内存。下次触发GC时将那块中存活的的又复制到这块，然后抹掉那块，循环往复。 图解 优点 相对于标记–清理算法解决了内存的碎片化问题。 效率更高（清理内存时，记住首尾地址，一次性抹掉）。 缺点： 内存利用率不高，每次只能使用一半内存。 改进 研究表明，新生代中的对象大都是“朝生夕死”的，即生命周期非常短而且对象活得越久则越难被回收。在发生GC时，需要回收的对象特别多，存活的特别少，因此需要搬移到另一块内存的对象非常少，所以不需要1：1划分内存空间。而是将整个新生代按照8 ： 1 ： 1的比例划分为三块，最大的称为Eden（伊甸园）区，较小的两块分别称为To Survivor和From Survivor。 首次GC时，只需要将Eden存活的对象复制到To。然后将Eden区整体回收。再次GC时，将Eden和To存活的复制到From，循环往复这个过程。这样每次新生代中可用的内存就占整个新生代的90%，大大提高了内存利用率。 但不能保证每次存活的对象就永远少于新生代整体的10%，此时复制过去是存不下的，因此这里会用到另一块内存，称为老年代，进行分配担保，将对象存储到老年代。若还不够，就会抛出OOM。 老年代：存放新生代中经过多次回收仍然存活的对象（默认15次）。 3. 标记-整理算法因为前面的复制算法当对象的存活率比较高时，这样一直复制过来，复制过去，没啥意义，且浪费时间。所以针对老年代提出了“标记整理”算法。 执行步骤： 标记：对需要回收的进行标记 整理：让存活的对象，向内存的一端移动，然后直接清理掉没有用的内存。 图解： 4. 分代收集算法当前大多商用虚拟机都采用这种分代收集算法，这个算法并没有新的内容，只是根据对象的存活的时间的长短，将内存分为了新生代和老年代，这样就可以针对不同的区域，采取对应的算法。如： 新生代，每次都有大量对象死亡，有老年代作为内存担保，采取复制算法。 老年代，对象存活时间长，采用标记整理，或者标记清理算法都可。 为什么采用分代收集算法？ 这是基于两个共识 绝大多数对象都是朝生夕死的 熬过越多次垃圾收集过程的对象就越难以消亡 这两个分代假说共同奠定了多款常用的垃圾收集器的一致的设计原则:收集器应该将Java堆划分出不同的区域，然后将回收对象依据其年龄(年龄即对象熬过垃圾收集过程的次数)分配到不同的区域之中存储。显而易见，如果一个区域中大多数对象都是朝生夕灭，难以熬过垃圾收集过程的话，那么把它们集中放在一起，每次回收时只关注如何保留少量存活而不是去标记那些大量将要被回收的对象，就能以较低代价回收到大量的空间;如果剩下的都是难以消亡的对象，那把它们集中放在一块，虚拟机便可以使用较低的频率来回收这个区域，这就同时兼顾了垃圾收集的时间开销和内存的空间有效利用。 在Java堆划分出不同的区域之后，垃圾收集器才可以每次只回收其中某一个或者某些部分的区域 ——因而才有了“Minor GC”“Major GC”“Full GC”这样的回收类型的划分;也才能够针对不同的区域安 排与里面存储对象存亡特征相匹配的垃圾收集算法——因而发展出了“标记-复制算法”“标记-清除算 法”“标记-整理算法”等针对性的垃圾收集算法。这里笔者提前提及了一些新的名词，它们都是本章的 重要角色，稍后都会逐一登场，现在读者只需要知道，这一切的出现都始于分代收集理论。 MinorGC和Majaor/Full GC的区别 MinorGC：发生在新生代的垃圾回收，因为新生代的特点，MinorGC非常频繁，且回收速度比较快，每次回收的量也很大。 Majaor/Full GC：发生在老年代的垃圾回收，也称MajorGC，速度比较慢，相对于MinorGC慢10倍左右。进行一次FullGC通常会伴有多次多次MinorGC。 参考JVM垃圾回收算法 java为什么要分代回收_JVM为什么要分代回收","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://xmmarlowe.github.io/tags/JVM/"},{"name":"垃圾回收","slug":"垃圾回收","permalink":"https://xmmarlowe.github.io/tags/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"}],"author":"Marlowe"},{"title":"Java类加载过程","slug":"Java/Java类加载过程","date":"2020-03-31T13:54:49.000Z","updated":"2021-05-03T13:35:14.394Z","comments":true,"path":"2020/03/31/Java/Java类加载过程/","link":"","permalink":"https://xmmarlowe.github.io/2020/03/31/Java/Java%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B/","excerpt":"","text":"类的生命周期一个类的完整生命周期如下： 类加载过程系统加载 Class 类型的文件主要三步:加载-&gt;连接-&gt;初始化。连接过程又可分为三步:验证-&gt;准备-&gt;解析。 加载类加载过程的第一步，主要完成下面3件事情： 通过全类名获取定义此类的二进制字节流 将字节流所代表的静态存储结构转换为方法区的运行时数据结构 在内存中生成一个代表该类的 Class 对象,作为方法区这些数据的访问入口 一个非数组类的加载阶段（加载阶段获取类的二进制字节流的动作）是可控性最强的阶段，这一步我们可以去完成还可以自定义类加载器去控制字节流的获取方式（重写一个类加载器的 loadClass() 方法）。数组类型不通过类加载器创建，它由 Java 虚拟机直接创建。 验证 准备准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配。对于该阶段有以下几点需要注意： 这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在 Java 堆中。 这里所设置的初始值”通常情况”下是数据类型默认的零值（如0、0L、null、false等），比如我们定义了public static int value=111 ，那么 value 变量在准备阶段的初始值就是 0 而不是111（初始化阶段才会赋值）。特殊情况：比如给 value 变量加上了 fianl 关键字public static final int value=111 ，那么准备阶段 value 的值就被赋值为 111。 解析解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用限定符7类符号引用进行。 符号引用就是一组符号来描述目标，可以是任何字面量。直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。在程序实际运行时，只有符号引用是不够的，举个例子：在程序执行方法时，系统需要明确知道这个方法所在的位置。Java 虚拟机为每个类都准备了一张方法表来存放类中所有的方法。当需要调用一个类的方法的时候，只要知道这个方法在方法表中的偏移量就可以直接调用该方法了。通过解析操作符号引用就可以直接转变为目标方法在类中方法表的位置，从而使得方法可以被调用。 综上，解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，也就是得到类或者字段、方法在内存中的指针或者偏移量。 初始化初始化是类加载的最后一步，也是真正执行类中定义的 Java 程序代码(字节码)，初始化阶段是执行初始化方法 &lt;clinit&gt; ()方法的过程。 对于&lt;clinit&gt;（） 方法的调用，虚拟机会自己确保其在多线程环境中的安全性。因为 &lt;clinit&gt;（） 方法是带锁线程安全，所以在多线程环境下进行类初始化的话可能会引起死锁，并且这种死锁很难被发现。 对于初始化阶段，虚拟机严格规范了有且只有5种情况下，必须对类进行初始化(只有主动去使用类才会初始化类)： 当遇到 new 、 getstatic、putstatic或invokestatic 这4条直接码指令时，比如 new 一个类，读取一个静态字段(未被 final 修饰)、或调用一个类的静态方法时。 当jvm执行new指令时会初始化类。即当程序创建一个类的实例对象。 当jvm执行getstatic指令时会初始化类。即程序访问类的静态变量(不是静态常量，常量会被加载到运行时常量池)。 当jvm执行putstatic指令时会初始化类。即程序给类的静态变量赋值。 当jvm执行invokestatic指令时会初始化类。即程序调用类的静态方法。 使用 java.lang.reflect 包的方法对类进行反射调用时如Class.forname(“…”),newInstance()等等。 ，如果类没初始化，需要触发其初始化。 初始化一个类，如果其父类还未初始化，则先触发该父类的初始化。 当虚拟机启动时，用户需要定义一个要执行的主类 (包含 main 方法的那个类)，虚拟机会先初始化这个类。M5. ethodHandle和VarHandle可以看作是轻量级的反射调用机制，而要想使用这2个调用， 就必须先使用findStaticVarHandle来初始化要调用的类。 「补充」 当一个接口中定义了JDK8新加入的默认方法（被default关键字修饰的接口方法）时，如果有这个接口的实现类发生了初始化，那该接口要在其之前被初始化。 卸载卸载类即该类的Class对象被GC。 卸载类需要满足3个要求: 该类的所有的实例对象都已被GC，也就是说堆不存在该类的实例对象。 该类没有在其他任何地方被引用 该类的类加载器的实例已被GC 所以，在JVM生命周期类，由jvm自带的类加载器加载的类是不会被卸载的。但是由我们自定义的类加载器加载的类是可能被卸载的。 只要想通一点就好了，jdk自带的BootstrapClassLoader,ExtClassLoader,AppClassLoader负责加载jdk提供的类，所以它们(类加载器的实例)肯定不会被回收。而我们自定义的类加载器的实例是可以被回收的，所以使用我们自定义加载器加载的类是可以被卸载掉的。 一个对象从加载到JVM,再到被GC清除，都经历了什么过程? 用户创建一一个对象，JVM首先需要到方法区去找对象的类型信息。然后再创建对象。 JVM要实例化一个对象， 首先要在堆当中先创建一个对象。 -&gt; 半初始化状态 对象首先会分配在堆内存中新生代的Eden。然后经过一次Minor GC,对象如果存活，就会进入S区。在后续的每次GC中，如果对象一直存活，就会在S区来回拷贝，每移动一次， 年龄加1。-&gt;多大年龄才会移入老年代?年龄最大15(markword中大小为4bit)， 超过一定年龄后，对象转入老年代。 当方法执行结束后，栈中的指针会先移除掉。 堆中的对象，经过Full GC,就会被标记为垃圾，然后被GC线程清理掉。 参考类的生命周期","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://xmmarlowe.github.io/tags/JVM/"}],"author":"Marlowe"},{"title":"Java8四大函数式接口","slug":"Java/Java8四大函数式接口","date":"2020-03-25T05:00:01.000Z","updated":"2021-04-23T14:22:10.954Z","comments":true,"path":"2020/03/25/Java/Java8四大函数式接口/","link":"","permalink":"https://xmmarlowe.github.io/2020/03/25/Java/Java8%E5%9B%9B%E5%A4%A7%E5%87%BD%E6%95%B0%E5%BC%8F%E6%8E%A5%E5%8F%A3/","excerpt":"只有一个方法的接口叫做函数式接口。Function、Predicate、Consumer、Supplier","text":"只有一个方法的接口叫做函数式接口。Function、Predicate、Consumer、Supplier 函数式接口的作用：简化编程模型，在新版本的框架底层大量应用！ 12345@FunctionalInterfacepublic interface Runnable &#123; public abstract void run();&#125;// foreach（消费者类型的函数式接口） Function函数型接口：有一个输入参数，有一个输出。 源码： 1234567891011@FunctionalInterfacepublic interface Function&lt;T, R&gt; &#123; /** * Applies this function to the given argument. * * @param t the function argument * @return the function result */ R apply(T t);&#125; 代码示例： 123456789101112131415161718// 只要是函数型接口，可以用lambda表达式简化public static void main(String[] args) &#123; Function function = new Function&lt;String, String&gt;() &#123; @Override public String apply(String string) &#123; return string; &#125; &#125;; System.out.println(function.apply(&quot;hello&quot;)); &#125;// 简化写法public static void main(String[] args) &#123; Function&lt;String, String&gt; function = (str) -&gt; &#123; return str; &#125;; System.out.println(function.apply(&quot;hello&quot;)); &#125; 结果： 1hello Predicate断定型接口：有一个输入参数，返回值只能是布尔值。 源码： 123456789101112@FunctionalInterfacepublic interface Predicate&lt;T&gt; &#123; /** * Evaluates this predicate on the given argument. * * @param t the input argument * @return &#123;@code true&#125; if the input argument matches the predicate, * otherwise &#123;@code false&#125; */ boolean test(T t);&#125; 代码示例： 12345678910/** * 判断字符串是否为空 * @param args */public static void main(String[] args) &#123; Predicate&lt;String&gt; predicate = (str) -&gt;&#123; return str.isEmpty(); &#125;; System.out.println(predicate.test(&quot;11&quot;));&#125; 结果： 12falsetrue Consumer消费型接口：只有输入，没有返回值。 源码： 12345678910@FunctionalInterfacepublic interface Consumer&lt;T&gt; &#123; /** * Performs this operation on the given argument. * * @param t the input argument */ void accept(T t);&#125; 代码示例： 1234567891011/** * 打印字符串 * * @param args */public static void main(String[] args) &#123; Consumer&lt;String&gt; consumer = str -&gt; &#123; System.out.println(str); &#125;; consumer.accept(&quot;consumer&quot;);&#125; 结果： 1consumer Supplier供给型接口：没有参数，只有返回值。 12345678910@FunctionalInterfacepublic interface Supplier&lt;T&gt; &#123; /** * Gets a result. * * @return a result */ T get();&#125; 代码示例： 1234567891011/** * 返回固定值 1024 * * @param args */public static void main(String[] args) &#123; Supplier&lt;Integer&gt; supplier = () -&gt; &#123; return 1024; &#125;; System.out.println(supplier.get());&#125; 结果： 11024","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"函数式接口","slug":"函数式接口","permalink":"https://xmmarlowe.github.io/tags/%E5%87%BD%E6%95%B0%E5%BC%8F%E6%8E%A5%E5%8F%A3/"}],"author":"Marlowe"},{"title":"JVM GC调优","slug":"Java/JVM-GC调优","date":"2020-03-22T16:46:49.000Z","updated":"2021-05-04T11:30:49.162Z","comments":true,"path":"2020/03/23/Java/JVM-GC调优/","link":"","permalink":"https://xmmarlowe.github.io/2020/03/23/Java/JVM-GC%E8%B0%83%E4%BC%98/","excerpt":"JVM调优入门…","text":"JVM调优入门… JVM调优是必须的吗？GC调优对于java服务是必须的吗？实际上，我感觉80%的java的程序员在实际工作中都没有碰到过GC调优吧，这是因为多数的Java应用不需要在服务器上进行GC优化，多数导致GC问题的Java应用，都不是因为我们参数设置错误，而是代码问题，需要记住一点：GC调优是最后要做的工作。 GC调优的目的可以总结为下面两点： 减少对象晋升到老年代的数量 减少FullGC的执行时间 减少对象晋升到老年代的数量分代垃圾回收是Oracle JVM中回收思想。 我们知道在Eden区创建的对象，在from Survivor 复制到to Survivor区之后，达到一定年龄就进入了老年代(15次)。有些对象因为比较大就直接进入了老年代。在老年代的GC时间相比于年轻代时间更长。因此，减少对象进入老年代可以降低Full GC的频率。 减少FullGC的执行时间Full GC的时间比Minor GC要长。所以如果执行太长时间的Full GC（超过1秒），就会发生超时错误 如果你试着减少老年代的大小来降低Full GC的执行时间，可能会引发OutOfMemoryError或者导致Full GC的频率升高。 如果是通过增加老年代的大小来降低Full GC的频率，执行时间将会增加。 影响GC的参数JVM调优主要用到参数罗列在下面的两张表中。主要分为内存参数和垃圾类型参数。GC优化的过程就是在调试这些参数的过程。 下表是与JVM内存相关的参数： 1.针对JVM堆的设置，一般可以通过-Xms -Xmx限定其最小、最大值，为了防止垃圾收集器在最小、最大之间收缩堆而产生额外的时间，通常把最大、最小设置为相同的值; 2.年轻代和年老代将根据默认的比例（1：2）分配堆内存， 可以通过调整二者之间的比率NewRadio来调整二者之间的大小，也可以针对回收代。 比如年轻代，通过 -XX:newSize -XX:MaxNewSize来设置其绝对大小。同样，为了防止年轻代的堆收缩，我们通常会把-XX:newSize -XX:MaxNewSize设置为同样大小。 一些问题如何进行JVM调优？JVM调优主要就是通过定制VM运行参数来提高JAVA应用程度的运行数据 JVM参数有哪些？JVM参数大致可以分为三类: 标注指令: -开头，这些是所有的HotSpot都支持的参数。可以用java -help打印出来。 非标准指令: -X开头， 这些指令通常是跟特定的HotSpot版本对应的。可以用java -X打印出来。 不稳定参数: -XX开头，这一 -类参数是跟特定HotSpot版本对应的，并且变化非常大。详细的文档资料非常少。在JDK1.8版本下，有几个常用的不稳定指令: java -XX:+ PrintCommandLineFlags :查看当前命令的不稳定指令。 java -XX:+ PrintFlagsInitial :查看所有不稳定指令的默认值。 java -XX: + PrintFlagsFinal:查看所有不稳定指令 最终生效的实际值。 总结JVM调优在实际工作中用到的比较少，但是这也是作为java程序员必须掌握的基本技能。真正熟练的使用GC调优，是建立在多次进行GC监控和调优的实战经验上的。 下面罗列了几个数据作为参考，如果GC执行时间满足下列所有条件，就没有必要进行GC优化了： Minor GC执行非常迅速（50ms以内） Minor GC没有频繁执行（大约10s执行一次） Full GC执行非常迅速（1s以内） Full GC没有频繁执行（大约10min执行一次） 参考JVM GC调优入门","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://xmmarlowe.github.io/tags/JVM/"},{"name":"GC","slug":"GC","permalink":"https://xmmarlowe.github.io/tags/GC/"}],"author":"Marlowe"},{"title":"HashMap扩容机制","slug":"Java/HashMap扩容机制","date":"2020-03-16T02:33:21.000Z","updated":"2021-08-26T10:53:36.930Z","comments":true,"path":"2020/03/16/Java/HashMap扩容机制/","link":"","permalink":"https://xmmarlowe.github.io/2020/03/16/Java/HashMap%E6%89%A9%E5%AE%B9%E6%9C%BA%E5%88%B6/","excerpt":"聊聊HashMap扩容机制","text":"聊聊HashMap扩容机制 1、什么时候才需要扩容 在首次调用put方法的时候，初始化数组table 当HashMap中的元素个数超过数组大小(数组长度)*loadFactor(负载因子)时，就会进行数组扩容，loadFactor的默认值(DEFAULT_LOAD_FACTOR)是0.75,这是一个折中的取值。也就是说，默认情况下，数组大小为16，那么当HashMap中的元素个数超过16×0.75=12(这个值就是阈值或者边界值threshold值)的时候，就把数组的大小扩展为2×16=32，即扩大一倍，然后重新计算每个元素在数组中的位置，而这是一个非常耗性能的操作，所以如果我们已经预知HashMap中元素的个数，那么预知元素的个数能够有效的提高HashMap的性能。 当HashMap中的其中一个链表的对象个数如果达到了8个，此时如果数组长度没有达到64，那么HashMap会先扩容解决，如果已经达到了64，那么这个链表会变成红黑树，节点类型由Node变成TreeNode类型。当然，如果映射关系被移除后，下次执行resize方法时判断树的节点个数低于6，也会再把树转换为链表。 为什么选择长度为6的时候转回链表？6和8，中间有个差值7可以有效防止链表和树频繁转换。假设一下，如果设计成链表个数超过8则链表转换成树结构，链表个数小于8则树结构转换成链表，如果一个HashMap不停的插入、删除元素，链表个数在8左右徘徊，就会频繁的发生树转链表、链表转树，效率会很低。 什么是红黑树?红黑树是一种自平衡的二叉查找树。 性质： 节点是红色或黑色。 根节点是黑色。 每个叶子节点都是黑色的空节点（NIL节点）。 每个红色节点的两个子节点都是黑色。(从每个叶子到根的所有路径上不能有两个连续的红色节点) 从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。 红黑树相比于BST和AVL树有什么优点？ 红黑树是牺牲了严格的高度平衡的优越条件为代价，它只要求部分地达到平衡要求，降低了对旋转的要求，从而提高了性能。红黑树能够以O(log2 n)的时间复杂度进行搜索、插入、删除操作。此外，由于它的设计，任何不平衡都会在三次旋转之内解决。当然，还有一些更好的，但实现起来更复杂的数据结构能够做到一步旋转之内达到平衡，但红黑树能够给我们一个比较“便宜”的解决方案。 相比于BST，因为红黑树可以能确保树的最长路径不大于两倍的最短路径的长度，所以可以看出它的查找效果是有最低保证的。在最坏的情况下也可以保证O(logN)的，这是要好于二叉查找树的。因为二叉查找树最坏情况可以让查找达到O(N)。 红黑树的算法时间复杂度和AVL相同，但统计性能比AVL树更高，所以在插入和删除中所做的后期维护操作肯定会比红黑树要耗时好多，但是他们的查找效率都是O(logN)，所以红黑树应用还是高于AVL树的. 实际上插入 AVL 树和红黑树的速度取决于你所插入的数据.如果你的数据分布较好,则比较宜于采用 AVL树(例如随机产生系列数),但是如果你想处理比较杂乱的情况,则红黑树是比较快的。 2、HashMap的扩容是什么进行扩容，会伴随着一次重新hash分配，并且会遍历hash表中所有的元素，是非常耗时的。在编写程序中，要尽量避免resize。 HashMap在进行扩容时，使用的rehash方式非常巧妙，因为每次扩容都是翻倍，与原来计算的 (n-1)&amp;hash的结果相比，只是多了一个bit位，所以节点要么就在原来的位置，要么就被分配到”原位置+旧容量“这个位置。 说明：5是假设计算出来的原来的索引。这样就验证了上述所描述的：扩容之后所以节点要么就在原来的位置，要么就被分配到”原位置+旧容量”这个位置。 因此，我们在扩充HashMap的时候，不需要重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就可以了，是0的话索引没变，是1的话索引变成“原索引+oldCap(原位置+旧容量)”。 正是因为这样巧妙的rehash方式，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，在resize的过程中保证了rehash之后每个桶上的节点数一定小于等于原来桶上的节点数，保证了rehash之后不会出现更严重的hash冲突，均匀的把之前的冲突的节点分散到新的桶中了。 3、HashMap源码分析构造方法HashMap 中有四个构造方法，它们分别如下： 123456789101112131415161718192021222324252627// 默认构造函数。public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted &#125; // 包含另一个“Map”的构造函数 public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);//下面会分析到这个方法 &#125; // 指定“容量大小”的构造函数 public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; // 指定“容量大小”和“加载因子”的构造函数 public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity); &#125; putMapEntries 方法： 123456789101112131415161718192021222324final void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) &#123; int s = m.size(); if (s &gt; 0) &#123; // 判断table是否已经初始化 if (table == null) &#123; // pre-size // 未初始化，s为m的实际元素个数 float ft = ((float)s / loadFactor) + 1.0F; int t = ((ft &lt; (float)MAXIMUM_CAPACITY) ? (int)ft : MAXIMUM_CAPACITY); // 计算得到的t大于阈值，则初始化阈值 if (t &gt; threshold) threshold = tableSizeFor(t); &#125; // 已初始化，并且m元素个数大于阈值，进行扩容处理 else if (s &gt; threshold) resize(); // 将m中的所有元素添加至HashMap中 for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) &#123; K key = e.getKey(); V value = e.getValue(); putVal(hash(key), key, value, false, evict); &#125; &#125;&#125; put 方法HashMap 只提供了 put 用于添加元素，putVal 方法只是给 put 方法调用的一个方法，并没有提供给用户使用。 对 putVal 方法添加元素的分析如下： 如果定位到的数组位置没有元素 就直接插入。 如果定位到的数组位置有元素就和要插入的 key 比较，如果 key 相同就直接覆盖，如果 key 不相同，就判断 p 是否是一个树节点，如果是就调用e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value)将元素添加进入。如果不是就遍历链表插入(插入的是链表尾部)。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // table未初始化或者长度为0，进行扩容 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // (n - 1) &amp; hash 确定元素存放在哪个桶中，桶为空，新生成结点放入桶中(此时，这个结点是放在数组中) if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); // 桶中已经存在元素 else &#123; Node&lt;K,V&gt; e; K k; // 比较桶中第一个元素(数组中的结点)的hash值相等，key相等 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) // 将第一个元素赋值给e，用e来记录 e = p; // hash值不相等，即key不相等；为红黑树结点 else if (p instanceof TreeNode) // 放入树中 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 为链表结点 else &#123; // 在链表最末插入结点 for (int binCount = 0; ; ++binCount) &#123; // 到达链表的尾部 if ((e = p.next) == null) &#123; // 在尾部插入新结点 p.next = newNode(hash, key, value, null); // 结点数量达到阈值(默认为 8 )，执行 treeifyBin 方法 // 这个方法会根据 HashMap 数组来决定是否转换为红黑树。 // 只有当数组长度大于或者等于 64 的情况下，才会执行转换红黑树操作，以减少搜索时间。否则，就是只是对数组扩容。 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); // 跳出循环 break; &#125; // 判断链表中结点的key值与插入的元素的key值是否相等 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) // 相等，跳出循环 break; // 用于遍历桶中的链表，与前面的e = p.next组合，可以遍历链表 p = e; &#125; &#125; // 表示在桶中找到key值、hash值与插入元素相等的结点 if (e != null) &#123; // 记录e的value V oldValue = e.value; // onlyIfAbsent为false或者旧值为null if (!onlyIfAbsent || oldValue == null) //用新值替换旧值 e.value = value; // 访问后回调 afterNodeAccess(e); // 返回旧值 return oldValue; &#125; &#125; // 结构性修改 ++modCount; // 实际大小大于阈值则扩容 if (++size &gt; threshold) resize(); // 插入后回调 afterNodeInsertion(evict); return null;&#125; 我们再来对比一下 JDK1.7 put 方法的代码 对于 put 方法的分析如下： ① 如果定位到的数组位置没有元素 就直接插入。② 如果定位到的数组位置有元素，遍历以这个元素为头结点的链表，依次和插入的 key 比较，如果 key 相同就直接覆盖，不同就采用头插法插入元素。 12345678910111213141516171819202122public V put(K key, V value) if (table == EMPTY_TABLE) &#123; inflateTable(threshold);&#125; if (key == null) return putForNullKey(value); int hash = hash(key); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; // 先遍历 Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(hash, key, value, i); // 再插入 return null;&#125; get 方法12345678910111213141516171819202122232425262728public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 数组元素相等 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 桶中不止一个节点 if ((e = first.next) != null) &#123; // 在树中get if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 在链表中get do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; resize()方法源码进行扩容，会伴随着一次重新 hash 分配，并且会遍历 hash 表中所有的元素，是非常耗时的。在编写程序中，要尽量避免 resize。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105final Node&lt;K,V&gt;[] resize() &#123; //得到当前数组 Node&lt;K,V&gt;[] oldTab = table; //如果当前数组等于null长度返回0，否则返回当前数组的长度 int oldCap = (oldTab == null) ? 0 : oldTab.length; //当前阀值点 默认是12(16*0.75) int oldThr = threshold; int newCap, newThr = 0; //如果老的数组长度大于0 //开始计算扩容后的大小 if (oldCap &gt; 0) &#123; // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; //修改阈值为int的最大值 threshold = Integer.MAX_VALUE; return oldTab; &#125; /* 没超过最大值，就扩充为原来的2倍 1)(newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY 扩大到2倍之后容量要小于最大容量 2)oldCap &gt;= DEFAULT_INITIAL_CAPACITY 原数组长度大于等于数组初始化长度16 */ else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) //阈值扩大一倍 newThr = oldThr &lt;&lt; 1; // double threshold &#125; //老阈值点大于0 直接赋值 else if (oldThr &gt; 0) // 老阈值赋值给新的数组长度 newCap = oldThr; else &#123;// 直接使用默认值 newCap = DEFAULT_INITIAL_CAPACITY;//16 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 计算新的resize最大上限 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; //新的阀值 默认原来是12 乘以2之后变为24 threshold = newThr; //创建新的哈希表 @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) //newCap是新的数组长度--&gt;32 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; //判断旧数组是否等于空 if (oldTab != null) &#123; // 把每个bucket都移动到新的buckets中 //遍历旧的哈希表的每个桶，重新计算桶里元素的新位置 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; //原来的数据赋值为null 便于GC回收 oldTab[j] = null; //判断数组是否有下一个引用 if (e.next == null) //没有下一个引用，说明不是链表，当前桶上只有一个键值对，直接插入 newTab[e.hash &amp; (newCap - 1)] = e; //判断是否是红黑树 else if (e instanceof TreeNode) //说明是红黑树来处理冲突的，则调用相关方法把树分开 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // 采用链表处理冲突 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; //通过上述讲解的原理来计算节点的新位置 do &#123; // 原索引 next = e.next; //这里来判断如果等于true e这个节点在resize之后不需要移动位置 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引+oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引+oldCap放到bucket里 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; treeifyBin方法123456789101112131415161718192021222324252627282930final void treeifyBin(Node&lt;K, V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K, V&gt; e; if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) // resize()方法这里不过多介绍，感兴趣的可以去看上面的链接。 resize(); // 通过hash求出bucket的位置。 else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; TreeNode&lt;K, V&gt; hd = null, tl = null; do &#123; // 将每个节点包装成TreeNode。 TreeNode&lt;K, V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else &#123; // 将所有TreeNode连接在一起此时只是链表结构。 p.prev = tl; tl.next = p; &#125; tl = p; &#125; while ((e = e.next) != null); if ((tab[index] = hd) != null) // 对TreeNode链表进行树化。 hd.treeify(tab); &#125; &#125; treeify方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162final void treeify(Node&lt;K, V&gt;[] tab) &#123; TreeNode&lt;K, V&gt; root = null; // 以for循环的方式遍历刚才我们创建的链表。 for (TreeNode&lt;K, V&gt; x = this, next; x != null; x = next) &#123; // next向前推进。 next = (TreeNode&lt;K, V&gt;) x.next; x.left = x.right = null; // 为树根节点赋值。 if (root == null) &#123; x.parent = null; x.red = false; root = x; &#125; else &#123; // x即为当前访问链表中的项。 K k = x.key; int h = x.hash; Class&lt;?&gt; kc = null; // 此时红黑树已经有了根节点，上面获取了当前加入红黑树的项的key和hash值进入核心循环。 // 这里从root开始，是以一个自顶向下的方式遍历添加。 // for循环没有控制条件，由代码内break跳出循环。 for (TreeNode&lt;K, V&gt; p = root;;) &#123; // dir：directory，比较添加项与当前树中访问节点的hash值判断加入项的路径，-1为左子树，+1为右子树。 // ph：parent hash。 int dir, ph; K pk = p.key; if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); // xp：x parent。 TreeNode&lt;K, V&gt; xp = p; // 找到符合x添加条件的节点。 if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; x.parent = xp; // 如果xp的hash值大于x的hash值，将x添加在xp的左边。 if (dir &lt;= 0) xp.left = x; // 反之添加在xp的右边。 else xp.right = x; // 维护添加后红黑树的红黑结构。 root = balanceInsertion(root, x); // 跳出循环当前链表中的项成功的添加到了红黑树中。 break; &#125; &#125; &#125; &#125; // Ensures that the given root is the first node of its bin，自己翻译一下。 moveRootToFront(tab, root); &#125; 第一次循环会将链表中的首节点作为红黑树的根，而后的循环会将链表中的的项通过比较hash值然后连接到相应树节点的左边或者右边，插入可能会破坏树的结构所以接着执行balanceInsertion。 balanceInsertion方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697static &lt;K, V&gt; TreeNode&lt;K, V&gt; balanceInsertion(TreeNode&lt;K, V&gt; root, TreeNode&lt;K, V&gt; x) &#123; // 正如开头所说，新加入树节点默认都是红色的，不会破坏树的结构。 x.red = true; // 这些变量名不是作者随便定义的都是有意义的。 // xp：x parent，代表x的父节点。 // xpp：x parent parent，代表x的祖父节点 // xppl：x parent parent left，代表x的祖父的左节点。 // xppr：x parent parent right，代表x的祖父的右节点。 for (TreeNode&lt;K, V&gt; xp, xpp, xppl, xppr;;) &#123; // 如果x的父节点为null说明只有一个节点，该节点为根节点，根节点为黑色，red = false。 if ((xp = x.parent) == null) &#123; x.red = false; return x; &#125; // 进入else说明不是根节点。 // 如果父节点是黑色，那么大吉大利（今晚吃鸡），红色的x节点可以直接添加到黑色节点后面，返回根就行了不需要任何多余的操作。 // 如果父节点是红色的，但祖父节点为空的话也可以直接返回根此时父节点就是根节点，因为根必须是黑色的，添加在后面没有任何问题。 else if (!xp.red || (xpp = xp.parent) == null) return root; // 一旦我们进入到这里就说明了两件是情 // 1.x的父节点xp是红色的，这样就遇到两个红色节点相连的问题，所以必须经过旋转变换。 // 2.x的祖父节点xpp不为空。 // 判断如果父节点是否是祖父节点的左节点 if (xp == (xppl = xpp.left)) &#123; // 父节点xp是祖父的左节点xppr // 判断祖父节点的右节点不为空并且是否是红色的 // 此时xpp的左右节点都是红的，所以直接进行上面所说的第三种变换，将两个子节点变成黑色，将xpp变成红色，然后将红色节点x顺利的添加到了xp的后面。 // 这里大家有疑问为什么将x = xpp？ // 这是由于将xpp变成红色以后可能与xpp的父节点发生两个相连红色节点的冲突，这就又构成了第二种旋转变换，所以必须从底向上的进行变换，直到根。 // 所以令x = xpp，然后进行下下一层循环，接着往上走。 if ((xppr = xpp.right) != null &amp;&amp; xppr.red) &#123; xppr.red = false; xp.red = false; xpp.red = true; x = xpp; &#125; // 进入到这个else里面说明。 // 父节点xp是祖父的左节点xppr。 // 祖父节点xpp的右节点xppr是黑色节点或者为空，默认规定空节点也是黑色的。 // 下面要判断x是xp的左节点还是右节点。 else &#123; // x是xp的右节点，此时的结构是：xpp左-&gt;xp右-&gt;x。这明显是第二中变换需要进行两次旋转，这里先进行一次旋转。 // 下面是第一次旋转。 if (x == xp.right) &#123; root = rotateLeft(root, x = xp); xpp = (xp = x.parent) == null ? null : xp.parent; &#125; // 针对本身就是xpp左-&gt;xp左-&gt;x的结构或者由于上面的旋转造成的这种结构进行一次旋转。 if (xp != null) &#123; xp.red = false; if (xpp != null) &#123; xpp.red = true; root = rotateRight(root, xpp); &#125; &#125; &#125; &#125; // 这里的分析方式和前面的相对称只不过全部在右测不再重复分析。 else &#123; if (xppl != null &amp;&amp; xppl.red) &#123; xppl.red = false; xp.red = false; xpp.red = true; x = xpp; &#125; else &#123; if (x == xp.left) &#123; root = rotateRight(root, x = xp); xpp = (xp = x.parent) == null ? null : xp.parent; &#125; if (xp != null) &#123; xp.red = false; if (xpp != null) &#123; xpp.red = true; root = rotateLeft(root, xpp); &#125; &#125; &#125; &#125; &#125; &#125; 参考面试题：HashMap扩容机制","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"HashMap","slug":"HashMap","permalink":"https://xmmarlowe.github.io/tags/HashMap/"}],"author":"Marlowe"},{"title":"ConcurrentHashMap 线程安全的具体实现方式/底层具体实现","slug":"Java/ConcurrentHashMap-线程安全的具体实现方式-底层具体实现","date":"2020-03-16T01:57:08.000Z","updated":"2021-04-23T14:21:23.078Z","comments":true,"path":"2020/03/16/Java/ConcurrentHashMap-线程安全的具体实现方式-底层具体实现/","link":"","permalink":"https://xmmarlowe.github.io/2020/03/16/Java/ConcurrentHashMap-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F-%E5%BA%95%E5%B1%82%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"JDK1.7 首先将数据分为一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问。 ConcurrentHashMap 是由 Segment 数组结构和 HashEntry 数组结构组成。 Segment 实现了 ReentrantLock,所以 Segment 是一种可重入锁，扮演锁的角色。HashEntry 用于存储键值对数据。 12static class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123;&#125; 一个 ConcurrentHashMap 里包含一个 Segment 数组。Segment 的结构和 HashMap 类似，是一种数组和链表结构，一个 Segment 包含一个 HashEntry 数组，每个 HashEntry 是一个链表结构的元素，每个 Segment 守护着一个 HashEntry 数组里的元素，当对 HashEntry 数组的数据进行修改时，必须首先获得对应的 Segment 的锁。 JDK1.8ConcurrentHashMap 取消了 Segment 分段锁，采用 CAS 和 synchronized 来保证并发安全。数据结构跟 HashMap1.8 的结构类似，数组+链表/红黑二叉树。Java 8 在链表长度超过一定阈值（8）时将链表（寻址时间复杂度为 O(N)）转换为红黑树（寻址时间复杂度为 O(log(N))） synchronized 只锁定当前链表或红黑二叉树的首节点，这样只要 hash 不冲突，就不会产生并发，效率又提升 N 倍。","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"HashMap","slug":"HashMap","permalink":"https://xmmarlowe.github.io/tags/HashMap/"},{"name":"线程安全","slug":"线程安全","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8/"}],"author":"Marlowe"},{"title":"ConcurrentHashMap 和 Hashtable 的区别","slug":"Java/ConcurrentHashMap-和-Hashtable-的区别","date":"2020-03-16T01:56:52.000Z","updated":"2021-04-23T14:21:20.027Z","comments":true,"path":"2020/03/16/Java/ConcurrentHashMap-和-Hashtable-的区别/","link":"","permalink":"https://xmmarlowe.github.io/2020/03/16/Java/ConcurrentHashMap-%E5%92%8C-Hashtable-%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"ConcurrentHashMap 和 Hashtable 的区别主要体现在实现线程安全的方式上不同。","text":"ConcurrentHashMap 和 Hashtable 的区别主要体现在实现线程安全的方式上不同。 底层数据结构： JDK1.7 的 ConcurrentHashMap 底层采用 分段的数组+链表 实现，JDK1.8 采用的数据结构跟 HashMap1.8 的结构一样，数组+链表/红黑二叉树。Hashtable 和 JDK1.8 之前的 HashMap 的底层数据结构类似都是采用 数组+链表 的形式，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的； 实现线程安全的方式（重要）： ① 在 JDK1.7 的时候，ConcurrentHashMap（分段锁） 对整个桶数组进行了分割分段(Segment)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。 到了 JDK1.8 的时候已经摒弃了 Segment 的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并发控制使用 synchronized 和 CAS 来操作。（JDK1.6 以后 对 synchronized 锁做了很多优化） 整个看起来就像是优化过且线程安全的 HashMap，虽然在 JDK1.8 中还能看到 Segment 的数据结构，但是已经简化了属性，只是为了兼容旧版本；② Hashtable(同一把锁) :使用 synchronized 来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用 put 添加元素，另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低。 两者的对比图：HashTable: JDK1.7 的 ConcurrentHashMap： JDK1.8 的 ConcurrentHashMap：JDK1.8 的 ConcurrentHashMap 不再是 Segment 数组 + HashEntry 数组 + 链表，而是 Node 数组 + 链表 / 红黑树。不过，Node 只能用于链表的情况，红黑树的情况需要使用 TreeNode。当冲突链表达到一定长度时，链表会转换成红黑树。","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"HashMap","slug":"HashMap","permalink":"https://xmmarlowe.github.io/tags/HashMap/"},{"name":"线程安全","slug":"线程安全","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8/"}],"author":"Marlowe"},{"title":"HashMap 和 Hashtable 的区别","slug":"Java/HashMap-和-Hashtable-的区别","date":"2020-03-15T13:21:12.000Z","updated":"2021-04-23T14:21:48.079Z","comments":true,"path":"2020/03/15/Java/HashMap-和-Hashtable-的区别/","link":"","permalink":"https://xmmarlowe.github.io/2020/03/15/Java/HashMap-%E5%92%8C-Hashtable-%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"线程是否安全： HashMap 是非线程安全的，HashTable 是线程安全的,因为 HashTable 内部的方法基本都经过synchronized 修饰。（如果你要保证线程安全的话就使用 ConcurrentHashMap 吧！）； 效率： 因为线程安全的问题，HashMap 要比 HashTable 效率高一点。另外，HashTable 基本被淘汰，不要在代码中使用它； 对 Null key 和 Null value 的支持： HashMap 可以存储 null 的 key 和 value，但 null 作为键只能有一个，null 作为值可以有多个；HashTable 不允许有 null 键和 null 值，否则会抛出 NullPointerException。 初始容量大小和每次扩充容量大小的不同 ： ① 创建时如果不指定容量初始值，Hashtable 默认的初始大小为 11，之后每次扩充，容量变为原来的 2n+1。HashMap 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。② 创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为 2 的幂次方大小（HashMap 中的tableSizeFor()方法保证，下面给出了源代码）。也就是说 HashMap 总是使用 2 的幂作为哈希表的大小,后面会介绍到为什么是 2 的幂次方。 底层数据结构： JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。Hashtable 没有这样的机制。 HashMap 中带有初始容量的构造函数： 12345678910111213141516public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);&#125; public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125; 下面这个方法保证了 HashMap 总是使用 2 的幂作为哈希表的大小。 12345678910111213/** * Returns a power of two size for the given target capacity. */static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125;","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"HashMap","slug":"HashMap","permalink":"https://xmmarlowe.github.io/tags/HashMap/"}],"author":"Marlowe"},{"title":"ArrayList和LinkedList的区别","slug":"Java/ArrayList和LinkedList的区别","date":"2020-03-15T06:23:58.000Z","updated":"2021-04-23T14:21:09.264Z","comments":true,"path":"2020/03/15/Java/ArrayList和LinkedList的区别/","link":"","permalink":"https://xmmarlowe.github.io/2020/03/15/Java/ArrayList%E5%92%8CLinkedList%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"Arraylist 与 LinkedList 区别? 是否保证线程安全： ArrayList 和 LinkedList 都是不同步的，也就是不保证线程安全； 底层数据结构： Arraylist 底层使用的是 Object 数组；LinkedList 底层使用的是 双向链表 数据结构（JDK1.6 之前为循环链表，JDK1.7 取消了循环。） 插入和删除是否受元素位置的影响： ① ArrayList 采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。 比如：执行add(E e)方法的时候， ArrayList 会默认在将指定的元素追加到此列表的末尾，这种情况时间复杂度就是 O(1)。但是如果要在指定位置 i 插入和删除元素的话（add(int index, E element)）时间复杂度就为 O(n-i)。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位/向前移一位的操作。 ② LinkedList 采用链表存储，所以对于add(E e)方法的插入，删除元素时间复杂度不受元素位置的影响，近似 O(1)，如果是要在指定位置i插入和删除元素的话（(add(int index, E element)） 时间复杂度近似为o(n))因为需要先移动到指定位置再插入。 是否支持快速随机访问： LinkedList 不支持高效的随机元素访问，而 ArrayList 支持。快速随机访问就是通过元素的序号快速获取元素对象(对应于get(int index)方法)。 内存空间占用： ArrayList 的空 间浪费主要体现在在 list 列表的结尾会预留一定的容量空间，而 LinkedList 的空间花费则体现在它的每一个元素都需要消耗比 ArrayList 更多的空间（因为要存放直接后继和直接前驱以及数据）。 参考Arraylist 与 LinkedList 区别?","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"List","slug":"List","permalink":"https://xmmarlowe.github.io/tags/List/"}],"author":"Marlowe"},{"title":"ArrayList扩容机制","slug":"Java/ArrayList扩容机制","date":"2020-03-15T06:23:43.000Z","updated":"2021-04-23T14:21:12.057Z","comments":true,"path":"2020/03/15/Java/ArrayList扩容机制/","link":"","permalink":"https://xmmarlowe.github.io/2020/03/15/Java/ArrayList%E6%89%A9%E5%AE%B9%E6%9C%BA%E5%88%B6/","excerpt":"","text":"1234567891、添加元素时，首先进行判断是否大于默认容量102、如果，小于默认容量，直接在原来基础上+1，元素添加完毕3、如果，大于默认容量，则需要进行扩容，扩容核心是grow()方法 3.1 扩容之前，首先创建一个新的数组，且旧数组被复制到新的数组中 这样就得到了一个全新的副本，我们在操作时就不会影响原来数组了 3.2 然后通过位运算符将新的容量更新为旧容量的 1.5 倍 3.3 如果新的容量比最小需要容量小，则最小需要容量为当前数组新容量， 如果minCapacity大于最大容量，则新容量则为`Integer.MAX_VALUE`，否则，新容量大小则为 MAX_ARRAY_SIZE 即为 `Integer.MAX_VALUE - 8`。 grow()方法： 12345678910111213141516171819202122232425/** * 要分配的最大数组大小 */private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;/** * ArrayList扩容的核心方法。 */private void grow(int minCapacity) &#123; // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量， if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; // 如果新容量大于 MAX_ARRAY_SIZE,进入(执行) `hugeCapacity()` 方法来比较 minCapacity 和 MAX_ARRAY_SIZE， //如果minCapacity大于最大容量，则新容量则为`Integer.MAX_VALUE`，否则，新容量大小则为 MAX_ARRAY_SIZE 即为 `Integer.MAX_VALUE - 8`。 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125;","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"List","slug":"List","permalink":"https://xmmarlowe.github.io/tags/List/"}],"author":"Marlowe"},{"title":"HashMap是线程安全的吗？","slug":"Java/HashMap是线程安全的吗？","date":"2020-03-15T06:23:22.000Z","updated":"2021-04-23T14:22:06.668Z","comments":true,"path":"2020/03/15/Java/HashMap是线程安全的吗？/","link":"","permalink":"https://xmmarlowe.github.io/2020/03/15/Java/HashMap%E6%98%AF%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E5%90%97%EF%BC%9F/","excerpt":"HashMap的线程不安全体现在会造成死循环、数据丢失、数据覆盖这些问题。其中死循环和数据丢失是在JDK1.7中出现的问题，在JDK1.8中已经得到解决，然而1.8中仍会有数据覆盖这样的问题。","text":"HashMap的线程不安全体现在会造成死循环、数据丢失、数据覆盖这些问题。其中死循环和数据丢失是在JDK1.7中出现的问题，在JDK1.8中已经得到解决，然而1.8中仍会有数据覆盖这样的问题。 扩容引发的线程不安全HashMap的线程不安全主要是发生在扩容函数中，即根源是在transfer函数中，JDK1.7中HashMap的transfer函数如下： 123456789101112131415void transfer(Entry[] newTable, boolean rehash) &#123; int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) &#123; while(null != e) &#123; Entry&lt;K,V&gt; next = e.next; if (rehash) &#123; e.hash = null == e.key ? 0 : hash(e.key); &#125; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; &#125; &#125; 这段代码是HashMap的扩容操作，重新定位每个桶的下标，并采用头插法将元素迁移到新数组中。头插法会将链表的顺序翻转，这也是形成死循环的关键点。 JDK1.8中的线程不安全根据上面JDK1.7出现的问题，在JDK1.8中已经得到了很好的解决，如果你去阅读1.8的源码会发现找不到transfer函数，因为JDK1.8直接在resize函数中完成了数据迁移。另外说一句，JDK1.8在进行元素插入时使用的是尾插法。 为什么说JDK1.8会出现数据覆盖的情况喃，我们来看一下下面这段JDK1.8中的put操作代码： 123456789101112131415161718192021222324252627282930313233343536373839404142final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) // 如果没有hash碰撞则直接插入元素 tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; 其中第六行代码是判断是否出现hash碰撞，假设两个线程A、B都在进行put操作，并且hash函数计算出的插入下标是相同的，当线程A执行完第六行代码后由于时间片耗尽导致被挂起，而线程B得到时间片后在该下标处插入了元素，完成了正常的插入，然后线程A获得时间片，由于之前已经进行了hash碰撞的判断，所有此时不会再进行判断，而是直接进行插入，这就导致了线程B插入的数据被线程A覆盖了，从而线程不安全。 除此之前，还有就是代码的第38行处有个++size，我们这样想，还是线程A、B，这两个线程同时进行put操作时，假设当前HashMap的zise大小为10，当线程A执行到第38行代码时，从主内存中获得size的值为10后准备进行+1操作，但是由于时间片耗尽只好让出CPU，线程B快乐的拿到CPU还是从主内存中拿到size的值10进行+1操作，完成了put操作并将size=11写回主内存，然后线程A再次拿到CPU并继续执行(此时size的值仍为10)，当执行完put操作后，还是将size=11写回内存，此时，线程A、B都执行了一次put操作，但是size的值只增加了1，所有说还是由于数据覆盖又导致了线程不安全。 总结HashMap的线程不安全主要体现在下面两个方面： 在JDK1.7中，当并发执行扩容操作时会造成环形链和数据丢失的情况。 在JDK1.8中，在并发执行put操作时会发生数据覆盖的情况。 参考HashMap的实现原理，以及在JDK1.7和1.8的区别","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"HashMap","slug":"HashMap","permalink":"https://xmmarlowe.github.io/tags/HashMap/"}],"author":"Marlowe"},{"title":"HashMap底层原理","slug":"Java/HashMap底层原理","date":"2020-03-15T06:23:01.000Z","updated":"2021-04-23T14:21:58.841Z","comments":true,"path":"2020/03/15/Java/HashMap底层原理/","link":"","permalink":"https://xmmarlowe.github.io/2020/03/15/Java/HashMap%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/","excerpt":"","text":"JDK1.7数据结构则是采用的位桶和链表相结合的形式完成了，即拉链法。具体如下图所示： HashMap里面存储的是静态内部类Entry的对象，这个对象其实也是一个key-value的结构。 hash源码： 12345678static int hash(int h) &#123; // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125; JDK1.8相比于之前的版本， JDK1.8 之后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。 hash源码： 1234567 static final int hash(Object key) &#123; int h; // key.hashCode()：返回散列值也就是hashcode // ^ ：按位异或 // &gt;&gt;&gt;:无符号右移，忽略符号位，空位都以0补齐 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; JDK 1.8 的 hash 方法 相比于 JDK 1.7 hash 方法更加简化，但是原理不变。 HashMap 的长度为什么是 2 的幂次方为了能让 HashMap 存取高效，尽量较少碰撞，也就是要尽量把数据分配均匀。我们上面也讲到了过了，Hash 值的范围值-2147483648 到 2147483647，前后加起来大概 40 亿的映射空间，只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。但问题是一个 40 亿长度的数组，内存是放不下的。所以这个散列值是不能直接拿来用的。用之前还要先做对数组的长度取模运算，得到的余数才能用来要存放的位置也就是对应的数组下标。这个数组下标的计算方法是“ (n - 1) &amp; hash”。（n 代表数组长度）。这也就解释了 HashMap 的长度为什么是 2 的幂次方。 这个算法应该如何设计呢？ 我们首先可能会想到采用%取余的操作来实现。但是，重点来了：“取余(%)操作中如果除数是 2 的幂次则等价于与其除数减一的与(&amp;)操作（也就是说 hash%length==hash&amp;(length-1)的前提是 length 是 2 的 n 次方；）。” 并且 采用二进制位操作 &amp;，相对于%能够提高运算效率，这就解释了 HashMap 的长度为什么是 2 的幂次方。","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"HashMap","slug":"HashMap","permalink":"https://xmmarlowe.github.io/tags/HashMap/"}],"author":"Marlowe"},{"title":"常见的IO模型有哪些？Java中的BIO，NIO，AIO的区别","slug":"操作系统/常见的IO模型有哪些？Java中的BIO，NIO，AIO的区别","date":"2020-03-15T06:22:22.000Z","updated":"2021-05-28T05:40:45.961Z","comments":true,"path":"2020/03/15/操作系统/常见的IO模型有哪些？Java中的BIO，NIO，AIO的区别/","link":"","permalink":"https://xmmarlowe.github.io/2020/03/15/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%B8%B8%E8%A7%81%E7%9A%84IO%E6%A8%A1%E5%9E%8B%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9FJava%E4%B8%AD%E7%9A%84BIO%EF%BC%8CNIO%EF%BC%8CAIO%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"从应用程序的视角来看的话，我们的应用程序对操作系统的内核发起 IO 调用（系统调用），操作系统负责的内核执行具体的 IO 操作。也就是说，我们的应用程序实际上只是发起了 IO 操作的调用而已，具体 IO 的执行是由操作系统的内核来完成的。","text":"从应用程序的视角来看的话，我们的应用程序对操作系统的内核发起 IO 调用（系统调用），操作系统负责的内核执行具体的 IO 操作。也就是说，我们的应用程序实际上只是发起了 IO 操作的调用而已，具体 IO 的执行是由操作系统的内核来完成的。 简介当应用程序发起I/O调用后，会经历两个步骤： 内核等待 I/O 设备准备好数据 内核将数据从内核空间拷贝到用户空间。 UNIX系统5种I/O模型： 同步阻塞 I/O 同步非阻塞 I/O I/O 多路复用 信号驱动 I/O 异步 I/O Java中三种常见的I/O模型BIO (Blocking I/O)BIO 属于同步阻塞 IO 模型 。 同步阻塞 IO 模型中，应用程序发起 read 调用后，会一直阻塞，直到在内核把数据拷贝到用户空间。 在客户端连接数量不高的情况下，是没问题的。但是，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。因此，我们需要一种更高效的 I/O 处理模型来应对更高的并发量。 NIO (Non-blocking/New I/O)Java 中的 NIO 于 Java 1.4 中引入，对应 java.nio 包，提供了 Channel , Selector，Buffer 等抽象。NIO 中的 N 可以理解为 Non-blocking，不单纯是 New。它支持面向缓冲的，基于通道的 I/O 操作方法。 对于高负载、高并发的（网络）应用，应使用 NIO 。 Java 中的 NIO 可以看作是 I/O 多路复用模型。也有很多人认为，Java 中的 NIO 属于同步非阻塞 IO 模型。 同步非阻塞 IO 模型中，应用程序会一直发起 read 调用，等待数据从内核空间拷贝到用户空间的这段时间里，线程依然是阻塞的，直到在内核把数据拷贝到用户空间。 相比于同步阻塞 IO 模型，同步非阻塞 IO 模型确实有了很大改进。通过轮询操作，避免了一直阻塞。 但是，这种 IO 模型同样存在问题：应用程序不断进行 I/O 系统调用轮询数据是否已经准备好的过程是十分消耗 CPU 资源的。 这个时候，I/O 多路复用模型 就上场了。 IO 多路复用模型中，线程首先发起 select 调用，询问内核数据是否准备就绪，等内核把数据准备好了，用户线程再发起 read 调用。read 调用的过程（数据从内核空间-&gt;用户空间）还是阻塞的。 IO 多路复用模型，通过减少无效的系统调用，减少了对 CPU 资源的消耗。 Java 中的 NIO ，有一个非常重要的选择器 ( Selector ) 的概念，也可以被称为 多路复用器。通过它，只需要一个线程便可以管理多个客户端连接。当客户端数据到了之后，才会为其服务。 ChannelJava NIO中的所有I/O操作都基于Channel对象，就像流操作都要基于Stream对象一样，因此很有必要先了解Channel是什么。以下内容摘自JDK 1.8的文档 A channel represents an open connection to an entity such as a hardware device, a file, a network socket, or a program component that is capable of performing one or more distinct I/O operations, for example reading or writing. 从上述内容可知，一个Channel（通道）代表和某一实体的连接，这个实体可以是文件、网络套接字等。也就是说，通道是Java NIO提供的一座桥梁，用于我们的程序和操作系统底层I/O服务进行交互。 通道是一种很基本很抽象的描述，和不同的I/O服务交互，执行不同的I/O操作，实现不一样，因此具体的有FileChannel、SocketChannel等。 通道使用起来跟Stream比较像，可以读取数据到Buffer中，也可以把Buffer中的数据写入通道。 当然，也有区别，主要体现在如下两点： 一个通道，既可以读又可以写，而一个Stream是单向的（所以分 InputStream 和 OutputStream） 通道有非阻塞I/O模式 实现Java NIO中最常用的通道实现是如下几个，可以看出跟传统的 I/O 操作类是一一对应的。 FileChannel：读写文件 DatagramChannel: UDP协议网络通信 SocketChannel：TCP协议网络通信 ServerSocketChannel：监听TCP连接 BufferNIO中所使用的缓冲区不是一个简单的byte数组，而是封装过的Buffer类，通过它提供的API，我们可以灵活的操纵数据，下面细细道来。 与Java基本类型相对应，NIO提供了多种 Buffer 类型，如ByteBuffer、CharBuffer、IntBuffer等，区别就是读写缓冲区时的单位长度不一样（以对应类型的变量为单位进行读写）。 Buffer中有3个很重要的变量，它们是理解Buffer工作机制的关键，分别是: capacity （总容量） position （指针当前位置） limit （读/写边界位置） Buffer的工作方式跟C语言里的字符数组非常的像，类比一下，capacity就是数组的总长度，position就是我们读/写字符的下标变量，limit就是结束符的位置。Buffer初始时3个变量的情况如下图: 在对Buffer进行读/写的过程中，position会往后移动，而 limit 就是 position 移动的边界。由此不难想象，在对Buffer进行写入操作时，limit应当设置为capacity的大小，而对Buffer进行读取操作时，limit应当设置为数据的实际结束位置。（注意：将Buffer数据 写入 通道是Buffer 读取 操作，从通道 读取 数据到Buffer是Buffer 写入 操作） 在对Buffer进行读/写操作前，我们可以调用Buffer类提供的一些辅助方法来正确设置 position 和 limit 的值，主要有如下几个: flip(): 设置 limit 为 position 的值，然后 position 置为0。对Buffer进行读取操作前调用。 rewind(): 仅仅将 position 置0。一般是在重新读取Buffer数据前调用，比如要读取同一个Buffer的数据写入多个通道时会用到。 clear(): 回到初始状态，即 limit 等于 capacity，position 置0。重新对Buffer进行写入操作前调用。 compact(): 将未读取完的数据（position 与 limit 之间的数据）移动到缓冲区开头，并将 position 设置为这段数据末尾的下一个位置。其实就等价于重新向缓冲区中写入了这么一段数据。 Selector简介 Selector（选择器）是一个特殊的组件，用于采集各个通道的状态（或者说事件）。我们先将通道注册到选择器，并设置好关心的事件，然后就可以通过调用select()方法，静静地等待事件发生。 通道有如下4个事件可供我们监听： Accept：有可以接受的连接 Connect：连接成功 Read：有数据可读 Write：可以写入数据了 为什么要用Selector 前文说了，如果用阻塞I/O，需要多线程（浪费内存），如果用非阻塞I/O，需要不断重试（耗费CPU）。Selector的出现解决了这尴尬的问题，非阻塞模式下，通过Selector，我们的线程只为已就绪的通道工作，不用盲目的重试了。比如，当所有通道都没有数据到达时，也就没有Read事件发生，我们的线程会在select()方法处被挂起，从而让出了CPU资源。 三大组件总结 channel类似于一流。 每个channel对应一 个buffer缓冲区。 channel会注册到selector。 select会根据channel上发生的读写事件，将请求交由某个空闲的线程处理。selector对应一 个或者多个线程。 Buffer和Channel都是可读可写的。 AIO (Asynchronous I/O)AIO 也就是 NIO 2。Java 7 中引入了 NIO 的改进版 NIO 2,它是异步 IO 模型。 异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。 目前来说 AIO 的应用还不是很广泛。Netty 之前也尝试使用过 AIO，不过又放弃了。这是因为，Netty 使用了 AIO 之后，在 Linux 系统上的性能并没有多少提升。 最后，来一张图，简单总结一下 Java 中的 BIO、NIO、AIO。 参考京东数科二面:常见的10模型有哪些? Java中的BIO、NIO、 AIO有啥区别?","categories":[{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"I/O模型","slug":"I-O模型","permalink":"https://xmmarlowe.github.io/tags/I-O%E6%A8%A1%E5%9E%8B/"}],"author":"Marlowe"},{"title":"final,static,this,super关键字总结","slug":"Java/final-static-this-super关键字总结","date":"2020-03-13T07:55:17.000Z","updated":"2021-04-23T14:21:32.347Z","comments":true,"path":"2020/03/13/Java/final-static-this-super关键字总结/","link":"","permalink":"https://xmmarlowe.github.io/2020/03/13/Java/final-static-this-super%E5%85%B3%E9%94%AE%E5%AD%97%E6%80%BB%E7%BB%93/","excerpt":"","text":"final 关键字final关键字，意思是最终的、不可修改的，最见不得变化 ，用来修饰类、方法和变量，具有以下特点： final修饰的类不能被继承，final类中的所有成员方法都会被隐式的指定为final方法； final修饰的方法不能被重写； final修饰的变量是常量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改；如果是引用类型的变量，则在对其初始化之后便不能让其指向另一个对象。 说明： 使用final方法的原因有两个。第一个原因是把方法锁定，以防任何继承类修改它的含义；第二个原因是效率。在早期的Java实现版本中，会将final方法转为内嵌调用。但是如果方法过于庞大，可能看不到内嵌调用带来的任何性能提升（现在的Java版本已经不需要使用final方法进行这些优化了）。类中所有的private方法都隐式地指定为final。 static 关键字static 关键字主要有以下四种使用场景： 修饰成员变量和成员方法: 被 static 修饰的成员属于类，不属于单个这个类的某个对象，被类中所有对象共享，可以并且建议通过类名调用。被static 声明的成员变量属于静态成员变量，静态变量 存放在 Java 内存区域的方法区。调用格式：类名.静态变量名 类名.静态方法名() 静态代码块: 静态代码块定义在类中方法外, 静态代码块在非静态代码块之前执行(静态代码块—&gt;非静态代码块—&gt;构造方法)。 该类不管创建多少对象，静态代码块只执行一次. 静态内部类（static修饰类的话只能修饰内部类）： 静态内部类与非静态内部类之间存在一个最大的区别: 非静态内部类在编译完成之后会隐含地保存着一个引用，该引用是指向创建它的外围类，但是静态内部类却没有。没有这个引用就意味着：1. 它的创建是不需要依赖外围类的创建。2. 它不能使用任何外围类的非static成员变量和方法。 静态导包(用来导入类中的静态资源，1.5之后的新特性): 格式为：import static 这两个关键字连用可以指定导入某个类中的指定静态资源，并且不需要使用类名调用类中静态成员，可以直接使用类中静态成员变量和成员方法。 this 关键字this关键字用于引用类的当前实例。例如： 1234567891011class Manager &#123; Employees[] employees; void manageEmployees() &#123; int totalEmp = this.employees.length; System.out.println(&quot;Total employees: &quot; + totalEmp); this.report(); &#125; void report() &#123; &#125;&#125; 在上述代码中，this关键字运用于两个地方: this.employees.length：访问Manager的当前实例的变量。 this.report（）：调用类Manager的当前实例的方法。此关键字是可选的，这意味着如果上面的示例在不使用此关键字的情况下表现相同。 但是，使用此关键字可能会使代码更易读或易懂。super 关键字 使用 this 和 super 要注意的问题： 在构造器中使用 super() 调用父类中的其他构造方法时，该语句必须处于构造器的首行，否则编译器会报错。另外，this 调用本类中的其他构造方法时，也要放在首行。 this、super不能用在static方法中。 简单解释一下： 被 static 修饰的成员属于类，不属于单个这个类的某个对象，被类中所有对象共享。而 this 代表对本类对象的引用，指向本类对象；而 super 代表对父类对象的引用，指向父类对象；所以， this和super是属于对象范畴的东西，而静态方法是属于类范畴的东西。 参考final,static,this,super 关键字总结","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"关键字","slug":"关键字","permalink":"https://xmmarlowe.github.io/tags/%E5%85%B3%E9%94%AE%E5%AD%97/"}],"author":"Marlowe"},{"title":"Arrays.asList()使用指南","slug":"Java/Arrays-asList-使用指南","date":"2020-03-13T02:40:40.000Z","updated":"2021-04-23T14:21:15.510Z","comments":true,"path":"2020/03/13/Java/Arrays-asList-使用指南/","link":"","permalink":"https://xmmarlowe.github.io/2020/03/13/Java/Arrays-asList-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/","excerpt":"Arrays.asList()将数组转换为集合后,底层其实还是数组","text":"Arrays.asList()将数组转换为集合后,底层其实还是数组 1234567891011public class Test1 &#123; public static void main(String[] args) &#123; String[] str = new String[]&#123;&quot;111&quot;, &quot;222&quot;&#125;; List&lt;String&gt; list = Arrays.asList(str); list.add(&quot;333&quot;); list.forEach(a-&gt;&#123; System.out.println(a); &#125;); &#125;&#125; 12345运行报错：Exception in thread &quot;main&quot; java.lang.UnsupportedOperationException at java.util.AbstractList.add(AbstractList.java:148) at java.util.AbstractList.add(AbstractList.java:108) at test.Test1.main(Test1.java:16) 使用注意事项传递的数组必须是对象数组，而不是基本类型 1234567int[] myArray = &#123;1, 2, 3&#125;;List myList = Arrays.asList(myArray);System.out.println(myList.size());//1System.out.println(myList.get(0));//数组地址值System.out.println(myList.get(1));//报错：ArrayIndexOutOfBoundsException: 1int[] array = (int[]) myList.get(0);System.out.println(array[0]);//1 当传入一个原生数据类型数组时，Arrays.asList() 的真正得到的参数就不是数组中的元素，而是数组对象本身！此时List 的唯一元素就是这个数组，这也就解释了上面的代码。 我们使用包装类型数组就可以解决这个问题。 12345Integer[] myArray = &#123;1, 2, 3&#125;;List myList = Arrays.asList(myArray);System.out.println(myList.size());//3System.out.println(myList.get(0));//1System.out.println(myList.get(1));//2 使用集合的修改方法:add()、remove()、clear()会抛出异常。 1234List myList = Arrays.asList(1, 2, 3);myList.add(4);//运行时报错：UnsupportedOperationExceptionmyList.remove(1);//运行时报错：UnsupportedOperationExceptionmyList.clear();//运行时报错：UnsupportedOperationException Arrays.asList() 方法返回的并不是 java.util.ArrayList ，而是 java.util.Arrays 的一个内部类,这个内部类并没有实现集合的修改方法或者说并没有重写这些方法。 12List myList = Arrays.asList(1, 2, 3);System.out.println(myList.getClass());//class java.util.Arrays$ArrayList 查看remove() 方法，可以知道为啥抛出UnsupportedOperationException。 123public E remove(int index) &#123; throw new UnsupportedOperationException();&#125; 如何正确的将数组转换为ArrayList？1、最简便的方法1List list = new ArrayList&lt;&gt;(Arrays.asList(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) 2、使用Java8的Stream12345Integer [] myArray = &#123; 1, 2, 3 &#125;;List myList = Arrays.stream(myArray).collect(Collectors.toList());//基本类型也可以实现转换（依赖boxed的装箱操作）int [] myArray2 = &#123; 1, 2, 3 &#125;;List myList = Arrays.stream(myArray2).boxed().collect(Collectors.toList()); 参考Arrays.asList()使用指南","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"Arrays","slug":"Arrays","permalink":"https://xmmarlowe.github.io/tags/Arrays/"}],"author":"Marlowe"},{"title":"JVM-GC如何判断对象可以被回收","slug":"Java/JVM-GC如何判断对象可以被回收","date":"2020-03-09T08:26:44.000Z","updated":"2021-04-25T02:51:27.475Z","comments":true,"path":"2020/03/09/Java/JVM-GC如何判断对象可以被回收/","link":"","permalink":"https://xmmarlowe.github.io/2020/03/09/Java/JVM-GC%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E5%AF%B9%E8%B1%A1%E5%8F%AF%E4%BB%A5%E8%A2%AB%E5%9B%9E%E6%94%B6/","excerpt":"","text":"引用计数法方式：每个对象有一个引用计数属性，新增一个引用时计数加1，引用释放时计数减1，计数为0时可以回收。优点：实现简单，效率高。缺点：无法解决循环引用。 可达性分析法方式：从一系列被称为GC ROOT的对象开始，向下搜索，搜索走过的路径称为引用链，当一个对象到GC ROOT之间没有引用链，说明这个对象不可用。 GC ROOT对象： 虚拟机栈中引用的对象 方法区内类的静态属性引用的对象 方法区常量引用的对象 本地方法栈中引用的对象 finalize()当一个对象被判定为不可达对象后，也并不是非死不可。在通过可达性分析算法判断没有引用链使之与GC ROOT相连，会判断该对象是否有必要执行finalize方法:假如重写了finalize，并且未调用过，则说明有必要执行。 判断有必要执行finalize的对象，会被放入一个队列，有jvm建立的低优先级的Finalizer线程去执行。 当在finalize中自救成功的对象，就会在第二次标记时移除即将回收的集合。自救失败的就会被回收，不会再执行finalize。 所谓自救就是把自己与引用链上的一个对象关联起来。","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"JVM","slug":"JVM","permalink":"https://xmmarlowe.github.io/tags/JVM/"},{"name":"GC","slug":"GC","permalink":"https://xmmarlowe.github.io/tags/GC/"}],"author":"Marlowe"},{"title":"Java中的异常体系","slug":"Java/Java中的异常体系","date":"2020-03-09T07:58:09.000Z","updated":"2021-05-14T09:04:37.823Z","comments":true,"path":"2020/03/09/Java/Java中的异常体系/","link":"","permalink":"https://xmmarlowe.github.io/2020/03/09/Java/Java%E4%B8%AD%E7%9A%84%E5%BC%82%E5%B8%B8%E4%BD%93%E7%B3%BB/","excerpt":"","text":"Java 异常类层次结构图 在 Java 中，所有的异常都有一个共同的祖先 java.lang 包中的 Throwable 类。Throwable 类有两个重要的子类 Exception（异常）和 Error（错误）。Exception 能被程序本身处理(try-catch)， Error是无法处理的(只能尽量避免)。 Exception 和 Error 二者都是 Java 异常处理的重要子类，各自都包含大量子类。 Exception: 程序本身可以处理的异常，可以通过 catch 来进行捕获。Exception 又可以分为 受检查异常(必须处理) 和 不受检查异常(可以不处理)。 Error： Error 属于程序无法处理的错误 ，我们没办法通过 catch 来进行捕获 。例如，Java 虚拟机运行错误（Virtual MachineError）、虚拟机内存不够错误(OutOfMemoryError)、类定义错误（NoClassDefFoundError）等 。这些异常发生时，Java 虚拟机（JVM）一般会选择线程终止。 受检查异常Java 代码在编译过程中，如果受检查异常没有被 catch/throw 处理的话，就没办法通过编译 。除了RuntimeException及其子类以外，其他的Exception类及其子类都属于受检查异常 。常见的受检查异常有： IO 相关的异常、ClassNotFoundException 、SQLException…。 不受检查异常Java 代码在编译过程中 ，我们即使不处理不受检查异常也可以正常通过编译。 RuntimeException及其子类都统称为非受检查异常，例如：NullPointerException、NumberFormatException（字符串转换为数字）、ArrayIndexOutOfBoundsException（数组越界）、ClassCastException（类型转换错误）、ArithmeticException（算术错误）等。 自定义异常实现自定义异常类步骤: 创建一个类继承异常父类Exception 在具体的实现方法首部抛出异常类(自己创建的那个类)，throws的运用 在具体的实现方法的内部抛出异常信息,throw的运用 创建一个类继承异常父类Exception 1234567public class EmailException extends Exception &#123; EmailException(String msg) &#123; super(msg); &#125;&#125; Throwable 类常用方法 public string getMessage(): 返回异常发生时的简要描述 public string toString(): 返回异常发生时的详细信息 public string getLocalizedMessage(): 返回异常对象的本地化信息。使用 Throwable 的子类覆盖这个方法，可以生成本地化信息。如果子类没有覆盖该方法，则该方法返回的信息与 getMessage（）返回的结果相同 public void printStackTrace(): 在控制台上打印 Throwable 对象封装的异常信息 try-catch-finally try块： 用于捕获异常。其后可接零个或多个 catch 块，如果没有 catch 块，则必须跟一个 finally 块。 catch块： 用于处理 try 捕获到的异常。 finally 块： 无论是否捕获或处理异常，finally 块里的语句都会被执行。当在 try 块或 catch 块中遇到 return 语句时，finally 语句块将在方法返回之前被执行。 在以下 3 种特殊情况下，finally 块不会被执行： 在 try 或 finally 块中用了 System.exit(int) 退出程序。但是，如果 System.exit(int) 在异常语句之后，finally 还是会被执行 程序所在的线程死亡。 关闭 CPU。 注意： 当 try 语句和 finally 语句中都有 return 语句时，在方法返回之前，finally 语句的内容将被执行，并且 finally 语句的返回值将会覆盖原始的返回值。如下： 1234567891011public class Test &#123; public static int f(int value) &#123; try &#123; return value * value; &#125; finally &#123; if (value == 2) &#123; return 0; &#125; &#125; &#125;&#125; 如果调用 f(2)，返回值将是 0，因为 finally 语句的返回值覆盖了 try 语句块的返回值。 使用 try-with-resources 来代替try-catch-finally 适用范围（资源的定义）： 任何实现 java.lang.AutoCloseable或者 java.io.Closeable 的对象 关闭资源和 finally 块的执行顺序： 在 try-with-resources 语句中，任何 catch 或 finally 块在声明的资源关闭后运行 《Effecitve Java》中明确指出： 面对必须要关闭的资源，我们总是应该优先使用 try-with-resources 而不是try-finally。随之产生的代码更简短，更清晰，产生的异常对我们也更有用。try-with-resources语句让我们更容易编写必须要关闭的资源的代码，若采用try-finally则几乎做不到这点。 Java 中类似于InputStream、OutputStream 、Scanner 、PrintWriter等的资源都需要我们调用close()方法来手动关闭，一般情况下我们都是通过try-catch-finally语句来实现这个需求，如下： 1234567891011121314//读取文本文件的内容Scanner scanner = null;try &#123; scanner = new Scanner(new File(&quot;D://read.txt&quot;)); while (scanner.hasNext()) &#123; System.out.println(scanner.nextLine()); &#125;&#125; catch (FileNotFoundException e) &#123; e.printStackTrace();&#125; finally &#123; if (scanner != null) &#123; scanner.close(); &#125;&#125; 使用 Java 7 之后的 try-with-resources 语句改造上面的代码: 1234567try (Scanner scanner = new Scanner(new File(&quot;test.txt&quot;))) &#123; while (scanner.hasNext()) &#123; System.out.println(scanner.nextLine()); &#125;&#125; catch (FileNotFoundException fnfe) &#123; fnfe.printStackTrace();&#125; 当然多个资源需要关闭的时候，使用 try-with-resources 实现起来也非常简单，如果你还是用try-catch-finally可能会带来很多问题。 通过使用分号分隔，可以在try-with-resources块中声明多个资源。 12345678910try (BufferedInputStream bin = new BufferedInputStream(new FileInputStream(new File(&quot;test.txt&quot;))); BufferedOutputStream bout = new BufferedOutputStream(new FileOutputStream(new File(&quot;out.txt&quot;)))) &#123; int b; while ((b = bin.read()) != -1) &#123; bout.write(b); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; 总结 所有异常类都是Throwable的子类 异常可分为Error(错误)和Exception(异常)两类 Exception又可分为RuntimeException(运行时异常)和非运行时异常两类 Error是程序无法处理的错误，一旦出现这个错误，则程序被迫停止运行。 Exception不会导致程序停止，分为RuntimeException运行时异常和CheckedException检查异常。 RuntimeException常常发生在程序运行过程中，会导致程序当前线程执行失败。CheckedException常常发生在程序编译过程中，会导致程序编译不通过。","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"异常","slug":"异常","permalink":"https://xmmarlowe.github.io/tags/%E5%BC%82%E5%B8%B8/"}],"author":"Marlowe"},{"title":"Java类加载机制和类加载器概述","slug":"Java/Java类加载机制和类加载器概述","date":"2020-03-09T05:44:40.000Z","updated":"2021-05-03T13:06:45.847Z","comments":true,"path":"2020/03/09/Java/Java类加载机制和类加载器概述/","link":"","permalink":"https://xmmarlowe.github.io/2020/03/09/Java/Java%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%E5%92%8C%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8%E6%A6%82%E8%BF%B0/","excerpt":"当程序主动使用某个类时，如果该类还未被加载到内存中，则JVM会通过加载、连接、初始化3个步骤来对该类进行初始化。如果没有意外，JVM将会连续完成3个步骤，所以有时候也把这3个步骤统称为类加载或类初始化。","text":"当程序主动使用某个类时，如果该类还未被加载到内存中，则JVM会通过加载、连接、初始化3个步骤来对该类进行初始化。如果没有意外，JVM将会连续完成3个步骤，所以有时候也把这3个步骤统称为类加载或类初始化。 一、类加载过程1. 加载加载指的是将类的Class文件读入到内存，并为之创建一个java.lang.Class对象，也就是说，当程序中使用任何类时，系统都会为之建立一个java.lang.Class对象。 2. 链接当类被加载之后，系统为之生成一个对应的Class对象，接着将会进入连接阶段，连接阶段负责把类的二进制数据合并到JRE中。 3. 初始化初始化是为类的静态变量赋予正确的初始值，准备阶段和初始化阶段看似有点矛盾，其实是不矛盾的，如果类中有语句：private static int a = 10，它的执行过程是这样的，首先字节码文件被加载到内存后，先进行链接的验证这一步骤，验证通过后准备阶段，给a分配内存，因为变量a是static的，所以此时a等于int类型的默认初始值0，即a=0,然后到解析（后面在说），到初始化这一步骤时，才把a的真正的值10赋给a,此时a=10。 二、类加载时机 创建类的实例，也即new一个对象 访问某个类或接口的静态变量，或者对该静态变量赋值 调用类的静态方法 反射（Class.forName(“com.lyj.load”)） 初始化一个类的子类（会首先初始化子类的父类） JVM启动时标明的启动类，即文件名和类名相同的那个类 除此之外，下面几种情形需要特别指出： 对于一个final类型的静态变量，如果该变量的值在编译时就可以确定下来，那么这个变量相当于“宏变量”。Java编译器会在编译时直接把这个变量出现的地方替换成它的值，因此即使程序使用该静态变量，也不会导致该类的初始化。反之，如果final类型的静态Field的值不能在编译时确定下来，则必须等到运行时才可以确定该变量的值，如果通过该类来访问它的静态变量，则会导致该类被初始化。 三、类加载器 根类加载器（bootstrap class loader）:它用来加载 Java 的核心类，是用原生代码来实现的，并不继承自 java.lang.ClassLoader（负责加载$JAVA_HOME中jre/lib/rt.jar里所有的class，由C++实现，不是ClassLoader子类）。由于引导类加载器涉及到虚拟机本地实现细节，开发者无法直接获取到启动类加载器的引用，所以不允许直接通过引用进行操作。 扩展类加载器（extensions class loader）：它负责加载JRE的扩展目录，lib/ext或者由java.ext.dirs系统属性指定的目录中的JAR包的类。由Java语言实现，父类加载器为null。 系统类加载器（system class loader）：被称为系统（也称为应用）类加载器，它负责在JVM启动时加载来自Java命令的-classpath选项、java.class.path系统属性，或者CLASSPATH换将变量所指定的JAR包和类路径。程序可以通过ClassLoader的静态方法getSystemClassLoader()来获取系统类加载器。如果没有特别指定，则用户自定义的类加载器都以此类加载器作为父加载器。由Java语言实现，父类加载器为ExtClassLoader。 四、类加载机制 全盘负责：所谓全盘负责，就是当一个类加载器负责加载某个Class时，该Class所依赖和引用其他Class也将由该类加载器负责载入，除非显示使用另外一个类加载器来载入。 双亲委派：所谓的双亲委派，则是先让父类加载器试图加载该Class，只有在父类加载器无法加载该类时才尝试从自己的类路径中加载该类。通俗的讲，就是某个特定的类加载器在接到加载类的请求时，首先将加载任务委托给父加载器，依次递归，如果父加载器可以完成类加载任务，就成功返回；只有父加载器无法完成此加载任务时，才自己去加载。 缓存机制。缓存机制将会保证所有加载过的Class都会被缓存，当程序中需要使用某个Class时，类加载器先从缓存区中搜寻该Class，只有当缓存区中不存在该Class对象时，系统才会读取该类对应的二进制数据，并将其转换成Class对象，存入缓冲区中。这就是为很什么修改了Class后，必须重新启动JVM，程序所做的修改才会生效的原因。 双亲委派机制 工作原理双亲委派机制，其工作原理的是，如果一个类加载器收到了类加载请求，它并不会自己先去加载，而是把这个请求委托给父类的加载器去执行，如果父类加载器还存在其父类加载器，则进一步向上委托，依次递归，请求最终将到达顶层的启动类加载器，如果父类加载器可以完成类加载任务，就成功返回，倘若父类加载器无法完成此加载任务，子加载器才会尝试自己去加载，这就是双亲委派模式，即每个儿子都很懒，每次有活就丢给父亲去干，直到父亲说这件事我也干不了时，儿子自己才想办法去完成。 优势采用双亲委派模式的是好处是Java类随着它的类加载器一起具备了一种带有优先级的层次关系，通过这种层级关可以避免类的重复加载，当父亲已经加载了该类时，就没有必要子ClassLoader再加载一次。其次是考虑到安全因素，java核心api中定义类型不会被随意替换，假设通过网络传递一个名为java.lang.Integer的类，通过双亲委托模式传递到启动类加载器，而启动类加载器在核心Java API发现这个名字的类，发现该类已被加载，并不会重新加载网络传递的过来的java.lang.Integer，而直接返回已加载过的Integer.class，这样便可以防止核心API库被随意篡改。 好处 主要是为了安全性，避免用户自己编写的类动态替换Java的一些核心类，比如String，Integer。 同时避免了类的重新加载，因为JVM中区分不同类，不仅仅是根据类名，相同的class文件被不同的ClassLoader加载就是不同的两个类。 不想用双亲委派模型怎么办？自定义加载器的话，需要继承 ClassLoader 。如果我们不想打破双亲委派模型，就重写 ClassLoader 类中的 findClass() 方法即可，无法被父类加载器加载的类最终会通过这个方法被加载。但是，如果想打破双亲委派模型则需要重写 loadClass() 方法 五、参考文档jvm之java类加载机制和类加载器(ClassLoader)的详解","categories":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"JVM","slug":"JVM","permalink":"https://xmmarlowe.github.io/tags/JVM/"}],"author":"Marlowe"}],"categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"https://xmmarlowe.github.io/categories/NoSQL/"},{"name":"架构","slug":"架构","permalink":"https://xmmarlowe.github.io/categories/%E6%9E%B6%E6%9E%84/"},{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"个人项目","slug":"个人项目","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE/"},{"name":"分布式","slug":"分布式","permalink":"https://xmmarlowe.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"算法与数据结构","slug":"算法与数据结构","permalink":"https://xmmarlowe.github.io/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/categories/Spring/"},{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/categories/Java/"},{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/categories/%E5%B9%B6%E5%8F%91/"},{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"常用工具","slug":"常用工具","permalink":"https://xmmarlowe.github.io/categories/%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7/"},{"name":"权限","slug":"权限","permalink":"https://xmmarlowe.github.io/categories/%E6%9D%83%E9%99%90/"},{"name":"中间件","slug":"中间件","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"计算机网络","slug":"计算机网络","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"网络安全","slug":"网络安全","permalink":"https://xmmarlowe.github.io/categories/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"},{"name":"个人博客","slug":"个人博客","permalink":"https://xmmarlowe.github.io/categories/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"},{"name":"随笔","slug":"随笔","permalink":"https://xmmarlowe.github.io/categories/%E9%9A%8F%E7%AC%94/"},{"name":"设计模式","slug":"设计模式","permalink":"https://xmmarlowe.github.io/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"春招面试","slug":"春招面试","permalink":"https://xmmarlowe.github.io/categories/%E6%98%A5%E6%8B%9B%E9%9D%A2%E8%AF%95/"},{"name":"题解","slug":"题解","permalink":"https://xmmarlowe.github.io/categories/%E9%A2%98%E8%A7%A3/"},{"name":"算法","slug":"算法","permalink":"https://xmmarlowe.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"自定义工具类","slug":"自定义工具类","permalink":"https://xmmarlowe.github.io/categories/%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B7%A5%E5%85%B7%E7%B1%BB/"},{"name":"学习方法","slug":"学习方法","permalink":"https://xmmarlowe.github.io/categories/%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"},{"name":"环境配置之踩坑","slug":"环境配置之踩坑","permalink":"https://xmmarlowe.github.io/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E4%B9%8B%E8%B8%A9%E5%9D%91/"},{"name":"LeetCode题解","slug":"LeetCode题解","permalink":"https://xmmarlowe.github.io/categories/LeetCode%E9%A2%98%E8%A7%A3/"},{"name":"Docker","slug":"Docker","permalink":"https://xmmarlowe.github.io/categories/Docker/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://xmmarlowe.github.io/tags/Redis/"},{"name":"微服务","slug":"微服务","permalink":"https://xmmarlowe.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"六边形架构","slug":"六边形架构","permalink":"https://xmmarlowe.github.io/tags/%E5%85%AD%E8%BE%B9%E5%BD%A2%E6%9E%B6%E6%9E%84/"},{"name":"MySQL","slug":"MySQL","permalink":"https://xmmarlowe.github.io/tags/MySQL/"},{"name":"调优","slug":"调优","permalink":"https://xmmarlowe.github.io/tags/%E8%B0%83%E4%BC%98/"},{"name":"token","slug":"token","permalink":"https://xmmarlowe.github.io/tags/token/"},{"name":"Jwt","slug":"Jwt","permalink":"https://xmmarlowe.github.io/tags/Jwt/"},{"name":"Session","slug":"Session","permalink":"https://xmmarlowe.github.io/tags/Session/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://xmmarlowe.github.io/tags/SpringBoot/"},{"name":"Shiro","slug":"Shiro","permalink":"https://xmmarlowe.github.io/tags/Shiro/"},{"name":"RPC","slug":"RPC","permalink":"https://xmmarlowe.github.io/tags/RPC/"},{"name":"LFU","slug":"LFU","permalink":"https://xmmarlowe.github.io/tags/LFU/"},{"name":"Spring","slug":"Spring","permalink":"https://xmmarlowe.github.io/tags/Spring/"},{"name":"循环依赖","slug":"循环依赖","permalink":"https://xmmarlowe.github.io/tags/%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96/"},{"name":"注入","slug":"注入","permalink":"https://xmmarlowe.github.io/tags/%E6%B3%A8%E5%85%A5/"},{"name":"JVM","slug":"JVM","permalink":"https://xmmarlowe.github.io/tags/JVM/"},{"name":"美团","slug":"美团","permalink":"https://xmmarlowe.github.io/tags/%E7%BE%8E%E5%9B%A2/"},{"name":"ID","slug":"ID","permalink":"https://xmmarlowe.github.io/tags/ID/"},{"name":"LRU","slug":"LRU","permalink":"https://xmmarlowe.github.io/tags/LRU/"},{"name":"缓存","slug":"缓存","permalink":"https://xmmarlowe.github.io/tags/%E7%BC%93%E5%AD%98/"},{"name":"AQS","slug":"AQS","permalink":"https://xmmarlowe.github.io/tags/AQS/"},{"name":"异常","slug":"异常","permalink":"https://xmmarlowe.github.io/tags/%E5%BC%82%E5%B8%B8/"},{"name":"Maven","slug":"Maven","permalink":"https://xmmarlowe.github.io/tags/Maven/"},{"name":"线程","slug":"线程","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B/"},{"name":"Condition","slug":"Condition","permalink":"https://xmmarlowe.github.io/tags/Condition/"},{"name":"CPU","slug":"CPU","permalink":"https://xmmarlowe.github.io/tags/CPU/"},{"name":"GitHub","slug":"GitHub","permalink":"https://xmmarlowe.github.io/tags/GitHub/"},{"name":"Gateway","slug":"Gateway","permalink":"https://xmmarlowe.github.io/tags/Gateway/"},{"name":"Nginx","slug":"Nginx","permalink":"https://xmmarlowe.github.io/tags/Nginx/"},{"name":"分布式","slug":"分布式","permalink":"https://xmmarlowe.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"WeChat","slug":"WeChat","permalink":"https://xmmarlowe.github.io/tags/WeChat/"},{"name":"Payment","slug":"Payment","permalink":"https://xmmarlowe.github.io/tags/Payment/"},{"name":"Login","slug":"Login","permalink":"https://xmmarlowe.github.io/tags/Login/"},{"name":"Docker","slug":"Docker","permalink":"https://xmmarlowe.github.io/tags/Docker/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://xmmarlowe.github.io/tags/SpringCloud/"},{"name":"oAuth2","slug":"oAuth2","permalink":"https://xmmarlowe.github.io/tags/oAuth2/"},{"name":"Linux","slug":"Linux","permalink":"https://xmmarlowe.github.io/tags/Linux/"},{"name":"MQ","slug":"MQ","permalink":"https://xmmarlowe.github.io/tags/MQ/"},{"name":"Kafka","slug":"Kafka","permalink":"https://xmmarlowe.github.io/tags/Kafka/"},{"name":"问题教程","slug":"问题教程","permalink":"https://xmmarlowe.github.io/tags/%E9%97%AE%E9%A2%98%E6%95%99%E7%A8%8B/"},{"name":"git","slug":"git","permalink":"https://xmmarlowe.github.io/tags/git/"},{"name":"RBAC","slug":"RBAC","permalink":"https://xmmarlowe.github.io/tags/RBAC/"},{"name":"Eureka","slug":"Eureka","permalink":"https://xmmarlowe.github.io/tags/Eureka/"},{"name":"Swagger","slug":"Swagger","permalink":"https://xmmarlowe.github.io/tags/Swagger/"},{"name":"MyBatis-Plus","slug":"MyBatis-Plus","permalink":"https://xmmarlowe.github.io/tags/MyBatis-Plus/"},{"name":"存储过程","slug":"存储过程","permalink":"https://xmmarlowe.github.io/tags/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/"},{"name":"索引","slug":"索引","permalink":"https://xmmarlowe.github.io/tags/%E7%B4%A2%E5%BC%95/"},{"name":"DP","slug":"DP","permalink":"https://xmmarlowe.github.io/tags/DP/"},{"name":"贪心","slug":"贪心","permalink":"https://xmmarlowe.github.io/tags/%E8%B4%AA%E5%BF%83/"},{"name":"Mybatis","slug":"Mybatis","permalink":"https://xmmarlowe.github.io/tags/Mybatis/"},{"name":"图","slug":"图","permalink":"https://xmmarlowe.github.io/tags/%E5%9B%BE/"},{"name":"堆","slug":"堆","permalink":"https://xmmarlowe.github.io/tags/%E5%A0%86/"},{"name":"队列","slug":"队列","permalink":"https://xmmarlowe.github.io/tags/%E9%98%9F%E5%88%97/"},{"name":"算法","slug":"算法","permalink":"https://xmmarlowe.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"对象","slug":"对象","permalink":"https://xmmarlowe.github.io/tags/%E5%AF%B9%E8%B1%A1/"},{"name":"哈希","slug":"哈希","permalink":"https://xmmarlowe.github.io/tags/%E5%93%88%E5%B8%8C/"},{"name":"Dijkstra","slug":"Dijkstra","permalink":"https://xmmarlowe.github.io/tags/Dijkstra/"},{"name":"邻接表","slug":"邻接表","permalink":"https://xmmarlowe.github.io/tags/%E9%82%BB%E6%8E%A5%E8%A1%A8/"},{"name":"领接矩阵","slug":"领接矩阵","permalink":"https://xmmarlowe.github.io/tags/%E9%A2%86%E6%8E%A5%E7%9F%A9%E9%98%B5/"},{"name":"树","slug":"树","permalink":"https://xmmarlowe.github.io/tags/%E6%A0%91/"},{"name":"注解","slug":"注解","permalink":"https://xmmarlowe.github.io/tags/%E6%B3%A8%E8%A7%A3/"},{"name":"IPv4","slug":"IPv4","permalink":"https://xmmarlowe.github.io/tags/IPv4/"},{"name":"数据库","slug":"数据库","permalink":"https://xmmarlowe.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Java","slug":"Java","permalink":"https://xmmarlowe.github.io/tags/Java/"},{"name":"数组","slug":"数组","permalink":"https://xmmarlowe.github.io/tags/%E6%95%B0%E7%BB%84/"},{"name":"连接池","slug":"连接池","permalink":"https://xmmarlowe.github.io/tags/%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"name":"CSRF","slug":"CSRF","permalink":"https://xmmarlowe.github.io/tags/CSRF/"},{"name":"ES","slug":"ES","permalink":"https://xmmarlowe.github.io/tags/ES/"},{"name":"并发","slug":"并发","permalink":"https://xmmarlowe.github.io/tags/%E5%B9%B6%E5%8F%91/"},{"name":"分库分表","slug":"分库分表","permalink":"https://xmmarlowe.github.io/tags/%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8/"},{"name":"博客","slug":"博客","permalink":"https://xmmarlowe.github.io/tags/%E5%8D%9A%E5%AE%A2/"},{"name":"内存","slug":"内存","permalink":"https://xmmarlowe.github.io/tags/%E5%86%85%E5%AD%98/"},{"name":"阻塞","slug":"阻塞","permalink":"https://xmmarlowe.github.io/tags/%E9%98%BB%E5%A1%9E/"},{"name":"ConcurrentLinkedQueue","slug":"ConcurrentLinkedQueue","permalink":"https://xmmarlowe.github.io/tags/ConcurrentLinkedQueue/"},{"name":"扩容","slug":"扩容","permalink":"https://xmmarlowe.github.io/tags/%E6%89%A9%E5%AE%B9/"},{"name":"ConcurrentHashMap","slug":"ConcurrentHashMap","permalink":"https://xmmarlowe.github.io/tags/ConcurrentHashMap/"},{"name":"IOC","slug":"IOC","permalink":"https://xmmarlowe.github.io/tags/IOC/"},{"name":"AOP","slug":"AOP","permalink":"https://xmmarlowe.github.io/tags/AOP/"},{"name":"Bean","slug":"Bean","permalink":"https://xmmarlowe.github.io/tags/Bean/"},{"name":"Factory","slug":"Factory","permalink":"https://xmmarlowe.github.io/tags/Factory/"},{"name":"设计模式","slug":"设计模式","permalink":"https://xmmarlowe.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"事务","slug":"事务","permalink":"https://xmmarlowe.github.io/tags/%E4%BA%8B%E5%8A%A1/"},{"name":"限流","slug":"限流","permalink":"https://xmmarlowe.github.io/tags/%E9%99%90%E6%B5%81/"},{"name":"RESTful","slug":"RESTful","permalink":"https://xmmarlowe.github.io/tags/RESTful/"},{"name":"ReadWriteLock","slug":"ReadWriteLock","permalink":"https://xmmarlowe.github.io/tags/ReadWriteLock/"},{"name":"源码","slug":"源码","permalink":"https://xmmarlowe.github.io/tags/%E6%BA%90%E7%A0%81/"},{"name":"ForkJoin","slug":"ForkJoin","permalink":"https://xmmarlowe.github.io/tags/ForkJoin/"},{"name":"精度","slug":"精度","permalink":"https://xmmarlowe.github.io/tags/%E7%B2%BE%E5%BA%A6/"},{"name":"LockSupport","slug":"LockSupport","permalink":"https://xmmarlowe.github.io/tags/LockSupport/"},{"name":"JDK","slug":"JDK","permalink":"https://xmmarlowe.github.io/tags/JDK/"},{"name":"InnoDB","slug":"InnoDB","permalink":"https://xmmarlowe.github.io/tags/InnoDB/"},{"name":"Compact","slug":"Compact","permalink":"https://xmmarlowe.github.io/tags/Compact/"},{"name":"cache","slug":"cache","permalink":"https://xmmarlowe.github.io/tags/cache/"},{"name":"ThreadLocal","slug":"ThreadLocal","permalink":"https://xmmarlowe.github.io/tags/ThreadLocal/"},{"name":"序列化","slug":"序列化","permalink":"https://xmmarlowe.github.io/tags/%E5%BA%8F%E5%88%97%E5%8C%96/"},{"name":"反序列化","slug":"反序列化","permalink":"https://xmmarlowe.github.io/tags/%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/"},{"name":"OOM","slug":"OOM","permalink":"https://xmmarlowe.github.io/tags/OOM/"},{"name":"SOF","slug":"SOF","permalink":"https://xmmarlowe.github.io/tags/SOF/"},{"name":"List","slug":"List","permalink":"https://xmmarlowe.github.io/tags/List/"},{"name":"hashcode","slug":"hashcode","permalink":"https://xmmarlowe.github.io/tags/hashcode/"},{"name":"equals","slug":"equals","permalink":"https://xmmarlowe.github.io/tags/equals/"},{"name":"信号","slug":"信号","permalink":"https://xmmarlowe.github.io/tags/%E4%BF%A1%E5%8F%B7/"},{"name":"信号量","slug":"信号量","permalink":"https://xmmarlowe.github.io/tags/%E4%BF%A1%E5%8F%B7%E9%87%8F/"},{"name":"UDP","slug":"UDP","permalink":"https://xmmarlowe.github.io/tags/UDP/"},{"name":"可靠传输","slug":"可靠传输","permalink":"https://xmmarlowe.github.io/tags/%E5%8F%AF%E9%9D%A0%E4%BC%A0%E8%BE%93/"},{"name":"memcached","slug":"memcached","permalink":"https://xmmarlowe.github.io/tags/memcached/"},{"name":"Top K","slug":"Top-K","permalink":"https://xmmarlowe.github.io/tags/Top-K/"},{"name":"进程","slug":"进程","permalink":"https://xmmarlowe.github.io/tags/%E8%BF%9B%E7%A8%8B/"},{"name":"调度","slug":"调度","permalink":"https://xmmarlowe.github.io/tags/%E8%B0%83%E5%BA%A6/"},{"name":"多线程","slug":"多线程","permalink":"https://xmmarlowe.github.io/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"命令","slug":"命令","permalink":"https://xmmarlowe.github.io/tags/%E5%91%BD%E4%BB%A4/"},{"name":"僵尸进程","slug":"僵尸进程","permalink":"https://xmmarlowe.github.io/tags/%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B/"},{"name":"孤儿进程","slug":"孤儿进程","permalink":"https://xmmarlowe.github.io/tags/%E5%AD%A4%E5%84%BF%E8%BF%9B%E7%A8%8B/"},{"name":"URL","slug":"URL","permalink":"https://xmmarlowe.github.io/tags/URL/"},{"name":"URI","slug":"URI","permalink":"https://xmmarlowe.github.io/tags/URI/"},{"name":"学习","slug":"学习","permalink":"https://xmmarlowe.github.io/tags/%E5%AD%A6%E4%B9%A0/"},{"name":"书籍","slug":"书籍","permalink":"https://xmmarlowe.github.io/tags/%E4%B9%A6%E7%B1%8D/"},{"name":"锁","slug":"锁","permalink":"https://xmmarlowe.github.io/tags/%E9%94%81/"},{"name":"redo log","slug":"redo-log","permalink":"https://xmmarlowe.github.io/tags/redo-log/"},{"name":"binlog","slug":"binlog","permalink":"https://xmmarlowe.github.io/tags/binlog/"},{"name":"undo log","slug":"undo-log","permalink":"https://xmmarlowe.github.io/tags/undo-log/"},{"name":"SQL","slug":"SQL","permalink":"https://xmmarlowe.github.io/tags/SQL/"},{"name":"HTTP","slug":"HTTP","permalink":"https://xmmarlowe.github.io/tags/HTTP/"},{"name":"as-if-serial","slug":"as-if-serial","permalink":"https://xmmarlowe.github.io/tags/as-if-serial/"},{"name":"happens-before","slug":"happens-before","permalink":"https://xmmarlowe.github.io/tags/happens-before/"},{"name":"内存布局","slug":"内存布局","permalink":"https://xmmarlowe.github.io/tags/%E5%86%85%E5%AD%98%E5%B8%83%E5%B1%80/"},{"name":"volatile","slug":"volatile","permalink":"https://xmmarlowe.github.io/tags/volatile/"},{"name":"CAS","slug":"CAS","permalink":"https://xmmarlowe.github.io/tags/CAS/"},{"name":"Synchronized","slug":"Synchronized","permalink":"https://xmmarlowe.github.io/tags/Synchronized/"},{"name":"ReentrantLock","slug":"ReentrantLock","permalink":"https://xmmarlowe.github.io/tags/ReentrantLock/"},{"name":"线程安全","slug":"线程安全","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8/"},{"name":"Object","slug":"Object","permalink":"https://xmmarlowe.github.io/tags/Object/"},{"name":"join","slug":"join","permalink":"https://xmmarlowe.github.io/tags/join/"},{"name":"操作系统","slug":"操作系统","permalink":"https://xmmarlowe.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"内存管理","slug":"内存管理","permalink":"https://xmmarlowe.github.io/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"name":"Unsafe","slug":"Unsafe","permalink":"https://xmmarlowe.github.io/tags/Unsafe/"},{"name":"Thread","slug":"Thread","permalink":"https://xmmarlowe.github.io/tags/Thread/"},{"name":"Runnable","slug":"Runnable","permalink":"https://xmmarlowe.github.io/tags/Runnable/"},{"name":"Future","slug":"Future","permalink":"https://xmmarlowe.github.io/tags/Future/"},{"name":"过滤器","slug":"过滤器","permalink":"https://xmmarlowe.github.io/tags/%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"name":"BST","slug":"BST","permalink":"https://xmmarlowe.github.io/tags/BST/"},{"name":"AVL","slug":"AVL","permalink":"https://xmmarlowe.github.io/tags/AVL/"},{"name":"红黑树","slug":"红黑树","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%A2%E9%BB%91%E6%A0%91/"},{"name":"JMM","slug":"JMM","permalink":"https://xmmarlowe.github.io/tags/JMM/"},{"name":"通信","slug":"通信","permalink":"https://xmmarlowe.github.io/tags/%E9%80%9A%E4%BF%A1/"},{"name":"反射","slug":"反射","permalink":"https://xmmarlowe.github.io/tags/%E5%8F%8D%E5%B0%84/"},{"name":"结构型模式","slug":"结构型模式","permalink":"https://xmmarlowe.github.io/tags/%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F/"},{"name":"装饰器","slug":"装饰器","permalink":"https://xmmarlowe.github.io/tags/%E8%A3%85%E9%A5%B0%E5%99%A8/"},{"name":"上下文","slug":"上下文","permalink":"https://xmmarlowe.github.io/tags/%E4%B8%8A%E4%B8%8B%E6%96%87/"},{"name":"IO","slug":"IO","permalink":"https://xmmarlowe.github.io/tags/IO/"},{"name":"SpringMVC","slug":"SpringMVC","permalink":"https://xmmarlowe.github.io/tags/SpringMVC/"},{"name":"虚拟内存","slug":"虚拟内存","permalink":"https://xmmarlowe.github.io/tags/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98/"},{"name":"缺页中断","slug":"缺页中断","permalink":"https://xmmarlowe.github.io/tags/%E7%BC%BA%E9%A1%B5%E4%B8%AD%E6%96%AD/"},{"name":"页面置换算法","slug":"页面置换算法","permalink":"https://xmmarlowe.github.io/tags/%E9%A1%B5%E9%9D%A2%E7%BD%AE%E6%8D%A2%E7%AE%97%E6%B3%95/"},{"name":"进程通信","slug":"进程通信","permalink":"https://xmmarlowe.github.io/tags/%E8%BF%9B%E7%A8%8B%E9%80%9A%E4%BF%A1/"},{"name":"死锁","slug":"死锁","permalink":"https://xmmarlowe.github.io/tags/%E6%AD%BB%E9%94%81/"},{"name":"二叉树","slug":"二叉树","permalink":"https://xmmarlowe.github.io/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"name":"链表","slug":"链表","permalink":"https://xmmarlowe.github.io/tags/%E9%93%BE%E8%A1%A8/"},{"name":"dp","slug":"dp","permalink":"https://xmmarlowe.github.io/tags/dp/"},{"name":"TopK","slug":"TopK","permalink":"https://xmmarlowe.github.io/tags/TopK/"},{"name":"ABA","slug":"ABA","permalink":"https://xmmarlowe.github.io/tags/ABA/"},{"name":"异步","slug":"异步","permalink":"https://xmmarlowe.github.io/tags/%E5%BC%82%E6%AD%A5/"},{"name":"B+Tree","slug":"B-Tree","permalink":"https://xmmarlowe.github.io/tags/B-Tree/"},{"name":"BTree","slug":"BTree","permalink":"https://xmmarlowe.github.io/tags/BTree/"},{"name":"排序","slug":"排序","permalink":"https://xmmarlowe.github.io/tags/%E6%8E%92%E5%BA%8F/"},{"name":"JUC","slug":"JUC","permalink":"https://xmmarlowe.github.io/tags/JUC/"},{"name":"原子类","slug":"原子类","permalink":"https://xmmarlowe.github.io/tags/%E5%8E%9F%E5%AD%90%E7%B1%BB/"},{"name":"线程池","slug":"线程池","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"name":"synchronized","slug":"synchronized","permalink":"https://xmmarlowe.github.io/tags/synchronized/"},{"name":"动态代理","slug":"动态代理","permalink":"https://xmmarlowe.github.io/tags/%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/"},{"name":"JWT","slug":"JWT","permalink":"https://xmmarlowe.github.io/tags/JWT/"},{"name":"DNS","slug":"DNS","permalink":"https://xmmarlowe.github.io/tags/DNS/"},{"name":"TCP","slug":"TCP","permalink":"https://xmmarlowe.github.io/tags/TCP/"},{"name":"HTTPS","slug":"HTTPS","permalink":"https://xmmarlowe.github.io/tags/HTTPS/"},{"name":"OSI","slug":"OSI","permalink":"https://xmmarlowe.github.io/tags/OSI/"},{"name":"Ping","slug":"Ping","permalink":"https://xmmarlowe.github.io/tags/Ping/"},{"name":"网络分层","slug":"网络分层","permalink":"https://xmmarlowe.github.io/tags/%E7%BD%91%E7%BB%9C%E5%88%86%E5%B1%82/"},{"name":"get","slug":"get","permalink":"https://xmmarlowe.github.io/tags/get/"},{"name":"post","slug":"post","permalink":"https://xmmarlowe.github.io/tags/post/"},{"name":"Cookie","slug":"Cookie","permalink":"https://xmmarlowe.github.io/tags/Cookie/"},{"name":"BufferPoll","slug":"BufferPoll","permalink":"https://xmmarlowe.github.io/tags/BufferPoll/"},{"name":"MyISAM","slug":"MyISAM","permalink":"https://xmmarlowe.github.io/tags/MyISAM/"},{"name":"隔离级别","slug":"隔离级别","permalink":"https://xmmarlowe.github.io/tags/%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/"},{"name":"mvcc","slug":"mvcc","permalink":"https://xmmarlowe.github.io/tags/mvcc/"},{"name":"countDownLatch","slug":"countDownLatch","permalink":"https://xmmarlowe.github.io/tags/countDownLatch/"},{"name":"CyclicBarrier","slug":"CyclicBarrier","permalink":"https://xmmarlowe.github.io/tags/CyclicBarrier/"},{"name":"Semaphore","slug":"Semaphore","permalink":"https://xmmarlowe.github.io/tags/Semaphore/"},{"name":"Callable","slug":"Callable","permalink":"https://xmmarlowe.github.io/tags/Callable/"},{"name":"redis","slug":"redis","permalink":"https://xmmarlowe.github.io/tags/redis/"},{"name":"Jedis","slug":"Jedis","permalink":"https://xmmarlowe.github.io/tags/Jedis/"},{"name":"数据类型","slug":"数据类型","permalink":"https://xmmarlowe.github.io/tags/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"},{"name":"效率","slug":"效率","permalink":"https://xmmarlowe.github.io/tags/%E6%95%88%E7%8E%87/"},{"name":"工作方法","slug":"工作方法","permalink":"https://xmmarlowe.github.io/tags/%E5%B7%A5%E4%BD%9C%E6%96%B9%E6%B3%95/"},{"name":"API","slug":"API","permalink":"https://xmmarlowe.github.io/tags/API/"},{"name":"踩坑","slug":"踩坑","permalink":"https://xmmarlowe.github.io/tags/%E8%B8%A9%E5%9D%91/"},{"name":"Java基础","slug":"Java基础","permalink":"https://xmmarlowe.github.io/tags/Java%E5%9F%BA%E7%A1%80/"},{"name":"面经","slug":"面经","permalink":"https://xmmarlowe.github.io/tags/%E9%9D%A2%E7%BB%8F/"},{"name":"配置","slug":"配置","permalink":"https://xmmarlowe.github.io/tags/%E9%85%8D%E7%BD%AE/"},{"name":"DI","slug":"DI","permalink":"https://xmmarlowe.github.io/tags/DI/"},{"name":"list","slug":"list","permalink":"https://xmmarlowe.github.io/tags/list/"},{"name":"线程不安全","slug":"线程不安全","permalink":"https://xmmarlowe.github.io/tags/%E7%BA%BF%E7%A8%8B%E4%B8%8D%E5%AE%89%E5%85%A8/"},{"name":"行为型模式","slug":"行为型模式","permalink":"https://xmmarlowe.github.io/tags/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F/"},{"name":"策略","slug":"策略","permalink":"https://xmmarlowe.github.io/tags/%E7%AD%96%E7%95%A5/"},{"name":"Json","slug":"Json","permalink":"https://xmmarlowe.github.io/tags/Json/"},{"name":"Utils","slug":"Utils","permalink":"https://xmmarlowe.github.io/tags/Utils/"},{"name":"代理","slug":"代理","permalink":"https://xmmarlowe.github.io/tags/%E4%BB%A3%E7%90%86/"},{"name":"单例","slug":"单例","permalink":"https://xmmarlowe.github.io/tags/%E5%8D%95%E4%BE%8B/"},{"name":"创建型模式","slug":"创建型模式","permalink":"https://xmmarlowe.github.io/tags/%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F/"},{"name":"工厂","slug":"工厂","permalink":"https://xmmarlowe.github.io/tags/%E5%B7%A5%E5%8E%82/"},{"name":"模板","slug":"模板","permalink":"https://xmmarlowe.github.io/tags/%E6%A8%A1%E6%9D%BF/"},{"name":"java","slug":"java","permalink":"https://xmmarlowe.github.io/tags/java/"},{"name":"插入排序","slug":"插入排序","permalink":"https://xmmarlowe.github.io/tags/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/"},{"name":"外观","slug":"外观","permalink":"https://xmmarlowe.github.io/tags/%E5%A4%96%E8%A7%82/"},{"name":"HashMap","slug":"HashMap","permalink":"https://xmmarlowe.github.io/tags/HashMap/"},{"name":"Leetcode","slug":"Leetcode","permalink":"https://xmmarlowe.github.io/tags/Leetcode/"},{"name":"test","slug":"test","permalink":"https://xmmarlowe.github.io/tags/test/"},{"name":"随笔","slug":"随笔","permalink":"https://xmmarlowe.github.io/tags/%E9%9A%8F%E7%AC%94/"},{"name":"public","slug":"public","permalink":"https://xmmarlowe.github.io/tags/public/"},{"name":"private","slug":"private","permalink":"https://xmmarlowe.github.io/tags/private/"},{"name":"protected","slug":"protected","permalink":"https://xmmarlowe.github.io/tags/protected/"},{"name":"default","slug":"default","permalink":"https://xmmarlowe.github.io/tags/default/"},{"name":"包装类","slug":"包装类","permalink":"https://xmmarlowe.github.io/tags/%E5%8C%85%E8%A3%85%E7%B1%BB/"},{"name":"常量池","slug":"常量池","permalink":"https://xmmarlowe.github.io/tags/%E5%B8%B8%E9%87%8F%E6%B1%A0/"},{"name":"String","slug":"String","permalink":"https://xmmarlowe.github.io/tags/String/"},{"name":"finally","slug":"finally","permalink":"https://xmmarlowe.github.io/tags/finally/"},{"name":"final","slug":"final","permalink":"https://xmmarlowe.github.io/tags/final/"},{"name":"finalize","slug":"finalize","permalink":"https://xmmarlowe.github.io/tags/finalize/"},{"name":"集合类","slug":"集合类","permalink":"https://xmmarlowe.github.io/tags/%E9%9B%86%E5%90%88%E7%B1%BB/"},{"name":"类图","slug":"类图","permalink":"https://xmmarlowe.github.io/tags/%E7%B1%BB%E5%9B%BE/"},{"name":"抽象类","slug":"抽象类","permalink":"https://xmmarlowe.github.io/tags/%E6%8A%BD%E8%B1%A1%E7%B1%BB/"},{"name":"接口","slug":"接口","permalink":"https://xmmarlowe.github.io/tags/%E6%8E%A5%E5%8F%A3/"},{"name":"GC","slug":"GC","permalink":"https://xmmarlowe.github.io/tags/GC/"},{"name":"垃圾回收","slug":"垃圾回收","permalink":"https://xmmarlowe.github.io/tags/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"},{"name":"函数式接口","slug":"函数式接口","permalink":"https://xmmarlowe.github.io/tags/%E5%87%BD%E6%95%B0%E5%BC%8F%E6%8E%A5%E5%8F%A3/"},{"name":"I/O模型","slug":"I-O模型","permalink":"https://xmmarlowe.github.io/tags/I-O%E6%A8%A1%E5%9E%8B/"},{"name":"关键字","slug":"关键字","permalink":"https://xmmarlowe.github.io/tags/%E5%85%B3%E9%94%AE%E5%AD%97/"},{"name":"Arrays","slug":"Arrays","permalink":"https://xmmarlowe.github.io/tags/Arrays/"}]}